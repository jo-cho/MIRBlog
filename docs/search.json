[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "음악정보검색(Music Information Retrieval)에 대한 포스트를 올리는 블로그입니다. 음악정보검색을 독학하며 배운 내용을 기록 중입니다. 초보자의 입문기라고 생각해주세요.\nE-mail: jhcho1016@naver.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "블로그 글",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1.1. MIR은 무엇인가\n\n\n\n음악정보검색\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.1. 악보 표현\n\n\n\n음악표현\n\n\n악보\n\n\n화성\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2. 기호 표현\n\n\n\n음악표현\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3. 오디오 표현\n\n\n\n음악표현\n\n\n화성\n\n\n스펙트로그램\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1. 수학리뷰 - 복소수와 지수함수\n\n\n\n푸리에변환\n\n\n수학\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)\n\n\n\n푸리에변환\n\n\nFFT\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3. 단기 푸리에 변환 (STFT) (1)\n\n\n\n푸리에변환\n\n\nSTFT\n\n\n스펙트로그램\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.4. 단기 푸리에 변환 (STFT) (2)\n\n\n\n푸리에변환\n\n\nSTFT\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5. 디지털 신호\n\n\n\n푸리에변환\n\n\n신호\n\n\n샘플링\n\n\n양자화\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1. 오디오 동기화 피쳐\n\n\n\n음악동기화\n\n\n오디오피쳐\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2. 동적 시간 워핑 (DTW)\n\n\n\n음악동기화\n\n\nDTW\n\n\n동적프로그래밍\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.1. 음악 구조와 분할\n\n\n\n음악구조분석\n\n\n분할\n\n\n크로마그램\n\n\nMFCC\n\n\n템포그램\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.2. 자기 유사성 행렬 (SSM)\n\n\n\n음악구조분석\n\n\nSSM\n\n\n크로마그램\n\n\nMFCC\n\n\n템포그램\n\n\n조옮김\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.3. 오디오 썸네일\n\n\n\n음악구조분석\n\n\n썸네일\n\n\n경로\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.4. 노벨티 기반 분할\n\n\n\n음악구조분석\n\n\n노벨티\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.5. 음악 처리의 평가 방법\n\n\n\n음악구조분석\n\n\n평가지표\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.1. 화성의 기본 이론\n\n\n\n화음인식\n\n\n화성\n\n\n음정\n\n\n화음\n\n\n음계\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2. 템플릿 기반 화음 인식\n\n\n\n화음인식\n\n\n평가지표\n\n\n크로마그램\n\n\n템플릿\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3. HMM 기반 화음 인식\n\n\n\n화음인식\n\n\nHMM\n\n\nViterbi\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.4. 화음 인식 예시: 비틀즈\n\n\n\n화음인식\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1. 온셋 감지\n\n\n\n템포\n\n\n온셋\n\n\n피크\n\n\n노벨티\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.2. 템포 분석\n\n\n\n템포\n\n\n비트\n\n\n템포그램\n\n\n자기상관\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.3. 비트와 펄스 추적\n\n\n\n템포\n\n\n비트\n\n\n동적프로그래밍\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.1. 내용 기반 오디오 검색: 개요\n\n\n\n내용 기반 오디오 검색\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2. 오디오 식별\n\n\n\n내용 기반\n\n\n오디오 검색\n\n\n피크\n\n\n매칭 함수\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.3. 오디오 매칭\n\n\n\n내용 기반\n\n\n오디오 검색\n\n\n매칭 함수\n\n\n크로마그램\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.4. 버전 식별\n\n\n\n내용 기반\n\n\n오디오 검색\n\n\n커버송\n\n\n평가지표\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1. 화성-타악 분리 (HPS)\n\n\n\n오디오 분해\n\n\nHPS\n\n\n크로마그램\n\n\n노벨티\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.2. 멜로디 추출\n\n\n\n오디오 분해\n\n\n멜로디\n\n\n스펙트로그램\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.3. NMF 기반 오디오 분해\n\n\n\n오디오 분해\n\n\n스펙트로그램\n\n\nNMF\n\n\n\n\n\n\n\nCheonghyo Cho\n\n\nFeb 18, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1. Introduction/1.1.Introduction.html",
    "href": "posts/1. Introduction/1.1.Introduction.html",
    "title": "1.1. MIR은 무엇인가",
    "section": "",
    "text": "음악정보검색 (Music Information Retrieval, MIR)은 무엇인가?\n\n대체로 MIR(Music Information Retrieval) 연구 분야는 오디오 신호, 기호 표현 또는 웹 페이지와 같은 소스 등으로부터 음악의 의미 있는 특징을 추출하고 추론하며, 이러한 특징을 사용하여 음악을 인덱싱(indexing)하고, 정의한 다양한 검색 체계(예: 내용 기반 검색, 음악 추천 시스템 또는 대규모 음악 컬렉션을 탐색하기 위한 사용자 인터페이스 등)를 개발하는 데 목표를 둔다.\nMIR 연구 분야는 30년도 되지 않은 짧은 역사를 가지고 있지만 최근 들어 많은 관심을 끌고 있는 분야이다. 그 이유는 1990년대말의 오디오 압축 기술이 발달하고 컴퓨터의 계산 능력이 크게 향상되었고, 따라서 음악 특징들을 추출할 수 있는 비용이 감소했기 때문이다. 또한 모바일로 음악을 자유롭게 감상할 수 있게 되며, 스포티파이(Spotify), 애플 뮤직(Apple Music), 그루브샤크(Grooveshark), 디저(Deezer) 등의 스트리밍 서비스들이 탄생하며 음악의 소비가 훨씬 자유로워진 점도 있다.\n\n\nfrom IPython.display import Image\nImage(\"../img/1.intro/music_perception.png\", width=400)\n\n\n\n\n\n위 그림은 음악 검색을 네 가지 유형을 나눈다.\n\n음악 내용(music content)은 오디오 특징들(예: 음색, 리듬, 멜로디 등),\n음악 맥락(music context)은 오디오 외적인 요소들(음악가의 배경, 명성, 앨범 커버 등) 위주를 다루며,\n사용자 맥락(user context)은 특정 청취자의 특징(예: 분위기, 활동, 심리적 요인 등),\n사용자 속성(user properties)은 청취 그룹의 특징(예: 청취 인구, 음악 평론, 인기 등)을 위주로 다룬다.\n\n이 블로그에서는 주로 음악내용(music content) 기반의 정보 검색을 다루기로 한다.\n\n\nMIR에는 다양한 응용 연구가 존재한다. 다음의 응용 방법을 살펴보자\n\n음악 분류 (Music Classification)\n\n장르 분류, 무드(mood) 분류, 아티스트 분류, 악기 식별, 음악 태깅(tagging) 등이 있다. 방대한 음악 데이터에 장르, 무드, 아티스트, 사용 악기, 인위적인 태그 등이 각각 레이블링(labeling)되어 있다면 이를 분석/학습하여 새로운 음악에 대해 장르, 분위기 등을 지정할 수 있다.\n\n음악 유사성 (Music Simliarity) / 추천(Recommendation)\n\n음악 간 유사성을 측정하는 연구로 커버 송 감지 혹은 음악 추천 시스템에 활용될 수 있다.\n\n허밍 기반 검색 (Query By Humming)\n\n사람이 흥얼거리는 소리를 쿼리(질의)로 하여 음악을 찾아내는 연구이다.\n\n음악 소스 분리 (Music Source Separation)\n\n오디오 신호로부터 보컬, 악기 등 한 오디오에 섞인 개별적 신호를 분리해내는 연구이다. 악기 인지, 노래방 버전 생성 등에 활용될 수 있으며, 작곡가, 편곡가 등에게 좋은 도구로 쓰일 수 있다.\n\n자동 음악 전사 (Automatic Music Transcription)\n\n오디오로부터 악보나 미디 등 기호적 표현으로 변환하는 연구이다. 피치(pitch) 감지, 온셋(onset) 감지, 악기 인지, 화성 정보 추출 등의 기술이 함께 연구된다.\n\n음악 생성 (Music Generation)\n\nMIR을 통해 얻은 음악적 정보 등으로 음악을 생성하는 AI 모형 등을 생각해볼 수 있다.\n\n\n\nfrom IPython.display import Image\nImage(\"../img/1.intro/01introduction-1.JPG\", width=300)\n\n\n\n\n위 그림은 2014년에 변가람, 김무영. “Music Information Retrieval 기술 동향”에서 조사한 MIR 분야별 연구 동향이다.\n\n\n참고 (References)\n\nhttps://en.wikipedia.org/wiki/Music_information_retrieval\nByeon, Ga-Ram, and Mu-Yeong Kim. “Music Information Retrieval 기술 동향.” Broadcasting and Media Magazine 19.1 (2014): 31-36.\nKnees, Peter, and Markus Schedl. Music similarity and retrieval: an introduction to audio-and web-based strategies. Vol. 9. Heidelberg: Springer, 2016.\nSchedl, Markus, Emilia Gómez, and Julián Urbano. “Music information retrieval: Recent developments and applications.” Foundations and Trends® in Information Retrieval 8.2-3 (2014): 127-261."
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html",
    "title": "2.1. 악보 표현",
    "section": "",
    "text": "음악의 표현 방법 중 악보(sheet)와 기보법(notation), 음(note), 피치(pitch), 크로마(chroma) 등에 대해 다룬다."
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#악보",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#악보",
    "title": "2.1. 악보 표현",
    "section": "악보",
    "text": "악보\nFull Score (전체 악보) - 위에서부터 악기별로 악보가 정렬되어 있다.\n\nImage(\"../img/2.music_representation/FMP_C1_F10.png\", width=400, height=400)\n\n\n\n\n\n예전에는 고품질의 표기를 그리는 것이 중요했으며, 이는 “music engraving”이라고 불렸다.\n하지만 요즘은 컴퓨터 소프트웨어가 악보를 그릴 수 있다. 아래는 위의 악보를 컴퓨터가 똑같이 제작한 버전의 악보이다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F10_Beethoven_Fifth-MM1-21_Sibelius-Orchestra.png\", width=500)"
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#기보법-music-notation",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#기보법-music-notation",
    "title": "2.1. 악보 표현",
    "section": "기보법 (Music Notation)",
    "text": "기보법 (Music Notation)\n\n오선보(staff)는 5개의 수평선들과 네 개의 공백의 집합으로, 각기 다른 음 높낮이를 표현한다.\n5선 만으로는 음의 높이를 알 수 없다. 따라서, 음의 자리를 정해주는 음자리표(clef)를 5선의 맨 앞에 그려 넣는데, 이렇게 음자리표까지 그려져 음의 자리가 정해져야 비로소 보표가 된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F04.png\", width=500)\n\n\n\n\n\n조표(key signature)란 악보에서 음자리표와 박자표 사이에 붙는 올림표나 내림표를 말하며, 음표 앞에 표기하는 임시표와는 달리 보통의 음표보다 반음이 지속적으로 높거나 낮은 상태를 나타내기 위해 사용된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F05.png\", width=500)\n\n\n\n\n\n악보는 음표(note), 쉼표(rest)로 형성되어 있다. (음표에 대한 자세한 설명은 생략한다.)\n\n\nImage(\"../img/2.music_representation/FMP_C1_F07.png\", width=500)\n\n\n\n\n\n박자표(time signature)는 악곡의 박자 종류를 가리킨다. 박자표는 모두 분수의 꼴로 쓴다\n\n\nImage(\"../img/2.music_representation/FMP_C1_F06.png\", width=500)\n\n\n\n\n\n여러 오선을 합쳐 staff system을 만들 수 있다. 다양한 악기를 동시에 연주할 때 사용된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F08.png\", width=500)\n\n\n\n\n\n템포, 다이나믹, 표현 등을 위한 설명으로 아티큘레이션(articulation)을 쓸 수 있다. 아래의 그림에 나와있다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F09.png\", width=500)"
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#음과-피치",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#음과-피치",
    "title": "2.1. 악보 표현",
    "section": "음과 피치",
    "text": "음과 피치\n\n피치(=음고, 음높낮이)(pitch)란 음(note)이 얼마나 높은지 낮은지를 다루는 속성이다. 피치는 음파의 기본 주파수(fundamental frequency)와 긴밀히 연관되어 있다.\n옥타브(ocatve)는 두 음의 간격을 의미하는데, 한 옥타브 높은 음은 낮은 음은 두배의 기본 주파수이다. 예를 들어 440Hz의 A와 880Hz의 A는 한 옥타브를 사이에 두고 나눠진다.\n피치 클래스(pitch class)란 옥타브를 간격으로 있는 모든 음의 집합이다. 예를 들어 C {…, C1, C2, …}는 하나의 피치 클래스, D {…, D1, D2, …}는 또다른 피치 클래스이다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_PitchClassC.png\", width=500)\n\n\n\n\n\nimport numpy as np\n\ndef generate_sinusoid_pitches(pitches=[69], dur=0.5, Fs=4000, amp=0.25):\n    \"\"\"Generation of sinusoids for a given list of MIDI pitches\n\n    Args:\n        pitches (list): List of MIDI pitches (Default value = [69])\n        dur (float): Duration (in seconds) of each sinusoid (Default value = 0.5)\n        Fs (scalar): Sampling rate (Default value = 4000)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = []\n    for p in pitches:\n        freq = 2 ** ((p - 69) / 12) * 440\n        x = np.append(x, np.sin(2 * np.pi * freq * t))\n    x = amp * x / np.max(x)\n    return x, t\n\n\nFs = 22050\n\npitches = [36,48,60,72,84,96,108]\nx, t = generate_sinusoid_pitches(pitches=pitches, Fs=Fs)\nprint('Pitch class C = {..., C1, C2, C3, C4, C5, C6, C7, ...}', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nPitch class C = {..., C1, C2, C3, C4, C5, C6, C7, ...}\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n음계(scale)는 음악에서 피치(pitch) 순서로 된 음의 집합을 말한다. 악곡을 주로 구성하는 음을 나타내며, 음계의 종류에 따라 곡의 분위기가 달라진다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_MusicalScales.png\", width=500)\n\n\n\n\n\ndur = 0.5\nFs = 22050\n\nx_maj, t = generate_sinusoid_pitches(pitches=[60,62,64,65,67,69,71,72], Fs=Fs)\nx_min, t = generate_sinusoid_pitches(pitches=[60,62,63,65,67,68,70,72], Fs=Fs)\n\nprint('C major scale', flush=True)\nipd.display(ipd.Audio(data=x_maj, rate=Fs))\nprint('C minor scale', flush=True)\nipd.display(ipd.Audio(data=x_min, rate=Fs))\n\nC major scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nC minor scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n평균율(equal temperament)이란 한 옥타브를 12개의 동일한 음계 단계로 나눈 것을 의미한다.\n두 연속된 음계 사이의 차이를 반음(semitone)이라고 하는데, 이는 12음 음계의 가장 작은 간격이다. 음악인들은 이를 ’half-step’이라고도 말한다.\n12음 평균율 음계에는 12개의 피치 클래스가 있다. 서양 음악 표기법에서 이러한 피치 클래스는 알파벳과 임시표(accidental)를 결합하여 표시된다. 7개의 피치 클래스(C 장조에 해당)는 문자 C, D, E, F, G, A 및 B로 표시된다. 이러한 피치 클래스는 피아노 건반의 흰색 건반에 해당된다. 나머지 5개의 피치 등급은 피아노 건반의 검은 건반에 해당하며 알파벳과 임시표(♯ ,♭)의 조합으로 표시된다. 샵(♯)은 음을 반음 올리고 플랫(♭)은 반음 내린 것으로 음 이름 뒤에 표시된다: C♯, D♯, F♯, G♯, A♯ 혹은 D♭, E♭, G♭, A♭, B♭. 이 때 C♯과 D♭는 같은 피치 클래스를 나타낸다. 이는 “enharmonic equivalence”로도 알려져 있다.\n\n과학적 피치 표기\n\n12음 평균율의 음에 이름을 지정하기 위해 피치 클래스를 표시하는 것 외에도 옥타브에 대한 식별자가 필요하다. 과학적 피치 표기법에 따라 각 음은 피치 클래스 이름과 옥타브를 나타내는 숫자로 지정된다. 음 A4는 440Hz의 기본 주파수를 갖는 것으로 결정되어 기준 역할을 한다. 옥타브 수는 피치 클래스 B의 음에서 피치 클래스 C의 음으로 올라갈 때 1씩 증가한다.\n다음 그림은 C3에서 C5까지의 건반과 서양 음악 표기법을 사용하는 해당 음표가 있는 피아노 건반 부분을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F02.png\", width=500)\n\n\n\n\n\ndur = 0.2\nFs = 22050\npitches = range(48,73)\n\nx_chromatic, t = generate_sinusoid_pitches(pitches=pitches, dur=dur, Fs=Fs)\n\nprint('Sinusoidal sonification of the chromatic scale ranging from C3 (p=48) to C5 (p=72):', flush=True)\nipd.display(ipd.Audio(data=x_chromatic, rate=Fs))\n\nSinusoidal sonification of the chromatic scale ranging from C3 (p=48) to C5 (p=72):\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#크로마chroma와-셰퍼드-톤shepard-tones",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#크로마chroma와-셰퍼드-톤shepard-tones",
    "title": "2.1. 악보 표현",
    "section": "크로마(Chroma)와 셰퍼드 톤(Shepard Tones)",
    "text": "크로마(Chroma)와 셰퍼드 톤(Shepard Tones)\n크로마란?\n\n피치에 따라 평균율 음계의 모든 음을 순서대로 배열하면, 음계의 모든 음이 같은 간격으로 배열된 평균율의 크로마틱 음계(chromatic scale)를 얻을 수 있다.\n“Chromatic”이라는 용어는 색을 의미하는 그리스어 “chroma”에서 유래했다. 음악적 맥락에서 크로마(chroma)라는 용어는 12개의 다른 피치 클래시와 밀접한 관련이 있다. 예를 들어, C2와 C5 음은 모두 같은 크로마 값 C를 가지고 있다. 즉, 크로마 값이 같은 모든 음은 동일한 피치 클래스에 속한다.\n같은 피치클래스에 속하거나 크로마 값이 같은 음은 유사하게 인식된다. 반면에, 다른 피치 클래스에 속하거나 다른 크로마 값을 갖는 음은 서로 다른 것으로 인식된다.\n크로마 값의 주기적 특성은 아래 그림과 같이 크로마 원에 의해 설명된다.\n이 개념을 확장하면, 로저 셰퍼드(1929)의 이름을 딴 셰퍼드의 피치 나선(Shepard’s helix of pitch)은 선형 피치 공간을 하나의 수직선을 따라 옥타브 관련 피치가 놓이도록 원통을 감싸고 있는 나선으로 표현한다. 실린더가 수평면에 투영되면 크로마원이 생성된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F03.png\", width=500)\n\n\n\n\n셰퍼드 톤\n\nShepard의 피치 나선은 Shepard 톤을 사용하여 음향화할 수 있으며, 각 톤은 옥타브로 구분된 사인파의 가중 중첩이다.\n반음계 위로 올라가는 이 음조를 연주할 때, 계속해서 위로 올라가는 음조의 청각적 환영을 얻는다(펜로즈 계단의 시각적 착시와 유사; 아래 그림).\n\n\nImage(\"../img/2.music_representation/FMP_C1_PenroseStairs.png\", width=200)\n\n\n\n\n\n뒤의 코드 예시에서 인간이 들을 수 있는 사인파 (20~20000헤르츠의 주파수)만 사용해보자. 특정 가중은 사용되지 않는다(모든 사인파는 1의 크기를 가짐).\n마지막으로 셰퍼드 톤은 크로마틱 스케일로 C3 (MIDI pitch 48) 부터 C5 (MIDI pitch 72)까지로 생성된다.\n\n\ndef generate_shepard_tone(freq=440, dur=0.5, Fs=44100, amp=1):\n    \"\"\"Generate Shepard tone\n\n    Args:\n        freq (float): Frequency of Shepard tone (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 0.5)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Shepard tone\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    num_sin = 1\n    x = np.sin(2 * np.pi * freq * t)\n    freq_lower = freq / 2\n    while freq_lower > 20:\n        num_sin += 1\n        x = x + np.sin(2 * np.pi * freq_lower * t)\n        freq_lower = freq_lower / 2\n    freq_upper = freq * 2\n    while freq_upper < 20000:\n        num_sin += 1\n        x = x + np.sin(2 * np.pi * freq_upper * t)\n        freq_upper = freq_upper * 2\n    x = x / num_sin\n    x = amp * x / np.max(x)\n    return x, t\n\ndef f_pitch(p):\n    F_A4 = 440\n    return F_A4 * 2 ** ((p - 69) / 12)\n    \nFs = 44100\ndur = 0.5\n\npitch_start = 48\npitch_end = 72\nscale = []\nfor p in range(pitch_start, pitch_end + 1):\n    freq = f_pitch(p)    \n    s, t = generate_shepard_tone(freq=freq, dur=dur, Fs=Fs, amp = 0.5)\n    scale = np.concatenate((scale, s))\n    \nipd.display(ipd.Audio(scale, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nShepard–Risset Glissando\n\n이산적인 스케일을 사용하는 대신 연속적인 셰퍼든 톤의 등락을 생성할 수 있다.: (Shepard–Risset glissando)\n뒤의 코드 예시는 상승하는 glissando를 생성한다.\n\n첫 째로 기하급수적으로 상승하는 chirp 신호가 정의된다. 이때 순간 주파수(instantaneous frequency)는 정현파 변수의 미분으로 주어진다.\n생성된 chirp 신호는 정확히 1옥타브를 커버한다.\n그런 다음 Shepared 톤과 유사하게 옥타브로 분리된 처프의 중첩(superposition)이 생성된다.\n한 옥타브를 커버하고 Shepard–Risset glissando의 끝 부분은 (지각적으로) 시작 부분과 일치한다. 따라서 여러 glissando를 연결하여 논스톱 버전을 얻는다.\n\n\n\ndef generate_chirp_exp_octave(freq_start=440, dur=8, Fs=44100, amp=1):\n    \"\"\"Generate one octave of a chirp with exponential frequency increase\n\n    Args:\n        freq_start (float): Start frequency of chirp (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Chirp signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = np.sin(2 * np.pi * freq_start * np.power(2, t / dur) / np.log(2) * dur)\n    x = amp * x / np.max(x)\n    return x, t\n\n\ndef generate_shepard_glissando(num_octaves=3, dur_octave=8, Fs=44100):\n    \"\"\"Generate several ocatves of a Shepared glissando\n\n    Args:\n        num_octaves (int): Number of octaves (Default value = 3)\n        dur_octave (int): Duration (in seconds) per octave (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n\n    Returns:\n        x (np.ndarray): Shepared glissando\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    freqs_start = 10 * 2**np.arange(0, 11)\n    # Generate Shepard glissando by superimposing chirps that differ by octaves\n    for freq in freqs_start:\n        if freq == 10:\n            x, t = generate_chirp_exp_octave(freq_start=freq, dur=dur_octave, Fs=Fs, amp=1)\n        else:\n            chirp, t = generate_chirp_exp_octave(freq_start=freq, dur=dur_octave, Fs=Fs, amp=1)\n            x = x + chirp\n    x = x / len(freqs_start)\n    # Concatenate several octaves\n    x = np.tile(x, num_octaves)\n    N = len(x)\n    t = np.arange(N) / Fs\n    return x, t\n    \nglissando, t = generate_shepard_glissando(num_octaves=3, dur_octave=8)\nipd.display(ipd.Audio(glissando, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_SheetMusic.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_MusicalNotesPitches.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_ChromaShepard.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html",
    "title": "2.2. 기호 표현",
    "section": "",
    "text": "음악의 표현 방법 중 기호(심볼릭) 표현에 대해 알아본다. 피아노-롤(piano-roll), 미디(MIDI) 등이 있다."
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#피아노-롤piano-roll-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#피아노-롤piano-roll-표현",
    "title": "2.2. 기호 표현",
    "section": "피아노-롤(piano-roll) 표현",
    "text": "피아노-롤(piano-roll) 표현\n\n피아노-롤은 피아노와 관련된 음 정보들을 모아 가시화한 것을 일반적으로 말한다.\n드뷔시와 베토벤 음악의 피아노롤을 아래 영상과 같이 표현할 수 있다.\n\n\nipd.display( ipd.YouTubeVideo(\"LlvUepMa31o\", start=15) )\n\n\n        \n        \n\n\n\nipd.display( ipd.YouTubeVideo(\"Kri2jWr08S4\", start=11) )"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#미디-midi-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#미디-midi-표현",
    "title": "2.2. 기호 표현",
    "section": "미디 (MIDI) 표현",
    "text": "미디 (MIDI) 표현\n\n또다른 기호 표현으로는 MIDI(Musical Instrument Digital Interface) 스탠다드가 있다. MIDI는 1980년대 초반 전자 음악 악기 시장의 급성장과 함께 출현했다.\nMIDI 메시지는 음(note) 온셋, 음 오프셋, 강도(intensity or “velocity”)와 같은 정보를 인코딩한다. 컴퓨터에서 MIDI 파일은 MIDI 메시지들과 다른 메타데이터를 보관한다.\nMIDI 노트넘버(MIDI note number)는 0과 127 사이의 정수로 노트의 피치를 인코딩한다. 가장 중요한 것으로는 C4(중간 C)는 MIDI 노트넘버 60이고, A4(concert A440)은 MIDI 노트넘버 69이다. MIDI 노트넘버는 12개로 나누어져있으며 한 옥타브씩 나누어진다 (e.g. 72 = C5, 84 = C6, etc.)\n\n\nImage(\"../img/2.music_representation/FMP_C1_MIDI-NoteNumbers.png\", width=500)\n\n\n\n\n\n키 벨로시티(key velocity)는 0과 127 사이의 정수로 소리의 강도를 조정한다.\nMIDI 채널은 0과 15 사이의 정수로 신디사이저가 특정 악기를 사용하도록 안내한다.\nMIDI는 사분음표를 clock pulses 또는 틱으로 세분화한다. 예를 들어, 분기 음 당 펄스 수(PPQN)를 120으로 정의하면 60개의 틱이 8번째 음의 길이를 나타낸다.\n또한 MIDI는 템포를 BPM으로 인코딩하여 절대적인 시간 정보를 알려준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F13.png\", width=600)\n\n\n\n\n\nmidi_data = pretty_midi.PrettyMIDI(\"../data_FMP/FMP_C1_F01_Beethoven_FateMotive_Sibelius-Tracks.mid\")\nmidi_list = []\n\nfor instrument in midi_data.instruments:\n    for note in instrument.notes:\n        start = note.start\n        end = note.end\n        pitch = note.pitch\n        velocity = note.velocity\n        midi_list.append([start, end, pitch, velocity, instrument.name])\n        \nmidi_list = sorted(midi_list, key=lambda x: (x[0], x[2]))\n\ndf = pd.DataFrame(midi_list, columns=['Start', 'End', 'Pitch', 'Velocity', 'Instrument'])\nhtml = df.to_html(index=False)\nipd.HTML(html)\n\n\n\n  \n    \n      Start\n      End\n      Pitch\n      Velocity\n      Instrument\n    \n  \n  \n    \n      0.25\n      0.50\n      43\n      113\n      Piano\n    \n    \n      0.25\n      0.50\n      55\n      76\n      Piano\n    \n    \n      0.25\n      0.50\n      67\n      76\n      Piano\n    \n    \n      0.50\n      0.75\n      43\n      113\n      Piano\n    \n    \n      0.50\n      0.75\n      55\n      76\n      Piano\n    \n    \n      0.50\n      0.75\n      67\n      76\n      Piano\n    \n    \n      0.75\n      1.00\n      43\n      113\n      Piano\n    \n    \n      0.75\n      1.00\n      55\n      76\n      Piano\n    \n    \n      0.75\n      1.00\n      67\n      76\n      Piano\n    \n    \n      1.00\n      2.00\n      39\n      126\n      Piano\n    \n    \n      1.00\n      2.00\n      51\n      126\n      Piano\n    \n    \n      1.00\n      2.00\n      63\n      70\n      Piano\n    \n    \n      2.25\n      2.50\n      41\n      113\n      Piano\n    \n    \n      2.25\n      2.50\n      53\n      76\n      Piano\n    \n    \n      2.25\n      2.50\n      65\n      76\n      Piano\n    \n    \n      2.50\n      2.75\n      41\n      113\n      Piano\n    \n    \n      2.50\n      2.75\n      53\n      76\n      Piano\n    \n    \n      2.50\n      2.75\n      65\n      76\n      Piano\n    \n    \n      2.75\n      3.00\n      41\n      113\n      Piano\n    \n    \n      2.75\n      3.00\n      53\n      76\n      Piano\n    \n    \n      2.75\n      3.00\n      65\n      76\n      Piano\n    \n    \n      3.00\n      5.00\n      38\n      112\n      Piano\n    \n    \n      3.00\n      5.00\n      50\n      126\n      Piano\n    \n    \n      3.00\n      5.00\n      62\n      71\n      Piano\n    \n  \n\n\n\n\nFs = 22050\naudio_data = midi_data.synthesize(fs=Fs)\nipd.Audio(audio_data, rate=Fs)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef midi_to_list(midi):\n    \"\"\"Convert a midi file to a list of note events\n\n    Args:\n        midi (str or pretty_midi.pretty_midi.PrettyMIDI): Either a path to a midi file or PrettyMIDI object\n\n    Returns:\n        score (list): A list of note events where each note is specified as\n            ``[start, duration, pitch, velocity, label]``\n    \"\"\"\n\n    if isinstance(midi, str):\n        midi_data = pretty_midi.pretty_midi.PrettyMIDI(midi)\n    elif isinstance(midi, pretty_midi.pretty_midi.PrettyMIDI):\n        midi_data = midi\n    else:\n        raise RuntimeError('midi must be a path to a midi file or pretty_midi.PrettyMIDI')\n\n    score = []\n\n    for instrument in midi_data.instruments:\n        for note in instrument.notes:\n            start = note.start\n            duration = note.end - start\n            pitch = note.pitch\n            velocity = note.velocity / 128.\n            score.append([start, duration, pitch, velocity, instrument.name])\n    return score\n\n\ndef visualize_piano_roll(score, xlabel='Time (seconds)', ylabel='Pitch', colors='FMP_1', velocity_alpha=False,\n                         figsize=(12, 4), ax=None, dpi=72):\n    \"\"\"Plot a pianoroll visualization\n    Args:\n        score: List of note events\n        xlabel: Label for x axis (Default value = 'Time (seconds)')\n        ylabel: Label for y axis (Default value = 'Pitch')\n        colors: Several options: 1. string of FMP_COLORMAPS, 2. string of matplotlib colormap,\n            3. list or np.ndarray of matplotlib color specifications,\n            4. dict that assigns labels  to colors (Default value = 'FMP_1')\n        velocity_alpha: Use the velocity value for the alpha value of the corresponding rectangle\n            (Default value = False)\n        figsize: Width, height in inches (Default value = (12)\n        ax: The Axes instance to plot on (Default value = None)\n        dpi: Dots per inch (Default value = 72)\n    Returns:\n        fig: The created matplotlib figure or None if ax was given.\n        ax: The used axes\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig = plt.figure(figsize=figsize, dpi=dpi)\n        ax = plt.subplot(1, 1, 1)\n\n    labels_set = sorted(set([note[4] for note in score]))\n    colors = color_argument_to_dict(colors, labels_set)\n\n    pitch_min = min(note[2] for note in score)\n    pitch_max = max(note[2] for note in score)\n    time_min = min(note[0] for note in score)\n    time_max = max(note[0] + note[1] for note in score)\n\n    for start, duration, pitch, velocity, label in score:\n        if velocity_alpha is False:\n            velocity = None\n        rect = patches.Rectangle((start, pitch - 0.5), duration, 1, linewidth=1,\n                                 edgecolor='k', facecolor=colors[label], alpha=velocity)\n        ax.add_patch(rect)\n\n    ax.set_ylim([pitch_min - 1.5, pitch_max + 1.5])\n    ax.set_xlim([min(time_min, 0), time_max + 0.5])\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.grid()\n    ax.set_axisbelow(True)\n    ax.legend([patches.Patch(linewidth=1, edgecolor='k', facecolor=colors[key]) for key in labels_set],\n              labels_set, loc='upper right', framealpha=1)\n\n    if fig is not None:\n        plt.tight_layout()\n\n    return fig, \n\n\nscore = midi_to_list(midi_data)\nvisualize_piano_roll(score, figsize=(8, 3), velocity_alpha=True);\n\n\n\n\n\nmidi_data = pretty_midi.PrettyMIDI(\"../data_FMP/FMP_C1_F12_Bach_BWV846_Sibelius-Tracks.mid\")\nscore = midi_to_list(midi_data)\nvisualize_piano_roll(score, figsize=(8, 3), velocity_alpha=True);\n\n\n\n\n\nimport music21 as m21\n\ns = m21.converter.parse(\"../data_FMP/FMP_C1_F12_Bach_BWV846_Sibelius-Tracks.mid\")\ns.plot('pianoroll', figureSize=(10, 3))"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#악보적score-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#악보적score-표현",
    "title": "2.2. 기호 표현",
    "section": "악보적(score) 표현",
    "text": "악보적(score) 표현\n\n기호적 악보 표현은 “2.1.Sheet_Music.ipynb”에서 설명한 음악적 기호들을 인코딩한다. (음자리표, 조표 등등) 하지만 이를 악보로 가시화하는 것이 아니라 저장하는데, MusicXML같은 파일로 저장한다.\n아래 그 예시가 있다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F15.png\", width=400)\n\n\n\n\n\ndef xml_to_list(xml):\n    \"\"\"Convert a music xml file to a list of note events\n\n    Args:\n        xml (str or music21.stream.Score): Either a path to a music xml file or a music21.stream.Score\n\n    Returns:\n        score (list): A list of note events where each note is specified as\n            ``[start, duration, pitch, velocity, label]``\n    \"\"\"\n\n    if isinstance(xml, str):\n        xml_data = m21.converter.parse(xml)\n    elif isinstance(xml, m21.stream.Score):\n        xml_data = xml\n    else:\n        raise RuntimeError('midi must be a path to a midi file or music21.stream.Score')\n\n    score = []\n\n    for part in xml_data.parts:\n        instrument = part.getInstrument().instrumentName\n\n        for note in part.flat.notes:\n\n            if note.isChord:\n                start = note.offset\n                duration = note.quarterLength\n\n                for chord_note in note.pitches:\n                    pitch = chord_note.ps\n                    volume = note.volume.realized\n                    score.append([start, duration, pitch, volume, instrument])\n\n            else:\n                start = note.offset\n                duration = note.quarterLength\n                pitch = note.pitch.ps\n                volume = note.volume.realized\n                score.append([start, duration, pitch, volume, instrument])\n\n    score = sorted(score, key=lambda x: (x[0], x[2]))\n    return score\n\n\nxml_data = m21.converter.parse(\"../data_FMP/FMP_C1_F01_Beethoven_FateMotive_Sibelius-Tracks.xml\")\nxml_list = xml_to_list(xml_data)\n\ndf = pd.DataFrame(xml_list[:9], columns=['Start', 'End', 'Pitch', 'Velocity', 'Instrument'])\nhtml = df.to_html(index=False, float_format='%.2f', max_rows=8)\nipd.HTML(html)\n\n\n\n  \n    \n      Start\n      End\n      Pitch\n      Velocity\n      Instrument\n    \n  \n  \n    \n      0.50\n      0.50\n      55.00\n      0.71\n      Piano (2)\n    \n    \n      0.50\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      1.00\n      0.50\n      55.00\n      0.71\n      Piano (2)\n    \n    \n      1.00\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1.50\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      2.00\n      2.00\n      63.00\n      0.71\n      Piano (2)\n    \n    \n      2.50\n      0.50\n      43.00\n      1.00\n      Piano (2)\n    \n    \n      3.00\n      0.50\n      43.00\n      1.00\n      Piano (2)\n    \n  \n\n\n\n\nvisualize_piano_roll(xml_list, figsize=(8, 3), velocity_alpha=True,\n                               xlabel='Time (quarter lengths)');\n\n\n\n\n\nxml_data = m21.converter.parse(\"../data_FMP/FMP_C1_F10_Beethoven_Fifth-MM1-21_Sibelius-Orchestra.xml\")\nxml_list = xml_to_list(xml_data)\n\nvisualize_piano_roll(xml_list, figsize=(10, 7), velocity_alpha=False,\n                               colors='gist_rainbow', xlabel='Time (quarter lengths)');\n\n\n\n\n기호 음악 표현법을 사용하는 파이썬 라이브러리\n\nPrettyMIDI: MIDI 읽기, 컨버팅 등\nmusic21: musicxml파일 다루기\npypianoroll: 피아노롤 비주얼\n\n\n출처:\n\nhttps://musicinformationretrieval.com/\nhttps://www.audiolabs-erlangen.de/FMP\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html",
    "title": "2.3. 오디오 표현",
    "section": "",
    "text": "음악의 표현 방법 중 오디오 표현에 대해 알아본다. 음파(wave), 주파수(frequency), 고조파(harmonics), 강도(intensity), 라우드니스(loudness), 음색(timbre) 등의 중요한 개념을 포함한다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#들을-수-있는-주파수-범위",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#들을-수-있는-주파수-범위",
    "title": "2.3. 오디오 표현",
    "section": "들을 수 있는 주파수 범위",
    "text": "들을 수 있는 주파수 범위\n\n정현파(sinusoidal wave)의 주파수가 높을수록 더 높은 소리를 낸다.\n인간의 가청 주파수 범위는 약 20Hz와 20000Hz(20 kHz) 사이이다. 다른 동물들은 다른 청력 범위를 가지고 있다. 예를 들어, 개의 청력 범위의 상단은 약 45kHz이고 고양이의 청력은 64kHz인 반면, 박쥐는 심지어 100kHz 이상의 주파수를 감지할 수 있다. 이는 사람의 청각 능력을 뛰어넘는 초음파를 내는 개 호루라기를 이용해 주변 사람들을 방해하지 않고 동물을 훈련시키고 명령할 수 있는 이유이다.\n다음 실험에서 주파수가 초당 2배(1옥타브) 증가하는 처프(chirp) 신호를 생성한다.\n\n\ndef generate_chirp_exp_octave(freq_start=440, dur=8, Fs=44100, amp=1):\n    \"\"\"Generate one octave of a chirp with exponential frequency increase\n    Args:\n        freq_start (float): Start frequency of chirp (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n    Returns:\n        x (np.ndarray): Chirp signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = np.sin(2 * np.pi * freq_start * np.power(2, t / dur) / np.log(2) * dur)\n    x = amp * x / np.max(x)\n    return x, t\n\n\n# 20Hz부터 시작하여 주파수는 총 10초 동안 20480Hz까지 상승한다.\nFs = 44100\ndur = 1\nfreq_start = 20 * 2**np.arange(10)\nfor f in freq_start:\n    if f==freq_start[0]:\n        chirp, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=.25)\n    else:\n        chirp_oct, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=.25)\n        chirp = np.concatenate((chirp, chirp_oct))\n\nipd.display(ipd.Audio(chirp, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# 640Hz부터 시작하여 주파수는 총 10초 동안 20Hz까지 하락한다.\n\nFs = 8000\ndur = 2\nfreq_start = 20 * 2**np.arange(5)\nfor f in freq_start:\n    if f==freq_start[0]:\n        chirp, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=1)\n    else:\n        chirp_oct, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=1)\n        chirp = np.concatenate((chirp,chirp_oct))    \n        \nchirp = chirp[::-1]    \nipd.display(ipd.Audio(chirp, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#피치와-중심-주파수center-frequency",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#피치와-중심-주파수center-frequency",
    "title": "2.3. 오디오 표현",
    "section": "피치와 중심 주파수(center frequency)",
    "text": "피치와 중심 주파수(center frequency)\n\n정현파는 음의 음향적 실현의 원형으로 간주될 수 있다. 때때로 정현파에서 나오는 소리를 하모닉 사운드(harmonic sound) 또는 순수 음색(pure tone)이라고 한다.\n주파수의 개념은 소리의 피치를 결정하는 것과 밀접한 관련이 있다. 일반적으로 피치는 소리의 주관적인 속성이다.\n복잡한 혼합음의 경우, 주파수와의 관계가 특히 모호할 수 있다. 그러나 순수 음색의 경우 주파수와 피치의 관계가 명확하다. 예를 들어, 440 Hz의 주파수를 갖는 정현파는 피치 A4에 해당한다. 이 특정한 피치는 콘서트 피치(concert pitch)로 알려져 있으며, 이는 연주를 위해 악기가 튜닝되는 기준 피치로 사용된다.\n주파수의 약간의 변화가 반드시 지각할 수 있는 변화로 이어지는 것은 아니기 때문에, 일반적으로 주파수의 전체 범위를 단일 피치와 연관시킨다.\n두 주파수가 2의 거듭제곱에 의해 차이가 나는 경우, 이는 옥타브의 개념과 연관된다.\n\n예를 들어, 피치 A3(220 Hz)와 피치 A4(440 Hz) 사이의 인식 거리는 피치 A4와 피치 A5(880 Hz) 사이의 인식 거리와 동일하다.\n즉, 피치에 대한 인간의 인식은 본질적으로 로그(logarithm)이다. 이 지각 특성은 이미 로그 주파수 축을 기준으로 옥타브를 12개의 반음으로 세분화하는 평균율(“equal-tempered scale”)을 정의하는 데 사용되었다.\n\n더 공식적으로, MIDI 노트 번호를 사용하여, 다음과 같이 정의된 중심 주파수(center frequency) \\(F_{pitch}(p)\\)(Hz 단위로 측정)를 각 피치 \\(p∈[0:127]\\) 에 연결할 수 있다.\n\n\\(F_{pitch}(p)=2^{(p−69)/12} \\cdot 440\\)\n\nMIDI 노트 번호 \\(p=69\\)는 기준으로 사용되며 피치 A4(440Hz)에 해당된다. 피치 넘버를 12(옥타브) 증가시키면 2배 증가한다. \\(F_{pitch} ( p + 12) = 2 \\cdot F_{pitch} ( p)\\)\n\n\n\n\nImage(\"../img/2.music_representation/FMP_C1_MIDI-NoteNumbers.png\", width=500)\n\n\n\n\n\ndef f_pitch(p):\n    \"\"\"Compute center frequency for (single or array of) MIDI note numbers\n    Args:\n        p (float or np.ndarray): MIDI note numbers\n    Returns:\n        freq_center (float or np.ndarray): Center frequency\n    \"\"\"\n    freq_center = 2 ** ((p - 69) / 12) * 440\n    return freq_center\n\nchroma = ['A ', 'A#', 'B ', 'C ', 'C#', 'D ', 'D#', 'E ', 'F ', 'F#', 'G ', 'G#']\n\nfor p in range(21, 109):\n    print('p = %3d (%2s%1d), freq = %7.2f ' % (p, chroma[(p-69) % 12], (p//12-1), f_pitch(p)))\n\np =  21 (A 0), freq =   27.50 \np =  22 (A#0), freq =   29.14 \np =  23 (B 0), freq =   30.87 \np =  24 (C 1), freq =   32.70 \np =  25 (C#1), freq =   34.65 \np =  26 (D 1), freq =   36.71 \np =  27 (D#1), freq =   38.89 \np =  28 (E 1), freq =   41.20 \np =  29 (F 1), freq =   43.65 \np =  30 (F#1), freq =   46.25 \np =  31 (G 1), freq =   49.00 \np =  32 (G#1), freq =   51.91 \np =  33 (A 1), freq =   55.00 \np =  34 (A#1), freq =   58.27 \np =  35 (B 1), freq =   61.74 \np =  36 (C 2), freq =   65.41 \np =  37 (C#2), freq =   69.30 \np =  38 (D 2), freq =   73.42 \np =  39 (D#2), freq =   77.78 \np =  40 (E 2), freq =   82.41 \np =  41 (F 2), freq =   87.31 \np =  42 (F#2), freq =   92.50 \np =  43 (G 2), freq =   98.00 \np =  44 (G#2), freq =  103.83 \np =  45 (A 2), freq =  110.00 \np =  46 (A#2), freq =  116.54 \np =  47 (B 2), freq =  123.47 \np =  48 (C 3), freq =  130.81 \np =  49 (C#3), freq =  138.59 \np =  50 (D 3), freq =  146.83 \np =  51 (D#3), freq =  155.56 \np =  52 (E 3), freq =  164.81 \np =  53 (F 3), freq =  174.61 \np =  54 (F#3), freq =  185.00 \np =  55 (G 3), freq =  196.00 \np =  56 (G#3), freq =  207.65 \np =  57 (A 3), freq =  220.00 \np =  58 (A#3), freq =  233.08 \np =  59 (B 3), freq =  246.94 \np =  60 (C 4), freq =  261.63 \np =  61 (C#4), freq =  277.18 \np =  62 (D 4), freq =  293.66 \np =  63 (D#4), freq =  311.13 \np =  64 (E 4), freq =  329.63 \np =  65 (F 4), freq =  349.23 \np =  66 (F#4), freq =  369.99 \np =  67 (G 4), freq =  392.00 \np =  68 (G#4), freq =  415.30 \np =  69 (A 4), freq =  440.00 \np =  70 (A#4), freq =  466.16 \np =  71 (B 4), freq =  493.88 \np =  72 (C 5), freq =  523.25 \np =  73 (C#5), freq =  554.37 \np =  74 (D 5), freq =  587.33 \np =  75 (D#5), freq =  622.25 \np =  76 (E 5), freq =  659.26 \np =  77 (F 5), freq =  698.46 \np =  78 (F#5), freq =  739.99 \np =  79 (G 5), freq =  783.99 \np =  80 (G#5), freq =  830.61 \np =  81 (A 5), freq =  880.00 \np =  82 (A#5), freq =  932.33 \np =  83 (B 5), freq =  987.77 \np =  84 (C 6), freq = 1046.50 \np =  85 (C#6), freq = 1108.73 \np =  86 (D 6), freq = 1174.66 \np =  87 (D#6), freq = 1244.51 \np =  88 (E 6), freq = 1318.51 \np =  89 (F 6), freq = 1396.91 \np =  90 (F#6), freq = 1479.98 \np =  91 (G 6), freq = 1567.98 \np =  92 (G#6), freq = 1661.22 \np =  93 (A 6), freq = 1760.00 \np =  94 (A#6), freq = 1864.66 \np =  95 (B 6), freq = 1975.53 \np =  96 (C 7), freq = 2093.00 \np =  97 (C#7), freq = 2217.46 \np =  98 (D 7), freq = 2349.32 \np =  99 (D#7), freq = 2489.02 \np = 100 (E 7), freq = 2637.02 \np = 101 (F 7), freq = 2793.83 \np = 102 (F#7), freq = 2959.96 \np = 103 (G 7), freq = 3135.96 \np = 104 (G#7), freq = 3322.44 \np = 105 (A 7), freq = 3520.00 \np = 106 (A#7), freq = 3729.31 \np = 107 (B 7), freq = 3951.07 \np = 108 (C 8), freq = 4186.01 \n\n\n\n이 공식으로부터, 두 개의 연속된 피치 p+1과 p의 주파수 비율은 일정하다.\n\n\\(F_\\mathrm{pitch}(p+1)/F_\\mathrm{pitch}(p) = 2^{1/12} \\approx 1.059463\\)\n\n반음의 개념을 일반화한 센트(cent)는 음악 간격에 사용되는 로그 단위를 나타낸다. 정의에 따라 옥타브는 \\(1200\\) 센트로 나뉘며, 각 반음은 \\(100\\)센트에 해당한다. 두 주파수(예: \\(\\omega_1\\) 및 \\(\\omega_2\\)) 사이의 센트 차이는 다음과 같다.\n\n\\(\\log_2\\left(\\frac{\\omega_1}{\\omega_2}\\right)\\cdot 1200.\\)\n\n1센트의 간격은 너무 작아서 연속된 음 사이를 지각할 수 없다. 지각할 수 있는 문턱은 사람마다 다르고 음색과 음악적 맥락과 같은 측면에 따라 달라진다.\n경험적으로 일반 성인은 25센트의 작은 피치 차이를 매우 안정적으로 인식할 수 있으며, 10센트의 차이는 훈련된 청취자만이 인식할 수 있다.\n그림에서와 같이, 기준으로 사용되는 \\(440~\\mathrm{Hz}\\)의 정현파와 다양한 차이를 가진 추가 정현파를 생성하여 본다.\n\n\ndef difference_cents(freq_1, freq_2):\n    \"\"\"Difference between two frequency values specified in cents\n\n    Args:\n        freq_1 (float): First frequency\n        freq_2 (float): Second frequency\n\n    Returns:\n        delta (float): Difference in cents\n    \"\"\"\n    delta = np.log2(freq_1 / freq_2) * 1200\n    return delta\n \ndef generate_sinusoid(dur=1, Fs=1000, amp=1, freq=1, phase=0):\n    \"\"\"Generation of sinusoid\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 1)\n        freq (float): Frequency of sinusoid (Default value = 1)\n        phase (float): Phase of sinusoid (Default value = 0)\n\n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    x = amp * np.sin(2*np.pi*(freq*t-phase))\n    return x, t\n\n\ndur = 1\nFs = 4000\npitch = 69\nref = f_pitch(pitch)\nfreq_list = ref + np.array([0,2,5,10,ref])\nfor freq in freq_list:\n    x, t = generate_sinusoid(dur=dur, Fs=Fs, freq=freq)\n    print('freq = %0.1f Hz (MIDI note number 69 + %0.2f cents)' % (freq, difference_cents(freq,ref)))\n    ipd.display(ipd.Audio(data=x, rate=Fs))  \n\nfreq = 440.0 Hz (MIDI note number 69 + 0.00 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 442.0 Hz (MIDI note number 69 + 7.85 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 445.0 Hz (MIDI note number 69 + 19.56 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 450.0 Hz (MIDI note number 69 + 38.91 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 880.0 Hz (MIDI note number 69 + 1200.00 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#데시벨-스케일-decibel-scale",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#데시벨-스케일-decibel-scale",
    "title": "2.3. 오디오 표현",
    "section": "데시벨 스케일 (Decibel Scale)",
    "text": "데시벨 스케일 (Decibel Scale)\n\n음악의 중요한 특성은 음량을 나타내는 음악 기호뿐만 아니라 음량을 나타내는 일반적인 용어인 다이나믹(dynamics)과 관련이 있다.\n물리적 관점에서 소리 힘(sound power)은 공기를 통해 모든 방향으로 흐르는 음원에 의해 단위 시간당 얼마나 많은 에너지가 방출되는지를 나타낸다.\n소리 강도/인텐시티(sound intensity)는 단위 면적당 소리 힘을 나타낸다. 실제로 소리 힘과 소리 강도는 인간 청취자와 연관된 극히 작은 값을 보여줄 수 있다.\n예를 들어, 인간이 들을 수 있는 순수 음색(pure tone)의 최소 소리 강도인 청각의 임계값(threshold of hearing, TOH)은 다음과 같이 작다.\n\n\\(I_\\mathrm{TOH}:=10^{-12}~\\mathrm{W}/\\mathrm{m}^2\\)\n\n게다가, 인간이 지각할 수 있는 강도의 범위는 \\(I_\\mathrm{TOP}:=10~\\mathrm{W}/\\mathrm{m}^2\\) (통증 임계값(threshold of pain, TOP))으로 매우 크다.\n실질적인 이유로, 힘과 강도를 표현하기 위해 로그 척도로 전환한다. 더 정확하게는 두 값 사이의 비율을 나타내는 로그 단위인 데시벨(dB) 척도를 사용한다.\n일반적으로 소리 강도의 경우 \\(I_\\mathrm{TOH}\\) 같은 값이 참조 역할을 한다.\n그런 다음 dB로 측정된 강도는 다음과 같이 정의된다.\n\n$ (I) := 10_{10}()$\n\n위의 정의에서 \\(\\mathrm{dB}(I_\\mathrm{TOH})=0\\)를 얻을 수 있고, 강도가 두배로 증가하면 대략 3dB 증가한다:\n\n\\(\\mathrm{dB}(2\\cdot I) = 10\\cdot \\log_{10}(2) + \\mathrm{dB}(I) \\approx 3 + \\mathrm{dB}(I)\\)\n\n데시벨 단위로 강도 값을 지정할 때 강도 수준(intensity levels)도 같이 언급된다.\n다음 표는 \\(\\mathrm{W}/\\mathrm{m}^2\\) 와 데시벨 단위로 몇 가지 일반적인 강도값(intensity value)을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_T01.png\", width=400)\n\n\n\n\n\n예시로 이를 보자.\n\n베토벤 5번 교항곡 시작 부분\n\n\n\nipd.Audio(\"../audio/beeth5_orch_21bars.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef compute_power_db(x, Fs, win_len_sec=0.1, power_ref=10**(-12)):\n    \"\"\"Computation of the signal power in dB\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        Fs (scalar): Sampling rate\n        win_len_sec (float): Length (seconds) of the window (Default value = 0.1)\n        power_ref (float): Reference power level (0 dB) (Default value = 10**(-12))\n\n    Returns:\n        power_db (np.ndarray): Signal power in dB\n    \"\"\"\n    win_len = round(win_len_sec * Fs)\n    win = np.ones(win_len) / win_len\n    power_db = 10 * np.log10(np.convolve(x**2, win, mode='same') / power_ref)\n    return power_db\n\n\nFs = 22050\nx, Fs = librosa.load(\"../audio/beeth5_orch_21bars.wav\", sr=Fs, mono=True)\n\nwin_len_sec = 0.2\npower_db = compute_power_db(x, win_len_sec=win_len_sec, Fs=Fs)\n\n\nplot_signal(x, Fs, ylabel='Amplitude', color='gray')\nplot_signal(power_db, Fs, ylabel='Power (dB)', color='red')\nplt.show()"
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#라우드니스loudness",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#라우드니스loudness",
    "title": "2.3. 오디오 표현",
    "section": "라우드니스(Loudness)",
    "text": "라우드니스(Loudness)\n\n다이나믹과 소리 강도는 소리가 조용한 것에서 큰 것으로 확장되는 규모로 소리를 정렬할 수 있는 라우드니스(loudness)라고 불리는 지각적 특성과 관련이 있다.\n라우드니스는 주관적인 측정이며, 이는 개별 청취자(예: 나이는 소리에 대한 인간의 귀의 반응에 영향을 미치는 요인 중 하나)뿐만 아니라 지속 시간(duration) 또는 주파수와 같은 다른 소리 특성에도 영향을 미친다.\n\n예를 들어, 사람은 200ms 동안 지속되는 소리가 50ms 동안만 지속되는 유사한 소리보다 더 크게 느껴진다.\n게다가, 강도는 같지만 주파수가 다른 두 소리는 일반적으로 동일한 라우드니스로 인식되지 않는다.\n정상적인 청력을 가진 사람은 2~4kHz 정도의 소리에 가장 민감하며, 낮은 주파수뿐만 아니라 높은 주파수에서도 감도가 감소한다.\n\n정신음향(psychoacoustic) 실험을 바탕으로 주파수에 따른 순수 음색의 라우드니스는 단위 폰(unit phon)으로 결정되고 표현되어 왔다.\n다음 그림은 동일한 음량 윤곽선(equal loudness contours)을 보여준다. 각 윤곽선은 폰(phon)으로 주어진 고정된 음량에 대해 (로그로 간격을 둔) 주파수 축에 대한 소리 강도를 지정한다. 하나의 폰 단위는 1000Hz의 주파수에 대해 정규화되며, 여기서 하나의 폰 값은 dB 단위의 강도 수준과 같다. 0폰의 윤곽선은 주파수에 따라 청각 임계값(threshold of hearing)이 어떻게 달라지는지를 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F21.png\", width=400)\n\n\n\n\n\n윤곽선은 가중치 함수에 의해 대략적으로 설명될 수 있다. 다음 코드 셀에서 동일한 음량 윤곽선에 질적으로 근사하는 함수의 예를 찾을 수 있다.\n\n\ndef compute_equal_loudness_contour(freq_min=30, freq_max=15000, num_points=100):\n    \"\"\"Computation of the equal loudness contour\n\n    Args:\n        freq_min (float): Lowest frequency to be evaluated (Default value = 30)\n        freq_max (float): Highest frequency to be evaluated (Default value = 15000)\n        num_points (int): Number of evaluation points (Default value = 100)\n\n    Returns:\n        equal_loudness_contour (np.ndarray): Equal loudness contour (in dB)\n        freq_range (np.ndarray): Evaluated frequency points\n    \"\"\"\n    freq_range = np.logspace(np.log10(freq_min), np.log10(freq_max), num=num_points)\n    freq = 1000\n    # Function D from https://bar.wikipedia.org/wiki/Datei:Acoustic_weighting_curves.svg\n    h_freq = ((1037918.48 - freq**2)**2 + 1080768.16 * freq**2) / ((9837328 - freq**2)**2 + 11723776 * freq**2)\n    n_freq = (freq / (6.8966888496476 * 10**(-5))) * np.sqrt(h_freq / ((freq**2 + 79919.29) * (freq**2 + 1345600)))\n    h_freq_range = ((1037918.48 - freq_range**2)**2 + 1080768.16 * freq_range**2) / ((9837328 - freq_range**2)**2\n                                                                                     + 11723776 * freq_range**2)\n    n_freq_range = (freq_range / (6.8966888496476 * 10**(-5))) * np.sqrt(h_freq_range / ((freq_range**2 + 79919.29) *\n                                                                         (freq_range**2 + 1345600)))\n    equal_loudness_contour = 20 * np.log10(np.abs(n_freq / n_freq_range))\n    return equal_loudness_contour, freq_range\n\n\nequal_loudness_contour, freq_range = compute_equal_loudness_contour()\n\nplot_signal(equal_loudness_contour, T_coef=freq_range, figsize=(6,3), xlabel='Frequency (Hz)',\n            ylabel='Intensity (dB)', title='Equal loudness contour', color='red')\nplt.xscale('log')\nplt.grid()\nplt.show()\n\n\n\n\n\n동일 힘을 가지는 처프 신호\n\n이제 30Hz에서 시작하여 10000Hz로 끝나는, 주파수가 기하급수적으로 증가하는 차프 신호에 대한 작은 실험을 해보자.\n먼저, 전체 시간 간격에 걸쳐 동일한 강도의 차프 신호를 생성한다. 이 신호를 들을 때는 주파수가 증가함에 따라 신호가 먼저 커지고 약 4000Hz의 주파수를 지나면 다시 부드러워지는 느낌이 든다.\n\n\n\ndef generate_chirp_exp(dur, freq_start, freq_end, Fs=22050):\n    \"\"\"Generation chirp with exponential frequency increase\n\n    Args:\n        dur (float): Length (seconds) of the signal\n        freq_start (float): Start frequency of the chirp\n        freq_end (float): End frequency of the chirp\n        Fs (scalar): Sampling rate (Default value = 22050)\n\n    Returns:\n        x (np.ndarray): Generated chirp signal\n        t (np.ndarray): Time axis (in seconds)\n        freq (np.ndarray): Instant frequency (in Hz)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    freq = np.exp(np.linspace(np.log(freq_start), np.log(freq_end), N))\n    phases = np.zeros(N)\n    for n in range(1, N):\n        phases[n] = phases[n-1] + 2 * np.pi * freq[n-1] / Fs\n    x = np.sin(phases)\n    return x, t, freq\n\n\nFs = 22050\nfreq_start = 30 \nfreq_end = 10000\ndur = 10\nx, t, freq = generate_chirp_exp(dur, freq_start, freq_end, Fs=Fs)\n\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [3, 2]}, figsize=(7, 5))\nN, H = 1024, 512\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\nplot_matrix(np.log(1+np.abs(X)), Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0], ax[0,1]], \n            title='Spectrogram of chirp', colorbar=True)\n\nwin_len_sec = 0.1\npower_db = compute_power_db(x, win_len_sec=win_len_sec, Fs=Fs)\nplot_signal(power_db, Fs=Fs, ax=ax[1,0], title='Sound power level', ylabel='Power (dB)', color='red')\nax[1,0].set_ylim([103, 137])\nax[1,1].set_axis_off()\nplt.tight_layout()\nplt.show()\n\ndisplay(Audio(x, rate=Fs) )\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n동일 라우드니스를 가지는 처프 신호\n\n둘째로, 위에서 생성된 동일 라우드니스 윤곽에 따라 신호의 진폭(amplitude)을 조정한다.\n이 경우 전체 주파수 범위를 통해 스위핑할 때 결과로 발생하는 처프 신호의 라우드니스가 동일한 것으로 보인다.\n\n\n\ndef generate_chirp_exp_equal_loudness(dur, freq_start, freq_end, Fs=22050):\n    \"\"\"Generation chirp with exponential frequency increase and equal loudness\n\n    Args:\n        dur (float): Length (seconds) of the signal\n        freq_start (float): Starting frequency of the chirp\n        freq_end (float): End frequency of the chirp\n        Fs (scalar): Sampling rate (Default value = 22050)\n\n    Returns:\n        x (np.ndarray): Generated chirp signal\n        t (np.ndarray): Time axis (in seconds)\n        freq (np.ndarray): Instant frequency (in Hz)\n        intensity (np.ndarray): Instant intensity of the signal\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    intensity, freq = compute_equal_loudness_contour(freq_min=freq_start, freq_max=freq_end, num_points=N)\n    amp = 10**(intensity / 20)\n    phases = np.zeros(N)\n    for n in range(1, N):\n        phases[n] = phases[n-1] + 2 * np.pi * freq[n-1] / Fs\n    x = amp * np.sin(phases)\n    return x, t, freq, intensity\n\n\nx_equal_loudness, t, freq, intensity = generate_chirp_exp_equal_loudness(dur, freq_start, freq_end, Fs=Fs)\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [3, 2]}, figsize=(7, 5))\nN, H = 1024, 512\nX = librosa.stft(x_equal_loudness, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\nplot_matrix(np.log(1+np.abs(X)), Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0], ax[0,1]], \n                     title='Spectrogram of chirp with equal loudness', colorbar=True)\n\nwin_len_sec = 0.1\npower_db = compute_power_db(x_equal_loudness, win_len_sec=win_len_sec, Fs=Fs)\nplot_signal(power_db, Fs=Fs, ax=ax[1,0], title='Sound power level', ylabel='Power (dB)', color='red')\nax[1,0].set_ylim([103, 137])\nax[1,1].set_axis_off()\nplt.tight_layout()\nplt.show()\n\ndisplay( Audio(x_equal_loudness, rate=Fs) )\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프와-adsr-모형envelope-and-adsr-model",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프와-adsr-모형envelope-and-adsr-model",
    "title": "2.3. 오디오 표현",
    "section": "엔벨로프와 ADSR 모형(Envelope and ADSR Model)",
    "text": "엔벨로프와 ADSR 모형(Envelope and ADSR Model)\n\n소리의 음색에 영향을 미치는 한 가지 소리 특성은 파형의 엔벨로프(envelope)이며, 이는 진폭에서 극단을 나타내는 매끄러운 곡선으로 간주될 수 있다.\n음향 합성에서 생성되는 신호의 엔벨로프는 어택(attack, A), 디케이(decay, D), 서스테인(sustain, S), 릴리스(release, R) 단계로 구성된 ADSR이라는 모델에 의해 종종 설명된다.\n4단계의 상대적 지속 시간과 진폭은 합성된 음색이 어떻게 들릴지에 큰 영향을 미친다.\n다음 그림은 이상적인 ADSR 모델과 피아노와 바이올린 사운드의 엔벨로프(단음 C4 재생)를 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F22a-23.png\", width=800)\n\n\n\n\n\n그림에서 알 수 있듯이, 하나의 음을 연주하면 이미 주기적인 요소뿐만 아니라 비주기적인 요소를 포함하여 시간이 지남에 따라 지속적으로 변화할 수 있는 특성을 가진 복잡한 음향 혼합물이 생성된다.\n어택(attack) 단계 동안, 소리는 보통 넓은 주파수 범위에 걸쳐 소음 같은 구성 요소로 축적된다. 소리가 시작될 때 소음과 같은 짧은 지속 시간의 소리를 과도음/트랜지언트(transient)라고 한다.\n디케이(decay) 단계 동안, 소리는 안정화되고 일정한 주기 패턴에 도달한다.\n서스테인(sustain) 단계 동안, 에너지는 꽤 일정하게 유지된다.\n릴리스(release) 단계에서는 소리가 사라진다.\n다음 코드 셀에서, 이상화된 ADSR 모델을 생성한다.\n\n\ndef compute_adsr(len_A=10, len_D=10, len_S=60, len_R=10, height_A=1.0, height_S=0.5):\n    \"\"\"Computation of idealized ADSR model\n\n    Args:\n        len_A (int): Length (samples) of A phase (Default value = 10)\n        len_D (int): Length (samples) of D phase (Default value = 10)\n        len_S (int): Length (samples) of S phase (Default value = 60)\n        len_R (int): Length (samples) of R phase (Default value = 10)\n        height_A (float): Height of A phase (Default value = 1.0)\n        height_S (float): Height of S phase (Default value = 0.5)\n\n    Returns:\n        curve_ADSR (np.ndarray): ADSR model\n    \"\"\"\n    curve_A = np.arange(len_A) * height_A / len_A\n    curve_D = height_A - np.arange(len_D) * (height_A - height_S) / len_D\n    curve_S = np.ones(len_S) * height_S\n    curve_R = height_S * (1 - np.arange(1, len_R + 1) / len_R)\n    curve_ADSR = np.concatenate((curve_A, curve_D, curve_S, curve_R))\n    return curve_ADSR\n\n\ncurve_ADSR = compute_adsr(len_A=10, len_D=10, len_S=60, len_R=10, height_A=1.0, height_S=0.5)\n\nplot_signal(curve_ADSR, figsize=(4,2.5), ylabel='Amplitude', title='ADSR model', color='red')\nplt.show()\n\ncurve_ADSR = compute_adsr(len_A=20, len_D=2, len_S=60, len_R=1, height_A=2.0, height_S=1.2)\nplot_signal(curve_ADSR, figsize=(4,2.5), ylabel='Amplitude', title='ADSR model', color='red')\nplt.show()\n\n\n\n\n\n\n\n\nADSR 모델은 단순화된 형태이며 특정 악기에서 생성되는 톤의 진폭 엔벨로프에 대한 의미 있는 근사치만 산출한다.\n예를 들어, 위와 같은 바이올린 소리는 ADSR 모델에 의해 잘 근사되지 않는다.\n\n우선 음량을 점차 늘려가며 부드럽게 연주하기 때문에 어택 국면이 퍼진다. 게다가, 디케이 단계가 없는 것처럼 보이고 그 이후의 서스테인 단계는 일정하지 않다; 대신 진폭 엔벨로프는 규칙적인 방식으로 진동한다. 바이올린 연주자가 활로 현을 켜는 것을 멈추면 릴리즈 단계가 시작된다. 그리고 나서 그 소리는 빠르게 사라진다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프-계산",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프-계산",
    "title": "2.3. 오디오 표현",
    "section": "엔벨로프 계산",
    "text": "엔벨로프 계산\n\n파형의 엔벨로프를 계산하는 방법은 여러 가지가 있다. 다음에서는 각 윈도우 섹션에 최대 필터를 적용하여 간단한 슬라이딩 윈도우 방식을 사용한다. 다음 코드 셀에서는 주어진 파형의 상한 엔벨로프와 하한 엔벨로프 및 파형의 크기 엔벨로프를 계산한다.\n\n\ndef compute_envelope(x, win_len_sec=0.01, Fs=4000):\n    \"\"\"Computation of a signal's envelopes\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        win_len_sec (float): Length (seconds) of the window (Default value = 0.01)\n        Fs (scalar): Sampling rate (Default value = 4000)\n\n    Returns:\n        env (np.ndarray): Magnitude envelope\n        env_upper (np.ndarray): Upper envelope\n        env_lower (np.ndarray): Lower envelope\n    \"\"\"\n    win_len_half = round(win_len_sec * Fs * 0.5)\n    N = x.shape[0]\n    env = np.zeros(N)\n    env_upper = np.zeros(N)\n    env_lower = np.zeros(N)\n    for i in range(N):\n        i_start = max(0, i - win_len_half)\n        i_end = min(N, i + win_len_half)\n        env[i] = np.amax(np.abs(x)[i_start:i_end])\n        env_upper[i] = np.amax(x[i_start:i_end])\n        env_lower[i] = np.amin(x[i_start:i_end])\n    return env, env_upper, env_lower\n    \n    \ndef compute_plot_envelope(x, win_len_sec, Fs, figsize=(6, 3), title=''):\n    \"\"\"Computation and subsequent plotting of a signal's envelope\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        win_len_sec (float): Length (seconds) of the window\n        Fs (scalar): Sampling rate\n        figsize (tuple): Size of the figure (Default value = (6, 3))\n        title (str): Title of the figure (Default value = '')\n\n    Returns:\n        fig (mpl.figure.Figure): Generated figure\n    \"\"\"\n    t = np.arange(x.size)/Fs\n    env, env_upper, env_lower = compute_envelope(x, win_len_sec=win_len_sec, Fs=Fs)\n    fig = plt.figure(figsize=figsize)\n    plt.plot(t, x, color='gray', label='Waveform')\n    plt.plot(t, env_upper, linewidth=2, color='cyan', label='Upper envelope')\n    plt.plot(t, env_lower, linewidth=2, color='blue', label='Lower envelope')\n    plt.plot(t, env, linewidth=2, color='red', label='Magnitude envelope')\n    plt.title(title)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.xlim([t[0], t[-1]])\n    #plt.ylim([-0.7, 0.7])\n    plt.legend(loc='lower right')\n    plt.show()\n    ipd.display(ipd.Audio(data=x, rate=Fs))\n    return fig\n\n\nFs = 11025\nwin_len_sec=0.05\n\nx, Fs = librosa.load(\"../audio/piano_c4.wav\", sr=Fs)\nfig = compute_plot_envelope(x, win_len_sec=win_len_sec, Fs=Fs, title='piano sound')\n\nx, Fs = librosa.load(\"../audio/violin_c4.wav\", sr=Fs)\nfig = compute_plot_envelope(x, win_len_sec=win_len_sec, Fs=Fs, title='violin sound')\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#비브라토-트레몰로-vibrato-and-tremolo",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#비브라토-트레몰로-vibrato-and-tremolo",
    "title": "2.3. 오디오 표현",
    "section": "비브라토, 트레몰로 Vibrato and Tremolo",
    "text": "비브라토, 트레몰로 Vibrato and Tremolo\n\n바이올린의 예에서 음색과 관련된 다른 현상들이 나타난다. 예를 들어, 진폭의 주기적인 변화를 관찰할 수 있다. 이러한 진폭 변조는 트레몰로(tremolo)로도 알려져 있다.\n트레몰로의 효과는 진동수의 규칙적인 변화(주파수 변조)로 구성된 음악적 효과인 비브라토와 함께 종종 동반된다.\n현악 이외에도 비브라토는 인간 가수들이 표현을 더하기 위해 주로 사용된다. 트레몰로와 비브라토가 단순히 강도와 주파수의 국소적인 변화라고 할지라도, 그것들이 반드시 전체적인 음조의 음량이나 음조의 지각된 변화를 불러일으키지는 않는다. 오히려, 그것들은 음악적 음색에 영향을 미치는 특징들이다.\n다음 코드 셀에서, 단순한 정현파, 비브라토가 있는 정현파, 트레몰로가 있는 정현파를 생성한다.\n\n\ndef generate_sinusoid_vibrato(dur=5, Fs=1000, amp=0.5, freq=440, vib_amp=1, vib_rate=5):\n    \"\"\"Generation of a sinusoid signal with vibrato\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 0.5)\n        freq (float): Frequency (Hz) of sinusoid (Default value = 440)\n        vib_amp (float): Amplitude (Hz) of the frequency oscillation (Default value = 1)\n        vib_rate (float): Rate (Hz) of the frequency oscillation (Default value = 5)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    freq_vib = freq + vib_amp * np.sin(t * 2 * np.pi * vib_rate)\n    phase_vib = np.zeros(num_samples)\n    for i in range(1, num_samples):\n        phase_vib[i] = phase_vib[i-1] + 2 * np.pi * freq_vib[i-1] / Fs\n    x = amp * np.sin(phase_vib)\n    return x, t\n\ndef generate_sinusoid_tremolo(dur=5, Fs=1000, amp=0.5, freq=440, trem_amp=0.1, trem_rate=5):\n    \"\"\"Generation of a sinusoid signal with tremolo\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 0.5)\n        freq (float): Frequency (Hz) of sinusoid (Default value = 440)\n        trem_amp (float): Amplitude of the amplitude oscillation (Default value = 0.1)\n        trem_rate (float): Rate (Hz) of the amplitude oscillation (Default value = 5)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    amps = amp + trem_amp * np.sin(t * 2 * np.pi * trem_rate)\n    x = amps * np.sin(2*np.pi*(freq*t))\n    return x, t\n\n\nFs = 4000\ndur = 5\nfreq = 220\namp = 0.5\nfigsize = (8,2)\n\nx, t = generate_sinusoid(dur=dur, Fs=Fs, amp=amp, freq=freq)\nx_vib, t = generate_sinusoid_vibrato(dur=dur, Fs=Fs, amp=amp, freq=freq, vib_amp=6, vib_rate=5)\nx_trem, t = generate_sinusoid_tremolo(dur=dur, Fs=Fs, amp=amp, freq=freq, trem_amp=0.3, trem_rate=5)\n\n\nplot_signal(x, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nplot_signal(x_vib, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid with vibrato')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x_vib, rate=Fs))\n\nplot_signal(x_trem, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid with tremolo')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x_trem, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#부분음부분파-partials",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#부분음부분파-partials",
    "title": "2.3. 오디오 표현",
    "section": "부분음/부분파 (Partials)",
    "text": "부분음/부분파 (Partials)\n\n아마도 음색을 특징 짓는 가장 중요하고 잘 알려진 속성은 특정 부분파(Partials)의 존재와 그 상대적 강점일 것이다.\n부분파는 음악 톤에서 가장 낮은 부분이 기본 주파수(fundamental frequency) 인 지배적 주파수이다.\n소리의 부분파는 스펙트로그램으로 시각화된다. 스펙트로그램(spectrogram)은 시간 경과에 따른 주파수 성분의 강도를 보여준다\n비조화(inharmonicity)는 가장 가까운 이상고조파(ideal harmonic)에서 벗어나는 부분적 정도를 나타낸다.\n명확하게 인식할 수 있는 음정을 가진 음악적 톤과 같은 소리의 경우, 대부분의 부분파는 고조파(harmonics)에 가깝다. 그러나 모든 부분파가 동일한 강도로 발생할 필요는 없다. 다음 그림은 서로 다른 악기에서 재생되는 단일 노트 C4에 대한 스펙트로그램 표현을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F23_FourInstruments.png\", width=800)\n\n\n\n\n\n음의 기본 주파수(261.6Hz)와 고조파(261.6Hz의 정수 배수) 모두 수평 구조로 보인다.\n음악 톤의 디케이는 각각의 부분파에서 그에 상응하는 디케이에 의해 반영된다.\n톤의 에너지의 대부분은 낮은 부분에 포함되어 있고, 높은 부분에 대한 에너지는 낮은 경향이 있다. 이러한 분포는 많은 악기에서 일반적이다. 현악기의 경우, 소리는 풍부한 부분 스펙트럼을 갖는 경향이 있는데, 여기서 많은 에너지가 상부 고조파(harmonics)에도 포함될 수 있다. 이 그림은 또한 트레몰로(특히 플루트의 경우)와 비브라토(특히 바이올린의 경우)를 보여준다.\n\n다른 예\n\n# pure tone\nT = 2.0 # seconds\nf0 = 1047.0\nsr = 22050\nt = np.linspace(0, T, int(T*sr), endpoint=False) # time variable\nx = 0.1*np.sin(2*np.pi*f0*t)\nipd.Audio(x, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[:4096])\nX_mag = np.absolute(X)        # spectral magnitude\nf = np.linspace(0, sr, 4096)  # frequency variable\nplt.figure(figsize=(6, 2))\nplt.title('pure tone')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\n# oboe C6\nx, sr = librosa.load('../audio/oboe_c6.wav')\nprint(x.shape)\nipd.Audio(x, rate=sr)\n\n(23625,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[10000:14096])\nX_mag = np.absolute(X)\nplt.figure(figsize=(6, 2))\nplt.title('oboe C6')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\nx, sr = librosa.load('../audio/clarinet_c6.wav')\nprint(x.shape)\nipd.Audio(x, rate=sr)\n\n(51386,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[10000:14096])\nX_mag = np.absolute(X)\nplt.figure(figsize=(6, 2))\nplt.title('clarinet C6')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\n부분파의 구성 요소의 상대적 진폭 차이에 주목해보자. 세 신호 모두 거의 동일한 피치와 기본 주파수를 가지고 있지만, 음색은 다르다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#missing-fundamental",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#missing-fundamental",
    "title": "2.3. 오디오 표현",
    "section": "Missing Fundamental",
    "text": "Missing Fundamental\n\n앞서 말했듯이, 소리의 음색은 결정적으로 고조파에 걸친 신호의 에너지 분포에 따라 달라진다. 또한, 인식된 피치의 지각은 기본 주파수뿐만 아니라 더 높은 고조파와 그들의 관계에 따라 달라진다. 예를 들어, 인간은 이 피치와 관련된 기본 주파수가 완전히 누락된 경우에도 톤의 피치를 감지할 수 있다. 이 현상은 “missing fundamental”로 알려져 있다.\n다음 코드 예제에서는 음의 중심 주파수(center frequency)의 정수 배수인 주파수를 가진 (가중된) 정현파를 추가하여 소리를 생성한다. 특히 순수 톤(MIDI 피치 𝑝), 고조파가 있는 톤, missing fundamental의 고조파가 있는 톤, 두번 째 순수 톤(MIDI 피치 𝑝+12)을 생성한다.\n\n\ndef generate_tone(p=60, weight_harmonic=np.ones([16, 1]), Fs=11025, dur=2):\n    \"\"\"Generation of a tone with harmonics\n\n    Args:\n        p (float): MIDI pitch of the tone (Default value = 60)\n        weight_harmonic (np.ndarray): Weights for the different harmonics (Default value = np.ones([16, 1])\n        Fs (scalar): Sampling frequency (Default value = 11025)\n        dur (float): Duration (seconds) of the signal (Default value = 2)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    freq = 2 ** ((p - 69) / 12) * 440\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    x = np.zeros(t.shape)\n    for h, w in enumerate(weight_harmonic):\n        x = x + w * np.sin(2 * np.pi * freq * (h + 1) * t)\n    return x, t\n\ndef plot_spectrogram(x, Fs=11025, N=4096, H=2048, figsize=(4, 2)):\n    \"\"\"Computation and subsequent plotting of the spectrogram of a signal\n\n    Args:\n        x: Signal (waveform) to be analyzed\n        Fs: Sampling rate (Default value = 11025)\n        N: FFT length (Default value = 4096)\n        H: Hopsize (Default value = 2048)\n        figsize: Size of the figure (Default value = (4, 2))\n\n    \"\"\"\n    N, H = 2048, 1024\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Y = np.abs(X)\n    plt.figure(figsize=figsize)\n    librosa.display.specshow(librosa.amplitude_to_db(Y, ref=np.max),\n                             y_axis='linear', x_axis='time', sr=Fs, hop_length=H, cmap='gray_r')\n    plt.ylim([0, 3000])\n    # plt.colorbar(format='%+2.0f dB')\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Frequency (Hz)')\n    plt.tight_layout()\n    plt.show()\n\n\nFs = 11025\np = 60\n\nprint('Pure tone (p = %s):' % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0.2])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Tone with harmonics (p = %s):' % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Tone with harmonics and missing fundamental (p = %s):'  % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Pure tone (p = %s):' % (p + 12))\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0, 0.2])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nPure tone (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nTone with harmonics (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nTone with harmonics and missing fundamental (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPure tone (p = 72):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://musicinformationretrieval.com/\nhttps://www.audiolabs-erlangen.de/FMP\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "",
    "text": "푸리에 변환(Fourier transform)을 보기 전에 이와 관련한 몇가지 수학적 개념(복소수, 지수함수)을 리뷰해보록 한다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#기본-개념",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#기본-개념",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "기본 개념",
    "text": "기본 개념\n\n실수 부분 \\(\\mathrm{Re}(c) = a\\), 허수 부분 \\(\\mathrm{Im}(c) = b\\) 및 허수 단위 \\(i = \\sqrt{-1}\\)로 복소수 \\(c = a + ib\\)를 쓸 수 있다. 파이썬에서 기호 j는 허수의 단위를 나타내기 위해 사용된다. 또한 j 앞의 계수가 필요하다. 복소수를 지정하기 위해 complex라는 생성자를 사용할 수도 있다.\n\n\na = 1.5\nb = 0.8\nc = a + b*1j\nprint(c)\nc2 = complex(a,b)\nprint(c2)\n\n(1.5+0.8j)\n(1.5+0.8j)\n\n\n\nprint(np.real(c))\nprint(np.imag(c))\n\n1.5\n0.8\n\n\n\ndef generate_figure(figsize=(6, 2), xlim=[0, 1], ylim=[0, 1]):\n    \"\"\"Generate figure for plotting complex numbers\n\n    Args:\n        figsize: Figure size (Default value = (2, 2))\n        xlim: Limits of x-axis (Default value = [0, 1])\n        ylim: Limits of y-axis (Default value = [0, 1])\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.grid()\n    plt.xlim(xlim)\n    plt.ylim(ylim)\n    plt.xlabel(r'$\\mathrm{Re}$')\n    plt.ylabel(r'$\\mathrm{Im}$')\n\ndef plot_vector(c, color='k', start=0, linestyle='-'):\n    \"\"\"Plot arrow corresponding to difference of two complex numbers\n\n    Args:\n        c: Complex number\n        color: Color of arrow (Default value = 'k')\n        start: Complex number encoding the start position (Default value = 0)\n        linestyle: Linestyle of arrow (Default value = '-')\n\n    Returns:\n        arrow (matplotlib.patches.FancyArrow): Arrow\n    \"\"\"\n    return plt.arrow(np.real(start), np.imag(start), np.real(c), np.imag(c),\n                     linestyle=linestyle, head_width=0.05, fc=color, ec=color, overhang=0.3,\n                     length_includes_head=True)\n\n\nc = 1.5 + 0.8j\n\ngenerate_figure(xlim=[0, 2.5], ylim=[0, 1])\nv = plot_vector(c, color='k')\n\nplt.text(1.5, 0.8, '$c$', size='16')\nplt.text(0.8, 0.55, '$|c|$', size='16')\nplt.text(0.25, 0.05, '$\\gamma$', size='16');"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#극좌표-표현-polar-representation",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#극좌표-표현-polar-representation",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "극좌표 표현 (Polar Representation)",
    "text": "극좌표 표현 (Polar Representation)\n\n복소수 \\(a+ib\\) 의 절대값 (absolute value or modulus)은 다음과 같이 정의된다.\n\n\\(|c| := \\sqrt{a^2 + b^2}.\\)\n\n(radian으로 주어진) 각도(angle)는 다음과 같다.\n\n\\(\\gamma := \\mathrm{atan2}(b, a).\\)\n\n이는 \\((-\\pi,\\pi]\\) 간격의 숫자를 생성하며, 이 값은 음의 값에 \\(2\\pi\\)를 추가하여 \\([0,2\\pi)\\)에 매핑될 수 있다. 각도(degree 단위)는 다음과 같이 구한다.\n\n\\(360 \\cdot \\frac{\\gamma}{2\\pi}\\)\n\n\n\nprint('Absolute value:', np.abs(c))\nprint('Angle (in radians):', np.angle(c))\nprint('Angle (in degree):', np.rad2deg(np.angle(c)))\nprint('Angle (in degree):', 360 * np.angle(c)/(2*np.pi) )\n\nAbsolute value: 1.7\nAngle (in radians): 0.48995732625372834\nAngle (in degree): 28.07248693585296\nAngle (in degree): 28.07248693585296\n\n\n\n복소수 \\(c=a+ib\\)는 \\((|c|, \\gamma)\\) 쌍에 의해 고유하게 정의되며, 이는 \\(c\\)의 극좌표 표현(polar representation)이라고도 한다. 다음과 같이 극좌표 표현 \\((|c|,\\gamma)\\)에서 데카르트 표현(Cartesian representation) \\((a,b)\\)를 얻는다.\n\n\\[\\begin{eqnarray}\na &=& |c| \\cdot \\cos(\\gamma) \\\\\nb &=& |c| \\cdot \\sin(\\gamma)\n\\end{eqnarray}\\]"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#연산",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#연산",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "연산",
    "text": "연산\n\n두 복소수 \\(c_1=a_1+ib_1\\)와 \\(c_2=a_2+ib_2\\)의 경우, 합은 다음과 같다.\n\n\\[\nc_1 + c_2 = (a_1 + ib_1) + (a_2 + ib_2) := (a_1 + a_2) + i(b_1 + b_2)\n\\]\n\n실수 부분과 허수 부분을 개별적으로 합산하여 정의한다. 덧셈의 기하학적 직관은 평행사변형으로 시각화할 수 있다.\n\n\nc1 = 1.3 - 0.3j\nc2 = 0.3 + 0.5j\nc = c1 + c2\n\ngenerate_figure(xlim=[-0.3, 2.2], ylim=[-0.4, 0.6])\nv1 = plot_vector(c1, color='k')\nv2 = plot_vector(c2, color='b')\nplot_vector(c1, start=c2, linestyle=':', color='lightgray')\nplot_vector(c2, start=c1, linestyle=':', color='lightgray')\nv3 = plot_vector(c, color='r')\n\nplt.legend([v1, v2, v3], ['$c_1$', '$c_2$', '$c_1+c_2$']);\n\n\n\n\n\n두 숫자 \\(c_1=a_1+ib_1\\)와 \\(c_2=a_2+ib_2\\)의 복소수 곱셈은 다음과 같이 정의된다.\n\n\\[c = c_1 \\cdot c_2 = (a_1 + ib_1) \\cdot (a_2 + ib_2) := (a_1a_2 - b_1b_2) + i(a_1b_2 + b_1a_2).\\]\n\n기하학적으로, 이 곱은 각도를 더하고 절대값을 곱함으로써 얻어진다. 다시 말해, \\((|c_1|, \\gamma_1)\\)와 \\((|c_2|, \\gamma_2)\\)가 각각 \\(c_1\\)와 \\(c_1\\)의 극좌표 표현이라면, \\(c\\)의 극좌표 표현 \\((|c|, \\gamma)\\)는 다음과 같이 주어진다.\n\n\\[\\begin{eqnarray}\n\\gamma &=& \\gamma_1 + \\gamma_2 \\\\\n|c| &=& |c_1| \\cdot |c_2|\n\\end{eqnarray}\\]\n\nc1 = 1.0 - 0.5j\nc2 = 2.3 + 0.7j\nc = c1 * c2\n\ngenerate_figure(xlim=[-0.5, 4.0], ylim=[-0.75, 0.75])\nv1 = plot_vector(c1, color='k')\nv2 = plot_vector(c2, color='b')\nv3 = plot_vector(c, color='r')\nplt.legend([v1, v2, v3], ['$c_1$', '$c_2$', '$c_1 \\cdot c_2$']);"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#극좌표계-polar-coordinate-plot",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#극좌표계-polar-coordinate-plot",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "극좌표계 (Polar Coordinate Plot)",
    "text": "극좌표계 (Polar Coordinate Plot)\n\ndef plot_polar_vector(c, label=None, color=None, start=0, linestyle='-'):\n    # plot line in polar plane\n    line = plt.polar([np.angle(start), np.angle(c)], [np.abs(start), np.abs(c)], label=label, \n                     color=color, linestyle=linestyle)\n    # plot arrow in same color\n    this_color = line[0].get_color() if color is None else color\n    plt.annotate('', xytext=(np.angle(start), np.abs(start)), xy=(np.angle(c), np.abs(c)),\n                 arrowprops=dict(facecolor=this_color, edgecolor='none', \n                                 headlength=12, headwidth=10, shrink=1, width=0))\n\n\n#head_width=0.05, fc=color, ec=color, overhang=0.3, length_includes_head=True    \n    \nc_abs = 1.5\nc_angle = 45  # in degree\nc_angle_rad = np.deg2rad(c_angle) \na = c_abs * np.cos(c_angle_rad)\nb = c_abs * np.sin(c_angle_rad)\nc1 = a + b*1j    \nc2 = -0.5 + 0.75*1j\n\nplt.figure(figsize=(6, 6))\nplot_polar_vector(c1, label='$c_1$', color='k')\nplot_polar_vector(np.conj(c1), label='$\\overline{c}_1$', color='gray')\nplot_polar_vector(c2, label='$c_2$', color='b')\nplot_polar_vector(c1*c2, label='$c_1\\cdot c_2$', color='r')\nplot_polar_vector(c1/c2, label='$c_1/c_2$', color='g')\n\nplt.ylim([0, 1.8]);\nplt.legend(framealpha=1);"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#power-series-멱급수",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#power-series-멱급수",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "Power Series (멱급수)",
    "text": "Power Series (멱급수)\n\n실 지수 함수(real exponential function) \\(\\exp:\\mathbb{R}\\to \\mathbb{R}\\)는 많은 수학 응용에서 볼 수 있다. 그리고 이 함수는 많은 다른 방식으로 특성화될 수 있다.\n역사적으로 지수 함수는 금리를 고려할 때 \\(17^\\mathrm{th}\\) 세기에 Johann Bernoulli에 의해 연구된 바 있다.\n\n\\(1\\)의 이자를 매월 복합된 연간 금리로 이자 \\(a\\)를 얻는다고 가정하자. 그런 다음 매달 얻은 이자는 현재 값의 \\(\\frac{a}{12}\\)배이므로, 매달 총 값에 \\(\\left(1+\\frac{a}{12}\\right)\\)를 곱하고 연말의 값은 \\(\\left(1+\\frac{a}{12}\\right)^{12}\\)이다. 매일 이자가 복합되는 경우 \\(\\left(1+\\frac{a}{365}\\right)^{365}\\)가 된다.\n\n시간 간격을 더 짧게 함으로써 매년 증가하도록 하는 것은 지수 함수의 limit 정의로 이어진다.\n\n\\(\\exp(a) = \\mathrm{lim}_{n\\to\\infty} \\left(1+\\frac{a}{n}\\right)^{n},\\)\n\n상수 \\(e:=\\exp(1)\\approx 2.71828 \\ldots\\)는 Euler의 숫자로도 알려져 있다. 위의 정의에서 \\(n\\)-fold 곱을 확장하면 지수 함수가 다음과 같은 멱급수로 표현될 수 있음을 보여줄 수 있다. \\[\\exp(a) := \\sum_{n=0}^{\\infty} \\frac{a^n}{n!} = 1 + a + \\frac{a^2}{1 \\cdot 2} + \\frac{a^3}{1 \\cdot 2 \\cdot 3} + \\cdot\\]\n멱급수에서 실수값 변수 \\(a\\in\\mathbb{R}\\)를 복소수 값 변수 \\(c\\in\\mathbb{C}\\)로 바꾸면, 여전히 다음과 같이 주어진 복소수 지수 함수 \\(\\exp:\\mathbb{C}\\to \\mathbb{C}\\)를 얻는다.\n\n\\[\\exp(c) := \\sum_{n=0}^{\\infty} \\frac{c^n}{n!} = 1 + c + \\frac{c^2}{1 \\cdot 2} + \\frac{c^3}{1 \\cdot 2 \\cdot 3} + \\cdot\\]\n\n복소수 지수 함수의 정의를 기반으로 삼각 함수(예: \\(\\sin\\) 및 \\(\\cos\\))의 정의를 복소 인수로 확장할 수도 있다.\n다음 구현은 매개 변수 \\(N\\in\\mathbb{N}\\)에 의해 지정된 첫 번째 \\(N\\) 항만 고려하여 멱급수의 근사치를 산출한다. \\(c=1\\)의 경우, 숫자 \\(e\\)에 대한 근사치를 산출한다.\n\n\ndef exp_power_series(c, N):\n    \"\"\"Compute power series for exponential function\n\n    Args:\n        c: Complex number\n        N: Number of summands used for approximation\n\n    Returns:\n        exp_c: Approximation of exp(c)\n    \"\"\"    \n    exp_c = 1\n    c_power = 1\n    nfac = 1\n    for n in range(1, N):\n        nfac *= n\n        c_power *= c \n        exp_c += c_power / nfac\n    return exp_c\n\n\nc=1\nprint('Approximation (N =  1):', exp_power_series(c, 1))\nprint('Approximation (N =  2):', exp_power_series(c, 2))\nprint('Approximation (N =  4):', exp_power_series(c, 4))\nprint('Approximation (N =  8):', exp_power_series(c, 8))\nprint('Approximation (N = 12):', exp_power_series(c, 12))\nprint('Numpy:                 ', np.exp(c))\n\nApproximation (N =  1): 1\nApproximation (N =  2): 2.0\nApproximation (N =  4): 2.6666666666666665\nApproximation (N =  8): 2.7182539682539684\nApproximation (N = 12): 2.718281826198493\nNumpy:                  2.718281828459045"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#지수-항등식과-오일러-공식-exponentiation-identity-and-eulers-formula",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#지수-항등식과-오일러-공식-exponentiation-identity-and-eulers-formula",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "지수 항등식과 오일러 공식 (Exponentiation Identity and Euler’s Formula )",
    "text": "지수 항등식과 오일러 공식 (Exponentiation Identity and Euler’s Formula )\n\n멱급수 정의에 기초하여, 많은 속성을 설명하는 지수 함수의 두 가지 유명한 공식을 증명할 수 있다.\n첫 번째 공식은 지수 항등식 exponentation identity로 알려져 있으며 다음과 같다.\n\n\\[\n  \\exp(c_1 + c_2) = \\exp(c_1)\\cdot \\exp(c_2)\n\\]\nfor any complex numbers \\(c_1, c_2\\in\\mathbb{C}\\).\n\n특히, 이 속성은 실수 인수의 기하급수적인 증가를 설명한다. 예를들면,\n\n\\[\n  \\exp(n) = \\exp(1+1+\\ldots +1) = \\exp(1)^n = e^n\n\\]\nfor \\(n\\in\\mathbb{N}\\).\n\n오일러 공식 Euler’s formula로 알려진 두 번째 공식은 순 허수(pure imaginary)의 인수에서 지수 함수의 값을 삼각 함수와 연관시킨다. 이는 일부 실수 값 \\(\\beta\\in\\mathbb{R}\\)와 함께 복소수 \\(c = i\\gamma\\)에 대해, 다음의 항등식을 가진다.\n\n\\[\\mathrm{exp}(i\\gamma) = \\cos(\\gamma) + i\\sin(\\gamma)\\]\n\n실제로 실수 사인 및 코사인 함수를 시작으로 오일러 공식을 사용하여 \\(\\mathrm{exp}(i\\gamma)\\)를 정의하는 경우가 많다.\n다음 그림에 나온 것처럼 허수(수직) 축을 따라 \\(\\exp\\)의 실수 및 허수 부분의 주기적인 행동을 설명한다.\n\n\nA, B = np.meshgrid(np.arange(-2, 2, 0.1), np.arange(-12, 12, 0.1))\nC = A + B*1j\nf_exp = np.exp(C)\n\nplt.figure(figsize=(12, 4))\nextent = [-2, 2, -12, 12]\nplt.subplot(1, 3, 1)\nplt.imshow(np.real(f_exp),  aspect='auto', cmap='seismic', origin='lower', extent=extent)\nplt.title('Real part Re(exp(c))')\nplt.xlabel('a = Re(c)')\nplt.ylabel('b = Im(c)')\nplt.colorbar()\nplt.subplot(1, 3, 2)\nplt.imshow(np.imag(f_exp),  aspect='auto', cmap='seismic', origin='lower', extent=extent)\nplt.title('Imaginary part Im(exp(c))')\nplt.xlabel('a = Re(c)')\nplt.ylabel('b = Im(c)')\nplt.colorbar()\nplt.subplot(1, 3, 3)\nplt.imshow(np.abs(f_exp),  aspect='auto', cmap='gray_r', origin='lower', extent=extent)\nplt.title('Absolute value |exp(c)|')\nplt.xlabel('a = Re(c)')\nplt.ylabel('b = Im(c)')\nplt.colorbar()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#기본-속성",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#기본-속성",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "기본 속성",
    "text": "기본 속성\n\n지수 함수에는 여러 가지 흥미로운 속성이 있다:\n\n\\(\\exp(i\\gamma) = \\exp(i(\\gamma+2\\pi))\\)\n\\(|\\exp(i\\gamma)| = 1\\)\n\\(\\overline{\\exp(i\\gamma)} = \\exp(-i\\gamma)\\)\n\\(\\exp(i(\\gamma_1+\\gamma_2)) = \\exp(i\\gamma_1) \\exp(i\\gamma_2)\\)\n\\(\\frac{d\\exp(i\\gamma)}{d\\gamma} = i\\exp(i\\gamma)\\)\n\n특히, 복소수 값 \\(\\mathrm{exp}(i\\gamma)\\)은 모든 \\(\\gamma\\in\\mathbb{R}\\)에 대해 복소수 평면의 단위 원(unit circle)에 있다. 또한 주기성(periodicity)으로 인해 \\(\\gamma\\in[0,2\\pi)\\)을 고려하기에 충분하다.\n실제로 \\(\\gamma\\)는 복소수 \\(c = \\mathrm{exp}(i\\gamma)\\)의 각도(라디안 단위)를 인코딩한다.(\\(|c|=1\\))\n다음 그림은 각도 \\(\\gamma\\)를 \\(0\\)에서 \\(2\\pi\\)로 증가시킬 때 값 \\(\\mathrm{exp}(i\\gamma)\\)이 어떻게 변하는지 보여준다.\n\n\nfrom matplotlib import ticker \n%matplotlib inline\n\ncmap = plt.cm.get_cmap('hsv') # hsv is nice because it is a circular color map\n\nN = 64\n\nfig = plt.figure(figsize=(5 * 3, 5))\nax1 = fig.add_subplot(1, 3, 1, projection='polar')\nax2 = fig.add_subplot(1, 3, 2)\nax3 = fig.add_subplot(1, 3, 3)\n\nfor i in range(N):\n    gamma = 2 * np.pi * i / N\n    c = np.exp(1j * gamma)\n    color = cmap(i / N)\n    ax1.plot([0, np.angle(c)], [0, np.abs(c)], color=color)\n    ax1.plot(np.angle(c), np.abs(c), 'o', color=color)\n    ax2.plot(gamma, np.real(c), 'o', color=color)\n    ax3.plot(gamma, np.imag(c), 'o', color=color)\n    \nax2.grid()\nax2.set_xlabel('$\\gamma$ [radians]')\nax2.set_ylabel('$\\mathrm{Re}(\\exp(i \\gamma))$')\nax2.xaxis.set_major_formatter(ticker.FormatStrFormatter('$%s$')) \n\nax3.grid()\nax3.set_xlabel('$\\gamma$ [radians]')\nax3.set_ylabel('$\\mathrm{Im}(\\exp(i \\gamma))$')\nax3.xaxis.set_major_formatter(ticker.FormatStrFormatter('$%s$')) \nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#의-거듭제곱근-roots-of-unity",
    "href": "posts/3. Fourier Anaylsis of Signals/3.1.Math_Review.html#의-거듭제곱근-roots-of-unity",
    "title": "3.1. 수학리뷰 - 복소수와 지수함수",
    "section": "1의 거듭제곱근 (Roots of Unity)",
    "text": "1의 거듭제곱근 (Roots of Unity)\n\n\\(N \\in \\mathbb{N}_{>0}\\)를 양의 정수라고 하자. 복소수 \\(\\rho \\in \\mathbb{C}\\)는 \\(\\rho^N = 1\\)인 경우 \\(N^\\mathrm{th}\\) 1의 거듭제곱근(root of unity)라고 한다. 다시말해 1의 거듭제곱근은 거듭제곱하여 1이 되는 복소수이다. 정확히 \\(N\\)개의 뚜렷한 \\(N^\\mathrm{th}\\) root of unity가 있다는 것을 보이는 것은 어렵지 않다.\n또한, 모든 \\(n\\in [1:N-1]\\)에 대해 \\(\\rho^n \\neq 1\\)인 경우, 단위 n승근(primitive \\(N^\\mathrm{th}\\) root of unity)라고 한다.\n위에서 언급한 속성을 통해 \\(\\rho_N:=\\exp(2 \\pi i / N)\\) 이 단위 n승근임을 쉽게 알 수 있다.\n모든 \\(N^\\mathrm{th}\\) root of unity는 \\(\\rho_N\\)의 power을 고려하여 생성될 수 있다.:\n\n\\[1=\\rho_N^0, \\quad \\rho_N^1, \\quad \\rho_N^2, \\quad ...,\\quad \\rho_N^{N-1}\\]\n\n다음 그림은 서로 다른 정수 \\(N \\in \\mathbb{N}_{>0}\\)에 대한 모든 root of unity를 보여준다. primitive root는 빨간색으로 표시되어 있다.\n\n\ndef plot_root_unity(N, figsize=(5, 5)): \n    root_unity = np.exp(2j * np.pi / N)\n    root_unity_power = 1\n\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.grid()  \n    plt.xlim([-1.4, 1.4])\n    plt.ylim([-1.4, 1.4])\n    plt.xlabel('$\\mathrm{Re}$')\n    plt.ylabel('$\\mathrm{Im}$')\n    plt.title('Roots of unity for $N=%d$'%N)\n\n    for n in range(0, N):\n        colorPlot = 'r' if gcd(n, N) == 1 else 'k'\n        plot_vector(root_unity_power, color=colorPlot)\n        plt.text(np.real(1.2*root_unity_power), np.imag(1.2*root_unity_power), \n                 r'$\\rho_{%s}^{%s}$' % (N, n), size='18', \n                 color=colorPlot, ha='center', va='center')\n        root_unity_power *= root_unity\n\n    circle_unit = plt.Circle((0, 0), 1, color='lightgray', fill=0)   \n    ax.add_artist(circle_unit)\n\n\nplot_root_unity(N=8)    \nplot_root_unity(N=11)\nplot_root_unity(N=12)\n\n\n\n\n\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C2/C2.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "",
    "text": "이산 푸리에 변환(DFT)과 그 기본 속성과 함께, DFT를 평가하는 효율적인 알고리즘인 고속 푸리에 변환(FFT)을 소개한다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#내적-inner-product",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#내적-inner-product",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "내적 (Inner Product)",
    "text": "내적 (Inner Product)\n\n푸리에 변환(Fourier transform)을 이해하기 위한 중요한 개념으로 \\(N \\in \\mathbb{N}\\)에 대한 복소(complex) 벡터 공간 \\(\\mathbb{C}^N\\)에 대한 내적(inner product) 이 있다.\n두 개의 복소수 벡터 \\(x, y \\in \\mathbb{C}^N\\)가 주어지면, \\(x\\)와 \\(y\\) 사이의 내적은 다음과 같이 정의된다. \\[\\langle x | y \\rangle := \\sum_{n=0}^{N-1} x(n) \\overline{y(n)}.\\]\n내적의 절대값은 \\(x\\)와 \\(y\\) 사이의 유사성의 척도로 해석될 수 있다.\n\n\\(x\\)와 \\(y\\)가 동일한 방향을 가리키면(즉, \\(x\\)와 \\(y\\)가 유사함), 내적 \\(|\\langle x | y \\rangle|\\)이 크다.\n\\(x\\)와 \\(y\\)가 직교하면(즉, \\(x\\)와 \\(y\\)가 서로 다른 경우), 내적 \\(|\\langle x | y \\rangle|\\)는 0이다.\n\n함수 np.vdot을 사용하여 내적을 계산할 때 첫 번째 인수에 대해 복소 켤레(complex conjugate)가 수행된다는 점에 유의하자.\n따라서, 위에서 정의된 \\(\\langle x | y \\rangle\\)를 계산하려면, np.vdot(y, x)를 사용해야 한다.\n\n\nx = np.array([ 1.0, 1j, 1.0 + 1.0j ])\ny = np.array([ 1.1, 1j, 0.9 + 1.1j ])\nprint('Vectors of high similarity:', np.abs(np.vdot(y, x)))\n\nx = np.array([ 1.0,   1j, 1.0 + 1j ])\ny = np.array([ 1.1, -1j, 0.1      ])\nprint('Vectors of low similarity:', np.abs(np.vdot(y, x)))\n\nVectors of high similarity: 4.104875150354758\nVectors of low similarity: 0.22360679774997913"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#dft의-정의",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#dft의-정의",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "DFT의 정의",
    "text": "DFT의 정의\n\n\\(x\\in \\mathbb{C}^N\\)을 길이가 \\(N\\in\\mathbb{N}\\)인 벡터라고 하자. 음악 신호의 맥락에서 \\(x\\)는 샘플 \\(x(0), x(1), ..., x(N-1)\\)가 있는 이산(discrete) 신호로 해석될 수 있다.\n이산 푸리에 변환(DFT)은 다음과 같이 정의된다. \\[ X(k) := \\sum_{n=0}^{N-1} x(n) \\exp(-2 \\pi i k n / N) \\] for \\(k \\in [0:N-1]\\).\n벡터 \\(X\\in\\mathbb{C}^N\\)는 시간 영역(time-domain) 신호 \\(x\\)의 주파수 표현(frequency representation)으로 해석될 수 있다.\nDFT의 기하학적 해석을 얻기 위해 벡터 \\(\\mathbf{u}_k\\in\\mathbb{C}^N\\)를 다음과 같이 정의한다. \\[\\mathbf{u}_k(n) :=  \\exp(2 \\pi i k n / N) = \\cos(2 \\pi k n / N) + i \\sin(2 \\pi k n / N)\\] for \\(k \\in [0:N-1]\\).\n이 벡터는 주파수 \\(k/N\\)의 지수 함수의 샘플링된 버전으로 볼 수 있다. 그러면 DFT는 신호 \\(x\\) 및 샘플링된 지수 함수 \\(\\mathbf{u}_k\\)의 내적으로 표현될 수 있다. \\[ X(k) := \\sum_{n=0}^{N-1} x(n) \\overline{\\mathbf{u}_k} = \\langle x | \\mathbf{u}_k \\rangle\\]\n절대값 \\(|X(k)|\\)는 신호 \\(x\\)와 \\(\\mathbf{u}_k\\) 사이의 유사도를 나타낸다.\n\\(x\\in \\mathbb{R}^N\\)이 실수 값 벡터(음악 신호 시나리오의 경우, 항상 해당)인 경우 다음을 얻는다.\n\n\\[ X(k) := \\langle x |\\mathrm{Re}(\\mathbf{u}_k) \\rangle - i\\langle x | \\mathrm{Im}(\\mathbf{u}_k) \\rangle \\]\n\n다음 그림은 두 개의 서로 다른 주파수 파라미터 \\(k\\)에 대한 함수 \\(\\overline{\\mathbf{u}_k}\\)와 비교한 신호 \\(x\\)의 예를 보여준다.\n\n\\(\\overline{\\mathbf{u}_k}\\)의 실수부와 허수부는 각각  빨간색 및  파란색으로 표시된다.\n\n\n\nN = 64\nn = np.arange(N)\nk = 3\nx = np.cos(2 * np.pi * (k * n / N) + (1.2*np.random.rand(N) - 0.0))\n\nplt.figure(figsize=(6, 4))\n\nplt.subplot(2, 1, 1)\nplt.plot(n, x, 'k', marker='.', markersize='5', linewidth=2.0, label='$x$')\nplt.xlabel('Time (samples)')\nk = 3\nu_k_real = np.cos(2 * np.pi * k * n / N)\nu_k_imag = -np.sin(2 * np.pi * k * n / N)\nu_k = u_k_real + u_k_imag*1j\nsim_complex = np.vdot(u_k, x)\nsim_abs = np.abs(sim_complex)\nplt.title(r'Signal $x$ and some $u_k$ (k=3) having high similarity: Re($X(k)$) = %0.2f, Im($X(k)$) = %0.2f,  $|X(k)|$=%0.2f'%(sim_complex.real,sim_complex.imag,sim_abs))\nplt.plot(n, u_k_real, 'r', marker='.', markersize='3', \n         linewidth=1.0, linestyle=':', label='$\\mathrm{Re}(\\overline{\\mathbf{u}}_k)$');\nplt.plot(n, u_k_imag, 'b', marker='.', markersize='3', \n         linewidth=1.0, linestyle=':', label='$\\mathrm{Im}(\\overline{\\mathbf{u}}_k)$');\nplt.legend()\n\nplt.subplot(2, 1, 2)\nplt.plot(n, x, 'k', marker='.', markersize='5', linewidth=2.0, label='$x$')\nplt.xlabel('Time (samples)')\nk = 5\nu_k_real = np.cos(2 * np.pi * k * n / N)\nu_k_imag = -np.sin(2 * np.pi * k * n / N)\nu_k = u_k_real + u_k_imag*1j\nsim_complex = np.vdot(u_k, x)\nsim_abs = np.abs(sim_complex)\nplt.title(r'Signal $x$ and some $u_k$ (k=5) having low similarity: Re($X(k)$) = %0.2f, Im($X(k)$) = %0.2f,  $|X(k)|$=%0.2f'%(sim_complex.real,sim_complex.imag,sim_abs))\nplt.plot(n, u_k_real, 'r', marker='.', markersize='3', \n         linewidth=1.0, linestyle=':', label='$\\mathrm{Re}(\\overline{\\mathbf{u}}_k)$');\nplt.plot(n, u_k_imag, 'b', marker='.', markersize='3', \n         linewidth=1.0, linestyle=':', label='$\\mathrm{Im}(\\overline{\\mathbf{u}}_k)$');\nplt.legend()\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#dft-행렬-dft-matrix",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#dft-행렬-dft-matrix",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "DFT 행렬 (DFT Matrix)",
    "text": "DFT 행렬 (DFT Matrix)\n\n선형 연산자 \\(\\mathbb{C}^N \\to \\mathbb{C}^N\\)이면, DFT는 \\(N\\times N\\)-행렬로 표현할 수 있다. 이는 다음과 같은 유명한 DFT 행렬 \\(\\mathrm{DFT}_N \\in \\mathbb{C}^{N\\times N}\\) 행렬로 이어진다. \\[\\mathrm{DFT}_N(n, k) = \\mathrm{exp}(-2 \\pi i k n / N)\\] for \\(n\\in[0:N-1]\\) and \\(k\\in[0:N-1]\\).\n\\(\\rho_N:=\\exp(2 \\pi i / N)\\)를 단위 N승근 (primitive Nth roots of unity)이라고 한다. 또한 단위 N승근을 다음과 같이 정의한다.\n\n\\(\\sigma_N:= \\overline{\\rho_N} = \\mathrm{exp}(-2 \\pi i / N)\\)\n\n지수 함수의 속성으로부터 다음을 얻을 수 있다.\n\n\\(\\sigma_N^{kn} = \\mathrm{exp}(-2 \\pi i / N)^{kn} = \\mathrm{exp}(-2 \\pi i k n / N)\\)\n\n이로부터 다음의 행렬을 얻는다. \\[\n\\mathrm{DFT}_N =\n\\begin{pmatrix}\n  1 & 1 & 1 & \\dots  & 1 \\\\\n  1 & \\sigma_N & \\sigma_N^2 & \\dots  & \\sigma_N^{N-1} \\\\\n  1 & \\sigma_N^2 & \\sigma_N^4 & \\dots  & \\sigma_N^{2(N-1)} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  1 & \\sigma_N^{N-1} & \\sigma_N^{2(N-1)} & \\dots  & \\sigma_N^{(N-1)(N-1)} \\\\\n\\end{pmatrix}\n\\]\n다음 그림에서 \\(\\mathrm{DFT}_N\\)의 실수 및 허수 부분이 표시되며, 값은 적절한 색상으로 인코딩된다. \\(\\mathrm{DFT}_N\\)의 \\(k^\\mathrm{th}\\) 행은 위에 정의된 벡터 \\(\\mathbf{u}_k\\)에 해당한다.\n\n\ndef generate_matrix_dft(N, K):\n    \"\"\"Generates a DFT (discrete Fourier transfrom) matrix\n\n    Args:\n        N (int): Number of samples\n        K (int): Number of frequency bins\n\n    Returns:\n        dft (np.ndarray): The DFT matrix\n    \"\"\"\n    dft = np.zeros((K, N), dtype=np.complex128)\n    for n in range(N):\n        for k in range(K):\n            dft[k, n] = np.exp(-2j * np.pi * k * n / N)\n    return dft\n\ndef dft(x):\n    \"\"\"Compute the disrcete Fourier transfrom (DFT)\n\n    Args:\n        x (np.ndarray): Signal to be transformed\n\n    Returns:\n        X (np.ndarray): Fourier transform of x\n    \"\"\"\n    x = x.astype(np.complex128)\n    N = len(x)\n    dft_mat = generate_matrix_dft(N, N)\n    return np.dot(dft_mat, x)\n\n\nN = 32\ndft_mat = generate_matrix_dft(N, N)\n\nplt.figure(figsize=(10, 3))\n\nplt.subplot(1, 2, 1)\nplt.title('$\\mathrm{Re}(\\mathrm{DFT}_N)$')\nplt.imshow(np.real(dft_mat), origin='lower', cmap='seismic', aspect='equal')\nplt.xlabel('Time index $n$')\nplt.ylabel('Frequency index $k$')\nplt.colorbar()\n\nplt.subplot(1, 2, 2)\nplt.title('$\\mathrm{Im}(\\mathrm{DFT}_N)$')\nplt.imshow(np.imag(dft_mat), origin='lower', cmap='seismic', aspect='equal')\nplt.xlabel('Time index $n$')\nplt.ylabel('Frequency index $k$')\nplt.colorbar()\nplt.tight_layout()\n\n\n\n\n\nN = 128\nn = np.arange(N)\nk = 10\nx = np.cos(2 * np.pi * (k * n / N) + 2 * (np.random.rand(N) - 0.5)) \nX = dft(x)\n\nplt.figure(figsize=(8, 3))\n\nplt.subplot(1, 2, 1)\nplt.title('$x$')\nplt.plot(x)\nplt.xlabel('Time (index $n$)')\n\nplt.subplot(1, 2, 2)\nplt.title('$|X|$')\nplt.plot(np.abs(X))\nplt.xlabel('Frequency (index $k$)')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#계산-복잡성",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#계산-복잡성",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "계산 복잡성",
    "text": "계산 복잡성\n\nFFT는 전체 작업 수를 \\(N^2\\)(일반적인 행렬-벡터 곱 \\(\\mathrm{DFT}_N \\cdot x\\)를 계산할 때 필요함)에서 \\(N\\log_2N\\) 정도로 줄인다. 예를 들어 \\(N=2^{10}=1024\\)를 사용하면 원래의 접근 방식의 \\(N^2=1048576\\) 작업 대신 FFT에 대략 \\(N\\log_2N=10240\\)가 필요하다.\nPython 코드의 작은 비트의 시간을 측정하는 간단한 방법을 제공하는 timeit 모듈을 사용하여 실행 시간을 비교한다.\n\n\nN = 512\nn = np.arange(N)\nx = np.sin(2 * np.pi * 5 * n / N )\n\nprint('Timing for DFT: ', end='')\n%timeit dft(x)\nprint('Timing for FFT: ', end='')\n%timeit fft(x)\n\nTiming for DFT: 284 ms ± 6.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nTiming for FFT: 9.31 ms ± 216 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nimport timeit\n\nNs = [2 ** n for n in range(5, 11)]\ntimes_dft = []\ntimes_fft = []\nexecuctions = 5\n\nfor N in Ns:\n    n = np.arange(N)\n    x = np.sin(2 * np.pi * 5 * n / N )\n    \n    time_dft = timeit.timeit(lambda: dft(x), number=execuctions) / execuctions\n    time_fft = timeit.timeit(lambda: fft(x), number=execuctions) / execuctions\n    times_dft.append(time_dft)\n    times_fft.append(time_fft)\n    \nplt.figure(figsize=(6, 3))\n    \nplt.plot(Ns, times_dft, '-xk', label='DFT')\nplt.plot(Ns, times_fft, '-xr', label='FFT')\nplt.xticks(Ns)\nplt.legend()\nplt.grid()\nplt.xlabel('$N$')\nplt.ylabel('Runtime (seconds)');\n\n\n\n\n\nFFT의 경우 계산이 훨씬 빠른 것을 볼 수 있다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#fft-예시",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#fft-예시",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "FFT 예시",
    "text": "FFT 예시\n\nlibrosa\n\n\nx, sr = librosa.load(\"../audio/c_strum.wav\")\nprint(x.shape)\nprint(sr)\nipd.Audio(x, rate=sr)\n\n(102400,)\n22050\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x)\nX_mag = np.absolute(X)\nf = np.linspace(0, sr, len(X_mag)) # frequency variable\n\n\nplt.figure(figsize=(6, 2))\nplt.plot(f, X_mag) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\nplt.show()\n\n\n\n\n\n# zoom in\nplt.figure(figsize=(6, 2))\nplt.plot(f[:5000], X_mag[:5000])\nplt.xlabel('Frequency (Hz)')\nplt.show()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#푸리에-계수의-극좌표-표현-polar-representation-of-fourier-coefficients",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#푸리에-계수의-극좌표-표현-polar-representation-of-fourier-coefficients",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "푸리에 계수의 극좌표 표현 (Polar Representation of Fourier Coefficients)",
    "text": "푸리에 계수의 극좌표 표현 (Polar Representation of Fourier Coefficients)\n\n\\(x=(x(0), x(1), ..., x(N-1))\\)을 샘플 \\(x(n)\\in\\mathbb{R}\\) for \\(n\\in[0:N-1]\\)을 가지는 시그널이라고 하자. 복소 푸리에 계수 \\(c_k:=X(k)\\in\\mathbb{C}\\) for \\(k\\in[0:N-1]\\)는 DFT에 계산되어 다음과 같다. \\[\nc_k :=X(k) = \\sum_{n=0}^{N-1} x(n) \\exp(-2 \\pi i k n / N).\n\\]\n\\(c_k = a_k + i b_k\\)를 실수부 \\(a_k\\in\\mathbb{R}\\)와 허수부 \\(b_k\\in\\mathbb{R}\\)로 구성된 복소수라고 할 때,\n절대값은 \\(|c_k| := \\sqrt{a_k^2 + b_k^2}\\)이고,\n각도(래디안 단위)는 \\(\\gamma_k := \\mathrm{angle}(c_k) := \\mathrm{atan2}(b_k, a_k) \\in [0,2\\pi)\\)이다.\n지수 함수를 쓰면, 다음의 극좌표 표현을 얻는다. \\[\n  c_k = |c_k| \\cdot \\mathrm{exp}(i \\gamma_k).\n\\]"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#최적화-optimality-속성",
    "href": "posts/3. Fourier Anaylsis of Signals/3.2.Discrete_Fourier_Transform.html#최적화-optimality-속성",
    "title": "3.2. 이산 푸리에 변환 & 고속 푸리에 변환 (DFT & FFT)",
    "section": "최적화 Optimality 속성",
    "text": "최적화 Optimality 속성\n\n\\(\\mathbf{cos}_{k,\\varphi}:[0:N-1]\\to\\mathbb{R}\\)를 주파수 파라미터 \\(k\\) 및 위상 \\(\\varphi\\in[0 ,1)\\)와 함께 샘플 정현파(sinusoid)라고 하면 다음과 같이 정의된다. \\[\n  \\mathbf{cos}_{k,\\varphi}(n) = \\sqrt{2}\\mathrm{cos}\\big( 2\\pi (kn/N - \\varphi) \\big)\n\\] for \\(n\\in[0,N-1]\\)\n직관적으로 말하자면, 길이 \\(N\\)의 이산 신호 \\(x\\)와 주파수 파라미터 \\(k\\)에 대한 푸리에 변환을 계산할 때 신호 \\(x\\)와 정현파 \\(\\mathbf{cos}_{k,\\varphi_k}\\)의 내적(일종의 상관 관계)을 계산한다.\n\\(\\varphi_k\\) 위상은 \\(\\varphi\\in[0,1)\\)로 \\(x\\)와 모든 가능한 정현파 \\(\\mathbf{cos}_{k,\\varphi}\\) 사이의 상관관계를 최대화한다는 속성을 가지고 있다.\n\n\\[\n      \\varphi_k = \\mathrm{argmax}_{\\varphi\\in[0,1)} \\langle x | \\mathbf{cos}_{k,\\varphi} \\rangle.\n\\]\n\n복소 푸리에 계수 \\(X(k)\\)는 기본적으로 복소수의 각도로 주어지는 이 최적 위상을 인코딩한다. 보다 정확하게 \\(\\gamma_k\\)를 \\(X(k)\\)의 각도라고 하면, 최적 위상 \\(\\varphi_k\\)가 다음과 같이 주어진다는 것을 알 수 있다.\n\n\\[\n       \\varphi_k := - \\frac{\\gamma_k}{2 \\pi}.\n\\]\n\n# Generate a chirp-like test signal (details not important)\nN = 256\nt_index = np.arange(N)\nx = 1.8 * np.cos(2 * np.pi * (3 * (t_index * (1 + t_index / (4 * N))) / N))\n\nk = 4\nexponential = np.exp(-2 * np.pi * 1j * k * t_index / N)\nX_k = np.sum(x * exponential)\nphase_k = - np.angle(X_k) / (2 * np.pi)\n\ndef compute_plot_correlation(x, N, k, phase):\n    sinusoid = np.cos(2 * np.pi * (k * t_index / N - phase)) \n    d_k = np.sum(x * sinusoid)\n    plt.figure(figsize=(6,1.5))\n    plt.plot(t_index, x, 'k')\n    plt.plot(sinusoid, 'r')\n    plt.title('Phase = %0.2f; correlation = %0.2f (optimal  = %0.2f)' % (phase, d_k, np.abs(X_k)))\n    plt.tight_layout()\n    plt.show()\n\nprint('Sinusoid with phase from Fourier coefficient resulting in an optimal correlation.')    \ncompute_plot_correlation(x, N, k, phase=phase_k)\n\nprint('Sinusoid with an arbitrary phase resulting in a medium correlation.')  \ncompute_plot_correlation(x, N, k, phase=0.4)\n\nprint('Sinusoid with a phase that yields a correlation close to zero.')  \ncompute_plot_correlation(x, N, k, phase=0.51)\n\nSinusoid with phase from Fourier coefficient resulting in an optimal correlation.\n\n\n\n\n\nSinusoid with an arbitrary phase resulting in a medium correlation.\n\n\n\n\n\nSinusoid with a phase that yields a correlation close to zero.\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C2/C2.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "",
    "text": "단기 푸리에 변환(STFT)를 소개하고, 이와 관련된 윈도우(window), 스펙트로그램(spectrogram), 패딩(padding) 전략 등을 살펴본다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#missing-time-localization",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#missing-time-localization",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "“Missing Time Localization”",
    "text": "“Missing Time Localization”\n\n기존 푸리에 변환은 전체 시간 영역에서 평균화되는 주파수 정보를 생성한다. 그러나 이러한 주파수가 언제 벌생하는지에 대한 정보는 변환에 숨겨져 있다. 이 현상은 다음 그림을 보면 알 수 있다.\n\n\nFs = 128\nduration = 10\nomega1 = 1\nomega2 = 5\nN = int(duration * Fs)\nt = np.arange(N) / Fs\nt1 = t[:N//2]\nt2 = t[N//2:]\n\nx1 = 1.0 * np.sin(2 * np.pi * omega1 * t1)\nx2 = 0.7 * np.sin(2 * np.pi * omega2 * t2)\nx = np.concatenate((x1, x2))\n\nplt.figure(figsize=(8, 2))\nplt.subplot(1, 2, 1)\nplt.plot(t, x)\nplt.xlim([min(t), max(t)])\nplt.xlabel('Time (seconds)')\n\nplt.subplot(1, 2, 2)\nX = np.abs(np.fft.fft(x)) / Fs\nfreq = np.fft.fftfreq(N, d=1/Fs)\nX = X[:N//2]\nfreq = freq[:N//2]\nplt.plot(freq, X)\nplt.xlim([0, 7])\nplt.ylim([0, 3])\nplt.xlabel('Frequency (Hz)')\nplt.tight_layout()\n\n\n\n\n\nImage(\"../img/3.fourier_analysis/f.2.6.PNG\", width=600)"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#기본-개념",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#기본-개념",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "기본 개념",
    "text": "기본 개념\n\n숨겨진 시간 정보를 복구하기 위해 Dennis Gabor는 1946년에 단시간 푸리에 변환(STFT)을 도입했다.\n전체 신호를 고려하는 대신 STFT는 신호의 작은 부분만 고려한다. 이를 위해 짧은 시간 동안의 non-zero 함수, 소위 윈도우 함수(window function)를 고정한다. 그런 다음 원래 신호에 윈도우 함수를 곱하여 윈도우(windowed) 신호를 생성한다.\n서로 다른 시간 인스턴스에서 주파수 정보를 얻으려면 시간에 따라 윈도우 함수를 이동하고, 각 결과 윈도우 신호에 대해 푸리에 변환을 계산해야 한다.\n\n\ndef windowed_ft(t, x, Fs, w_pos_sec, w_len):\n    N = len(x)\n    w_pos = int(Fs * w_pos_sec)\n    w_padded = np.zeros(N)\n    w_padded[w_pos:w_pos + w_len] = 1\n    x = x * w_padded\n    \n    X = np.abs(np.fft.fft(x)) / Fs\n    freq = np.fft.fftfreq(N, d=1/Fs)\n    X = X[:N//2]\n    freq = freq[:N//2]\n    \n    plt.figure(figsize=(8, 2))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(t, x, c='k')\n    plt.plot(t, w_padded, c='r')\n    plt.xlim([min(t), max(t)])\n    plt.ylim([-1.1, 1.1])\n    plt.xlabel('Time (seconds)')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(freq, X, c='k')\n    plt.xlim([0, 7])\n    plt.ylim([0, 3])\n    plt.xlabel('Frequency (Hz)')\n    plt.tight_layout()\n    \nprint('서로다른 window 변화에 대한 실험:')\n\nw_len = 4 * Fs\nwindowed_ft(t, x, Fs, w_pos_sec=1, w_len=w_len) # 윈도우 신호 t=1 중심\nwindowed_ft(t, x, Fs, w_pos_sec=3, w_len=w_len) # t=3\nwindowed_ft(t, x, Fs, w_pos_sec=5, w_len=w_len) # t=5\nplt.show()\n\n서로다른 window 변화에 대한 실험:"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#윈도우window",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#윈도우window",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "윈도우(window)",
    "text": "윈도우(window)\n\nSTFT는 원래 신호의 속성뿐만 아니라 윈도우 함수의 속성도 반영한다는 점에 유의해야 한다.\n우선 STFT는 섹션의 크기를 결정하는 윈도우의 길이에 따라 달라진다. 그리고 STFT는 윈도우 모양의 영향을 받는다. 예를 들어 직사각형 윈도우를 사용하면 일반적으로 단면 경계에서 불연속성이 발생하기 때문에 큰 단점이 있다. 이러한 급격한 변화는 전체 주파수 스펙트럼에 걸쳐 전파되는 간섭으로 인해 부작용을 발생시킨다: ripple artifacts.\n\n윈도우 유형\n\n이러한 경계 효과를 줄이기 위해 원하는 섹션 내에서 섹션의 경계를 향해 연속적으로 0으로 떨어지는 음이 아닌 윈도우를 사용한다. 그러한 예 중 하나는 훨씬 더 작은 ripple artifact를 초래하는 삼각형 윈도우(triangular winow) 이다.\n신호 처리에 자주 사용되는 윈도우는 Hann 윈도우(기상학자 Julius von Hann의 이름을 따서 명명됨, 1839~1921)이다. Hann 윈도우는 상승하는 코사인 윈도우로, 섹션 경계에서 스무스(smoothly)하게 0으로 떨어진다. 이는 윈도우 신호의 푸리에 변환에서 부작용을 완화한다. 그러나 단점은 Hann 윈도우가 약간의 주파수 번짐을 유발한다는 것이다.\n결과적으로 신호의 윈도우 영역의 푸리에 변환은 신호의 속성이 제안하는 것보다 더 스무스해 보일 수 있다. 즉, ripple artifact의 감소는 더 안좋은 spectral localization을 통해 달성된다 (trade-off 관계).\n\n\ndef windowed_ft2(t, x, Fs, w_pos_sec, w_len, w_type, upper_y=1.0):\n    \n    N = len(x)\n    w_pos = int(Fs * w_pos_sec)\n    w = np.zeros(N)\n    w[w_pos:w_pos + w_len] = scipy.signal.get_window(w_type, w_len)\n    x = x * w\n    \n    plt.figure(figsize=(8, 2))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(t, x, c='k')\n    plt.plot(t, w, c='r')\n    plt.xlim([min(t), max(t)])\n    plt.ylim([-1.1, 1.1])\n    plt.xlabel('Time (seconds)')\n\n    plt.subplot(1, 2, 2)\n    X = np.abs(np.fft.fft(x)) / N * 2\n    freq = np.fft.fftfreq(N, d=1/Fs)\n    X = X[:N//2]\n    freq = freq[:N//2]\n    plt.plot(freq, X, c='k')\n    plt.xlim([0, 50])\n    plt.ylim([0, upper_y])\n    plt.xlabel('Frequency (Hz)')\n    plt.tight_layout()\n    plt.show()\n\n\nduration = 2.0\nFs = 2000\nomega = 10\nN = int(duration * Fs)\nt = np.arange(N) / Fs\nx = 0.9 * np.sin(2 * np.pi * omega * t * t)\n\nplt.figure(figsize=(8, 2))\n\nplt.subplot(1, 2, 1)\nplt.plot(t, x, c='k')\nplt.xlim([t[0], t[-1]])\nplt.ylim([-1.1, 1.1])\nplt.xlabel('Time (seconds)')\n\nplt.subplot(1, 2, 2)\nX = np.abs(np.fft.fft(x)) / N * 2\nfreq = np.fft.fftfreq(N, d=1/Fs)\nX = X[:N//2]\nfreq = freq[:N//2]\nplt.plot(freq, X, c='k')\nplt.xlim([0, 50])\nplt.ylim(bottom=0)\nplt.xlabel('Frequency (Hz)');\nplt.tight_layout()\n\n\n\n\n\nw_len = 1024\nw_pos = 1280\nprint('Rectangular window:')\nwindowed_ft2(t, x, Fs, 1.0, w_len, 'boxcar', upper_y=0.15)\nprint('Triangular window:')\nwindowed_ft2(t, x, Fs, 1.0, w_len, 'triang', upper_y=0.15)\nprint('Hann window:')\nwindowed_ft2(t, x, Fs, 1.0, w_len, 'hann', upper_y=0.15)\n\nRectangular window:\n\n\n\n\n\nTriangular window:\n\n\n\n\n\nHann window:"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#discrete-stft의-정의",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#discrete-stft의-정의",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "Discrete-STFT의 정의",
    "text": "Discrete-STFT의 정의\n\n이제 STFT의 이산적 사례를 고려하고 실제 적용에 필요한 가장 중요한 수학 공식을 구체화해보자.\n\\(x:[0:L-1]:=\\{0,1,\\ldots,L-1\\}\\to{\\mathbb R}\\)가 길이 \\(L\\)의 실수 값 이산 시간(DT) 신호라고 가정하자. 이는 헤르츠(Hertz)로 주어진 고정 샘플링 비율 \\(F_\\mathrm{s}\\)에 대해 등거리(equidistant) 샘플링에 의해 얻어진다.\n\\(w:[0:N-1]\\to\\mathbb{R}\\)를 길이 \\(N\\in\\mathbb{N}\\)의 표본 윈도우 함수라고 하자. 예를 들어 직사각형 윈도우의 경우 \\(w(n)=1\\) for \\(n\\in[0:N-1]\\)이다. 길이 파라미터 \\(N\\)는 해당 섹션의 기간을 결정하며 이는 \\(N/F_\\mathrm{s}\\)초에 해당된다.\n홉 크기(hop size)라고 하는 추가 파라미터 \\(H\\in\\mathbb{N}\\)를 도입한다. 홉 크기 파라미터는 샘플에 지정되며 신호에서 윈도우가 이동하는 단계 크기(step size)를 결정한다. 이러한 파라미터와 관련하여 신호 \\(x\\)의 이산 STFT \\(\\mathcal{X}\\)는 다음과 같다. \\[ \\mathcal{X}(m,k):= \\sum_{n=0}^{N-1} x(n+mH)w(n)\\mathrm{exp}(-2\\pi ikn/N) \\] with \\(m\\in[0:M]\\) and \\(k\\in[0:K]\\)\n\\(M:=\\lfloor \\frac{L-N}{H} \\rfloor\\)는 윈도우 시간 범위가 신호의 시간 범위에 완전히 포함되도록 하는 최대 프레임 인덱스(maximal frame index)이다(나중에 패딩(padding) 전략을 사용하는 일부 변형을 볼 것).\n또한 \\(K=N/2\\)(\\(N\\)이 짝수라고 가정)는 Nyquist 주파수에 해당하는 주파수 인덱스이다.\n복소수 $ (m,k)$는 \\(m^{\\mathrm{th}}\\) 시간 프레임에 대한 \\(k^{\\mathrm{th}}\\) 푸리에 계수를 나타낸다.\n각 고정 시간 프레임 \\(m\\)에 대해, 0에 대한 계수 \\(\\mathcal{X}(m,k)\\) for \\(k\\in[0:K]\\)에 의해 주어진 크기 \\(K+1\\)의 스펙트럼 벡터(spectral vector) 를 얻는다.\n이러한 각 스펙트럼 벡터의 계산은 \\(N\\) 크기의 DFT에 해당하며 FFT를 사용하여 효율적으로 수행될 수 있다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#시간-및-주파수-인덱스에-대한-해석",
    "href": "posts/3. Fourier Anaylsis of Signals/3.3.Short-term_Fourier_Transform_1.html#시간-및-주파수-인덱스에-대한-해석",
    "title": "3.3. 단기 푸리에 변환 (STFT) (1)",
    "section": "시간 및 주파수 인덱스에 대한 해석",
    "text": "시간 및 주파수 인덱스에 대한 해석\n\n시간 차원의 경우, 각 푸리에 계수 \\(\\mathcal{X}(m,k)\\)는 초 단위로 주어진 물리적 시간 위치와 연관된다. \\[\\begin{equation}\n       T_\\mathrm{coef}(m) := \\frac{m\\cdot H}{F_\\mathrm{s}}\n\\end{equation}\\]\n예를 들어 가능한 가장 작은 홉 크기 \\(H=1\\)의 경우 \\(T_\\mathrm{coef}(m)=m/F_\\mathrm{s}=m\\cdot T~\\sec\\)를 얻는다. 이 경우 DT 신호 \\(x\\)의 각 샘플에 대한 스펙트럼 벡터를 얻으므로 데이터 양이 크게 증가한다.\n또한, 하나의 샘플에 의해서만 이동된 섹션을 고려하면 일반적으로 매우 유사한 스펙트럼 벡터가 생성된다. 이러한 유형의 중복성을 줄이기 위해 일반적으로 홉 크기를 윈도우 길이 \\(N\\)에 연관시킨다. 예를 들어, \\(H=N/2\\)를 선택하는 경우가 많으며, 이는 생성된 모든 스펙트럼 계수를 포함하는 데이터 크기와 합리적인 시간의 분해 사이의 좋은 trade-off이다.\n주파수 차원의 경우 \\(\\mathcal{X}(m,k)\\)의 인덱스 \\(k\\)는 물리적 주파수에 해당된다.\n\n\\[\\begin{equation}\n         F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\n\\end{equation}\\]\n\nT_coef = np.arange(X.shape[1]) * H / Fs\nF_coef = np.arange(X.shape[0]) * Fs / N\n\nplt.figure(figsize=(3, 4))\n\nplt.subplot(2, 1, 1)\nplt.plot(t, x, c='k')\nplt.xlim([min(t), max(t)])\nplt.xlabel('Time (seconds)')\n\nplt.subplot(2, 1, 2)\nleft = min(T_coef)\nright = max(T_coef) + N / Fs\nlower = min(F_coef)\nupper = max(F_coef)\nplt.imshow(Y, origin='lower', aspect='auto', cmap='gray_r', \n           extent=[left, right, lower, upper])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "",
    "text": "단기 푸리에 변환(SFTF)의 변형을 다룬다. 주파수 그리드 밀도(density), 보간법(interpolation), 그리고 역(inverse) 푸리에 변환 등을 소개한다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#dft-주파수-그리드",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#dft-주파수-그리드",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "DFT 주파수 그리드",
    "text": "DFT 주파수 그리드\n\n\\(x\\in \\mathbb{R}^N\\) 를 길이 \\(N\\in\\mathbb{N}\\)의 샘플 \\(x(0), x(1), \\ldots, x(N-1)\\)의 이산 신호라고 하자.\n샘플링 레이트 \\(F_\\mathrm{s}\\)가 주어졌을 때, \\(x\\)는 연속 시간 신호 \\(f:\\mathbb{R}\\to\\mathbb{R}\\)를 샘플링하여 얻는다고 가정한다.\n그러면 이산 푸리에 변환(DFT) \\(X := \\mathrm{DFT}_N \\cdot x\\)은 특정 주파수에 대한 연속 푸리에 변환 \\(\\hat{f}\\)의 근사치로 해석될 수 있다. \\[\nX(k) := \\sum_{n=0}^{N-1} x(n) \\exp(-2 \\pi i k n / N)\n\\approx {F_\\mathrm{s}} \\cdot \\hat{f} \\left(k \\cdot \\frac{F_\\mathrm{s}}{N}\\right)\n\\] for \\(k\\in[0:N-1]\\).\n따라서 \\(X(k)\\)의 인덱스 \\(k\\)는 다음의 물리적 주파수(헤르츠단위)에 해당된다. \\[\\begin{equation}\n       F_\\mathrm{coef}^N(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\n\\end{equation}\\]\n즉, 이산 푸리에 변환은 \\(\\mathrm{DFT}_N\\)의 크기 \\(N\\)에 따라 달라지는, 해상도 \\(F_\\mathrm{s}/N\\)의 선형 주파수 그리드(frequency grid)를 얻는다.\n주파수 그리드의 밀도를 높이기 위한 한 가지 아이디어는 인위적으로 신호에 0을 추가하여 DFT의 크기를 늘리는 것이다.\n이를 위해 \\(L\\in\\mathbb{N}\\) with \\(L\\geq N\\)라고 하자. 그런 다음 \\(\\tilde{x}\\in \\mathbb{R}^L\\) 신호를 얻기 위해 \\(x\\) 신호 오른쪽에 제로 패딩(zero padding)을 적용한다.\n\n\\[\\tilde{x}(n) :=\\left\\{\\begin{array}{ll}\n    x(n) , \\,\\,\\mbox{for}\\,\\, n \\in[0:N-1]\\\\\n    0,     \\,\\,\\mbox{for}\\,\\, n \\in[N:L-1]\n\\end{array}\\right.\\]\n\n\\(\\mathrm{DFT}_L\\)을 적용하면 다음을 얻는다. \\[\n\\tilde{X}(k) = \\mathrm{DFT}_L \\cdot \\tilde{x}\n= \\sum_{n=0}^{L-1} \\tilde{x}(n) \\exp(-2 \\pi i k n / L)\n= \\sum_{n=0}^{N-1} x(n) \\exp(-2 \\pi i k n / L)\n\\approx {F_\\mathrm{s}} \\cdot \\hat{f} \\left(k \\cdot \\frac{F_\\mathrm{s}}{L}\\right)\n\\] for \\(k\\in[0:L-1]\\).\n이제 계수 \\(\\tilde{X}(k)\\)는 다음의 물리적 주파수에 대응된다. \\[\\begin{equation}\n       F_\\mathrm{coef}^L(k) := \\frac{k\\cdot F_\\mathrm{s}}{L},\n\\end{equation}\\]\n이는 선형 주파수 해상도 \\(F_\\mathrm{s}/L\\)를 보인다.\n예를 들어, \\(L=2N\\)인 경우 주파수 그리드 해상도는 2배 증가한다. 즉, DFT가 길수록 간격이 더 가까운 주파수 빈(bin)이 더 많아진다.\n그러나 이 트릭은 DFT의 근사 품질을 개선하지 않는다는 점에 유의해야 한다(리만(Riemann) 근사의 합계 수는 여전히 \\(N\\)임에 유의하자).\n단, \\(L\\geq N\\) 및 제로 패딩을 사용할 때 주파수 축의 선형 샘플링이 정제(refine)된다.\n다음 예는 \\(\\mathrm{DFT}_N \\cdot x\\)를 \\(\\mathrm{DFT}_L \\cdot \\tilde{x}\\)와 비교한다.\n\n\nFs = 32\nduration = 2\nfreq1 = 5\nfreq2 = 15\nN = int(duration * Fs)\nt = np.arange(N) / Fs\nt1 = t[:N//2]\nt2 = t[N//2:]\n\nx1 = 1.0 * np.sin(2 * np.pi * freq1 * t1)\nx2 = 0.7 * np.sin(2 * np.pi * freq2 * t2)\nx = np.concatenate((x1, x2))\n\nplt.figure(figsize=(8, 2))\n\nax1 = plt.subplot(1, 2, 1)\nplt.plot(x)\nplt.title('Orginal signal ($N$=%d)' % N)\nplt.xlabel('Time (samples)')\nplt.xlim([0, N - 1])\nplt.subplot(1, 2, 2)\nY = np.abs(np.fft.fft(x)) / Fs\nplt.plot(Y)\nplt.title('Magnitude DFT of original signal ($N$=%d)' % N)\nplt.xlabel('Frequency (bins)')\nplt.xlim([0, N - 1])\nplt.tight_layout()\n\nL = 2 * N\npad_len = L - N\nt_tilde = np.concatenate((t, np.arange(len(x), len(x) + pad_len) / Fs))\nx_tilde = np.concatenate((x, np.zeros(pad_len)))\n                         \nplt.figure(figsize=(8, 2))\nax1 = plt.subplot(1, 2, 1)\nplt.plot(x_tilde)\nplt.title('Padded signal ($L$=%d)' % L)\nplt.xlabel('Time (samples)')\nplt.xlim([0, L - 1])\nplt.subplot(1, 2, 2)\nY_tilde = np.abs(np.fft.fft(x_tilde)) / Fs\nplt.plot(Y_tilde)\nplt.title('Magnitude DFT of padded signal ($L$=%d)' % L)\nplt.xlabel('Frequency (bins)')\nplt.xlim([0, L - 1])\n\nplt.tight_layout()                       \n\n\n\n\n\n\n\n\n다음 코드 예제는 증가된 주파수 그리드 해상도로 DFT를 계산하는 함수를 구현한다.\n여기에서 모든 파라미터는 물리적 방식으로 해석된다(초 및 헤르츠 기준).\n\n\ndef compute_plot_DFT_extended(t, x, Fs, L):\n    N = len(x)\n    pad_len = L - N\n    t_tilde = np.concatenate((t, np.arange(len(x), len(x) + pad_len) / Fs))\n    x_tilde = np.concatenate((x, np.zeros(pad_len)))\n    Y = np.abs(np.fft.fft(x_tilde)) / Fs    \n    Y = Y[:L//2]\n    freq = np.arange(L//2)*Fs/L\n    # freq = np.fft.fftfreq(L, d=1/Fs)\n    # freq = freq[:L//2]\n    plt.figure(figsize=(10, 2))\n    \n    ax1 = plt.subplot(1, 3, 1)\n    plt.plot(t_tilde, x_tilde)\n    plt.title('Signal ($N$=%d)' % N)\n    plt.xlabel('Time (seconds)')\n    plt.xlim([t[0], t[-1]])\n    \n    ax2 = plt.subplot(1, 3, 2)\n    plt.plot(t_tilde, x_tilde)\n    plt.title('Padded signal (of size $L$=%d)' % L)\n    plt.xlabel('Time (seconds)')\n    plt.xlim([t_tilde[0], t_tilde[-1]])    \n    \n    ax3 = plt.subplot(1, 3, 3)\n    plt.plot(freq, Y)\n    plt.title('Magnitude DFT of padded signal ($L$=%d)' % L)\n    plt.xlabel('Frequency (Hz)')\n    plt.xlim([freq[0], freq[-1]])\n    plt.tight_layout()           \n\n    return ax1, ax2, ax3\n\n\nN = len(x)\n\nL = N\nax1, ax2, ax3 = compute_plot_DFT_extended(t, x, Fs, L)\n\nL = 2 * N\nax1, ax2, ax3 = compute_plot_DFT_extended(t, x, Fs, L)\n\nL = 4 * N\nax1, ax2, ax3 = compute_plot_DFT_extended(t, x, Fs, L)"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-그리드-해상도가-증가된-stft",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-그리드-해상도가-증가된-stft",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "주파수 그리드 해상도가 증가된 STFT",
    "text": "주파수 그리드 해상도가 증가된 STFT\n\n이제 동일한 제로-패딩 전략을 사용하여 STFT의 주파수 그리드 해상도를 높이는 방법을 보자. librosa 함수 librosa.stft는 두 개의 파라미터 n_fft(\\(L\\)에 해당) 및 win_length(\\(N\\)에 해당)를 통해 이 아이디어를 구현한다. 파라미터를 물리적 도메인으로 변환할 때 주의해야 한다.\n바이올린이 연주하는 음 C4의 예를 보자\n\n\nx, Fs = librosa.load(\"../audio/violin_c4_legato.wav\")\nipd.display(ipd.Audio(x, rate=Fs))\n\nt_wav = np.arange(0, x.shape[0]) * 1 / Fs\nplt.figure(figsize=(5, 1.5))\nplt.plot(t_wav, x, c='gray')\nplt.xlim([t_wav[0], t_wav[-1]])\nplt.xlabel('Time (seconds)')\nplt.tight_layout()\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n이제 제로 패딩으로 STFT를 계산한다. 그림에서 축은 시간 프레임 및 주파수 빈으로 표시된다.\n\n\ndef compute_stft(x, Fs, N, H, L=N, pad_mode='constant', center=True):    \n    X = librosa.stft(x, n_fft=L, hop_length=H, win_length=N, \n                     window='hann', pad_mode=pad_mode, center=center)\n    Y = np.log(1 + 100 * np.abs(X) ** 2)\n    F_coef = librosa.fft_frequencies(sr=Fs, n_fft=L)\n    T_coef = librosa.frames_to_time(np.arange(X.shape[1]), sr=Fs, hop_length=H) \n    return Y, F_coef, T_coef\n\ndef plot_compute_spectrogram(x, Fs, N, H, L, color='gray_r'):\n    Y, F_coef, T_coef = compute_stft(x, Fs, N, H, L)\n    plt.imshow(Y, cmap=color, aspect='auto', origin='lower')\n    plt.xlabel('Time (frames)')\n    plt.ylabel('Frequency (bins)')\n    plt.title('L=%d' % L)\n    plt.colorbar()\n\n\nN = 256\nH = 64\ncolor = 'gray_r' \nplt.figure(figsize=(8, 3))\n\nL = N\nplt.subplot(1,3,1)\nplot_compute_spectrogram(x, Fs, N, H, L)\n\nL = 2 * N\nplt.subplot(1,3,2)\nplot_compute_spectrogram(x, Fs, N, H, L)\n\nL = 4 * N\nplt.subplot(1,3,3)\nplot_compute_spectrogram(x, Fs, N, H, L)\n\nplt.tight_layout()\n\n\n\n\n\n다음으로 동일한 계산을 반복한다. 여기서 축은 이제 초와 헤르츠로 지정된 물리적 단위를 표시하도록 변환된다. 또한 시간-주파수 평면을 확대하여 밀도가 높은 주파수 그리드 밀도의 효과를 강조한다.\n\n\ndef plot_compute_spectrogram_physical(x, Fs, N, H, L, xlim, ylim, color='gray_r'):\n    Y, F_coef, T_coef = compute_stft(x, Fs, N, H, L)\n    extent=[T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\n    plt.imshow(Y, cmap=color, aspect='auto', origin='lower', extent=extent)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Frequency (Hz)')\n    plt.title('L=%d' % L)\n    plt.ylim(ylim)\n    plt.xlim(xlim)\n    plt.colorbar()\n\n\nxlim_sec = [1, 2]\nylim_hz = [2000, 3000]\n\nplt.figure(figsize=(8, 3))\n\nL = N\nplt.subplot(1,3,1)\nplot_compute_spectrogram_physical(x, Fs, N, H, L, xlim=xlim_sec, ylim=ylim_hz)\n\nL = 2 * N\nplt.subplot(1,3,2)\nplot_compute_spectrogram_physical(x, Fs, N, H, L, xlim=xlim_sec, ylim=ylim_hz)\n\nL = 4 * N\nplt.subplot(1,3,3)\nplot_compute_spectrogram_physical(x, Fs, N, H, L, xlim=xlim_sec, ylim=ylim_hz)\n\nplt.tight_layout()\n\n\n\n\n\nlibrosa 함수 librosa.stft는 두 개의 파라미터 n_fft(패딩된 섹션의 크기 \\(L\\)에 해당) 및 win_length(\\(N\\)에 해당, 윈도우 부분)가 있다. 이 패딩 변형을 \\(L\\)(\\(N\\) 대신)와 함께 사용하면, 함수 \\(F_\\mathrm{coef}\\)의 계산을 다음과 같이 조정해야 한다. \\[\\begin{equation}\n       F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{L}\n\\end{equation}\\] for \\(k\\in [0:K]\\) with \\(K=L/2\\).\n반올림 문제를 방지하려면 짝수 \\(L\\)(아마도 2의 거듭제곱)를 선택하는 것이 좋다.\n\n\nN = 256\nL = 512\nH = 64\ncolor = 'gray_r' \n\nX = librosa.stft(x, n_fft=L, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=True) # center에 대해서는 밑에서 다루기로 한다\nY = np.log(1 + 100 * np.abs(X) ** 2)\n\nT_coef = np.arange(0, X.shape[1]) * H / Fs\n\nK = L // 2\nF_coef = np.arange(K + 1) * Fs / L # 공식에 따라\nF_coef_librosa = librosa.fft_frequencies(sr=Fs, n_fft=L) # librosa 내장\nprint('F_coef 결과가 같은가:', np.allclose(F_coef, F_coef_librosa))\nprint('Y.shape = (%d,%d)'%(Y.shape[0],Y.shape[1]))\n\nplt.figure(figsize=(6, 3))\nextent = [T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\nplt.imshow(Y, cmap=color, aspect='auto', origin='lower', extent=extent)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.colorbar()\nplt.tight_layout()\n\nF_coef 결과가 같은가: True\nY.shape = (257,1034)"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#보간법-interpolation",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#보간법-interpolation",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "보간법 (Interpolation)",
    "text": "보간법 (Interpolation)\n\n데이터 포인트 시퀀스가 주어지면 보간법 interpolation의 목표는 의미 있는 방식으로 시퀀스를 정제(refine)하는 중간(intermediate) 데이터 포인트를 계산하는 것이다.\n보다 구체적으로, 파라미터 \\(t\\in\\mathbb{R}\\)를 \\(f(t)\\in\\mathbb{R}\\)로 매핑하는 \\(f\\colon\\mathbb{R}\\to\\mathbb{R}\\) 함수를 고려해보자.\n\\(n\\in\\mathbb{Z}\\)에 대한 파라미터 \\(t_n\\in\\mathbb{R}\\)의 이산 집합에 대해서만 \\(f(t_n)\\) 값이 있다고 가정하자. \\(t_{n} > t_ {n-1}\\).\n그런 다음 보간법의 목표는 \\(f^\\ast(t)\\) 값을 추정하는 것이다. \\(f(t_n)\\)는 다음과 같이 주어진다.\n\n\\[f^\\ast(t) \\approx f(t)\\] for any \\(t\\in\\mathbb{R}\\)\n\n실제로는 \\(f\\) 함수는 알 수 없지만, 종종 \\(f\\)의 연속성, 평활성, 미분 가능성 등과 같은 특정 속성을 가정한다.\n가장 간단한 보간법 방법은 조각 상수 보간(piecewise constant interpolation)(또는 최근접 이웃(nearest neighbor) 보간법)이다. 파라미터 \\(t\\in\\mathbb{R}\\)가 주어지면, 가장 가까운 파라미터 \\(t_n\\)을 취하여 다음을 정의한다.\n\n\\(f^\\ast(t)=f(t_n).\\)\n\n\n\n# Simulates the original function\nt = np.arange(-0.5, 10.5, 0.01)\nf = np.sin(2 * t)\n\n# Known funcion values\nt_n = np.arange(0, 11)\nf_n = np.sin(2 * t_n)\n\n# Interpolation\nf_interpol_nearest = interp1d(t_n, f_n, kind='nearest', fill_value='extrapolate')(t)\n\nplt.figure(figsize=(8, 2)) \nplt.plot(t, f, color=(0.8, 0.8, 0.8))\nplt.plot(t, f_interpol_nearest, 'r-')\nplt.plot(t_n, f_n, 'ko')\nplt.ylim([-1.5,1.5])\nplt.title('Piecewise constant interpolation')\nplt.tight_layout()\n\n\n\n\n\n최근접 이웃 보간법은 일반적으로 이산 함수를 생성한다. 연속적인 보간법 함수를 얻기 위한 간단한 대안으로는 선형 보간법(linear interpolation)이 있다.\n\\(t_{n-1}\\)와 \\(t_n\\) 사이에 있는 파라미터 \\(t\\)가 주어지면, 다음과 같이 정의할 수 있다. \\[\nf^\\ast(t)=f(t_{n-1}) + (f(t_{n})-f(t_{n-1}))\\cdot\\frac{t-t_{n-1}}{t_{n} - t_{n-1}}.\n\\]\n\n\nf_interpol_linear = interp1d(t_n, f_n, kind='linear', fill_value='extrapolate')(t)\n\nplt.figure(figsize=(8, 2)) \nplt.plot(t, f, color=(0.8, 0.8, 0.8))\nplt.plot(t, f_interpol_linear, 'r-')\nplt.plot(t_n, f_n, 'ko')\nplt.ylim([-1.5,1.5])\nplt.title('Linear interpolation')\nplt.tight_layout()\n\n\n\n\n\nPython 클래스 scipy.inperploate.interp1d는 Nearest-neighbor, 선형 및 다양한 차수의 spline 보간을 포함하여 여러 종류의 보간 방법을 제공한다. 또 다른 예로, 3차 보간(3차 스플라인)에 대한 결과를 보자.\n\n\nf_interpol_cubic = interp1d(t_n, f_n, kind='cubic', fill_value='extrapolate')(t)\n\nplt.figure(figsize=(8, 2)) \nplt.plot(t, f, color=(0.8, 0.8, 0.8))\nplt.plot(t, f_interpol_cubic, 'r-')\nplt.plot(t_n, f_n, 'ko')\nplt.ylim([-1.5,1.5])\nplt.title('Cubic interpolation')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-보간법",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-보간법",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "주파수 보간법",
    "text": "주파수 보간법\n\n선형 주파수 그리드의 밀도를 증가시키기 위해 제로 패딩을 기반으로 더 큰 DFT를 사용할 수 있었다. 이제 주파수 영역에서 보간법을 적용하여 대안을 소개한다.\n\\(x\\in \\mathbb{R}^N\\)를 길이 \\(N\\in\\mathbb{N}\\), 샘플링 레이트 \\(F_\\mathrm{s}\\), DFT \\(X = \\mathrm{DFT} _N \\cdot x\\), 및 DFT 크기 \\(Y=|X|\\)의 이산 신호라고 하자. 그러면 \\(Y(k)\\)의 인덱스 \\(k\\)는 다음의 헤르츠로 주어진 물리적 주파수에 해당한다.\n\n\\[\\begin{equation}\n         F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\n\\end{equation}\\]\n\n즉, 주파수 그리드 결과의 해상도는 \\(F_\\mathrm{s}/N\\)이다. 팩터 \\(\\rho\\in\\mathbb{N}\\)를 도입하여, 해상도 \\(F_\\mathrm{s}/(\\rho\\cdot N)\\)를 고려하여 주파수 그리드를 정제(refine)한다. 이 주파수 그리드를 기반으로 보간 기술을 사용하여 크기 주파수 계수(magnitude frequency coefficient)를 계산한다.\n\n\nFs = 32\nduration = 2\nomega1 = 5\nomega2 = 15\nN = int(duration * Fs)\nt = np.arange(N) / Fs\nt1 = t[:N//2]\nt2 = t[N//2:]\n\nx1 = 1.0 * np.sin(2 * np.pi * omega1 * t1)\nx2 = 0.7 * np.sin(2 * np.pi * omega2 * t2)\nx = np.concatenate((x1, x2))\n\nplt.figure(figsize=(6, 2))\nplt.plot(t, x)\nplt.title('Orginal signal ($N$=%d)' % N)\nplt.xlabel('Time (seconds)')\nplt.xlim([t[0], t[-1]])   \nplt.tight_layout()\n\nY = np.abs(np.fft.fft(x)) / Fs\nY = Y[:N//2+1]\nF_coef = np.arange(N//2+1)*Fs/N\nplt.figure(figsize=(6, 2))\nplt.plot(F_coef,Y)\nplt.title('Magnitude DFT ($N$=%d)' % N)\nplt.xlabel('Frequency (Hz)')\nplt.xlim([F_coef[0], F_coef[-1]])     \nplt.tight_layout()\n\n\n\n\n\n\n\n\ndef interpolate_plot_DFT(N, Fs, F_coef, rho, int_method):\n    F_coef_interpol = np.arange(F_coef[0], F_coef[-1], Fs/(rho*N))\n    Y_interpol = interp1d(F_coef, Y, kind=int_method)(F_coef_interpol)\n    plt.figure(figsize=(6, 2))\n    plt.plot(F_coef_interpol, Y_interpol)\n    plt.title(r'Magnitude DFT (interpolation: %s, $\\rho$=%d)'%(int_method,rho))\n    plt.xlabel('Frequency (Hz)')\n    plt.xlim([F_coef[0], F_coef[-1]])\n    plt.tight_layout()\n\n\nrho = 4\ninterpolate_plot_DFT(N=N, Fs=Fs, F_coef=F_coef, rho=rho, int_method='nearest')\ninterpolate_plot_DFT(N=N, Fs=Fs, F_coef=F_coef, rho=rho, int_method='linear')\ninterpolate_plot_DFT(N=N, Fs=Fs, F_coef=F_coef, rho=rho, int_method='cubic')"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#stft를-위한-보간법",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#stft를-위한-보간법",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "STFT를 위한 보간법",
    "text": "STFT를 위한 보간법\n\nSTFT의 주파수 그리드를 정제하기 위해서 주파수 방향을 따라 보간법을 적용할 수 있다.\n이전과 같은 바이올린(C4)의 예로 보자.\n\n\nx, Fs = librosa.load(\"../audio/violin_c4_legato.wav\")\nipd.display(ipd.Audio(x, rate=Fs))\n\nt_wav = np.arange(0, x.shape[0]) * 1 / Fs\nplt.figure(figsize=(6, 1))\nplt.plot(t_wav, x, c='gray')\nplt.xlim([t_wav[0], t_wav[-1]])\nplt.xlabel('Time (seconds)');\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\ndef stft_convention_fmp(x, Fs, N, H, pad_mode='constant', center=True, mag=False, gamma=0):\n    \"\"\"Compute the discrete short-time Fourier transform (STFT)\n\n    Args:\n        x (np.ndarray): Signal to be transformed\n        Fs (scalar): Sampling rate\n        N (int): Window size\n        H (int): Hopsize\n        pad_mode (str): Padding strategy is used in librosa (Default value = 'constant')\n        center (bool): Centric view as used in librosa (Default value = True)\n        mag (bool): Computes magnitude STFT if mag==True (Default value = False)\n        gamma (float): Constant for logarithmic compression (only applied when mag==True) (Default value = 0)\n\n    Returns:\n        X (np.ndarray): Discrete (magnitude) short-time Fourier transform\n    \"\"\"\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N,\n                     window='hann', pad_mode=pad_mode, center=center)\n    if mag:\n        X = np.abs(X)**2\n        if gamma > 0:\n            X = np.log(1 + gamma * X)\n    F_coef = librosa.fft_frequencies(sr=Fs, n_fft=N)\n    T_coef = librosa.frames_to_time(np.arange(X.shape[1]), sr=Fs, hop_length=H)\n    # T_coef = np.arange(X.shape[1]) * H/Fs\n    # F_coef = np.arange(N//2+1) * Fs/N\n    return X, T_coef, F_coef\n\n\ndef compute_f_coef_linear(N, Fs, rho=1):\n    \"\"\"Refines the frequency vector by factor of rho\n\n    Args:\n        N (int): Window size\n        Fs (scalar): Sampling rate\n        rho (int): Factor for refinement (Default value = 1)\n\n    Returns:\n        F_coef_new (np.ndarray): Refined frequency vector\n    \"\"\"\n    L = rho * N\n    F_coef_new = np.arange(0, L//2+1) * Fs / L\n    return F_coef_new\n\n\ndef interpolate_freq_stft(Y, F_coef, F_coef_new):\n    \"\"\"Interpolation of STFT along frequency axis\n\n    Args:\n        Y (np.ndarray): Magnitude STFT\n        F_coef (np.ndarray): Vector of frequency values\n        F_coef_new (np.ndarray): Vector of new frequency values\n\n    Returns:\n        Y_interpol (np.ndarray): Interploated magnitude STFT\n    \"\"\"\n    compute_Y_interpol = interp1d(F_coef, Y, kind='cubic', axis=0)\n    Y_interpol = compute_Y_interpol(F_coef_new)\n    return Y_interpol\n\n\ndef plot_compute_spectrogram_physical(x, Fs, N, H, xlim, ylim, rho=1, color='gray_r'):\n    Y, T_coef, F_coef = stft_convention_fmp(x, Fs, N, H, mag=True, gamma=100)\n    F_coef_new = compute_f_coef_linear(N, Fs, rho=rho)\n    Y_interpol = interpolate_freq_stft(Y, F_coef, F_coef_new)    \n    extent=[T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\n    plt.imshow(Y_interpol, cmap=color, aspect='auto', origin='lower', extent=extent)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Frequency (Hz)')\n    plt.title(r'$\\rho$=%d' % rho)\n    plt.ylim(ylim)\n    plt.xlim(xlim)\n    plt.colorbar()\n\n\nxlim_sec = [1, 2]\nylim_hz = [2000, 3000]\n\nN = 256\nH = 64\nplt.figure(figsize=(10, 4))\n \nplt.subplot(1, 3, 1)\nplot_compute_spectrogram_physical(x, Fs, N, H, xlim=xlim_sec, ylim=ylim_hz, rho=1)\n\nplt.subplot(1, 3, 2)\nplot_compute_spectrogram_physical(x, Fs, N, H, xlim=xlim_sec, ylim=ylim_hz, rho=2)\n\nplt.subplot(1, 3, 3)\nplot_compute_spectrogram_physical(x, Fs, N, H, xlim=xlim_sec, ylim=ylim_hz, rho=4)\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#로그-주파수-stft",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#로그-주파수-stft",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "로그-주파수 STFT",
    "text": "로그-주파수 STFT\n\n이 보간법은 주파수 그리드의 비선형 변형에 사용될 수 있다. 예를 들어 선형 간격의 주파수 축(헤르츠로 측정)을 로그 간격의 주파수 축(피치 또는 센트로 측정)으로 변환할 수 있다.\n로그 간격의 주파수 그리드를 정의하는 것이 주요 단계이다. 이때 음정에 사용되는 대수 측정 단위인 센트라는 개념을 사용한다. 참조 주파수 \\(\\omega_0\\)가 주어지면 임의의 주파수 \\(\\omega\\)와 \\(\\omega_0\\) 사이의 거리는 다음과 같이 지정된다. \\[\n    \\log_2\\left(\\frac{\\omega}{\\omega_0}\\right)\\cdot 1200\n\\]\n다음 함수에서는 최소 주파수 값(매개변수 F_min이 기준 주파수 \\(\\omega_0\\)로 사용되므로 \\(0\\) cents에 해당함)에서 시작하여 로그 간격의 주파수 축을 계산한다.\n또한 이 함수에는 로그 해상도를 센트 단위로 지정하는 파라미터 R이 있다. 즉, 로그 주파수 축에서 두 개의 연속된 주파수 빈은 R 센트 떨어져 있다.\n최대 주파수는 다른 매개변수 F_max에 의해 지정된다.\n다음 예에서 주파수 F_min = 100(\\(0\\) 센트에 해당) 및 F_max = 3200(\\(6000\\) 센트에 해당)을 사용한다. 또한 해상도는 R=20(옥타브당 \\(60\\) 주파수 빈에 해당)으로 설정한다.\n\n\ndef compute_f_coef_log(R, F_min, F_max):\n    \"\"\"Adapts the frequency vector in a logarithmic fashion\n\n    Args:\n        R (scalar): Resolution (cents)\n        F_min (float): Minimum frequency\n        F_max (float): Maximum frequency (not included)\n\n    Returns:\n        F_coef_log (np.ndarray): Refined frequency vector with values given in Hz)\n        F_coef_cents (np.ndarray): Refined frequency vector with values given in cents.\n            Note: F_min serves as reference (0 cents)\n    \"\"\"\n    n_bins = np.ceil(1200 * np.log2(F_max / F_min) / R).astype(int)\n    F_coef_log = 2 ** (np.arange(0, n_bins) * R / 1200) * F_min\n    F_coef_cents = 1200 * np.log2(F_coef_log / F_min)\n    return F_coef_log, F_coef_cents\n\n\nN = 1024\nH = 256\nY, T_coef, F_coef = stft_convention_fmp(x, Fs, N, H, mag=True, gamma=100)\n\nF_min = 100\nF_max = 3200\nR = 20\nF_coef_log, F_coef_cents = compute_f_coef_log(R, F_min, F_max)\n\nprint('#bins=%3d, F_coef[0]      =%6.2f, F_coef[1]      =%6.2f, F_coef[-1]      =%6.2f'%(len(F_coef),F_coef[0], F_coef[1], F_coef[-1]))\nprint('#bins=%3d, F_coef_log[0]  =%6.2f, F_coef_log[1]  =%6.2f, F_coef_log[-1]  =%6.2f'%(len(F_coef_log),F_coef_log[0], F_coef_log[1], F_coef_log[-1]))\nprint('#bins=%3d, F_coef_cents[0]=%6.2f, F_coef_cents[1]=%6.2f, F_coef_cents[-1]=%6.2f'%(len(F_coef_cents),F_coef_cents[0], F_coef_cents[1], F_coef_cents[-1]))\n\n#bins=513, F_coef[0]      =  0.00, F_coef[1]      = 21.53, F_coef[-1]      =11025.00\n#bins=300, F_coef_log[0]  =100.00, F_coef_log[1]  =101.16, F_coef_log[-1]  =3163.24\n#bins=300, F_coef_cents[0]=  0.00, F_coef_cents[1]= 20.00, F_coef_cents[-1]=5980.00\n\n\n\nY_interpol = interpolate_freq_stft(Y, F_coef, F_coef_log)\ncolor = 'gray_r' \n\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 3, 1)\nextent=[T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\nplt.imshow(Y, cmap=color, aspect='auto', origin='lower', extent=extent)\ny_ticks_freq = np.array([100, 400, 800, 1200, 1600, 2000, 2400, 2800, 3200])\nplt.yticks(y_ticks_freq)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.title('Linear frequency axis')\nplt.ylim([F_min, F_max])\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nextent=[T_coef[0], T_coef[-1], F_coef_cents[0], F_coef_cents[-1]]\nplt.imshow(Y_interpol, cmap=color, aspect='auto', origin='lower', extent=extent)\ny_tick_freq_cents = 1200 * np.log2(y_ticks_freq / F_min)\nplt.yticks(y_tick_freq_cents, y_ticks_freq)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.title('Log-frequency axis')\nplt.title('Log-frequency axis with R=%d' % R)\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nextent=[T_coef[0], T_coef[-1], F_coef_cents[0], F_coef_cents[-1]]\nplt.imshow(Y_interpol, cmap=color, aspect='auto', origin='lower', extent=extent)\ny_ticks_cents = np.array([0, 1200, 2400, 3600, 4800, 6000])\nplt.yticks(y_ticks_cents)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (cents)')\nplt.title('Log-frequency axis')\nplt.title('Log-frequency axis with R=%d' % R)\nplt.colorbar()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#샘플-신호에-대한-시간-축",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#샘플-신호에-대한-시간-축",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "샘플 신호에 대한 시간 축",
    "text": "샘플 신호에 대한 시간 축\n\n\\(x=(x(0),x(1), \\ldots x(L-1))^\\top \\in \\mathbb{R}^L\\)을 길이가 \\(L\\in\\mathbb{N}\\)인 이산-시간 신호라고 가정하자. 또한 \\(F_\\mathrm{s}\\)를 샘플 레이트라고 하자.\n그런 다음 물리적 시간 위치(초 단위로 표시)의 벡터 \\(t=(t(0),t(1), \\ldots t(L-1))^\\top \\in \\mathbb{R}^L\\)을 \\(x\\)에 대입하면 다음과 같이 정의된다. \\[\n   t(n) = \\frac{n}{F_\\mathrm{s}}\n\\] for \\(n\\in[0:L-1]\\)\n다시 말해,\n\n샘플 \\(x(0)\\)는 물리적 시간 \\(t(0)=0\\)(초 단위로 표시됨)와 연관된다.\n신호 \\(x\\)의 기간(duration)(초 단위)은 샘플 수를 샘플링 속도로 나눈 것이다: \\(L/F_\\mathrm{s}\\). 단, 이것은 \\(t(L-1)=(L-1)/F_\\mathrm{s}\\)와 동일하지 않다.\n두 샘플 \\(x(n-1)\\) 및 \\(x(n)\\) 사이의 거리(샘플링 기간 sampling period이라고 함)는 \\(1/F_\\mathrm{s}\\)이다.\n\n\n\nx, Fs = librosa.load(\"../audio/violin_c4_legato.wav\")\nipd.Audio(x, rate=Fs)\nL = x.shape[0] #샘플 수\nt_wav = np.arange(L) / Fs\nx_duration = L / Fs #듀레이션\n\nprint('t[0] = %0.4f, t[-1] = (L-1)/Fs = %0.4f, Fs = %0.0f, L = %0.0f, dur_x=%0.4f'\n      % (t_wav[0], t_wav[-1], Fs, L, x_duration))\nipd.display(ipd.Audio(x, rate=Fs))\n\nplt.figure(figsize=(6, 2))\nplt.plot(t_wav, x, color='gray')\nplt.xlim([t_wav[0], t_wav[-1]])\nplt.xlabel('Time (seconds)')\nplt.tight_layout()\n\nt[0] = 0.0000, t[-1] = (L-1)/Fs = 3.0000, Fs = 22050, L = 66150, dur_x=3.0000\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n# librosa를 이용한 plot\n# 단 librosa는 파형의 샘플이 아닌 symmetric amplitude envelope를 그림\n\nplt.figure(figsize=(6, 2))\nlibrosa.display.waveshow(x, color='gray')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#중심-윈도우잉과-시간-변환-centered-windowing-and-time-conversion",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#중심-윈도우잉과-시간-변환-centered-windowing-and-time-conversion",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "중심 윈도우잉과 시간 변환 (Centered Windowing and Time Conversion)",
    "text": "중심 윈도우잉과 시간 변환 (Centered Windowing and Time Conversion)\n\n신호의 윈도우 부분을 고려할 때 중앙 (centered) 시점을 채택한다. 여기서 윈도우의 중심이란 물리적 도메인과 관련된 참조로 사용된다.\n특히 STFT를 계산할 때, 윈도우 길이의 절반의 제로-패딩을 적용하여 신호를 왼쪽으로 확장한다.\n보다 정확히 말하면, \\(w:[0:N-1]\\to\\mathbb{R}\\)를 짝수 윈도우 길이 \\(N\\in\\mathbb{N}\\)의 윈도우 함수라고 하고 \\(H\\in\\mathbb{ N}\\)를 홉(hop) 크기라고 하자. 그리고 \\(N/2\\)개 0값을 앞에 넣는다.\n\n\\[\n\\tilde{x}=(0,\\ldots,0,x(0),x(1), \\ldots x(L-1))^\\top \\in \\mathbb{R}^{L+N/2}\n\\]\n\\[\n   \\mathcal{X}(m,k):= \\sum_{n=0}^{N-1} \\tilde{x}(n+mH)w(n)\\mathrm{exp}(-2\\pi ikn/N).\n\\]\n\n또한 프레임 인덱스 \\(m\\)이 물리적 시간 위치와 연관되어 있는 규칙을 사용한다.\n\n\\(T_\\mathrm{coef}(m) := \\frac{m\\cdot H}{F_\\mathrm{s}}\\)\n\n특히 다음이 성립한다.\n\n프레임 인덱스 \\(m=0\\)는 물리적 시간 \\(T_\\mathrm{coef}(0)=0\\)(초 단위)에 해당한다.\n시간 해상도(즉, 연속되는 두 프레임 사이의 거리)은 \\(\\Delta t = H/F_\\mathrm{s}\\)(초 단위)이다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-변환-frequency-conversion",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#주파수-변환-frequency-conversion",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "주파수 변환 (Frequency Conversion)",
    "text": "주파수 변환 (Frequency Conversion)\n\n\\(x\\) 및 \\(w\\)가 실수이면 주파수 계수의 상위 절반이 중복된다. 따라서 계수 \\(k\\in[0:K]\\) (\\(K=N/2\\))만 사용된다.\n특히 인덱스 \\(k=N/2\\)는 나이퀴스트(Nyquist) 주파수 \\(\\omega=F_\\mathrm{s}/2\\)에 해당한다.\n또한 인덱스 \\(k\\)는 다음의 주파수에 해당한다(헤르츠 단위).\n\n\\(F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\\)"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#스펙트로그램-시각화",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#스펙트로그램-시각화",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "스펙트로그램 시각화",
    "text": "스펙트로그램 시각화\n\n다음 코드는 이러한 규칙을 구현하기 위해 librosa 함수 librosa.stft를 사용하는 방법을 보여준다. 파라미터 설정 center=True는 centered view를 활성화하고 pad_mode='constant'는 제로-패딩 모드로 전환한다.\n또한 이 코드는 변환 함수 \\(T_\\mathrm{coef}\\) 및 \\(F_\\mathrm{coef}\\)를 한 번은 위의 공식을 사용하고 한 번은 librosa 내장 함수를 사용하여 구현하는 방법을 보여준다. 홀수 윈도우 크기 \\(N\\)의 경우 반올림에 대한 다른 규칙이 있을 수 있다. 실제로는 일반적으로 짝수 윈도우 크기를 사용한다(특히 FFT algorithm 관점에서 2의 거듭제곱임).\n\n\nN = 256\nH = 64\ncolor = 'gray_r' \n\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=True)\nY = np.log(1 + 100 * np.abs(X) ** 2)\n\nT_coef = np.arange(X.shape[1]) * H / Fs #위의 공식\nT_coef_librosa = librosa.frames_to_time(np.arange(X.shape[1]), sr=Fs, hop_length=H) #librosa 내장\nprint('T_coef 계산 결과가 같은가:', np.allclose(T_coef, T_coef_librosa))\n\nK = N // 2\nF_coef = np.arange(K+1) * Fs / N #위의 공식\nF_coef_librosa = librosa.fft_frequencies(sr=Fs, n_fft=N) #librosa 내장\nprint('F_coef 계산 결과가 같은가:', np.allclose(F_coef, F_coef_librosa))\n\nplt.figure(figsize=(6, 3))\nextent = [T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\nplt.imshow(Y, cmap=color, aspect='auto', origin='lower', extent=extent)\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.colorbar()\nplt.tight_layout()\nplt.show()\n\nT_coef 계산 결과가 같은가: True\nF_coef 계산 결과가 같은가: True\n\n\n\n\n\n\n시각화에서 centered view를 채택하려면 프레임 길이의 절반만큼 왼쪽 및 오른쪽 여백을 조정하고, 빈(bin) 너비의 절반만큼 아래쪽 및 위쪽 여백을 조정해야 한다.\n그러나 큰 스펙트로그램의 경우 시각화의 이러한 작은 조정은 상관 없다.\n\n\nplt.figure(figsize=(6, 3))\nextent = [T_coef[0] - (H / 2) / Fs, T_coef[-1] + (H / 2) / Fs,\n          F_coef[0] - (Fs / N) / 2, F_coef[-1] + (Fs / N) / 2]\nplt.imshow(Y, cmap=color, aspect='auto', origin='lower', extent=extent)\nplt.xlim([T_coef[0], T_coef[-1]])\nplt.ylim([F_coef[0], F_coef[-1]])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\nplt.colorbar()\nplt.tight_layout()\n\n\n\n\n\n# librosa의 내장 함수로 결과를 비교하자\n\nplt.figure(figsize=(6, 3))\nlibrosa.display.specshow(Y, y_axis='linear', x_axis='time', sr=Fs, hop_length=H, cmap=color)\nplt.colorbar()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#역inverse-dft",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#역inverse-dft",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "역(Inverse) DFT",
    "text": "역(Inverse) DFT\n\n\\(N\\in\\mathbb{N}\\) 길이의 벡터 \\(x\\in \\mathbb{C}^N\\)가 주어지면 DFT은 행렬-벡터 곱으로 정의된다. \\[X = \\mathrm{DFT}_N \\cdot x\\]\n\nwith \\(\\mathrm{DFT}_N \\in \\mathbb{C}^{N\\times N}\\)\ngiven by \\(\\mathrm{DFT}_N(n, k) = \\mathrm{exp}(-2 \\pi i k n / N)\\)\nfor \\(n\\in[0:N-1]\\) and \\(k\\in[0:N-1]\\).\n\nDFT는 벡터 \\(x\\)를 스펙트럼 벡터 \\(X\\)에서 복구할 수 있다는 점에서 invertible하다. 역 DFT는 행렬-벡터 곱으로 다시 지정된다. \\[ x = \\mathrm{DFT}_N^{-1} \\cdot X, \\]\n여기서 \\(\\mathrm{DFT}_N^{-1}\\)는 DFT 행렬 \\(\\mathrm{DFT}_N\\)의 역을 나타낸다.\n\n\\(\\mathrm{DFT}_N^{-1}(n, k) = \\frac{1}{N}\\mathrm{exp}(2 \\pi i k n / N)\\)\nfor \\(n\\in[0:N-1]\\) and \\(k\\in[0:N-1]\\).\n\n즉, 역함수는 본질적으로 일부 정규화 인자(normalizing factor) 및 켤레 복소수(complex conjugation)까지 DFT 행렬과 일치한다.\n다음 코드 셀에서 DFT 행렬과 그 역행렬을 생성한다. 또한 두 행렬이 실제로 서로 역임을 보여준다.\n\n이를 위해 \\(\\mathrm{DFT}_N \\cdot \\mathrm{DFT}_N^{-1}\\)와 항등 행렬 \\(I_N\\in \\mathbb{R}^{N\\times N}\\)의 차이, 그리고 \\(\\mathrm{DFT}_N^{-1} \\cdot\\mathrm{DFT}_N\\)와 \\(I_N\\)의 차이를 측정한다.\n\n\n\ndef generate_matrix_dft(N, K):\n    \"\"\"Generates a DFT (discrete Fourier transfrom) matrix\n    Args:\n        N (int): Number of samples\n        K (int): Number of frequency bins\n    Returns:\n        dft (np.ndarray): The DFT matrix\n    \"\"\"\n    dft = np.zeros((K, N), dtype=np.complex128)\n    for n in range(N):\n        for k in range(K):\n            dft[k, n] = np.exp(-2j * np.pi * k * n / N)\n    return dft\n\ndef generate_matrix_dft_inv(N, K):\n    \"\"\"Generates an IDFT (inverse discrete Fourier transfrom) matrix\n    Args:\n        N (int): Number of samples\n        K (int): Number of frequency bins\n    Returns:\n        dft (np.ndarray): The IDFT matrix\n    \"\"\"\n    dft = np.zeros((K, N), dtype=np.complex128)\n    for n in range(N):\n        for k in range(K):\n            dft[k, n] = np.exp(2j * np.pi * k * n / N) / N\n    return dft\n\n\nN = 32\ndft_mat = generate_matrix_dft(N, N)\ndft_mat_inv = generate_matrix_dft_inv(N, N)\n\nI = np.eye(N)\nA =  np.dot(dft_mat, dft_mat_inv)\nB =  np.dot(dft_mat_inv, dft_mat)\n\nplt.figure(figsize=(11, 3))\n\nplt.subplot(1, 3, 1)\nplt.title(r'$I_N$ for $N = %d$'%N)\nplt.imshow(I, origin='lower', cmap='seismic', aspect='equal')\nplt.colorbar()\n\nplt.subplot(1, 3, 2)\nplt.title(r'$|I_N - \\mathrm{DFT}_N \\cdot \\mathrm{DFT}_N^{-1}|$')\nplt.imshow(np.abs(I-A), origin='lower', cmap='seismic', aspect='equal')\nplt.colorbar()\n\nplt.subplot(1, 3, 3)\nplt.title(r'$|I_N - \\mathrm{DFT}_N^{-1} \\cdot \\mathrm{DFT}_N|$')\nplt.imshow(np.abs(I-B), origin='lower', cmap='seismic', aspect='equal')\nplt.colorbar();\n\nplt.tight_layout()\n\n\n\n\n\nDFT는 FFT 알고리즘을 사용하여 효율적으로 계산할 수 있다. 역 DFT 계산에도 적용할 수 있다. 다음에서는 numpy.fft.fft 및 numpy.fft.ifft를 사용해 구현한다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#역inverse-stft",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#역inverse-stft",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "역(Inverse) STFT",
    "text": "역(Inverse) STFT\n\n다음으로 이산 STFT을 inverting하는 법을 보자.\n\\(x:\\mathbb{Z}\\to\\mathbb{R}\\)를 이산-시간 신호라고 하고, \\(\\mathcal{X}\\)를 그것의 STFT라고 하자.\n또한 \\(w:[0:N-1]\\to\\mathbb{R}\\)를 길이 \\(N\\in\\mathbb{N}\\) 및 홉 크기 파라미터 \\(H\\in\\mathbb{N}\\)의 실수 값 이산 윈도우 함수를 나타낸다고 하자.\n표기상 편의를 위해 제로-패딩을 사용하여 윈도우 함수를 \\(w:\\mathbb{Z}\\to\\mathbb{R}\\)로 확장한다.\n\\(x_n:\\mathbb{Z}\\to\\mathbb{R}\\)를 다음에 의해 정의된 윈도우 신호라고 하자.\n\n\\(x_n(r):=x(r+nH)w(r)\\) for \\(r\\in\\mathbb{Z}\\).\n\n그러면 STFT 계수 \\(\\mathcal{X}(n,k)\\) for \\(k\\in[0:N-1]\\)가 다음에 의해 얻어진다.\n\n\\((\\mathcal{X}(n,0),\\ldots, \\mathcal{X}(n,N-1))^\\top = \\mathrm{DFT}_N \\cdot (x_n(0),\\ldots, x_n(N-1))^\\top.\\)\n\n\\(\\mathrm{DFT}_N\\)가 invertible 행렬이기 때문에 STFT에서의 윈도우 신호 \\(x_n\\)를 다음과 같이 재구성할 수 있다. \\[(x_n(0),\\ldots x_n(N-1))^\\top = \\mathrm{DFT}_N^{-1} \\cdot (\\mathcal{X}(n,0),\\ldots, \\mathcal{X}(n,N-1))^\\top\\]\n원래 신호의 샘플 \\(x(r)\\)을 얻으려면 윈도우 프로세스를 반대로 해야 한다. 이것은 윈도우 프로세스에서 상대적으로 약한 조건에서 가능하다는 것을 볼 수 있다. 신호의 윈도우 섹션의 적절하게 이동된 모든 버전에 대한 중첩(superposition)을 고려해 보자. \\[\\sum_{n\\in\\mathbb{Z}} x_n(r-nH)\n  = \\sum_{n\\in\\mathbb{Z}} x(r-nH+nH)w(r-nH)\n  = x(r)\\sum_{n\\in\\mathbb{Z}} w(r-nH)\\]\n따라서 샘플 \\(x(r)\\)를 다음을 통해 복구할 수 있다. \\[x(r) = \\frac{\\sum_{n\\in\\mathbb{Z}} x_n(r-nH)}{\\sum_{n\\in\\mathbb{Z}} w(r-nH)}\\]\n\n\\(\\sum_{n\\in\\mathbb{Z}} w(r-nH)\\not= 0\\)\n\n이 전반적인 접근 방식은 소위 overlap–add technique을 기반으로 한다. 이 기술에서는 겹치는 재구성된 윈도우 섹션이 단순히 오버레이되고 합산된다(그런 다음 windowing을 보상하기 위해 정규화됨)."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#단위-분할-partition-of-unity",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#단위-분할-partition-of-unity",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "단위 분할 (Partition of Unity)",
    "text": "단위 분할 (Partition of Unity)\n\n위의 조건을 만족하는 홉 크기와 함께 윈도우 함수를 찾는 것은 어렵지 않다. 예를 들어 윈도우 함수 \\(w:[0:N-1]\\to\\mathbb{R}\\)가 양수이고 홉 크기가 윈도우 길이보다 작거나 같을 때 time-shifted 윈도우에 대한 합계는 항상 양수이다.\n종종 윈도우 함수와 홉 크기를 다음의 더 강한 조건으로 선택할 수도 있다.\n\n\\(\\sum_{n\\in\\mathbb{Z}} w(r-nH) = 1\\) (for all \\(r\\in\\mathbb{Z}\\) is fulfilled.)\n\n이 경우, time-shifted 윈도우 함수는 이산 시간 축 \\(\\mathbb{Z}\\)의 단위 분할 (partition of unity)을 정의한다고 한다.\n예를 들어, 다음과 같이 정의된 윈도우 \\(w:\\mathbb{Z}\\to\\mathbb{R}\\)로 squared sinusoidal을 사용할 때 단위 분할을 얻는다.\n\n\\(w(r):= \\left\\{ \\begin{array}{cl}  \\sin(\\pi r/N)^2 \\quad \\mbox{if}\\,\\,\\, r\\in[0:N-1] \\\\  0 \\quad \\mbox{otherwise}  \\end{array} \\right.\\)\n홉 사이즈는 \\(H=N/2\\)\n\n단위 분할이 되는 속성은 윈도우 함수 자체뿐만 아니라 홉 크기 파라미터에도 의존한다는 점에 유의해야 한다.\n다음 그림은 \\(N\\) 길이의 다양한 윈도우 함수와 홉 크기 \\(H\\)를 사용하는 time-shifted 버전을 보여준다. time-shifted 버전의 합은 두꺼운 빨간색 곡선으로 표시된다.\n\n\ndef plot_sum_window(w, H, L, title='', figsize=(6, 2)):\n    N = len(w)\n    M = np.floor((L - N) / H).astype(int) + 1\n    w_sum = np.zeros(L)\n    plt.figure(figsize=figsize)\n    for m in range(M):\n        w_shifted = np.zeros(L)\n        w_shifted[m * H:m * H + N] = w\n        plt.plot(w_shifted, 'k')\n        w_sum = w_sum + w_shifted\n    plt.plot(w_sum, 'r', linewidth=3)\n    plt.xlim([0, L-1])\n    plt.ylim([0, 1.1*np.max(w_sum)])\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n    return w_sum\n\n\nL = 256\nN = 64\n\nH = N//2\nw_type = 'triang'\nw = scipy.signal.get_window(w_type, N)\nplot_sum_window(w, H, L, title='Triangular window, H = N/2');\n\nH = N//2\nw_type = 'hann'\nw = scipy.signal.get_window(w_type, N)\nplot_sum_window(w, H, L, title='Hann window, H = N/2');\n\nH = 3*N//8\nw_type = 'hann'\nw = scipy.signal.get_window(w_type, N)\nplot_sum_window(w, H, L, title='Hann window, H = 3N/8');\n\nH = N//4\nw = scipy.signal.gaussian(N, std=8)\nplot_sum_window(w, H, L, title='Gaussian window, H = N/4');"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#구현",
    "href": "posts/3. Fourier Anaylsis of Signals/3.4.Short-term_Fourier_Transfrom_2.html#구현",
    "title": "3.4. 단기 푸리에 변환 (STFT) (2)",
    "section": "구현",
    "text": "구현\n\n역 DFT에서 작은 허수값을 피하기 위해 재구성된 윈도우 신호의 실수 부분만 유지하는게 이득일 수 있다.\n역 DFT를 적용한 후에는 윈도잉을 보상해야 한다. 프레임 기반 레벨에서(즉, 각 윈도우 섹션에 대해 개별적으로) 이 작업을 수행하는 것 보다는 모든 윈도우 신호와 이동된 모든 윈도우를 개별적으로 누적하여 전역적으로(globally) 보상을 수행해야 한다. 전역 보상이 가능한 이유는 DFT의 선형성에 있다. 또한 보상에서 0으로 나누기를 피해야 한다.\n주어진 홉 크기에 대해 이동된 윈도우가 단위 분할(a partition of unity)을 형성하는 경우, 보상을 수행할 필요가 없다.\nSTFT 계산에서 패딩이 적용된 경우 역 STFT 계산 시 이를 고려해야 한다.\n\n\ndef stft_basic(x, w, H=8, only_positive_frequencies=False):\n    \"\"\"Compute a basic version of the discrete short-time Fourier transform (STFT)\n    Args:\n        x (np.ndarray): Signal to be transformed\n        w (np.ndarray): Window function\n        H (int): Hopsize (Default value = 8)\n        only_positive_frequencies (bool): Return only positive frequency part of spectrum (non-invertible)\n            (Default value = False)\n    Returns:\n        X (np.ndarray): The discrete short-time Fourier transform\n    \"\"\"\n    N = len(w)\n    L = len(x)\n    M = np.floor((L - N) / H).astype(int) + 1\n    X = np.zeros((N, M), dtype='complex')\n    for m in range(M):\n        x_win = x[m * H:m * H + N] * w\n        X_win = np.fft.fft(x_win)\n        X[:, m] = X_win\n\n    if only_positive_frequencies:\n        K = 1 + N // 2\n        X = X[0:K, :]\n    return X\n\ndef istft_basic(X, w, H, L):\n    \"\"\"Compute the inverse of the basic discrete short-time Fourier transform (ISTFT)\n    Args:\n        X (np.ndarray): The discrete short-time Fourier transform\n        w (np.ndarray): Window function\n        H (int): Hopsize\n        L (int): Length of time signal\n    Returns:\n        x (np.ndarray): Time signal\n    \"\"\"\n    N = len(w)\n    M = X.shape[1]\n    x_win_sum = np.zeros(L)\n    w_sum = np.zeros(L)\n    for m in range(M):\n        x_win = np.fft.ifft(X[:, m])\n        # Avoid imaginary values (due to floating point arithmetic)\n        x_win = np.real(x_win)\n        x_win_sum[m * H:m * H + N] = x_win_sum[m * H:m * H + N] + x_win\n        w_shifted = np.zeros(L)\n        w_shifted[m * H:m * H + N] = w\n        w_sum = w_sum + w_shifted\n    # Avoid division by zero\n    w_sum[w_sum == 0] = np.finfo(np.float32).eps\n    x_rec = x_win_sum / w_sum\n    return x_rec, x_win_sum, w_sum\n\n\nL = 256\nt = np.arange(L) / L\nomega = 4\nx = np.sin(2 * np.pi * omega * t * t)\n\nN = 64\nH = 3 * N // 8\nw_type = 'hann'\nw = scipy.signal.get_window(w_type, N)\nX = stft_basic(x, w=w, H=H)\nx_rec, x_win_sum, w_sum = istft_basic(X, w=w, H=H, L=L)\n\nplt.figure(figsize=(8, 3))\nplt.plot(x, color=[0, 0, 0], linewidth=4, label='Original signal')\nplt.plot(x_win_sum, 'b', label='Summed windowed signals')\nplt.plot(w_sum, 'r', label='Summed windows')\nplt.plot(x_rec, color=[0.8, 0.8, 0.8], linestyle=':', linewidth=4, label='Reconstructed signal')\nplt.xlim([0,L-1])\nplt.legend(loc='lower left')\nplt.tight_layout()\nplt.show()\n\n\n\n\nlibrosa 예제\n\n단, librosa.istft에서는 다른 윈도우 보상 방법을 사용한다. \\[x(r) = \\frac{\\sum_{n\\in\\mathbb{Z}} w(r-nH)x_n(r-nH)}{\\sum_{n\\in\\mathbb{Z}} w(r-nH)^2}\\]\n\n\ndef print_plot(x, x_rec):\n    print('Number of samples of x:    ', x.shape[0])\n    print('Number of samples of x_rec:', x_rec.shape[0])\n    if x.shape[0] == x_rec.shape[0]:\n        print('Signals x and x_inv agree:', np.allclose(x, x_rec))\n        plt.figure(figsize=(6, 2))\n        plt.plot(x-x_rec, color='red')\n        plt.xlim([0, x.shape[0]])\n        plt.title('Differences between x and x_rec')\n        plt.xlabel('Time (samples)');\n        plt.tight_layout()\n        plt.show()\n    else:\n        print('Number of samples of x and x_rec does not agree.')\n\n\nx, Fs = librosa.load(\"../audio/violin_c4_legato.wav\")        \n        \nN = 4096\nH = 2048\nL = x.shape[0]\n\nprint('=== Centered Case ===')\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=True)\nx_rec = librosa.istft(X, hop_length=H, win_length=N, window='hann', center=True, length=L)\nprint('stft: center=True; istft: center=True')\nprint_plot(x, x_rec)\n\nprint('=== Non-Centered Case ===')\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=False)\nx_rec = librosa.istft(X, hop_length=H, win_length=N, window='hann', center=False, length=L)\nprint('stft: center=False; istft: center=False')\nprint_plot(x, x_rec)\n\nprint('=== Centered vs. Non-Centered Case ===')\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=True)\nx_rec = librosa.istft(X, hop_length=H, win_length=N, window='hann', center=False, length=L)\nprint('stft: center=True; istft: center=False')\nprint_plot(x, x_rec)\n\nplt.figure(figsize=(6, 2))\nplt.plot(x, color='black')\nplt.xlim([0, x.shape[0]])\nplt.plot(x_rec, color='gray')\nplt.title('Signal x (black) and x_rec (gray)')\nplt.xlabel('Time (samples)');\nplt.tight_layout()\nplt.show()\n\n=== Centered Case ===\nstft: center=True; istft: center=True\nNumber of samples of x:     66150\nNumber of samples of x_rec: 66150\nSignals x and x_inv agree: True\n\n\n\n\n\n=== Non-Centered Case ===\nstft: center=False; istft: center=False\nNumber of samples of x:     66150\nNumber of samples of x_rec: 66150\nSignals x and x_inv agree: False\n\n\n\n\n\n=== Centered vs. Non-Centered Case ===\nstft: center=True; istft: center=False\nNumber of samples of x:     66150\nNumber of samples of x_rec: 66150\nSignals x and x_inv agree: False\n\n\n\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C2/C2.html\nhttps://musicinformationretrieval.com/\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html",
    "title": "3.5. 디지털 신호",
    "section": "",
    "text": "오디오 신호(signal)의 디지털화에 필요한 샘플링(sampling)과 양자화(quantization)에 대해 소개하며, 또한 간섭(interference) 및 비팅(beating) 현상을 다룬다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#샘플링sampling",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#샘플링sampling",
    "title": "3.5. 디지털 신호",
    "section": "샘플링(Sampling)",
    "text": "샘플링(Sampling)\n\n신호 처리(signal processing)에서, 샘플링이란 연속 신호를 시간 축의 이산적 부분집합으로만 정의된 이산 신호로 축소시키는 것을 말한다.\n적절한 인코딩을 통해 이산 집합이 정수 집합 \\(\\mathbb{Z}\\)의 하위 집합 \\(I\\)라고 가정하고는 한다. 그런 다음 이산시간(discrete time, DT)-신호는 함수 \\(x\\colon I\\to\\mathbb{R}\\)로 정의되며 여기서 도메인 \\(I\\)는 시점에 해당한다.\n\\(\\mathbb{Z}\\setminus I\\)의 포인트에 대해 모든 값을 0으로 설정하는 것만으로 도메인 \\(I\\)에서 도메인 \\(\\mathbb{Z}\\)로 모든 DT 신호를 확장할 수 있으므로 \\(I=\\mathbb{Z}\\)를 가정할 수 있다.\n연속시간(continuous time, CT)-신호 \\(f\\colon\\mathbb{R}\\to\\mathbb{R}\\)를 DT-신호 \\(x\\colon\\mathbb{Z}\\to\\mathbb{R}\\)로 변환하는 가장 일반적인 샘플링 절차는 등거리 샘플링(equidistant sampling)이라고 한다.\n양의 실수 \\(T>0\\)를 고정하면, DT-신호 \\(x\\)는 다음과 같이 설정하여 얻는다.\n\n\\(x(n):= f(n \\cdot T)\\) for \\(n\\in\\mathbb{Z}\\)\n\n\\(x(n)\\) 값은 원래 아날로그 신호 \\(f\\)의 시간 \\(t=n\\cdot T\\)에서 가져온 샘플이라고 한다. 간단히 말해서 이 절차를 \\(T\\)-샘플링 이라고도 한다.\n숫자 \\(T\\)는 샘플링 주기 (sampling period) 라고 하고 역(inverse) \\(F_\\mathrm{s}:=1/T\\)는 샘플링 레이트/속도 (sampling rate) 라고 한다. 샘플링 레이트는 초당 샘플 수를 지정하며 헤르츠(Hz) 단위로 측정된다.\n다음 코드 셀에서 높은 샘플링 속도로 샘플링된 DT-신호의 선형 보간법을 통해 정의된 CT-신호 \\(f\\)로 시작한다. 그림에서 이 CT-신호는 검은색 곡선으로 표시된다. 등거리 샘플링을 적용하여 빨간색 줄기 플롯으로 시각화된 DT 신호 \\(x\\)를 얻는다.\n\n\ndef generate_function(Fs, dur=1):\n    \"\"\"Generate example function\n    \n    Args:\n        Fs (scalar): Sampling rate\n        dur (float): Duration (in seconds) of signal to be generated (Default value = 1)\n        \n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(Fs * dur)\n    t = np.arange(N) / Fs\n    x = 1 * np.sin(2 * np.pi * (2 * t - 0))\n    x += 0.5 * np.sin(2 * np.pi * (6 * t - 0.1))\n    x += 0.1 * np.sin(2 * np.pi * (20 * t - 0.2))\n    return x, t\n\n\ndef sampling_equidistant(x_1, t_1, Fs_2, dur=None):\n    \"\"\"Equidistant sampling of interpolated signal\n\n    Args:\n        x_1 (np.ndarray): Signal to be interpolated and sampled\n        t_1 (np.ndarray): Time axis (in seconds) of x_1\n        Fs_2 (scalar): Sampling rate used for equidistant sampling\n        dur (float): Duration (in seconds) of sampled signal (Default value = None)\n        \n    Returns:\n        x (np.ndarray): Sampled signal\n        t (np.ndarray): Time axis (in seconds) of sampled signal\n    \"\"\"\n    if dur is None:\n        dur = len(t_1) * t_1[1]\n    N = int(Fs_2 * dur)\n    t_2 = np.arange(N) / Fs_2\n    x_2 = interp1d(t_1, x_1, kind='linear', fill_value='extrapolate')(t_2)\n    return x_2, t_2\n\n\nFs_1 = 100\nx_1, t_1 = generate_function(Fs=Fs_1, dur=2)\n\nFs_2 = 20\nx_2, t_2 = sampling_equidistant(x_1, t_1, Fs_2)\n    \nplt.figure(figsize=(6, 2))\nplt.plot(t_1, x_1, 'k')\nplt.title('Original CT-signal')\nplt.xlabel('Time (seconds)')\nplt.ylim([-1.5, 1.5])\nplt.xlim([t_1[0], t_1[-1]])\nplt.tight_layout()\n\nplt.figure(figsize=(6, 2))\nplt.stem(t_2, x_2, linefmt='r', markerfmt='ro', basefmt='None')\nplt.plot(t_1, x_1, 'k', linewidth=1, linestyle='dotted')\nplt.title(r'Sampling rate $F_\\mathrm{s} = %.0f$'%Fs_2)\nplt.xlabel('Time (seconds)')\nplt.ylim([-1.5, 1.5])\nplt.xlim([t_1[0], t_1[-1]])\nplt.tight_layout()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#에일리어싱-aliasing",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#에일리어싱-aliasing",
    "title": "3.5. 디지털 신호",
    "section": "에일리어싱 (Aliasing)",
    "text": "에일리어싱 (Aliasing)\n또는 위신호현상\n\n일반적으로 샘플링은 처리 과정에서 정보가 손실되고 원본 CT-신호를 샘플링된 버전으로부터 복구할 수 없다는 점에서 손실적인(lossy) 작업이다.\nCT-신호에 주파수 스펙트럼 측면(대역 제한(bandlimited) 필요)에서 추가 속성이 있는 경우에만 완벽한 재구성이 가능하다. 이것이 유명한 샘플링 정리(sampling theorem)의 주장이다. 샘플링 이론은 또한 DT-신호의 샘플에 의해 가중된 적절하게 이동된 \\(\\mathrm{sinc}\\)-함수를 중첩하여 원래 CT-신호가 어떻게 재구성될 수 있는지 보여준다.\n추가적인 속성이 없다면 샘플링으로 인해 신호의 특정 주파수 구성 요소를 구분할 수 없게 되는 에일리어싱(aliasing)이라는 현상이 발생할 수 있다.\n이 효과는 다음 그림에 설명되어 있다. 높은 샘플링 속도를 사용하면 원본 CT-신호를 높은 정확도로 재구성할 수 있다. 그러나 샘플링 속도를 낮추면 더 높은 주파수 구성 요소가 잘 캡처되지 않고 원래 신호의 대략적인 근사치만 남는다.\n\n\ndef reconstruction_sinc(x, t, t_sinc):\n    \"\"\"Reconstruction from sampled signal using sinc-functions\n\n    Args:\n        x (np.ndarray): Sampled signal\n        t (np.ndarray): Equidistant discrete time axis (in seconds) of x\n        t_sinc (np.ndarray): Equidistant discrete time axis (in seconds) of signal to be reconstructed\n\n    Returns:\n        x_sinc (np.ndarray): Reconstructed signal having time axis t_sinc\n    \"\"\"\n    Fs = 1 / t[1]\n    x_sinc = np.zeros(len(t_sinc))\n    for n in range(0, len(t)):\n        x_sinc += x[n] * np.sinc(Fs * t_sinc - n)\n    return x_sinc\n\ndef plot_signal_reconstructed(t_1, x_1, t_2, x_2, t_sinc, x_sinc):\n    plt.figure(figsize=(6, 2))\n    plt.plot(t_1, x_1, 'k', linewidth=1, linestyle='dotted', label='Orignal signal')\n    plt.stem(t_2, x_2, linefmt='r:', markerfmt='r.', basefmt='None', label='Samples')\n    plt.plot(t_1, x_sinc, 'b', label='Reconstructed signal')\n    plt.title(r'Sampling rate $F_\\mathrm{s} = %.0f$'%(1/t_2[1]))\n    plt.xlabel('Time (seconds)')\n    plt.ylim([-1.5, 1.5])\n    plt.xlim([t_1[0], t_1[-1]])\n    plt.legend(loc='upper right')\n    plt.tight_layout()\n    plt.show()\n\n\nFs_2 = 40\nx_2, t_2 = sampling_equidistant(x_1, t_1, Fs_2)\nt_sinc = t_1\nx_sinc = reconstruction_sinc(x_2, t_2, t_sinc)\nplot_signal_reconstructed(t_1, x_1, t_2, x_2, t_sinc, x_sinc);\n\nFs_2 = 20\nx_2, t_2 = sampling_equidistant(x_1, t_1, Fs_2)\nt_sinc = t_1\nx_sinc = reconstruction_sinc(x_2, t_2, t_sinc)\nplot_signal_reconstructed(t_1, x_1, t_2, x_2, t_sinc, x_sinc);\n\nFs_2 = 10\nx_2, t_2 = sampling_equidistant(x_1, t_1, Fs_2)\nt_sinc = t_1\nx_sinc = reconstruction_sinc(x_2, t_2, t_sinc)\nplot_signal_reconstructed(t_1, x_1, t_2, x_2, t_sinc, x_sinc);\n\n\n\n\n\n\n\n\n\n\n\n다음 예는 에일리어싱이 음질에 미치는 영향을 나타낸다. 높은 샘플링 속도(\\(F_s=8192Hz\\))의 음악 신호로 시작한 다음 두배씩 줄여나가보자.\n\n\nx, Fs = librosa.load('../audio/piano_c_scale.wav', sr=8000)\nFs_orig = Fs\nlen_orig = len(x)\nfor i in range(5):\n    print('Sampling rate Fs = %s; Number of samples = %s' % (Fs, len(x)))\n    x_play = scipy.signal.resample(x, len_orig)\n    ipd.display(ipd.Audio(data=x_play, rate=Fs_orig))\n    Fs = Fs // 2\n    x = x[::2]\n\nSampling rate Fs = 8000; Number of samples = 54001\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSampling rate Fs = 4000; Number of samples = 27001\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSampling rate Fs = 2000; Number of samples = 13501\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSampling rate Fs = 1000; Number of samples = 6751\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSampling rate Fs = 500; Number of samples = 3376\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#샘플링-정리-sampling-theorem",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#샘플링-정리-sampling-theorem",
    "title": "3.5. 디지털 신호",
    "section": "샘플링 정리 (Sampling Theorem)",
    "text": "샘플링 정리 (Sampling Theorem)\n\n샘플링 정리 (sampling theorem)는 대역 제한이 있는(bandlimited) 연속 시간(CT) 신호를 특정 조건에서 완벽하게 재구성할 수 있다고 말한다.\n보다 정확하게는 \\(|\\omega|>\\Omega\\) 에 대해 푸리에 변환 \\(\\hat{f}\\)가 사라지면 CT 신호 \\(f\\in L^2(\\mathbb{R})\\)를 \\(\\Omega\\)-bandlimited 라고 한다 (즉, \\(\\hat{f}(\\omega) = 0\\) for \\(|\\omega|>\\Omega\\)).\n\\(f\\in L^2(\\mathbb{R})\\)를 \\(\\Omega\\)-bandlimited 함수라고 하고, \\(x\\)를 \\(f\\)의 (with \\(T:=1/(2\\Omega)\\)) \\(T\\)-샘플 버전이라고 하자 (즉, \\(x(n)=f(nT)\\), \\(n\\in\\mathbb{Z}\\)).\n그러면 \\(f\\)는 다음과 같이 \\(x\\)로부터 재구성될 수 있다.\n\n\\[\nf(t)=\\sum_{n\\in\\mathbb{Z}}x(n)\\mathrm{sinc}\\left(\\frac{t-nT}{T}\\right)\n=\\sum_{n\\in\\mathbb{Z}}f\\left(\\frac{n}{2\\Omega}\\right) \\mathrm{sinc}\\left(2\\Omega t-n\\right),\n\\]\nwhere the \\(\\mathrm{sinc}\\)-function is defined as\n\\[\\begin{equation}\n    \\mathrm{sinc}(t):=\\left\\{\\begin{array}{ll}\n    \\frac{\\sin \\pi t}{\\pi t},&\\mbox{ if $t\\not= 0$,}\\\\\n    1,&\\mbox{ if $t= 0$.}\n\\end{array}\\right.\n\\end{equation}\\]\n\n즉, 대역 제한이 샘플링 속도의 절반 이하인 경우, 등거리 샘플링으로 얻은 DT 신호에서 CT 신호 \\(f\\)를 완벽하게 재구성할 수 있다.\n\\(\\mathrm{sinc}\\) 함수를 기반으로 한 이 재구성은 reconstruction_sinc 함수에서 사용되었다."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#이산화-discretization",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#이산화-discretization",
    "title": "3.5. 디지털 신호",
    "section": "이산화 (Discretization)",
    "text": "이산화 (Discretization)\n\n위에서 연속-시간 축을 이산-시간 축으로 변환하는 과정으로서의 샘플링을 보았다. 이것은 아날로그-to-디지털의 첫번째 단계였다.\n두 번째 단계에서는 가능한 진폭(amplitude)의 연속 범위(\\(\\mathbb{R}\\)로 인코딩됨)를 가능한 값의 이산 범위(이산 집합 \\(\\Gamma\\subset \\mathbb{R}\\)로 인코딩됨)로 대체해야 한다. 이 프로세스를 일반적으로 양자화(quantization) 라고 한다.\n\n\nImage('../img/3.fourier_analysis/f.2.13.PNG', width=500)"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#균일uniform-양자화",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#균일uniform-양자화",
    "title": "3.5. 디지털 신호",
    "section": "균일(Uniform) 양자화",
    "text": "균일(Uniform) 양자화\n\n양자화는 각 진폭 값 \\(a\\in\\mathbb{R}\\)에 값 \\(Q(a)\\in\\Gamma\\)을 할당하는 quantizer라고 하는 함수 \\(Q:\\mathbb{R}\\to\\Gamma\\)로 모델링할 수 있다.\n사용되는 많은 quantizer는 단순히 아날로그 값을 일부 정밀도 단위로 반올림하거나 자른다(truncate). 예를 들어 어떤 값 \\(\\Delta\\)와 동일한 quantization step size를 갖는 일반적인 uniform quantizer는 양자화 수준을 균일하게 배치한다.\n\n\\(Q(a) := \\mathrm{sgn}(a) \\cdot \\Delta \\cdot \\left\\lfloor \\frac{|a|}{\\Delta} + \\frac{1}{2} \\right\\rfloor\\) for \\(a\\in\\mathbb{R}\\),\n\\(\\mathrm{sgn}(\\cdot)\\)는 실수의 부호를 생성하는 signum 함수이며, 대괄호 \\(\\lfloor \\cdot \\rfloor\\)는 실수를 잘라(truncate) 이 숫자 아래에서 가장 큰 정수를 생성하는 것이다.\n\\(\\Delta=1\\)의 경우 quantizer \\(Q\\)는 가장 가까운 정수로 간단히 반올림된다.\n\n\n\n양자화 오류 (Quantization Error)\n\n샘플링과 마찬가지로 양자화는 일반적으로 손실이 많은 작업이다. 다른 아날로그 값이 동일한 디지털 값에 매핑될 수 있기 때문이다. 실제 아날로그 값과 양자화된 값의 차이를 양자화 오류라고 한다.\n양자화 step size \\(\\Delta\\)를 줄이면 일반적으로 양자화 오류가 더 작아진다. 그러나 동시에 양자화된 값의 수(따라서 이러한 값을 인코딩하는 데 필요한 비트 수)도 증가한다.\n예를 들어 양자화 단계 크기 \\(\\Delta=1/3\\)가 사용되면 주어진 신호에 대해 \\(8\\)의 서로 다른 양자화 값이 생성된다. 따라서 \\(3\\) 비트 코딩 방식을 사용하여 양자화된 값을 나타낼 수 있다. CD 녹음의 경우 \\(65536\\) 가능한 값을 표현할 수 있는 \\(16\\) 비트 코딩 체계가 사용된다.\n\n\n\n균일 양자화 구현\n\n다음에서 모든 아날로그 값이 \\(s,t\\in\\mathbb{R}\\)에 대해 \\([s,t]\\) 범위 내에 있다고 가정한다. 또한 여러 양자화 수준이 \\(\\lambda\\in\\mathbb{N}\\)인 경우, 양자화 step size를 \\(\\Delta=|t-s|/(\\lambda-1)\\)로 정의한다. 이는 \\(\\lambda\\) 양자화 레벨로 구성되는 (\\(s\\) 값으로 시작하여 \\(t\\) 값으로 끝남), 값 \\(s\\)와 \\(t\\) 사이의 uniform 양자화로 정의한다.\n예를 들어 파형 기반 오디오 신호는 일반적으로 \\([-1,1]\\) 범위에 있다.\n\\(s=-1\\), \\(t=1\\) 및 \\(\\lambda=9\\)의 경우 결과는 \\(\\Delta=1/4\\)이다. 이 경우 결과 양자화 오류는 최대 1/8입니다. 다음 코드 셀에서 서로 다른 매개변수 \\(s\\), \\(t\\) 및 \\(\\lambda\\)에 대해 균일한 양자화를 산출하는 quantize_uniform 함수를 정의해보자.\n\n\ndef quantize_uniform(x, quant_min=-1.0, quant_max=1.0, quant_level=5):\n    \"\"\"Uniform quantization approach\n\n    Args:\n        x (np.ndarray): Original signal\n        quant_min (float): Minimum quantization level (Default value = -1.0)\n        quant_max (float): Maximum quantization level (Default value = 1.0)\n        quant_level (int): Number of quantization levels (Default value = 5)\n\n    Returns:\n        x_quant (np.ndarray): Quantized signal\n    \"\"\"\n    x_normalize = (x-quant_min) * (quant_level-1) / (quant_max-quant_min)\n    x_normalize[x_normalize > quant_level - 1] = quant_level - 1\n    x_normalize[x_normalize < 0] = 0\n    x_normalize_quant = np.around(x_normalize)\n    x_quant = (x_normalize_quant) * (quant_max-quant_min) / (quant_level-1) + quant_min\n    return x_quant\n\n\ndef plot_graph_quant_function(ax, quant_min=-1.0, quant_max=1.0, quant_level=256, mu=255.0, quant='uniform'):\n    \"\"\"Helper function for plotting a graph of quantization function and quantization error\n\n    Args:\n        ax (mpl.axes.Axes): Axis\n        quant_min (float): Minimum quantization level (Default value = -1.0)\n        quant_max (float): Maximum quantization level (Default value = 1.0)\n        quant_level (int): Number of quantization levels (Default value = 256)\n        mu (float): Encoding parameter (Default value = 255.0)\n        quant (str): Type of quantization (Default value = 'uniform')\n    \"\"\"\n    x = np.linspace(quant_min, quant_max, 1000)\n    if quant == 'uniform':\n        x_quant = quantize_uniform(x, quant_min=quant_min, quant_max=quant_max, quant_level=quant_level)\n        quant_stepsize = (quant_max - quant_min) / (quant_level-1)\n        title = r'$\\lambda = %d, \\Delta=%0.2f$' % (quant_level, quant_stepsize)\n    if quant == 'nonuniform':\n        x_quant = quantize_nonuniform_mu(x, mu=mu, quant_level=quant_level)\n        title = r'$\\lambda = %d, \\mu=%0.1f$' % (quant_level, mu)\n    error = np.abs(x_quant - x)\n    ax.plot(x, x, color='k', label='Original amplitude')\n    ax.plot(x, x_quant, color='b', label='Quantized amplitude')\n    ax.plot(x, error, 'r--', label='Quantization error')\n    ax.set_title(title)\n    ax.set_xlabel('Amplitude')\n    ax.set_ylabel('Quantized amplitude/error')\n    ax.set_xlim([quant_min, quant_max])\n    ax.set_ylim([quant_min, quant_max])\n    ax.grid('on')\n    ax.legend()\n\n\nplt.figure(figsize=(12,4))\nax = plt.subplot(1, 3, 1)\nplot_graph_quant_function(ax, quant_min=-1, quant_max=4, quant_level=3)\nax = plt.subplot(1, 3, 2)\nplot_graph_quant_function(ax, quant_min=-2, quant_max=2, quant_level=4)\nax = plt.subplot(1, 3, 3)\nplot_graph_quant_function(ax, quant_min=-1, quant_max=1, quant_level=9)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n다음 코드 셀에서는 정현파를 신호로 사용하여 다양한 파라미터 설정에 대한 균일(uniform) 양자화 결과를 본다.\n\n\ndef generate_sinusoid(dur=5, Fs=1000, amp=1, freq=1, phase=0):\n    \"\"\"Generation of sinusoid\n    \n    2.3.Audio_Represenation에 쓰인 바 있음\n    \n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 1)\n        freq (float): Frequency of sinusoid (Default value = 1)\n        phase (float): Phase of sinusoid (Default value = 0)\n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    x = amp * np.sin(2*np.pi*(freq*t-phase))\n    return x, t\n\n\ndef plot_signal_quant(x, t, x_quant, figsize=(6, 2), xlim=None, ylim=None, title=''):\n    \"\"\"Helper function for plotting a signal and its quantized version\n\n    Args:\n        x: Original Signal\n        t: Time\n        x_quant: Quantized signal\n        figsize: Figure size (Default value = (8, 2))\n        xlim: Limits for x-axis (Default value = None)\n        ylim: Limits for y-axis (Default value = None)\n        title: Title of figure (Default value = '')\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.plot(t, x, color='gray', linewidth=1.0, linestyle='-', label='Original signal')\n    plt.plot(t, x_quant, color='red', linewidth=2.0, linestyle='-', label='Quantized signal')\n    if xlim is None:\n        plt.xlim([0, t[-1]])\n    else:\n        plt.xlim(xlim)\n    if ylim is not None:\n        plt.ylim(ylim)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.title(title)\n    plt.legend(loc='upper right', framealpha=1)\n    plt.tight_layout()\n    plt.show()\n\n\ndur = 5\nx, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=1, phase=0.0)\n\nquant_min = -1\nquant_max = 1\nquant_level = 5\nx_quant = quantize_uniform(x, quant_min=quant_min, quant_max=quant_max, \n                          quant_level=quant_level)\nplot_signal_quant(x, t, x_quant, xlim=[0, dur], ylim=[-1.3,1.3], \n                  title=r'Uniform quantization with min=$%0.1f$, max=$%0.1f$, $\\lambda$=$%d$'%(quant_min, quant_max, quant_level));\n\nquant_min = -0.5\nquant_max = 1\nquant_level = 3\nx_quant = quantize_uniform(x, quant_min=quant_min, quant_max=quant_max, \n                          quant_level=quant_level)\nplot_signal_quant(x, t, x_quant, xlim=[0, dur], ylim=[-1.3,1.3], \n                  title=r'Uniform quantization with min=$%0.1f$, max=$%0.1f$, $\\lambda$=$%d$'%(quant_min, quant_max, quant_level));\n\nquant_min = -1.2\nquant_max = 1.2\nquant_level = 4\nx_quant = quantize_uniform(x, quant_min=quant_min, quant_max=quant_max, \n                          quant_level=quant_level)\nplot_signal_quant(x, t, x_quant, xlim=[0, dur], ylim=[-1.3,1.3], \n                  title=r'Uniform quantization with min=$%0.1f$, max=$%0.1f$, $\\lambda$=$%d$'%(quant_min, quant_max, quant_level));"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#비균일nonuniform-양자화",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#비균일nonuniform-양자화",
    "title": "3.5. 디지털 신호",
    "section": "비균일(Nonuniform) 양자화",
    "text": "비균일(Nonuniform) 양자화\n\n균일 양자화에서 양자화 수준은 등거리(equidistant) 방식으로 배치(spaced)된다. 그렇지 않은 경우 비균일(nonuniform) 양자화라고 한다. 예를 들어 오디오 신호의 경우 로그 방식으로 간격을 둔 양자화 수준을 선택하는 경우가 많다. 소리 강도에 대한 인간의 인식이 본질적으로 대수적이기 때문이다.\n따라서 인지적 관점에서 보면 높은 진폭 값을 인코딩하는 것보다 낮은 진폭 값을 인코딩하는 데 더 많은 비트를 활용하는 것이 유리할 수 있다(낮은 진폭 값에서 인간은 소리 강도의 변화에 더 민감함).\n로그 양자화에 대한 한 가지 접근 방식으로 다음과 같이 정의되는 \\(\\mu\\)-law 인코딩이 있다. \\[F_\\mu(v) = \\mathrm{sgn}(v) \\frac{\\ln(1+ \\mu |v|)}{\\ln(1+\\mu)}\\] for values \\(v\\in[-1,1]\\), where \\(\\mathrm{sgn}\\) denotes the signum function\n파라미터 \\(\\mu\\in\\mathbb{R}_{>0}\\)는 적용되는 compression 정도를 결정하는 정수이다.\n실제로는 비균일 \\(8\\)비트 양자화 체계를 도출하기 위해 \\(\\mu=255\\)를 자주 사용한다. 인코딩 \\(F_\\mu\\)는 strictly 증가 단조 함수로, \\([-1,1]\\) 간격을 낮은 값은 확장되고 높은 값은 압축되도록 자기 자체에 매핑한다.\n그 역인 \\(\\mu\\)-law 디코딩(decoding) 은 다음과 같다. \\[F_\\mu^{-1}(v) = \\mathrm{sgn}(v) \\frac{(1 + \\mu)^{|v|}- 1}{\\mu}\\] for values \\(v\\in[-1,1]\\)\n다음 코드 셀에서 \\(F_\\mu\\) 함수와 그 역 \\(F_\\mu^{-1}\\)를 구현하고 다른 파라미터 \\(\\mu\\)에 대한 결과를 설명한다.\n\n\ndef encoding_mu_law(v, mu=255.0):\n    \"\"\"mu-law encoding\n\n    Args:\n        v (float): Value between -1 and 1\n        mu (float): Encoding parameter (Default value = 255.0)\n\n    Returns:\n        v_encode (float): Encoded value\n    \"\"\"\n    v_encode = np.sign(v) * (np.log(1.0 + mu * np.abs(v)) / np.log(1.0 + mu))\n    return v_encode\n\n\ndef decoding_mu_law(v, mu=255.0):\n    \"\"\"mu-law decoding\n\n    Args:\n        v (float): Value between -1 and 1\n        mu (float): Dencoding parameter (Default value = 255.0)\n\n    Returns:\n        v_decode (float): Decoded value\n    \"\"\"\n    v_decode = np.sign(v) * (1.0 / mu) * ((1.0 + mu)**np.abs(v) - 1.0)\n    return v_decode\n\n\ndef plot_mu_law(mu=255.0, figsize=(8, 3)):\n    \"\"\"Helper function for plotting a signal and its quantized version\n\n    Args:\n        mu (float): Dencoding parameter (Default value = 255.0)\n        figsize (tuple): Figure size (Default value = (8.5, 2))\n    \"\"\"\n    values = np.linspace(-1, 1, 1000)\n    values_encoded = encoding_mu_law(values, mu=mu)\n    values_decoded = encoding_mu_law(values, mu=mu)\n\n    plt.figure(figsize=figsize)\n    ax = plt.subplot(1, 2, 1)\n    ax.plot(values, values, color='k', label='Original values')\n    ax.plot(values, values_encoded, color='b', label='Encoded values')\n    ax.set_title(r'$\\mu$-law encoding with $\\mu=%.0f$' % mu)\n    ax.set_xlabel('$v$')\n    ax.set_ylabel(r'$F_\\mu(v)$')\n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n    ax.grid('on')\n    ax.legend()\n\n    ax = plt.subplot(1, 2, 2)\n    ax.plot(values, values, color='k', label='Original values')\n    ax.plot(values, values_decoded, color='b', label='Decoded values')\n    ax.set_title(r'$\\mu$-law decoding with $\\mu=%.0f$' % mu)\n    ax.set_xlabel('$v$')\n    ax.set_ylabel(r'$F_\\mu^{-1}(v)$')\n    ax.set_xlim([-1, 1])\n    ax.set_ylim([-1, 1])\n    ax.grid('on')\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_mu_law(mu=255.0)\nplot_mu_law(mu=7.0)\n\n\n\n\n\n\n\n\n비균일 양자화 구현\n\n먼저 주어진 신호를 \\(F_\\mu\\)를 사용하여 인코딩한 다음, 균일 양자화를 적용하고, 마지막으로 \\(F_\\mu^{-1}\\)를 사용하여 양자화된 신호를 디코딩해보자. 다음 코드에서 신호 샘플을 \\([-1,1]\\) 범위에 있는 경우로 제한한다. 뒤의 그림은 위의 정현파 예제 형식을 따르는 균일 양자화와 비균일 양자화의 차이를 보여준다.\n\n\ndef quantize_nonuniform_mu(x, mu=255.0, quant_level=256):\n    \"\"\"Nonuniform quantization approach using mu-encoding\n\n    Args:\n        x (np.ndarray): Original signal\n        mu (float): Encoding parameter (Default value = 255.0)\n        quant_level (int): Number of quantization levels (Default value = 256)\n\n    Returns:\n        x_quant (np.ndarray): Quantized signal\n    \"\"\"\n    x_en = encoding_mu_law(x, mu=mu)\n    x_en_quant = quantize_uniform(x_en, quant_min=-1, quant_max=1, quant_level=quant_level)\n    x_quant = decoding_mu_law(x_en_quant, mu=mu)\n    return x_quant\n\n\ndur = 5\nx, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=1, phase=0.0)\n\nquant_level = 8\nx_quant = quantize_uniform(x, quant_min=-1, quant_max=1, quant_level=quant_level)\nplot_signal_quant(x, t, x_quant, xlim=[0, dur], ylim=[-1.3,1.3], \ntitle=r'Uniform quantization with $\\lambda$=$%d$'%(quant_level));\n\nmu = 7\nx_quant = quantize_nonuniform_mu(x, mu=mu, quant_level=quant_level)\nplot_signal_quant(x, t, x_quant, xlim=[0, dur], ylim=[-1.3,1.3], \ntitle=r'Nonuniform quantization with $\\mu$=$%d$ and $\\lambda$=$%d$'%(mu, quant_level));\n\n\n\n\n\n\n\n\n위의 접근 방식은 두 개의 파라미터, \\(\\mu\\)(인코딩 파라미타) 및 \\(\\lambda\\)(간격 \\([-1,1]\\)의 양자화 레벨 수)에 따라 달라지는 비균일 양자화 함수를 생성한다. 다음 코드 셀에서는 다양한 파라미터 설정에 대한 양자화 오류(quantization error)와 양자화 함수의 그래프를 보여준다. 양자화 오류는 \\(|v|\\approx 1\\)인 \\(v\\) 값보다 \\(|v|\\approx 0\\)인 \\(v\\) 값에서 훨씬 더 낮다.\n\n\nplt.figure(figsize=(12,3))\nax = plt.subplot(1, 3, 1)\nplot_graph_quant_function(ax, mu=3, quant_level=4, quant='nonuniform')\nax = plt.subplot(1, 3, 2)\nplot_graph_quant_function(ax, mu=7, quant_level=8, quant='nonuniform')\nax = plt.subplot(1, 3, 3)\nplot_graph_quant_function(ax, mu=15, quant_level=16, quant='nonuniform')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#양자화-잡음-quantization-noise",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#양자화-잡음-quantization-noise",
    "title": "3.5. 디지털 신호",
    "section": "양자화 잡음 (Quantization Noise)",
    "text": "양자화 잡음 (Quantization Noise)\n\n신호의 양자화는 손실이 많은 작업이다. 이 과정에서 발생하는 왜곡을 양자화 잡음이라고 한다. 다음 코드 셀에서 샘플 값을 인코딩하기 위해 다른 수의 비트를 사용하여 C 장음계의 피아노 녹음을 양자화하여 이러한 왜곡의 impression을 줘본다. \\(b\\in\\mathbb{N}\\) 비트를 사용하여 \\(2^b\\)의 서로 다른 양자화 레벨을 인코딩할 수 있다.\n\n\ndef display_signal_quant(x, Fs, number_of_bits):\n    quant_level = 2 ** number_of_bits\n    x_quant = quantize_uniform(x, quant_min=-1, quant_max=1, quant_level=quant_level)    \n    print('Signal after uniform quantization (%d bits) :'%number_of_bits, flush=True)\n    ipd.display(ipd.Audio(x_quant, rate=Fs))\n    return x_quant\n\n\nx, Fs = librosa.load(\"../audio/piano_c_scale.wav\", sr=11025)\n\nprint('Original audio signal (16 bits):', flush=True)\nipd.display(ipd.Audio(x, rate=Fs) )\n\nx_quant = display_signal_quant(x=x, Fs=Fs, number_of_bits=8)\nx_quant = display_signal_quant(x=x, Fs=Fs, number_of_bits=4)\nx_quant = display_signal_quant(x=x, Fs=Fs, number_of_bits=2)\n\nOriginal audio signal (16 bits):\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSignal after uniform quantization (8 bits) :\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSignal after uniform quantization (4 bits) :\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nSignal after uniform quantization (2 bits) :\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n균일 및 비균일 양자화에 대한 양자화 노이즈\n\n위의 C 메이저 스케일의 피아노 녹음을 사용하여 양자화 노이즈 결과 균일 및 비균일 양자화를 비교한다. 샘플 값을 인코딩하기 위해 서로 다른 비트 수를 고려한다. 특히, \\(8\\) 비트 균일 양자화를 사용할 때보다 \\(8\\) 비트 비균일 양자화를 사용할 때 더 낮은 노이즈 레벨을 인지한다.\n\n\ndef compare_quant_signal(x, Fs, number_of_bits):\n    quant_level = 2 ** number_of_bits\n    mu = quant_level-1\n    x_qu = quantize_uniform(x, quant_min=-1, quant_max=1, quant_level=quant_level)    \n    x_qn = quantize_nonuniform_mu(x, mu=mu, quant_level=quant_level)\n    audio_player_list([x, x_qu, x_qn], [Fs, Fs, Fs], width=160, \n                columns=['Original (16 bits)', 'Uniform (%d bits)'%number_of_bits, 'Nonuniform (%d bits)'%number_of_bits])\n\n\nx, Fs = librosa.load(\"../audio/piano_c_scale.wav\", sr=11025)\n\ncompare_quant_signal(x, Fs, number_of_bits=8)\ncompare_quant_signal(x, Fs, number_of_bits=4)\ncompare_quant_signal(x, Fs, number_of_bits=2)\n\n\n\n  \n    \n      Original (16 bits)\n      Uniform (8 bits)\n      Nonuniform (8 bits)\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\n\n\n  \n    \n      Original (16 bits)\n      Uniform (4 bits)\n      Nonuniform (4 bits)\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\n\n\n  \n    \n      Original (16 bits)\n      Uniform (2 bits)\n      Nonuniform (2 bits)\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element."
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#간섭interference",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#간섭interference",
    "title": "3.5. 디지털 신호",
    "section": "간섭(Interference)",
    "text": "간섭(Interference)\n\n신호 처리에서 간섭(interference)은 한 파동이 비슷한 주파수의 다른 파동과 중첩(superimposed)될 때 발생한다. 한 파동의 파고(crest)가 어떤 지점에서 다른 파동의 파고와 만나면 일정 기간 동안 개별적 크기가 합산되며 이를 보강 간섭(constructive interference)이라고 한다. 반대로, 한 파동의 파고가 다른 파동의 파고와 만나면 크기가 일정 시간 동안 상쇄되는데, 이를 상쇄 간섭(destructive interference)이라고 한다.\n\n\ndef plot_interference(x1, x2, t, figsize=(6, 2), xlim=None, ylim=None, title=''):\n    \"\"\"Helper function for plotting two signals and its superposition\n    Args:\n        x1: Signal 1\n        x2: Signal 2\n        t: Time\n        figsize: figure size (Default value = (8, 2))\n        xlim: x limits (Default value = None)\n        ylim: y limits (Default value = None)\n        title: figure title (Default value = '')\n    \"\"\"\n    plt.figure(figsize=figsize)\n    plt.plot(t, x1, color='gray', linewidth=.5, linestyle='-', label='x1', alpha=.6)\n    plt.plot(t, x2, color='cyan', linewidth=.5, linestyle='-', label='x2', alpha=.6)\n    plt.plot(t, x1+x2, color='red', linewidth=1.0, linestyle='-', label='x1+x2', alpha=.6)\n    if xlim is None:\n        plt.xlim([0, t[-1]])\n    else:\n        plt.xlim(xlim)\n    if ylim is not None:\n        plt.ylim(ylim)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.title(title)\n    plt.legend(loc='upper right')\n    plt.tight_layout()\n    plt.show()\n\n\ndur = 5\nx1, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=1.05, phase=0.0)\nx2, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=0.95, phase=0.8)\nplot_interference(x1, x2, t, xlim=[0, dur], ylim=[-2.2,2.2], title='Constructive Interference');\n\ndur = 5\nx1, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=1.05, phase=0.0)\nx2, t = generate_sinusoid(dur=dur, Fs=1000, amp=1, freq=1.00, phase=0.4)\nplot_interference(x1, x2, t, xlim=[0, dur], ylim=[-2.2,2.2], title='Destructive Interference');"
  },
  {
    "objectID": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#비팅beating",
    "href": "posts/3. Fourier Anaylsis of Signals/3.5.Digital_Signals.html#비팅beating",
    "title": "3.5. 디지털 신호",
    "section": "비팅(Beating)",
    "text": "비팅(Beating)\n\n앞의 그림은 주파수가 비슷한 두 정현파가 더해지거나(보강 간섭) 상쇄(상쇄 간섭)될 수 있음을 보여주었다.\n\\(f_1(t)=\\sin(2\\pi \\omega_1 t)\\) 및 \\(f_2(t)=\\sin(2\\pi \\omega_2 t)\\)를 뚜렷하지만 가까운 주파수 \\(\\omega_1\\approx\\omega_2\\)의 두 정현파라고 하자.\n이제 이 두 정현파의 중첩 \\(f_1+f_2\\)가 진폭이 천천히 변하는 단일 사인파처럼 보이는 함수를 생성한다는 것을 볼 수 있다. 이 현상은 비팅(beating) 이라고 한다. 수학적으로 이 현상은 삼각 항등식 (trigonometric identity)의 결과이다. \\[\\sin(2\\pi \\omega_1t)+\\sin(2\\pi \\omega_2t)=\n2\\cos\\left(2\\pi\\frac{\\omega_1-\\omega_2}{2}t\\right)\\sin\\left(2\\pi\\frac{\\omega_1+\\omega_2}{2}t\\right).\\]\n\\(\\omega_1-\\omega_2\\)의 차이가 작으면 코사인 항은 사인 항에 비해 빈도가 낮다.\n결과적으로 신호 \\(f_1+f_2\\)는 주파수 \\(|\\omega_1-\\omega_2|\\)의 천천히 변하는 진폭 포락선(amplitude envelope)을 가지는 주파수 \\((\\omega_1+\\omega_2)/2\\)의 사인파로 볼 수 있다.\n이 비율은 코사인 항의 빈도 \\((\\omega_1-\\omega_2)/2\\)의 두 배이다.\n\n\nFs = 4000\ndur = 5\nx1, t = generate_sinusoid(dur=dur, Fs=Fs, amp=0.5, freq=200)\nx2, t = generate_sinusoid(dur=dur, Fs=Fs, amp=0.5, freq=203)\nplot_interference(x1, x2, t, ylim=[-1.1,1.1], xlim=[0, dur],\n    title=r'Beating with beating frequency $|\\omega_1-\\omega_2|=3$ ($\\omega_1=200, \\omega_2=203$)');\nplot_interference(x1, x2, t, ylim=[-1.1,1.1], xlim=[1.115, 1.225], title=r'Zoom-in section');\n\nipd.display(ipd.Audio(x1+x2, rate=Fs))\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n처프(Chirp) 실험\n\n비팅(beating)효과를 설명하기 위해 처프 신호(시간에 따라 주파수가 증가)를 보자.\n\\(\\omega_0,\\omega_1\\in\\mathbb{R}\\)를 두 주파수 파라미터(헤르츠 단위)라고 하고, \\(T\\in\\mathbb{R}\\)를 듀레이션 파라미터(초단위)라고 하자.\n\\(\\omega_0\\)에서 시작하여 \\(\\omega_1\\)로 주파수가 선형적으로 증가하는 듀레이션 \\(d\\)의 선형 처프는 다음과 같이 계산된다.\n\n$ f(t)=( t^2 + 2_0t)$ for \\(t\\in[0,T]\\)\n\n시간 \\(t\\)에서 처프 신호 \\(f\\)의 순간 주파수 (instantaneous frequency) 는 정현파의 인수를 \\(2\\pi\\)로 나눈 것의 도함수로 주어진다.\n\n\\[\n   g(t) = \\frac{\\omega_1-\\omega_0}{T} t + \\omega_0.\n\\]\n\n주파수 \\(\\omega_0=220.0~\\mathrm{Hz}\\)(피치 \\(\\mathrm{A3}\\))에서 시작하여 주파수 \\(\\omega_1=311.1~\\mathrm{Hz}\\)(피치 \\(\\mathrm{E}^\\flat 4\\))로 끝나는 듀레이션 \\(T=20~\\mathrm{sec}\\)의 선형 처프 신호를 보자.\n또한 동일한 듀레이션의 주파수 \\(261.5~\\mathrm{Hz}\\)(피치 \\(\\mathrm{C4}\\))를 갖는 정현파를 생각해보자.\n이 신호의 중첩을 들을 때, 처음에는 \\(\\mathrm{A3}\\) 및 \\(\\mathrm{C4}\\) 두 개의 개별 피치를 인식한다.\n처프가 \\(\\mathrm{C4}\\)에 가까워지면 두 음이 하나의 소리로 합쳐지기 시작한다. 동시에 처음에는 속도가 느려졌다가 사라지고(처프가 \\(\\mathrm{C4}\\)에 도달하면), 다시 속도가 빨라지는 비팅 효과를 볼 수 있다. 마지막에 다시 \\(\\mathrm{E}^\\flat 4\\) 및 \\(\\mathrm{C4}\\)인 두 피치를 인식한다.\n\n\ndef generate_chirp_linear(dur, freq_start, freq_end, amp=1.0, Fs=22050):\n    \"\"\"Generation chirp with linear frequency increase\n\n    Args:\n        dur (float): Duration (seconds) of the signal\n        freq_start (float): Start frequency of the chirp\n        freq_end (float): End frequency of the chirp\n        amp (float): Amplitude of chirp (Default value = 1.0)\n        Fs (scalar): Sampling rate (Default value = 22050)\n\n    Returns:\n        x (np.ndarray): Generated chirp signal\n        t (np.ndarray): Time axis (in seconds)\n        freq (np.ndarray): Instant frequency (in Hz)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    a = (freq_end - freq_start) / dur\n    freq = a * t + freq_start\n    x = amp * np.sin(np.pi * a * t ** 2 + 2 * np.pi * freq_start * t)\n    return x, t, freq\n\n\nf_pitch = lambda p: 440 * 2 ** ((p - 69) / 12)\n\nFs = 4000\ndur = 20\nfreq_start = f_pitch(57)   # A3\nfreq_end = f_pitch(63)     # Eflatp4\nfreq_sin = f_pitch(60)     # C4\nx1, t, freq = generate_chirp_linear(dur=dur, freq_start=freq_start, freq_end=freq_end, amp=0.5, Fs=Fs)\nx2, t = generate_sinusoid(dur=dur, Fs=Fs, amp=0.5, freq=freq_sin)\n\ny = x1 + x2\nipd.display(ipd.Audio(y, rate=Fs))\nplot_interference(x1, x2, t, xlim=[0, dur], ylim=[-1.1,1.1], \n    title=r'Superposition of a linear chirp $x_1$ (A3 to E$^\\flat$4) and sinusoid $x_2$ (C4)');\nplot_interference(x1, x2, t, xlim=[7, 11], ylim=[-1.1,1.1], title='Zoom-in section');\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C2/C2.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "",
    "text": "음악 작품의 음악적 정보를 시간순으로 정렬하여 동기화(synchronize)하는데 필요한 오디오 피쳐(audio feature)를 소개한다. 특히 크로마(chroma) 기반 음악 피쳐의 개념을 알아본다."
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#stft와-피치-주파수",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#stft와-피치-주파수",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "STFT와 피치 주파수",
    "text": "STFT와 피치 주파수\n\n평균율(equal-tempered scale)에 따라 피치를 의미 있게 분류할 수 있는 음악을 다루고 있다고 가정하고, 오디오 녹음이 다양한 피치에 걸쳐 신호 에너지의 분포를 나타내는 특징 표현(feature representation)으로 변환될 수 있는 방법을 보자. (앞으로 “feature”를 피쳐 혹은 특징으로 해석해 서술하겠다.)\n이러한 특징(feature)은 선형 주파수 축(Hertz로 측정)을 로그(log) 축(피치로 측정)으로 변환하여 스펙트로그램에서 얻을 수 있다. 이 결과를 로그 주파수 스펙트로그램(log-frequency spectrogram) 이라고도 한다.\n시작하기 앞서 이산 STFT가 필요하다. \\(x\\)를 헤르츠로 주어진 고정 샘플링 속도 \\(F_\\mathrm{s}\\)에 대해 등거리(equidistant) 샘플링으로 얻은 실수 값 이산 신호라고 하자. 또한, \\(\\mathcal{X}\\)를 길이 \\(N\\in\\mathbb{N}\\) 및 홉(hop) 크기 \\(H\\in\\mathbb{N}\\)의 윈도우 \\(w\\)에 대한 이산 STFT라고 하자.\n제로-패딩을 적용하면 푸리에 계수 \\(\\mathcal{X}(n,k)\\)가 프레임 파라미터 \\(n\\in\\mathbb{Z}\\)와 주파수 파라미터 \\(k\\in[0: K]\\)로 인덱싱된다. 여기서 \\(K=N/2\\)는 Nyquist 주파수에 해당하는 주파수 인덱스이다.\n각 푸리에 계수 \\(\\mathcal{X}(n,k)\\)는 초 단위로 주어진 물리적 시간 위치 \\(T_\\mathrm{coef}(n) = nH/F_\\mathrm{s}\\)와 연관되어 있다.\n\n\\(F_\\mathrm{coef}(k) = \\frac{k \\cdot F_\\mathrm{s}}{N}\\)\n\n로그 주파수 스펙트로그램의 주요 아이디어는 평균율 스케일의 로그 간격 주파수 분포에 해당하도록 주파수 축을 재정의하는 것이다. MIDI 음표 번호로 피치를 식별하면(음표 A4는 MIDI 음표 번호 \\(p=69\\)에 해당), 중심 주파수는 다음과 같이 지정된다:\n\n\\(F_\\mathrm{pitch}(p) = 2^{(p-69)/12} \\cdot 440.\\)\n\n예를 들어 피아노 연주 반음계(chromatic scale)를 고려해보자. 결과의 스펙트로그램은 연주된 음의 피치에 대한 기본 주파수(fundamental frequency)의 기하급수적 종속성(exponential dependency)을 나타낸다. 또한 화음과 음의 온셋(onset) 위치(수직 구조)가 명확하게 표시된다.\n\n\nImage(\"../img/4.music_synchronization/f.4.1a.png\", width=500)\n\n\n\n\n\npiano_chromatic = \"../data_FMP/FMP_C3_F03.wav\"\nipd.Audio(piano_chromatic)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Load wav\nx, Fs = librosa.load(piano_chromatic)\n\n# Compute Magnitude STFT\nN = 4096\nH = 1024\nY, T_coef, F_coef = stft_convention_fmp(x, Fs, N, H, mag=True)\n# Y = np.abs(Y) ** 2 (if mag=False)\n\n\n# Plot spectrogram\nfig = plt.figure(figsize=(8, 3))\neps = np.finfo(float).eps\nplt.imshow(10 * np.log10(eps + Y), origin='lower', aspect='auto', cmap='gray_r', \n           extent=[T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]])\nplt.title(\"Magnitude spectrogram\")\nplt.clim([-30, 30])\nplt.ylim([0, 4500])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (Hz)')\ncbar = plt.colorbar()\ncbar.set_label('Magnitude (dB)')\nplt.tight_layout()\n\n# Plot rectangle corresponding to pitch C3 (p=48)\nrect = matplotlib.patches.Rectangle((29.3, 0.5), 1.2, 4490, linewidth=3, \n                                    edgecolor='r', facecolor='none')\nplt.gca().add_patch(rect)\nplt.text(28, -400, r'$\\mathrm{C3}$', color='r', fontsize='x-large');"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#로그-주파수-풀링pooling",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#로그-주파수-풀링pooling",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "로그 주파수 풀링(Pooling)",
    "text": "로그 주파수 풀링(Pooling)\n\n주파수에 대한 로그 인식은 평균율 음계의 피치로 레이블이 지정된 로그 주파수 축으로 시간-주파수 표현을 사용하도록 한다.\n주어진 스펙트로그램에서 그러한 표현을 도출하기 위한 기본 아이디어로는 각 스펙트럼 계수(sprectral coefficient) \\(\\mathcal{X}(n,k)\\)를 주파수 \\(F_\\mathrm{coef}(k)\\)에 가장 가까운 중심 주파수의 피치에 할당하는 것이다.\n보다 정확하게는 각 피치 \\(p\\in[0:127]\\)에 대해 다음의 집합을 정의한다. \\[\\begin{equation}\n  P(p) := \\{k:F_\\mathrm{pitch}(p-0.5) \\leq   F_\\mathrm{coef}(k) <  F_\\mathrm{pitch}(p+0.5)\\}.\n\\end{equation}\\]\n\\(P(p)\\)에 포함되는 주파수 범위는 로그 방식의 주파수에 따라 달라진다. 피치 \\(p\\)의 대역폭(bandwidth) \\(\\mathrm{BW}(p)\\)를 다음과 같이 정의한다.\n\n\\[\\mathrm{BW}(p):=F_\\mathrm{pitch}(p+0.5)-F_\\mathrm{pitch}(p-0.5)\\]\n\n대역폭 \\(\\mathrm{BW}(p)\\)는 피치가 낮아질수록 작아진다. 특히 피치를 한 옥타브 내리면 반으로 감소한다. 예를 들어, MIDI 피치 \\(p=66\\)의 경우 대역폭은 대략 \\(21.4~\\mathrm{Hz}\\)인 반면 \\(p=54\\)의 경우 대역폭은 \\(10.7~\\mathrm{Hz}\\) 아래로 떨어진다.\n다음 표는 다양한 음표와 MIDI 음표 번호 \\(p\\), 중심 주파수 \\(F_\\mathrm{pitch}(p)\\), 컷오프(cutoff) 주파수 \\(F_\\mathrm{pitch}(p-0.5)\\) 및 \\(F_\\mathrm{pitch}(p+0.5)\\) 및 대역폭 \\(\\mathrm{BW}(p)\\)을 보여준다.\n\n\ndef note_name(p):\n    \"\"\"Returns note name of pitch\n\n    Args:\n        p (int): Pitch value\n\n    Returns:\n        name (str): Note name\n    \"\"\"\n    chroma = ['A', 'A$^\\\\sharp$', 'B', 'C', 'C$^\\\\sharp$', 'D', 'D$^\\\\sharp$', 'E', 'F', 'F$^\\\\sharp$', 'G',\n              'G$^\\\\sharp$']\n    name = chroma[(p - 69) % 12] + str(p // 12 - 1)\n    return name\n\n\nf_pitch = lambda p: 440 * 2 ** ((p - 69) / 12)\n   \nnote_infos = []\nfor p in range(60, 73):\n    name = note_name(p)\n    p_pitch = f_pitch(p)\n    p_pitch_lower = f_pitch(p - 0.5)\n    p_pitch_upper = f_pitch(p + 0.5)\n    bw = p_pitch_upper - p_pitch_lower\n    note_infos.append([name, p, p_pitch, p_pitch_lower, p_pitch_upper, bw])\n\ndf = pd.DataFrame(note_infos, columns=['Note', '$p$', \n                                       '$F_\\mathrm{pitch}(p)$', \n                                       '$F_\\mathrm{pitch}(p-0.5)$', \n                                       '$F_\\mathrm{pitch}(p+0.5)$', \n                                       '$\\mathrm{BW}(p)$'])\n\n\nhtml = df.to_html(index=False, float_format='%.2f')\nhtml = html.replace('<table', '<table style=\"width: 80%\"')\nipd.HTML(html)\n\n\n\n  \n    \n      Note\n      $p$\n      $F_\\mathrm{pitch}(p)$\n      $F_\\mathrm{pitch}(p-0.5)$\n      $F_\\mathrm{pitch}(p+0.5)$\n      $\\mathrm{BW}(p)$\n    \n  \n  \n    \n      C4\n      60\n      261.63\n      254.18\n      269.29\n      15.11\n    \n    \n      C$^\\sharp$4\n      61\n      277.18\n      269.29\n      285.30\n      16.01\n    \n    \n      D4\n      62\n      293.66\n      285.30\n      302.27\n      16.97\n    \n    \n      D$^\\sharp$4\n      63\n      311.13\n      302.27\n      320.24\n      17.97\n    \n    \n      E4\n      64\n      329.63\n      320.24\n      339.29\n      19.04\n    \n    \n      F4\n      65\n      349.23\n      339.29\n      359.46\n      20.18\n    \n    \n      F$^\\sharp$4\n      66\n      369.99\n      359.46\n      380.84\n      21.37\n    \n    \n      G4\n      67\n      392.00\n      380.84\n      403.48\n      22.65\n    \n    \n      G$^\\sharp$4\n      68\n      415.30\n      403.48\n      427.47\n      23.99\n    \n    \n      A4\n      69\n      440.00\n      427.47\n      452.89\n      25.42\n    \n    \n      A$^\\sharp$4\n      70\n      466.16\n      452.89\n      479.82\n      26.93\n    \n    \n      B4\n      71\n      493.88\n      479.82\n      508.36\n      28.53\n    \n    \n      C5\n      72\n      523.25\n      508.36\n      538.58\n      30.23"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#로그-주파수-스펙트로그램",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#로그-주파수-스펙트로그램",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "로그-주파수 스펙트로그램",
    "text": "로그-주파수 스펙트로그램\n\n집합 \\(P(p)\\)를 기반으로 다음의 간단한 풀링 절차를 사용하여 로그-주파수 스펙트로그램 \\(\\mathcal{Y}_\\mathrm{LF}:\\mathbb{Z}\\times [0:127]\\)를 얻는다.\n\n\\[\\mathcal{Y}_\\mathrm{LF}(n,p) := \\sum_{k \\in P(p)}{|\\mathcal{X}(n,k)|^2}\\]\n\n이 절차에 따라 주파수 축은 로그적으로 분할되고 MIDI 피치에 따라 선형으로 레이블이 지정된다.\n\n\ndef f_pitch(p, pitch_ref=69, freq_ref=440.0):\n    \"\"\"Computes the center frequency/ies of a MIDI pitch\n\n    Args:\n        p (float): MIDI pitch value(s)\n        pitch_ref (float): Reference pitch (default: 69)\n        freq_ref (float): Frequency of reference pitch (default: 440.0)\n\n    Returns:\n        freqs (float): Frequency value(s)\n    \"\"\"\n    return 2 ** ((p - pitch_ref) / 12) * freq_ref\n\n\ndef pool_pitch(p, Fs, N, pitch_ref=69, freq_ref=440.0):\n    \"\"\"Computes the set of frequency indices that are assigned to a given pitch\n    \n    Args:\n        p (float): MIDI pitch value\n        Fs (scalar): Sampling rate\n        N (int): Window size of Fourier fransform\n        pitch_ref (float): Reference pitch (default: 69)\n        freq_ref (float): Frequency of reference pitch (default: 440.0)\n\n    Returns:\n        k (np.ndarray): Set of frequency indices\n    \"\"\"\n    lower = f_pitch(p - 0.5, pitch_ref, freq_ref)\n    upper = f_pitch(p + 0.5, pitch_ref, freq_ref)\n    k = np.arange(N // 2 + 1)\n    k_freq = k * Fs / N  # F_coef(k, Fs, N)\n    mask = np.logical_and(lower <= k_freq, k_freq < upper)\n    return k[mask]\n\n\ndef compute_spec_log_freq(Y, Fs, N):\n    \"\"\"Computes a log-frequency spectrogram\n\n    Args:\n        Y (np.ndarray): Magnitude or power spectrogram\n        Fs (scalar): Sampling rate\n        N (int): Window size of Fourier fransform\n\n    Returns:\n        Y_LF (np.ndarray): Log-frequency spectrogram\n        F_coef_pitch (np.ndarray): Pitch values\n    \"\"\"\n    Y_LF = np.zeros((128, Y.shape[1]))\n    for p in range(128):\n        k = pool_pitch(p, Fs, N)\n        Y_LF[p, :] = Y[k, :].sum(axis=0)\n    F_coef_pitch = np.arange(128)\n    return Y_LF, F_coef_pitch\n\n\nY_LF, F_coef_pitch = compute_spec_log_freq(Y, Fs, N)        \n\nfig = plt.figure(figsize=(8, 3))\nplt.imshow(10 * np.log10(eps + Y_LF), origin='lower', aspect='auto', cmap='gray_r', \n           extent=[T_coef[0], T_coef[-1], 0, 127])\nplt.title(\"Pitch-based log-frequency spectrogram\")\nplt.clim([-10, 50])\nplt.ylim([21, 108])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (pitch)')\ncbar = plt.colorbar()\ncbar.set_label('Magnitude (dB)')\nplt.tight_layout()\n\n# Create a Rectangle patch\nrect = matplotlib.patches.Rectangle((29.3, 21), 1.2, 86.5, linewidth=3, edgecolor='r', facecolor='none')\nplt.gca().add_patch(rect)\nplt.text(28, 15, r'$\\mathrm{C3}$', color='r', fontsize='x-large');\n\n\n\n\n\n스펙트로그램를 보면 다음과 같은 몇 가지 흥미로운 사실을 관찰할 수 있다.\n\n일반적으로 높은 음의 소리는 낮은 음의 소리보다 더 깨끗한 고조파 스펙트럼을 가진다. 낮은 음의 경우 신호의 에너지가 높은 고조파에 포함되는 경우가 많지만, 청취자는 여전히 낮은 피치의 소리로 인식할 수 있다.\n스펙트로그램에 표시된 수직 줄무늬(주파수 축을 따라)는 일부 신호 에너지가 스펙트럼의 큰 부분에 퍼져 있음을 나타낸다. 에너지 확산의 주요 원인은 건반 입력(기계적 소음)으로 인한 피아노 음향의 불협화음과 과도 및 공명 효과 때문이다.\n또한 소리의 주파수 내용은 마이크의 주파수 응답에 따라 달라진다. 예를 들어 마이크는 위의 오디오 예제의 경우와 같이 특정 임계값 이상의 주파수만 캡처할 수 있다. 이것은 또한 음 A0(\\(p=21\\))에서 B0(\\(p=32\\))에 대한 기본 주파수에서 가시적으로 에너지가 거의 없는 이유를 설명할 수 있다.\n\n음향적인 속성 외에도 이산 STFT를 기반으로 하는 풀링 전략을 사용할 때 낮은 피치를 제대로 표현하지 못하는 또 다른 이유가 있다. 이산 STFT는 주파수 축의 선형 샘플링을 도입하지만 풀링(pooling) 전략에 사용되는 대역폭은 로그 방식으로 주파수에 따라 달라진다. 그 결과, 집합 \\(P(p)\\)는 매우 적은 수의 스펙트럼 계수만 포함하거나 작은 \\(p\\) 값에 대해서는 비어 있을 수도 있다(위 그림에서 가로 흰색 줄무늬의 이유임).\n\n\nprint('Sampling rate: Fs = ', Fs)\nprint('Window size: N = ', N)\nprint('STFT frequency resolution (in Hz): Fs/N = %4.2f' % (Fs / N))\n\nfor p in [76, 64, 52, 40, 39, 38]:\n    print('Set P(%d) = %s' % (p, pool_pitch(p, Fs, N)))\n\nSampling rate: Fs =  22050\nWindow size: N =  4096\nSTFT frequency resolution (in Hz): Fs/N = 5.38\nSet P(76) = [119 120 121 122 123 124 125 126]\nSet P(64) = [60 61 62 63]\nSet P(52) = [30 31]\nSet P(40) = [15]\nSet P(39) = []\nSet P(38) = [14]"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#크로마그램-chromagram",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#크로마그램-chromagram",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "크로마그램 (Chromagram)",
    "text": "크로마그램 (Chromagram)\n\n이제 음색(timbre)과 악기에 따른 로그-주파수 스펙트로그램의 견고성을 높이는 전략에 대해 논의해본다. 하나 또는 여러 옥타브 차이의 피치에 해당하는 피치 밴드를 적절하게 결합하는 것이다.\n음정에 대한 인간의 인식은 두 음정이 한 옥타브 차이가 나는 경우 “색상”(유사한 하모닉 역할을 함)이 유사한 것으로 인식된다는 점에서 주기적이다. 이 관찰을 기반으로 피치는 톤 높이(tone height)와 크로마(chroma)라고 하는 두 가지 구성 요소로 분리될 수 있다. 톤 높이는 다음의 집합에 포함된 각 피치 스펠링 속성에 대한 옥타브 수와 크로마를 나타낸다.\n\n\\(\\{ \\mathrm{C},\\mathrm{C}^\\sharp,\\mathrm{D},\\mathrm{D}^\\sharp,\\ldots,\\mathrm{B} \\}\\)\n\n\n(크로마와 피치 클래스에 대해서는 2.1.악보 에서 다룬바 있다.)\n\n크로마 값을 enumerate하면서 이 집합을 \\([0:11]\\)로 식별할 수 있다. 여기서 \\(0\\)는 크로마 \\(\\mathrm{C}\\), \\(1\\)는 \\(\\mathrm{C}^\\sharp\\) 등을 나타낸다. 피치 클래스는 동일한 크로마를 공유하는 모든 피치의 집합으로 정의된다.\n크로마 특징의 기본 아이디어는 주어진 피치 클래스와 관련된 모든 스펙트럼 정보를 단일 계수로 집계하는 것이다. 피치 기반 로그-주파수 스펙트로그램 \\(\\mathcal{Y}_\\mathrm{LF}:\\mathbb{Z}\\times[0:127]\\to \\mathbb{R}_{\\geq 0}\\)가 주어지면, 크로마 표현 또는 크로마그램 \\(\\mathbb{Z}\\times[0:11]\\to \\mathbb{R}_{\\geq 0}\\)는 다음의 동일한 채도에 속한 모든 피치 계수를 더해 구할 수 있다.\n\n\\(\\mathcal{C}(n,c) := \\sum_{\\{p \\in [0:127]\\,:\\,p\\,\\mathrm{mod}\\,12 = c\\}}{\\mathcal{ Y}_\\mathrm{LF}(n,p)}\\) for \\(c\\in[0:11]\\).\n\n다음 코드 예에서 크로마 피쳐의 순환적(cyclic) 특성을 볼 수 있는 크로마틱 스케일의 크로마그램을 생성한다. 옥타브 등가(equivalence)로 인해 크로마틱 스케일의 증가하는 음은 크로마 축을 둘러싼다(wrapped around).\n로그-주파수 스펙트로그램과 마찬가지로 오디오 예제의 결과 크로마그램은 특히 낮은 음에 대해 노이즈가 많다. 게다가 고조파가 존재하기 때문에 에너지는 일반적으로 한 번에 하나의 음을 연주할 때에도 다양한 크로마 밴드에 분산된다.\n예를 들어 C3 음표를 연주하면 세 번째 배음(harmonic)은 G4에 해당하고 다섯 번째 배음은 E5에 해당한다. 따라서 C3음을 피아노로 연주할 때 크로마 밴드 \\(\\mathrm{C}\\)뿐만 아니라 크로마 밴드 \\(\\mathrm{G}\\) 및 \\(\\mathrm{E}\\)도 신호의 에너지의 상당한 부분을 포함하고 있다.\n\n\ndef compute_chromagram(Y_LF):\n    \"\"\"Computes a chromagram\n    \n    Args:\n        Y_LF (np.ndarray): Log-frequency spectrogram\n\n    Returns:\n        C (np.ndarray): Chromagram\n    \"\"\"\n    C = np.zeros((12, Y_LF.shape[1]))\n    p = np.arange(128)\n    for c in range(12):\n        mask = (p % 12) == c\n        C[c, :] = Y_LF[mask, :].sum(axis=0)\n    return C\n\n\nC = compute_chromagram(Y_LF)\n\nfig = plt.figure(figsize=(8, 3))\nplt.title('Chromagram')\nchroma_label = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\nplt.imshow(10 * np.log10(eps + C), origin='lower', aspect='auto', cmap='gray_r', \n           extent=[T_coef[0], T_coef[-1], 0, 12])\nplt.clim([10, 50])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Chroma')\ncbar = plt.colorbar()\ncbar.set_label('Magnitude (dB)')\nplt.yticks(np.arange(12) + 0.5, chroma_label)\nplt.tight_layout()\n\nrect = matplotlib.patches.Rectangle((29.3, 0.0), 1.2, 12, linewidth=3, edgecolor='r', facecolor='none')\nplt.gca().add_patch(rect)\nplt.text(28.5, -1.2, r'$\\mathrm{C3}$', color='r', fontsize='x-large');\n\n\n\n\nExample: Burgmüller\n\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F05.wav\")\nipd.display(ipd.Audio(x, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nN = 4096\nH = 512\nX, T_coef, F_coef = stft_convention_fmp(x, Fs, N, H)\neps = np.finfo(float).eps\nY = np.abs(X) ** 2\nY_LF, F_coef_pitch = compute_spec_log_freq(Y, Fs, N)\nC = compute_chromagram(Y_LF)\n\nfig = plt.figure(figsize=(8,3))\nplt.imshow(10 * np.log10(eps + Y_LF), origin='lower', aspect='auto', cmap='gray_r', \n           extent=[T_coef[0], T_coef[-1], 0, 128])\nplt.clim([-10, 50])\nplt.ylim([55, 92])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Frequency (pitch)')\ncbar = plt.colorbar()\ncbar.set_label('Magnitude (dB)')\nplt.tight_layout()\n\nfig = plt.figure(figsize=(8, 2.5))\nplt.imshow(10 * np.log10(eps + C), origin='lower', aspect='auto', cmap='gray_r', \n           extent=[T_coef[0], T_coef[-1], 0, 12])\nplt.clim([0, 50])\nplt.xlabel('Time (seconds)')\nplt.ylabel('Chroma')\ncbar = plt.colorbar()\ncbar.set_label('Magnitude (dB)')\nplt.yticks(np.arange(12) + 0.5, chroma_label)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n#librosa example \nN = 4096\nH = 512\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F05.wav\")\nC = librosa.feature.chroma_stft(y=x, tuning=0, norm=None, hop_length=H, n_fft=N)\nplt.figure(figsize=(8, 2))\nlibrosa.display.specshow(10 * np.log10(eps + C), x_axis='time', \n                         y_axis='chroma', sr=Fs, hop_length=H)\nplt.colorbar();"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축-함수-compression-function",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축-함수-compression-function",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "압축 함수 (Compression Function)",
    "text": "압축 함수 (Compression Function)\n\n음악 신호 처리에서 스펙트로그램이나 크로마그램의 문제는 그 값이 큰 동적 범위(dynamic range)를 갖는다는 것이다. 결과적으로 작지만 여전히 관련성이 높은 값이 큰 값에 의해 가려질 수 있다. 따라서 데시벨 단위(decibel scale)를 사용하는 경우가 많다. 큰 값과 작은 값의 차이를 줄이고 작은 값을 향상시키는 효과로 이러한 불일치의 균형을 맞추는 것이다. 보다 일반적으로 로그 압축(=컴프레션) 이라고 하는 단계인 다른 유형의 로그-기반 함수를 적용할 수 있습니다.\n\\(\\gamma\\in\\mathbb{R}_{>0}\\)를 양의 상수로 두고, 다음과 같이 함수 \\(\\Gamma_\\gamma:\\mathbb{R}_{>0} \\to \\mathbb{R}_{>0}\\)를 정의할 수 있다.\n\n\\(\\Gamma_\\gamma(v):=\\log(1+ \\gamma \\cdot v)\\) for some positive value \\(v\\in\\mathbb{R}_{>0}\\)\n\n\\(\\mathrm{dB}\\) 함수와 달리 \\(\\Gamma_\\gamma\\) 함수는 모든 양수 값 \\(v\\in\\mathbb{R}_{>0}\\)에 대해 양수 값 \\(\\Gamma_\\gamma(v)\\)를 생성한다.\n압축 정도는 상수 \\(\\gamma\\)로 조정할 수 있다. \\(\\gamma\\)가 클수록 압축이 커진다.\n\n\ndef log_compression(v, gamma=1.0):\n    \"\"\"Logarithmically compresses a value or array\n\n    Args:\n        v (float or np.ndarray): Value or array\n        gamma (float): Compression factor (Default value = 1.0)\n\n    Returns:\n        v_compressed (float or np.ndarray): Compressed value or array\n    \"\"\"\n    return np.log(1 + gamma * v)\n\n\nv = np.arange(1001) / 100\n\nplt.figure(figsize=(4, 3.5))\nplt.plot(v, v, color='black', linestyle=':', label='Identity')\nplt.plot(v, log_compression(v, gamma=1), color='blue', label='$\\gamma$ = 1')\nplt.plot(v, log_compression(v, gamma=10), color='gray', label='$\\gamma$ = 10')\nplt.plot(v, log_compression(v, gamma=100), color='red', label='$\\gamma$ = 100')\nplt.xlabel('Original values')\nplt.ylabel('Compressed values')\nplt.xlim([v[0], v[-1]])\nplt.ylim([v[0], v[-1]])\n# plt.tick_params(direction='in')\nplt.legend(loc='upper left', fontsize=12)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축된-스펙트로그램-compressed-spectrogram",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축된-스펙트로그램-compressed-spectrogram",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "압축된 스펙트로그램 (Compressed Spectrogram)",
    "text": "압축된 스펙트로그램 (Compressed Spectrogram)\n\n스펙트로그램과 같은 양수 값을 가진 표현의 경우 \\(\\Gamma_\\gamma\\) 함수를 각 값에 적용하여 압축된 버전을 얻는다. 예를 들어, 스펙트로그램 \\(\\mathcal{Y}\\)의 경우 압축된 버전은 다음과 같이 정의된 concatenation \\(\\Gamma_\\gamma \\circ \\mathcal{Y}\\)이다.\n\n\\((\\Gamma_\\gamma\\circ \\mathcal{Y})(n,k):=\\log(1+ \\gamma \\cdot \\mathcal{Y}(n,k))\\)\n\n여기서 \\(n\\)은 시간 프레임 파라미터이고 \\(k\\)는 주파수 빈(bin) 파라미터이다. \\(\\gamma\\)의 적절한 선택은 데이터 특성과 염두에 둔 적용에 따라 크게 달라진다. 특히, 잡음이 있는 경우, 약하지만 관련된 신호 구성 요소를 향상시키는 것과 바람직하지 않은 잡음 구성 요소를 너무 많이 증폭시키지 않는 것 사이에서 적절한 균형을 찾아야 한다.\n\n\nx, Fs = librosa.load(\"../audio/piano_c4.wav\")\n\nN = 2048\nH = 512\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=True)\nT_coef = np.arange(X.shape[1]) * H / Fs\nK = N // 2\nF_coef = np.arange(K + 1) * Fs / N\nY = np.abs(X) ** 2\n\nplt.figure(figsize=(10, 3))\nextent = [T_coef[0], T_coef[-1], F_coef[0], F_coef[-1]]\ngamma_set = [0, 1, 100, 10000]\nM = len(gamma_set)\nY = np.abs(X)\n\nfor m in range(M):\n    ax = plt.subplot(1, M, m + 1)\n    gamma = gamma_set[m]\n    if gamma == 0:\n        Y_compressed = Y\n        title = 'No compression'\n    else:\n        Y_compressed = log_compression(Y, gamma=gamma)\n        title = '$\\gamma$=%d' % gamma\n    plt.imshow(Y_compressed, cmap='gray_r', aspect='auto', origin='lower', extent=extent)\n    plt.xlabel('Time (seconds)')\n    plt.ylim([0, 4000])\n    plt.clim([0, Y_compressed.max()])\n    plt.ylabel('Frequency (Hz)')\n    plt.colorbar()\n    plt.title(title)\n\nplt.tight_layout()\n\n\n\n\n\n원본 스펙트로그램에서는 수평선이 거의 보이지 않지만 압축된 버전에서는 명확하게 나타난다. 단점으로는 스펙트로그램을 압축할 때 잡음과 같은 사운드 구성 요소도 향상된다는 것이다. 잡음이 있는 경우, 약하지만 관련된 신호 구성 요소를 강화하는 것과 바람직하지 않은 잡음 구성 요소를 너무 많이 증폭하지 않는 것 사이에서 적절한 균형을 찾아야 한다."
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축된-크로마그램-compressed-chromagram",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#압축된-크로마그램-compressed-chromagram",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "압축된 크로마그램 (Compressed Chromagram)",
    "text": "압축된 크로마그램 (Compressed Chromagram)\n\n다음으로 chromagram \\(\\mathcal{C}\\) 및 압축 버전 \\(\\Gamma_\\gamma \\circ \\mathcal{C}\\)를 생각해보자.\n\n\\((\\Gamma_\\gamma\\circ \\mathcal{C})(n,c):=\\log(1+ \\gamma \\cdot \\mathcal{C}(n,c))\\)\n\n\n\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F08_C-major-scale_pause.wav\")\n\nN = 4096\nH = 512\nC = librosa.feature.chroma_stft(y=x, tuning=0, norm=None, hop_length=H, n_fft=N)\nC = C / C.max()\n\nplt.figure(figsize=(6, 8))\ngamma_set = [0, 10, 1000, 100000]\nM = len(gamma_set)\nY = np.abs(X)\n\nfor m in range(M):\n    ax = plt.subplot(M, 1, m + 1)\n    gamma = gamma_set[m]\n    if gamma == 0:\n        C_compressed = C\n        title = 'No compression'\n    else:\n        C_compressed = log_compression(C, gamma=gamma)\n        title = '$\\gamma$=%d' % gamma\n    librosa.display.specshow(C_compressed, x_axis='time', \n                             y_axis='chroma', cmap='gray_r', sr=Fs, hop_length=H)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Chroma')\n    plt.clim([0, np.max(C_compressed)])\n    plt.title(title)\n    plt.colorbar()\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#노름norm",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#노름norm",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "노름(Norm)",
    "text": "노름(Norm)\n\n여태까지 스펙트럼 피쳐(spectral features), 로그-주파수 스펙트럼 피쳐 및 크로마 피쳐(chroma feautres)를 비롯한 다양한 기능 표현을 소개했다. 이러한 피쳐는 일반적으로 어떤 차원 \\(K\\in\\mathbb{N}\\)의 유클리드 공간의 요소이다.\n앞으로 이 피쳐 공간을 \\(\\mathcal{F}=\\mathbb{R}^K\\)로 표시한다. 종종 피처에 크기나 일종의 길이를 할당하는 측정이 필요하며, 이는 norm이라는 개념으로 이어진다. 수학 용어로 벡터 공간(예: \\(\\mathcal{F}=\\mathbb{R}^K\\))이 주어지면 norm은 다음의 세 가지 속성을 만족하는 음이 아닌 함수 \\(p:\\mathcal{F} \\to \\mathbb{R}_{\\geq 0}\\)이다:\n\nTriangle inequality: \\(p(x + y) \\leq p(x) + p(y)\\) for all \\(x,y\\in\\mathcal{F}\\).\nPositive scalability: \\(p(\\alpha x) = |\\alpha| p(x)\\) for all \\(x\\in\\mathcal{F}\\) and \\(\\alpha\\in\\mathbb{R}\\).\nPositive definiteness: \\(p(x) = 0\\) if an only if \\(x=0\\).\n\n벡터 \\(x\\in\\mathcal{F}\\)에 대한 숫자 \\(p(x)\\)를 벡터의 길이라고 한다. 또한 \\(p(x)=1\\)인 벡터를 단위 벡터라고 한다.\n여러 norm이 있음에 유의하자. 다음에서는 벡터 공간 \\(\\mathcal{F}=\\mathbb{R}^K\\)만 고려하고, 세 가지 norm을 살펴보자.\n\n\n유클리드 (Euclidean) 노름\n\n가장 보편적인 것은 유클리드(Euclidean) norm (or \\(\\ell^2\\)-norm)이다. 이 norm은 \\(\\|\\cdot\\|_2\\)로 표기되며 다음과 같이 정의할 수 있다.\n\n\\(||x||_2 = \\sqrt{\\langle x\\mid x\\rangle} = \\Big(\\sum_{k=1}^K x(k)^2\\Big)^{1/2}\\)\nfor a vector \\(x=(x(1),x(2),\\ldots,x(K))^\\top \\in\\mathbb{R}^K\\).\n\n유클리드 norm \\(||x||_2\\)는 원점 \\((0,0)\\)에서 점 \\(x\\)까지의 일반적인 거리를 나타낸다.\n유클리드 norm에 관한 단위 벡터 집합은 \\(S^{K-1}\\subset\\mathbb{R}^K\\)로 표시되는 단위 구(unit sphere) 를 형성한다.\n\\(K=2\\)인 경우 단위구는 원점이 \\((0,0)\\)인 단위원(unit circle) \\(S^1\\)이다.\n\n\ndef plot_vector(x,y, color='k', start=0, linestyle='-'):    \n    return plt.arrow(np.real(start), np.imag(start), x, y, \n                     linestyle=linestyle, head_width=0.05, \n                     fc=color, ec=color, overhang=0.3, length_includes_head=True)\n\n\ndef norm_Euclidean(x):\n    p = np.sqrt(np.sum(x ** 2))\n    return p\n\nfig, ax = plt.subplots(figsize=(3, 3))\nplt.grid()  \nplt.xlim([-1.5, 1.5])\nplt.ylim([-1.5, 1.5])\nplt.xlabel('Axis of first coordinate')\nplt.ylabel('Axis of second coordinate')\n\ncircle = plt.Circle((0, 0), 1, color='r', fill=0, linewidth=2)  \nax.add_artist(circle)\n\nx_list = [np.array([[1, 1], [0.6, 1.1]]),\n          np.array([[-np.sqrt(2)/2, np.sqrt(2)/2], [-1.45, 0.85]]),\n          np.array([[0, -1], [-0.4, -1.2]])]\n\nfor y in x_list:\n    x = y[0, :]\n    p = norm_Euclidean(x)\n    color = 'r' if p == 1 else 'k'\n    plot_vector(x[0], x[1], color=color)\n    plt.text(y[1, 0], y[1, 1], r'$||x||_2=%0.3f$'% p, size='12', color=color) \n\n\n\n\n\n\n맨해튼 (Manhattan) 노름\n\n맨해튼(Manhattan) norm (or \\(\\ell^1\\)-norm)에서 벡터의 길이는 벡터의 데카르트 좌표의 절대값을 합산하여 측정된다. 맨해튼 norm (\\(\\|\\cdot\\|_1\\))은 다음과 같이 정의된다,\n\n\\(||x||_1 = \\sum_{k=1}^K |x(k)|\\)\nfor a vector \\(x=(x(1),x(2),\\ldots,x(K))^\\top \\in\\mathbb{R}^K\\).\n\n맨해튼 norm에서 단위 벡터 집합은 좌표축에 대해 45도 각도로 향하는 면이 있는 정사각형을 형성한다. \\(K=2\\)인 경우 단위원은 \\(|x(1)| + |x(2)| = 1\\)로 설명된다.\n\n\ndef norm_Manhattan(x):\n    p = np.sum(np.abs(x))\n    return p\n    \nfig, ax = plt.subplots(figsize=(3, 3))\nplt.grid()  \nplt.xlim([-1.5, 1.5])\nplt.ylim([-1.5, 1.5])\nplt.xlabel('Axis of first coordinate')\nplt.ylabel('Axis of second coordinate')\n\nplt.plot([-1, 0, 1, 0, -1], [0, 1, 0, -1, 0], color='r', linewidth=2)\n\nfor y in x_list:\n    x = y[0, :]\n    p = norm_Manhattan(x)\n    color = 'r' if p == 1 else 'k'\n    plot_vector(x[0], x[1], color=color)\n    plt.text(y[1, 0], y[1, 1], r'$||x||_1=%0.3f$' % p, size='12', color=color)\n\n\n\n\n\n\n최대 (Maximum) 노름\n\n최대(maximum) norm (or \\(\\ell^\\infty\\)-norm)에서, 벡터의 길이는 최대 절대 데카르트 좌표로 측정된다. maximum norm (\\(\\|\\cdot\\|_\\infty\\))은 다음과 같이 정의된다.\n\n\\(||x||_\\infty = \\max\\big\\{|x(k)| \\,\\,\\mathrm{for}\\,\\, k\\in[1:K] \\big\\}\\)\nfor a vector \\(x=(x(1),x(2),\\ldots,x(K))^\\top \\in\\mathbb{R}^K\\).\n\n단위 벡터 집합은 가장자리 길이가 2인 hypercube의 표면을 형성한다.\n\n\ndef norm_max(x):\n    p = np.max(np.abs(x))\n    return p\n    \nfig, ax = plt.subplots(figsize=(3, 3))\nplt.grid()  \nplt.xlim([-1.5, 1.5])\nplt.ylim([-1.5, 1.5])\nplt.xlabel('Axis of first coordinate')\nplt.ylabel('Axis of second coordinate')\n\nplt.plot([-1, -1, 1, 1, -1], [-1, 1, 1, -1, -1], color='r', linewidth=2)\n\nfor y in x_list:\n    x = y[0, :]\n    p = norm_max(x)\n    color = 'r' if p == 1 else 'k'\n    plot_vector(x[0], x[1], color=color)\n    plt.text(y[1, 0], y[1, 1], r'$||x||_\\infty=%0.3f$' % p, size='12', color=color)"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#피쳐-정규화-feature-normalization-1",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#피쳐-정규화-feature-normalization-1",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "피쳐 정규화 (Feature Normalization)",
    "text": "피쳐 정규화 (Feature Normalization)\n\n음악 처리에서 오디오 녹음은 일반적으로 특징(feature) 표현으로 변환된다. 특징 표현은 \\(n\\in[1:N]\\)에 대한 특징 벡터 \\(x_n \\in \\mathcal{F}=\\mathbb{R}^K\\)가 있는 시퀀스 \\(X=(x_1,x_2,\\ldots x_N)\\)로 종종 구성된다.\n특징 표현을 더 잘 비교하기 위해 종종 정규화(normalization)를 적용한다.\n한 가지 정규화 전략은 적절한 norm \\(p\\)를 선택한 다음 각 특징 벡터 \\(x_n\\in\\mathcal{F}\\)를 \\(x_n/p(x_n)\\)로 바꾸는 것이다. 이 전략은 \\(x_n\\)이 0이 아닌 벡터인 한 작동한다. 정규화된 특징 벡터 \\(x_n/p(x_n)\\)는 norm \\(p\\)에 대한 단위 벡터이다.\n예를 들어 \\(X=(x_1,x_2,\\ldots x_N)\\)가 크로마 특징의 시퀀스인 경우를 생각해보자. 이 경우 특징 공간 \\(\\mathcal{F}=\\mathbb{R}^K\\)의 차원은 \\(K=12\\)이다. 위에서 설명한 정규화 절차는 각 크로마 벡터를 정규화된 버전으로 대체한다.\n결과적으로 정규화된 크로마 벡터는 12개의 크로마 계수 크기의 절대 차이가 아닌 상대만 차이만 인코딩한다. 직관적으로 말하면, 정규화는 다이나믹(dynamics) 또는 사운드 강도(sound intensity) 의 차이에 일종의 불변성(invariance) 을 도입한다.\n정규화 절차는 \\(p(x)\\not= 0\\)인 경우에만 가능하다. 실제 녹음 시작 전이나 긴 일시 중지 중에 무음 구간에서 발생할 수 있는 매우 작은 \\(p(x)\\) 값에 대해 정규화 할 시 다소 무작위적이고 무의미한 크로마 값 분포로 이어질 수 있다.\n따라서 \\(p(x)\\)가 특정 임계값 아래로 떨어지면, \\(p(x)\\)로 나누는 대신 \\(x\\) 벡터를 norm 1의 uniform 벡터와 같은 표준(standard) 벡터로 대체할 수 있다.\n수학적으로 이 정규화 절차는 다음과 같이 설명할 수 있다.\n\\(S^{K-1}\\subset\\mathbb{R}^{K}\\)를 norm 1의 모든 K-차원 벡터를 포함하는 단위 구라고 하자. 그런 다음 주어진 임계값 \\(\\varepsilon>0\\)에 대해 프로젝션 연산자 \\(\\pi^{\\varepsilon}:\\mathbb{R}^{K}\\to S^{K-1}\\)를 다음과 같이 정의한다.\n\n\\(\\pi^{\\varepsilon}(x):= \\left\\{\\begin{array}{cl} x / p(x) & \\,\\,\\,\\mbox{if}\\,\\, p(x) > \\varepsilon\\\\  u & \\,\\,\\,\\mbox{if}\\,\\, p(x) \\leq \\varepsilon \\end{array}\\right.\\)\n\\(u=v/p(v)\\) 는 all-one vector \\(v=(1,1,\\ldots,1)^\\top\\in\\mathbb{R}^K\\)의 unit vector\n\n이 연산자를 기반으로 각 chroma 벡터 \\(x\\)는 \\(\\pi^{\\varepsilon}(x)\\)로 대체될 수 있다.\n\\(\\varepsilon\\) 임계값은 신중하게 선택해야 하는 파라미터이다. 응용에 따라 적절한 값을 선택해야 한다.\n정규화에 대한 많은 변형도 생각할 수 있다. 분명히 정규화는 선택한 norm과 임계값에 따라 달라진다. 또한 균등하게 분산된 단위 벡터 \\(u\\)를 사용하는 대신, 작은 크기의 특징 벡터를 나타내는 다른 벡터를 사용할 수도 있다.\n\n\ndef normalize_feature_sequence(X, norm='2', threshold=0.0001, v=None):\n    \"\"\"Normalizes the columns of a feature sequence\n\n    Args:\n        X (np.ndarray): Feature sequence\n        norm (str): The norm to be applied. '1', '2', 'max' or 'z' (Default value = '2')\n        threshold (float): An threshold below which the vector ``v`` used instead of normalization\n            (Default value = 0.0001)\n        v (float): Used instead of normalization below ``threshold``. If None, uses unit vector for given norm\n            (Default value = None)\n\n    Returns:\n        X_norm (np.ndarray): Normalized feature sequence\n    \"\"\"\n    assert norm in ['1', '2', 'max', 'z']\n\n    K, N = X.shape\n    X_norm = np.zeros((K, N))\n\n    if norm == '1':\n        if v is None:\n            v = np.ones(K, dtype=np.float64) / K\n        for n in range(N):\n            s = np.sum(np.abs(X[:, n]))\n            if s > threshold:\n                X_norm[:, n] = X[:, n] / s\n            else:\n                X_norm[:, n] = v\n\n    if norm == '2':\n        if v is None:\n            v = np.ones(K, dtype=np.float64) / np.sqrt(K)\n        for n in range(N):\n            s = np.sqrt(np.sum(X[:, n] ** 2))\n            if s > threshold:\n                X_norm[:, n] = X[:, n] / s\n            else:\n                X_norm[:, n] = v\n\n    if norm == 'max':\n        if v is None:\n            v = np.ones(K, dtype=np.float64)\n        for n in range(N):\n            s = np.max(np.abs(X[:, n]))\n            if s > threshold:\n                X_norm[:, n] = X[:, n] / s\n            else:\n                X_norm[:, n] = v\n\n    if norm == 'z':\n        if v is None:\n            v = np.zeros(K, dtype=np.float64)\n        for n in range(N):\n            mu = np.sum(X[:, n]) / K\n            sigma = np.sqrt(np.sum((X[:, n] - mu) ** 2) / (K - 1))\n            if sigma > threshold:\n                X_norm[:, n] = (X[:, n] - mu) / sigma\n            else:\n                X_norm[:, n] = v\n\n    return X_norm\n\n\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F08_C-major-scale_pause.wav\")\n\nN, H = 4096, 512\nC = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\nC = C / C.max()\n\nfigsize=(8, 3)\nplot_chromagram(C, Fs=Fs/H, figsize=figsize, title='Original chromagram')\n\nthreshold = 0.000001\nC_norm = normalize_feature_sequence(C, norm='2', threshold=threshold)\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, \n        title = r'Normalized chromgram ($\\ell^2$-norm, $\\varepsilon=%f$)' % threshold)\n\nthreshold = 0.01\nC_norm = normalize_feature_sequence(C, norm='2', threshold=threshold)\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, \n        title = r'Normalized chromgram ($\\ell^2$-norm, $\\varepsilon=%0.2f$)' % threshold)\n\nthreshold = 0.01\nC_norm = normalize_feature_sequence(C, norm='1', threshold=threshold)\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, \n        title = r'Normalized chromgram ($\\ell^1$-norm, $\\varepsilon=%0.2f$)' % threshold)\n\nthreshold = 0.01\nC_norm = normalize_feature_sequence(C, norm='max', threshold=threshold)\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, \n        title = r'Normalized chromgram (maximum norm, $\\varepsilon=%0.2f$)' % threshold)\n\nthreshold = 0.01\nv = np.zeros(C.shape[0])\nC_norm = normalize_feature_sequence(C, norm='max', threshold=threshold, v=v)\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, \n        title = r'Normalized chromgram (maximum norm, $v$ is zero vector)');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n정규점수 (Standard score)\n\n피쳐 \\(x=(x(1),x(2),\\ldots,x(K))^\\top \\in\\mathbb{R}^K\\)의 평균(mean) \\(\\mu(x)\\) and 분산(variance) \\(\\sigma(x)\\)을 사용해 standard score를 고려하자.\n\n\\[z(x) = \\frac{x-\\mu(x)}{\\sigma(x)}\\]\n\ndef normalize_feature_sequence_z(X, threshold=0.0001, v=None):\n    K, N = X.shape\n    X_norm = np.zeros((K, N))\n    \n    if v is None:\n        v = np.zeros(K)\n        \n    for n in range(N):\n        mu = np.sum(X[:, n]) / K\n        sigma = np.sqrt(np.sum((X[:, n] - mu) ** 2) / (K - 1))\n        if sigma > threshold:\n            X_norm[:, n] = (X[:, n] - mu) / sigma\n        else:\n            X_norm[:, n] = v  \n            \n    return X_norm\n\n\nthreshold = 0.0000001\nC_norm = normalize_feature_sequence_z(C, threshold=threshold)\nm = np.max(np.abs(C_norm))\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, cmap='seismic', clim=[-m, m],\n        title = r'Normalized chromgram (standard score, $\\varepsilon=%0.7f$)' % threshold)\n\nthreshold = 0.01\nC_norm = normalize_feature_sequence_z(C, threshold=threshold)\nm = np.max(np.abs(C_norm))\nplot_chromagram(C_norm, Fs=Fs/H, figsize=figsize, cmap='seismic', clim=[-m, m],\n        title = r'Normalized chromgram (standard score, $\\varepsilon=%0.2f$)' % threshold);"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#템포-스무딩과-다운샘플링-temporal-smoothing-and-downsampling-1",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#템포-스무딩과-다운샘플링-temporal-smoothing-and-downsampling-1",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "템포 스무딩과 다운샘플링 (Temporal Smoothing and Downsampling)",
    "text": "템포 스무딩과 다운샘플링 (Temporal Smoothing and Downsampling)\n\n특정 음악 검색 응용의 경우 이러한 크로마그램이 너무 상세할 수 있다. 특히 이들 간의 유사도를 더욱 높이는 것이 바람직할 수 있다. 이제 후처리 단계에서 적용되는 스무딩(smoothing)(=평활화) 절차를 통해 이것이 어떻게 달성될 수 있는지 보자. 시간이 지남에 따라 각 크로마 차원에 대해 일종의 로컬 평균을 계산한다.\n보다 정확하게는, \\(X=(x_1,x_2, ..., x_N)\\)를 \\(x_n\\in\\mathbb{R}^K\\) for \\(n\\in[1:N]\\)인 피쳐 시퀀스라고 하고, \\(w\\)는 \\(L\\in\\mathbb{N}\\) 길이의 직사각형 윈도우(rectangular window)라고 하자. 그런 다음 각 \\(k\\in[1:K]\\)에 대해 \\(w\\)와 시퀀스 \\((x_1(k), x_2(k),\\ldots, x_N(k))\\) 사이의 컨볼루션(convolution)을 계산한다. 중앙보기(“centered view”)를 가정하면 컨볼루션 길이 \\(N\\)의 중앙 부분만 유지한다. 결과는 동일한 차원 \\(K\\times N\\)의 평활화된 피쳐 시퀀스이다.\n다음에서 scipy.signal.convolve 함수를 사용하여 2D 컨볼루션을 계산한다. 윈도우(커널이라고도 함)에 대해 차원을 \\(1\\times L\\)로 설정하면 대역별(bandwise) 1D 컨벌루션이 생성된다.\nmode='same' 파라미터를 사용하면 중앙보기(centered view)가 적용된다.\n윈도우 \\(w\\)는 Hann 윈도우와 같은 다른 윈도우 유형을 사용할 수도 있다.\n직사각형 윈도우 또는 Hann 윈도우를 사용하여 템포 스무딩을 적용하는 것은 특징 표현에서 빠른 템포 변동을 감쇠시키는 대역 저역 통과 필터링 (lowpass filtering) 으로 간주될 수 있다.\n종종 후속 처리 및 분석 단계의 효율성을 높이기 위해, 모든 \\(H^\\mathrm{th}\\) 피쳐만 유지하여 평활화된 표현을 단순화(decimation)하며, 여기서 \\(H\\in\\mathbb{N}\\)는 적절한 상수(일반적으로 창 길이 \\(L\\)보다 훨씬 작음)이다. 다운샘플링(donwsampling) 이라고도 하는 이 단순화는 \\(H\\)개의 팩터로 피쳐 레이트(feature rate)를 감소시킨다.\n다음 그림은 위의 쇼팽의 예에서 스무딩 절차의 효과를 보여준다. 길이가 \\(L=11\\)인 직사각형 윈도우로 평활화한 후 유클리드 norm을 사용하여 피쳐 정규화를 적용하고 마지막으로 \\(H=2\\)만큼 다운샘플링을 적용한다.\n\n\ndef smooth_downsample_feature_sequence(X, Fs, filt_len=41, down_sampling=10, w_type='boxcar'):\n    \"\"\"Smoothes and downsamples a feature sequence. Smoothing is achieved by convolution with a filter kernel\n\n    Args:\n        X (np.ndarray): Feature sequence\n        Fs (scalar): Frame rate of ``X``\n        filt_len (int): Length of smoothing filter (Default value = 41)\n        down_sampling (int): Downsampling factor (Default value = 10)\n        w_type (str): Window type of smoothing filter (Default value = 'boxcar')\n\n    Returns:\n        X_smooth (np.ndarray): Smoothed and downsampled feature sequence\n        Fs_feature (scalar): Frame rate of ``X_smooth``\n    \"\"\"\n    filt_kernel = np.expand_dims(signal.get_window(w_type, filt_len), axis=0)\n    X_smooth = signal.convolve(X, filt_kernel, mode='same') / filt_len\n    X_smooth = X_smooth[:, ::down_sampling]\n    Fs_feature = Fs / down_sampling\n    return X_smooth, Fs_feature\n\n\nfilt_len = 11\ndown_sampling = 2\nC_smooth_dict = {}    \nfor name in fn_wav_dict:   \n    C_smooth, Fs_C_smooth = smooth_downsample_feature_sequence(C_dict[name], Fs_C, \n                                        filt_len=filt_len, down_sampling=down_sampling)\n    C_smooth_dict[name] = normalize_feature_sequence(C_smooth, norm='2', threshold=threshold)\n    plot_chromagram(C_smooth_dict[name], Fs_C_smooth, figsize=figsize, \n                             ylabel=name, title='',  xlabel='', chroma_yticks=yticks)"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#중앙값-필터링medain-filtering을-통한-스무딩",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#중앙값-필터링medain-filtering을-통한-스무딩",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "중앙값 필터링(medain filtering)을 통한 스무딩",
    "text": "중앙값 필터링(medain filtering)을 통한 스무딩\n\n로컬(local) 평균 필터를 적용하는 것의 대안으로 중앙값(median) 필터링을 사용할 수 있다. 이 필터를 사용하면 날카로운 전환을 더 잘 보존하면서 약간의 평활화 효과도 얻을 수 있다. 유한한 숫자 목록의 중앙값은 숫자의 절반이 그값보다 낮고 절반이 그값보다 높은 속성을 가진 숫자 값이다. 중앙값은 가장 낮은 값에서 가장 높은 값으로 모든 숫자를 정렬하고 가운데 값을 선택하여 계산한다.\n중앙값 필터링의 아이디어는 시퀀스의 각 항목을 인접 항목의 중앙값으로 바꾸는 것이다. 이웃의 크기는 적용된 중간 필터의 길이인 \\(L\\in\\mathbb{N}\\) 파라미터에 의해 결정된다. 피쳐 벡터 \\(x_n\\in\\mathbb{R}^K\\)의 시퀀스 \\(X=(x_1,x_2, ..., x_N)\\)가 주어지면, 각 \\(k\\in[1:K]\\)에 대해 각 차원에 대한 중앙값 필터링을 적용합니다(평균 평활화에서 수행한 것과 동일).\n다음 구현에서는 scipy.signal.medfilt2d 함수를 사용하여 2D 중앙값 필터링을 계산한다. 커널 크기 \\(1\\times L\\)로 중앙값 필터링을 사용하면 대역별 1D 중앙값 필터링이 된다.\nscipy.signal.medfilt2d를 사용할 때 중간 필터 길이 \\(L\\)은 홀수여야 한다.\n중앙값 필터링의 output array는 input array과 동일한 차원을 가진다. 경계 문제는 제로-패딩으로 처리된다.\n\n\nX = np.array([[1, 2, 3, 4, 5], [5, 6, 7, 8, 9], [5, 3, 2, 8, 2]], dtype='float')\nL = 3\nfilt_len = [1, L]\nX_smooth = signal.medfilt2d(X, filt_len)\nprint('Input array X of dimension (K,N) with K=3 and N=5')\nprint(X)\nprint('Output array after median filtering with L=3')\nprint(X_smooth)\n\nInput array X of dimension (K,N) with K=3 and N=5\n[[1. 2. 3. 4. 5.]\n [5. 6. 7. 8. 9.]\n [5. 3. 2. 8. 2.]]\nOutput array after median filtering with L=3\n[[1. 2. 3. 4. 4.]\n [5. 6. 7. 8. 8.]\n [3. 3. 3. 2. 2.]]\n\n\n\n다음 그림은 원본 크로마그램과 평활화된 버전(한 번은 평균 필터링을 적용하고 한 번은 중앙값 필터링을 적용함)을 비교한다. 원래 크로마그램에서 음(note) 전환의 결과인 날카로운 edge를 관찰할 수 있다. 평균 평활화는 이러한 전환 사이에 번짐을 초래하지만 중앙값 평활화는 에지(edge)를 더 잘 보존하는 경향이 있다.\n\n\ndef median_downsample_feature_sequence(X, Fs, filt_len=41, down_sampling=10):\n    \"\"\"Smoothes and downsamples a feature sequence. Smoothing is achieved by median filtering\n    \n    Args:\n        X (np.ndarray): Feature sequence\n        Fs (scalar): Frame rate of ``X``\n        filt_len (int): Length of smoothing filter (Default value = 41)\n        down_sampling (int): Downsampling factor (Default value = 10)\n\n    Returns:\n        X_smooth (np.ndarray): Smoothed and downsampled feature sequence\n        Fs_feature (scalar): Frame rate of ``X_smooth``\n    \"\"\"\n    assert filt_len % 2 == 1  # L needs to be odd\n    filt_len = [1, filt_len]\n    X_smooth = signal.medfilt2d(X, filt_len)\n    X_smooth = X_smooth[:, ::down_sampling]\n    Fs_feature = Fs / down_sampling\n    return X_smooth, Fs_feature\n\n\nfilt_len = 11\ndown_sampling = 2\nC_median_dict = {}    \nfor name in fn_wav_dict:   \n    C_median, Fs_C_smooth = median_downsample_feature_sequence(C_dict[name],  Fs_C,\n                                                               filt_len=filt_len, down_sampling=down_sampling)\n    C_median_dict[name] = normalize_feature_sequence(C_median, norm='2', threshold=threshold)\n\nfigsize=(8, 2)\n\nname = 'Karajan'\nplot_chromagram(C_dict[name], Fs_C_smooth, figsize=figsize, ylabel = name,\n                         title='Original chromagram', xlabel='', chroma_yticks=yticks)\nplot_chromagram(C_smooth_dict[name], Fs_C_smooth, figsize=figsize, ylabel = name,\n                         title='Smoothed chromagram using average filtering', xlabel='', chroma_yticks=yticks) \nplot_chromagram(C_median_dict[name], Fs_C_smooth, figsize=figsize, ylabel = name, \n                         title='Smoothed chromagram using median filtering', xlabel='', chroma_yticks=yticks)  \n\nC_diff = C_smooth_dict[name] - C_median_dict[name]\nm = np.max(np.abs(C_diff))\nplot_chromagram(C_diff, Fs_C_smooth, cmap='seismic', clim=[-m, m], figsize=figsize,\n                         title='Difference between average- and median-filtered chromagram', xlabel='',\n                         ylabel=name, chroma_yticks=yticks);"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조옮김-transposition",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조옮김-transposition",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "조옮김 (Transposition)",
    "text": "조옮김 (Transposition)\n\n음악에서는 종종 멜로디나 음악 전체를 다른 키로 전환하는 경우가 있다. 이러한 개념을 조옮김(transposition) 이라고 한다.\n이러한 수정은 주어진 곡의 피치 범위를 다른 악기나 가수에 적용하기 위해 종종 적용된다. 엄밀히 말하면, 조옮김은 일련의 음표를 일정한 간격으로 피치를 올리거나 내리는 과정을 말한다. 예를 들어 C-major 스케일의 음표를 4 반음 위로 이동하면 E-major 스케일이 된다.\n크로마 레벨에서 음악적 조옮김을 쉽게 시뮬레이션할 수 있다. \\([0:11]\\) 세트로 식별한 12개의 크로마 값은 순환적으로 정렬되며, 순환 이동 연산자(cyclic shift operator) \\(\\rho:\\mathbb{R}^{12} \\to \\mathbb{R}^{12}\\)의 정의와 연관된다.\n크로마 벡터 \\(x=(x(0),x(1),\\ldots,x(10),x(11))^\\intercal\\in\\mathbb{R}^{12}\\)가 주어지면, 순환 이동 연산자를 다음과 같이 정의할 수 있다.\n\n\\(\\rho(x):=(x(11),x(0),x(1),\\ldots,x(10))^\\intercal\\)\n\n즉, \\(x\\)의 크로마 밴드(chroma band) \\(\\mathrm{C}\\)는 \\(\\rho(x)\\)에서 크로마-밴드 \\(\\mathrm{C}^\\sharp\\)가, 그리고 \\(\\mathrm{C}^\\sharp\\)가, 그리고 \\(\\mathrm{D}\\)가, 계속해서 마지막으로 \\(\\mathrm{B}\\)는 \\(\\mathrm{C}\\)가 된다.\n\\(i\\)의 순환 이동을 정의하는 \\(\\rho^i:=\\rho\\circ\\rho^{i-1}\\)(for \\(i\\in\\mathbb{N}\\))를 얻기 위해 순환 이동 연산자를 반음 위로 연속적으로 적용할 수 있다. \\(\\rho^{12}(x) = x\\)는 크로마 벡터를 12반음(1옥타브) 위로 주기적으로 이동하여 원래 벡터를 복구함을 의미한다. 크로마그램의 모든 프레임에 순환 이동 연산자를 동시에 적용하면 전체 크로마그램이 수직 방향으로 순환 이동하게 된다.\n다음 예에서 \\(\\mathrm{C}\\)-major 스케일의 원래 크로마그램이 4반음 위로 이동된 것을 확인할 수 있다. 그 결과 E-major 스케일 중 하나처럼 보이는 크로마그램이 생성된다.\n\n\ndef cyclic_shift(C, shift=1):\n    \"\"\"Cyclically shift a chromagram\n\n    Args:\n        C (np.ndarray): Chromagram\n        shift (int): Tranposition shift (Default value = 1)\n\n    Returns:\n        C_shift (np.ndarray): Cyclically shifted chromagram\n    \"\"\"\n    C_shift = np.roll(C, shift=shift, axis=0)\n    return C_shift\n\n\nx_cmajor, Fs = librosa.load('../data_FMP/FMP_C3_F08_C-major-scale.wav')\nx_emajor, Fs = librosa.load('../data_FMP/FMP_C3_F08_C-major-scale_400-cents-up.wav')\n\nN = 4096\nH = 1024\nfigsize = (8, 2)\nyticks = [0, 4, 7, 11]\n\nC = librosa.feature.chroma_stft(y=x_cmajor, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\ntitle = 'Chromagram of C-major scale'\nplot_chromagram(C, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks)\n\nC = librosa.feature.chroma_stft(y=x_cmajor, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\nC_shift = cyclic_shift(C, shift=4)\ntitle = 'Chromagram of C-major scale cyclically shifted four semitones upwards'\nplot_chromagram(C_shift, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks)\n\nC = librosa.feature.chroma_stft(y=x_emajor, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\ntitle = 'Chromagram of E-major scale'\nplot_chromagram(C, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks);"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조율-tuning",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조율-tuning",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "조율 (Tuning)",
    "text": "조율 (Tuning)\n\n조옮김은 반음(semitone) 수준의 피치 이동이지만, 이제 하위 세미톤(sub-semitone) 수준의 전역적 주파수 편차(global frequency deviations)에 대해 살펴보자. 이러한 편차는 중심 주파수가 \\(440~\\mathrm{Hz}\\)인 예상 기준 피치 \\(\\mathrm{A4}\\)보다 낮거나 높게 조율된 악기의 결과일 수 있다. 예를 들어, 많은 현대 오케스트라는 \\(440\\)Hz보다 약간 높은 튜닝 주파수를 사용하는 반면 바로크 음악을 연주하는 앙상블은 종종 \\(440\\)Hz보다 낮게 튜닝한다.\n조율 효과를 보상하려면 MIDI 피치의 중심 주파수를 조정하는 추가 조율 추정 단계와 피치 기반 로그 주파수 스펙트로그램을 계산하기 위한 로그 분할을 수행해야 한다.\n보다 정확하게는 \\(\\theta\\in[-50,50)\\)(센트 단위)를 전연적 조율 편차(global tuning deviation)라고 하자. 그러면 조정된 중심 주파수의 공식은 다음과 같다. \\[F^\\theta_\\mathrm{pitch}(p) = 2^{(p-69+\\theta/100)/12} \\cdot 440\\]\n로그 분할의 조정은 크로마그램을 계산하기 위해 librosa-함수에 내장되어 있다.\n\n\nx, Fs = librosa.load('../data_FMP/FMP_C3_F08_C-major-scale.wav')\nx_detune, Fs = librosa.load('../data_FMP/FMP_C3_F08_C-major-scale_40-cents-up.wav')\n\nyticks = [0, 4, 7, 11]\nN = 4096\nH = 1024\n\nC = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\ntitle = 'Chromagram of original signal'\nplot_chromagram(C, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks)\n\nC = librosa.feature.chroma_stft(y=x_detune, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\ntitle = 'Chromagram of detuned signal'\nplot_chromagram(C, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks);\n\n\n\n\n\n\n\n\n# librosa로 adjust하기\n# logarithmic partitioning\n\ntheta = 40\ntuning = theta / 100\nC = librosa.feature.chroma_stft(y=x_detune, sr=Fs, tuning=tuning, norm=2, hop_length=H, n_fft=N)\ntitle = 'Chromagram of detuned signal with adjusted chroma bands'\nplot_chromagram(C, Fs/H, figsize=figsize, title=title, clim=[0, 1], chroma_yticks=yticks);"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조율-추정-tuning-estimation",
    "href": "posts/4. Music Synchronization/4.1.Audio_Synchronization_Features.html#조율-추정-tuning-estimation",
    "title": "4.1. 오디오 동기화 피쳐",
    "section": "조율 추정 (Tuning Estimation)",
    "text": "조율 추정 (Tuning Estimation)\n\n악기의 조율은 일반적으로 고정된 기준 피치를 기반으로 한다. 서양 음악에서는 일반적으로 주파수가 \\(440~\\mathrm{Hz}\\)인 concert 피치 \\(\\mathrm{A4}\\)를 사용한다. 따라서 \\(\\mathrm{A4}\\) 음을 연주하면 \\(440~\\mathrm{Hz}\\)(기본 주파수(fundamental frequency)에 해당) 및 정수 배수(고조파(harmonics)에 해당) 근처에서 우세한 주파수를 기대할 수 있다.\n\\(x=(x(0), x(1), ..., x(N-1))\\)를 녹음된 음 \\(\\mathrm{A4}\\)의 샘플 신호(샘플링 속도 \\(F_\\mathrm{s}\\) 사용)라고 하자. 또한 \\(X = \\mathrm{DFT}_N \\cdot x\\)를 이산 푸리에 변환이라고 하자. 푸리에 계수 \\(X(k)\\)의 인덱스 \\(k\\in[0:N-1]\\)는 헤르츠로 주어진 다음의 물리적 주파수(physical frequency)에 해당된다.\n\n\\(F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\\)\n\n한 가지 간단한 방법은 \\(440~\\mathrm{Hz}\\) 부근(예: 반음 더하기/빼기)에서 최대 크기 계수 \\(|X(k_0)|\\)를 생성하는 주파수 인덱스 \\(k_0\\)를 찾는 것이다.\n\\(\\log_2\\left(\\frac{F_\\mathrm{coef}(k_0)}{440}\\right) \\cdot 1200\\)는 조율 편차의 추정치를 산출한다(cent로). 보다 강력한 추정치를 얻기 위해 고조파의 적절히 정의된 이웃에서 스펙트럼 피크 위치를 고려할 수도 있다. 물론 다른 음도 조율 편차를 추정하기 위한 기준으로 사용될 수 있다.\n일반적으로 음악 녹음이 주어지면 하나의 음을 따로 연주한다고 가정할 수는 없다. 다음에서는 녹음된 음악이 \\(12\\)음 평균율 음계를 기반으로 하지만 이상적인 조율에서 벗어날 가능성이 있는 시나리오를 고려해본다. 이 편차가 센트로 주어진 단일 파라미터 \\(\\theta\\)로 표현될 수 있다고 가정한다.\n예를 들어, 이전에 사용된 디튜닝된(detuned) \\(\\mathrm{C}\\)-major 스케일은 \\(\\theta=40\\) 센트가 사용됐다. 조율 추정의 목적은 이 파라미터 \\(\\theta\\)를 결정하는 것이다. 다음은 기본 절차를 설명한다.\n\n\n먼저 신호의 전체 주파수 분포를 계산한다. 함수 compute_freq_distribution에서 두 가지 옵션을 고려한다.\n\nLocal=True: 신호의 (로컬) 크기 STFT를 계산한 다음, 평균을 냄\nLocal=False: 전체 신호의 (전역) 크기 DFT를 계산함\n\n크기(magnitude) DFT 또는 STFT를 계산할 때 로그 압축(compression)을 적용하여 작은 신호 구성 요소(예: gamma=100)를 향상시킨다.\n그런 다음 보간 기술(interpolation technique)을 사용하여 주파수 축(헤르츠로 주어짐)을 로그 축(1센트 해상도(resolution)의 센트 단위로 주어짐)으로 변환환다. 그 결과 신호의 주파수 분포를 나타내는 벡터 \\(v\\in\\mathbb{R}^M\\)가 생성된다.\n로컬 주파수 평균을 빼고, 결과를 수정(양수 부분만 고려)하여 이 벡터를 더 향상시킬 수 있다. filt_len 파라미터는 로컬 평균을 계산하는 데 사용되는 필터 길이(센트 단위)를 지정한다. 예를 들어 filt_len=101은 대략 1반음에 해당된다.\n다음 단계에서는 모든 조율 파라미터 \\(\\theta\\in \\Theta\\)에 대해 “comb-like template vectors” \\(\\mathbf{t}_\\theta\\in\\mathbb{R}^M\\)(함수 template_comb로 계산됨)를 정의한다. 각 템플릿 \\(\\mathbf{t}_\\theta\\)는 기준 조율이 \\(\\theta\\) 센트 이동된 12음 평균율 음계를 기반으로 이상적인 피치 그리드를 인코딩한다. \\(\\Theta\\) 집합은 고려해야 할 모든 가능한 조율 편차 집합을 나타낸다. 1센트 해상도에서 한 반음의 범위를 나타내는 \\(\\Theta=[-50:49]=\\{-50, -49, \\ldots,-1, 0, 1, \\ldots, 48, 49\\}\\)를 사용할 수 있다.\n마지막으로, 적절한 유사성 척도를 사용하여 \\(v\\)를 \\(\\mathbf{t}_\\theta\\) for all \\(\\theta\\in \\Theta\\)와 비교한다. tuning_similarity 함수를 이용해, 이 비교를 위해 단순히 내적(inner product) \\(\\langle v\\mid\\mathbf{t}_\\theta\\rangle\\)을 사용한다. 조율 파라미터는 유사성 최대화 매개변수 (similarity-maximizing parameter) \\(\\theta_\\mathrm{max}\\)로 다음과 같이 정의된다.\n\n\\[\\theta_\\mathrm{max} = \\mathrm{argmax}_{\\theta\\in\\Theta}\\langle v\\mid\\mathbf{t}_\\theta\\rangle\\]\n\ndef compute_freq_distribution(x, Fs, N=16384, gamma=100.0, local=True, filt=True, filt_len=101):\n    \"\"\"Compute an overall frequency distribution\n\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sampling rate\n        N (int): Window size (Default value = 16384)\n        gamma (float): Constant for logarithmic compression (Default value = 100.0)\n        local (bool): Computes STFT and averages; otherwise computes global DFT (Default value = True)\n        filt (bool): Applies local frequency averaging and by rectification (Default value = True)\n        filt_len (int): Filter length for local frequency averaging (length given in cents) (Default value = 101)\n\n    Returns:\n        v (np.ndarray): Vector representing an overall frequency distribution\n        F_coef_cents (np.ndarray): Frequency axis (given in cents)\n    \"\"\"\n    if local:\n        # Compute an STFT and sum over time\n        if N > len(x)//2:\n            raise Exception('The signal length (%d) should be twice as long as the window length (%d)' % (len(x), N))\n        Y, T_coef, F_coef = stft_convention_fmp(x=x, Fs=Fs, N=N, H=N//2, mag=True, gamma=gamma)\n        # Error \"range() arg 3 must not be zero\" occurs when N is too large. Why?\n        Y = np.sum(Y, axis=1)\n    else:\n        # Compute a single DFT for the entire signal\n        N = len(x)\n        Y = np.abs(np.fft.fft(x)) / Fs\n        Y = Y[:N//2+1]\n        Y = np.log(1 + gamma * Y)\n        # Y = libfmp.c3.log_compression(Y, gamma=100)\n        F_coef = np.arange(N // 2 + 1).astype(float) * Fs / N\n\n    # Convert linearly spaced frequency axis in logarithmic axis (given in cents)\n    # The minimum frequency F_min corresponds 0 cents.\n    f_pitch = lambda p: 440 * 2 ** ((p - 69) / 12)\n    p_min = 24               # C1, MIDI pitch 24\n    F_min = f_pitch(p_min)   # 32.70 Hz\n    p_max = 108              # C8, MIDI pitch 108\n    F_max = f_pitch(p_max)   # 4186.01 Hz\n    F_coef_log, F_coef_cents = compute_f_coef_log(R=1, F_min=F_min, F_max=F_max)\n    Y_int = interp1d(F_coef, Y, kind='cubic', fill_value='extrapolate')(F_coef_log)\n    v = Y_int / np.max(Y_int)\n\n    if filt:\n        # Subtract local average and rectify\n        filt_kernel = np.ones(filt_len)\n        Y_smooth = signal.convolve(Y_int, filt_kernel, mode='same') / filt_len\n        # Y_smooth = signal.medfilt(Y_int, filt_len)\n        Y_rectified = Y_int - Y_smooth\n        Y_rectified[Y_rectified < 0] = 0\n        v = Y_rectified / np.max(Y_rectified)\n\n    return v, F_coef_cents\n\n\ndef template_comb(M, theta=0):\n    \"\"\"Compute a comb template on a pitch axis\n\n    Args:\n        M (int): Length template (given in cents)\n        theta (int): Shift parameter (given in cents); -50 <= theta < 50 (Default value = 0)\n\n    Returns:\n        template (np.ndarray): Comb template shifted by theta\n    \"\"\"\n    template = np.zeros(M)\n    peak_positions = (np.arange(0, M, 100) + theta)\n    peak_positions = np.intersect1d(peak_positions, np.arange(M)).astype(int)\n    template[peak_positions] = 1\n    return template\n\n\ndef tuning_similarity(v):\n    \"\"\"Compute tuning similarity\n\n    Args:\n        v (np.ndarray): Vector representing an overall frequency distribution\n\n    Returns:\n        theta_axis (np.ndarray): Axis consisting of all tuning parameters -50 <= theta < 50\n        sim (np.ndarray): Similarity values for all tuning parameters\n        ind_max (int): Maximizing index\n        theta_max (int): Maximizing tuning parameter\n        template_max (np.ndarray): Similiarty-maximizing comb template\n    \"\"\"\n    theta_axis = np.arange(-50, 50)  # Axis (given in cents)\n    num_theta = len(theta_axis)\n    sim = np.zeros(num_theta)\n    M = len(v)\n    for i in range(num_theta):\n        theta = theta_axis[i]\n        template = template_comb(M=M, theta=theta)\n        sim[i] = np.inner(template, v)\n    sim = sim / np.max(sim)\n    ind_max = np.argmax(sim)\n    theta_max = theta_axis[ind_max]\n    template_max = template_comb(M=M, theta=theta_max)\n    return theta_axis, sim, ind_max, theta_max, template_max\n\n\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F08_C-major-scale.wav\")\n\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, \n                                            local=True, filt_len=101)\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\n\nprint('Estimated tuning: %d cents' % theta_max)\n\nEstimated tuning: 8 cents\n\n\n\n매개변수 의존도 (Parameter dependency)\n\n이 조율 추정이 작동하는 방법을 잘 이해하기 위해 \\(\\theta\\in\\Theta=[-50:49]\\)에 대한 함수로 \\(v\\)와 \\(\\mathbf{t}_\\theta\\) 사이의 유사성을 시각화해보자(plot_tuning_similarity 함수 사용). 또한 유사성 최대화 템플릿(plot_freq_vector_template 함수 사용)과 함께 주파수 벡터를 그려보자.\n\n\ndef plot_tuning_similarity(sim, theta_axis, theta_max, ax=None, title=None, figsize=(4, 3)):\n    \"\"\"Plots tuning similarity\n\n    Args:\n        sim: Similarity values\n        theta_axis: Axis consisting of cent values [-50:49]\n        theta_max: Maximizing tuning parameter\n        ax: Axis (in case of ax=None, figure is generated) (Default value = None)\n        title: Title of figure (or subplot) (Default value = None)\n        figsize: Size of figure (only used when ax=None) (Default value = (4, 3))\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n        line: handle for line plot\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig = plt.figure(figsize=figsize)\n        ax = plt.subplot(1, 1, 1)\n    if title is None:\n        title = 'Estimated tuning: %d cents' % theta_max\n    line = ax.plot(theta_axis, sim, 'k')\n    ax.set_xlim([theta_axis[0], theta_axis[-1]])\n    ax.set_ylim([0, 1.1])\n    ax.plot([theta_max, theta_max], [0, 1.1], 'r')\n    ax.set_xlabel('Tuning parameter (cents)')\n    ax.set_ylabel('Similarity')\n    ax.set_title(title)\n    if fig is not None:\n        plt.tight_layout()\n    return fig, ax, line\n\n\ndef plot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=None, title=None, figsize=(8, 3)):\n    \"\"\"Plots frequency distribution and similarity-maximizing template\n\n    Args:\n        v: Vector representing an overall frequency distribution\n        F_coef_cents: Frequency axis\n        template_max: Similarity-maximizing template\n        theta_max: Maximizing tuning parameter\n        ax: Axis (in case of ax=None, figure is generated) (Default value = None)\n        title: Title of figure (or subplot) (Default value = None)\n        figsize: Size of figure (only used when ax=None) (Default value = (8, 3))\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n        line: handle for line plot\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig = plt.figure(figsize=figsize)\n        ax = plt.subplot(1, 1, 1)\n    if title is None:\n        title = r'Frequency distribution with maximizing comb template ($\\theta$ = %d cents)' % theta_max\n    line = ax.plot(F_coef_cents, v, c='k', linewidth=1)\n    ax.set_xlim([F_coef_cents[0], F_coef_cents[-1]])\n    ax.set_ylim([0, 1.1])\n    x_ticks_freq = np.array([0, 1200, 2400, 3600, 4800, 6000, 7200, 8000])\n    ax.plot(F_coef_cents, template_max * 1.1, 'r:', linewidth=0.5)\n    ax.set_xticks(x_ticks_freq)\n    ax.set_xlabel('Frequency (cents)')\n    plt.title(title)\n    if fig is not None:\n        plt.tight_layout()\n    return fig, ax, line\n\n\n# Load audio signal\nx, Fs = librosa.load(\"../data_FMP/FMP_C3_F08_C-major-scale.wav\")\n\nprint('Average STFT (Local=True), without enhancement (Filt=False):', flush=True)\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, \n                                            local=True, filt=False)\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                          'height_ratios': [1]}, figsize=(10, 2))\nplot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\nplot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1])\nplt.show()\n\nprint('Average STFT (Local=True), with enhancement (Filt=True):', flush=True)\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, \n                                            local=True, filt_len=101)\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                          'height_ratios': [1]}, figsize=(10, 2))\nplot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\nplot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1])\nplt.show()\n\nprint('Global DFT (Local=False), with enhancement (Filt=True):', flush=True)\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, \n                                            local=False, filt_len=101)\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                          'height_ratios': [1]}, figsize=(10, 2))\nplot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\nplot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1])\nplt.show()\n\nAverage STFT (Local=True), without enhancement (Filt=False):\n\n\n\n\n\nAverage STFT (Local=True), with enhancement (Filt=True):\n\n\n\n\n\nGlobal DFT (Local=False), with enhancement (Filt=True):\n\n\n\n\n\n\n# 음악 예\n\nfn_wav_dict = {}\nfn_wav_dict['Cmaj'] = '../data_FMP/FMP_C3_F08_C-major-scale.wav'\nfn_wav_dict['Cmaj40'] = '../data_FMP/FMP_C3_F08_C-major-scale_40-cents-up.wav'\nfn_wav_dict['C4violin'] = '../data_FMP/FMP_C3_NoteC4_Violin.wav'\nfn_wav_dict['Burgmueller'] = '../data_FMP/FMP_C3_F05_BurgmuellerFirstPart.wav'\n\nfor name in fn_wav_dict:\n    fn_wav = fn_wav_dict[name]\n    x, Fs = librosa.load(fn_wav)\n    print('Audio example: %s' % name)\n    ipd.display(ipd.Audio(x, rate=Fs) )\n    v, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, \n                                                local=True, filt_len=101)\n    theta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\n    fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                              'height_ratios': [1]}, figsize=(10, 2))\n    plot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\n    #title=r'Frequency distribution with maximizing comb template'\n    title = None\n    plot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1],  title=title)\n    plt.show()\n\nAudio example: Cmaj\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\nAudio example: Cmaj40\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\nAudio example: C4violin\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\nAudio example: Burgmueller\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n주파수 범위 (Frequency Range)\n\n조율 추정의 또 다른 중요한 측면은 고려되는 주파수 범위이다. 12음 음계에 따라 완벽하게 튜닝된 음악 녹음을 고려할 때에도 튜닝된 피치의 고조파는 평균율 그리드에서 상당한 편차를 유발할 수 있다.\n또한 피아노와 같은 특정 악기의 경우 “string stiffness”로 인한 부조화 (inharmonicities) 로 인해 높은 고조파가 늘어나는 경향이 있다. 따라서 상위 주파수 스펙트럼이 너무 큰 역할을 할 때, 조율 추정치를 약간 증가시킬 수 있다.\n마지막으로 특정 주파수 범위가 조율 추정에 도움이 될 수 있는 음악적 이유가 있다. 예를 들어 Weber 오페라 녹음에서 가수의 강한 비브라토 및 여타 피치 변동의 존재는 조율 추정을 흐릿하고 불안정하게 만든다. 특히 소프라노 가수의 상위 주파수 대역이 문제가 되기도 한다.\n기본 조율 추정 절차(compute_freq_distribution 함수의 세팅 참조)에서 \\(33.7Hz\\)(C1, 0센트)에서 \\(4186Hz\\)(C8, 8400센트) 사이의 로그 샘플링 주파수 범위를 사용한다. 다음 코드 셀에서는 tuning_similarity 함수를 적용하기 전에 6000센트 이상의 상위 주파수 범위를 잘라낸다.\n\n\nx, Fs = librosa.load(\"../data_FMP/FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40.wav\")\nipd.display(ipd.Audio(x, rate=Fs))\n\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, local=True, filt_len=101)\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                          'height_ratios': [1]}, figsize=(12, 2))\nplot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\nplot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1])\nplt.show()\n\nv, F_coef_cents = compute_freq_distribution(x, Fs=Fs, N=16384, gamma=10, local=True, filt_len=101)\nv[6000:] = 0\ntheta_axis, sim, ind_max, theta_max, template_max = tuning_similarity(v)\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3], \n                                          'height_ratios': [1]}, figsize=(12, 2))\nplot_tuning_similarity(sim, theta_axis, theta_max, ax=ax[0])\nplot_freq_vector_template(v, F_coef_cents, template_max, theta_max, ax=ax[1])\nplt.show()\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C3/C3.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "",
    "text": "두 음악/오디오 시퀀스를 최적의 방법으로 시간에 따라 정렬할 수 있는 동적 시간 워핑 (dynamic time warping, DTW)에 대해 설명한다."
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#기본-개념",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#기본-개념",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "기본 개념",
    "text": "기본 개념\n\n두 시퀀스, 길이 \\(N\\in\\mathbb{N}\\)의 \\(X:=(x_1,x_2,\\ldots,x_N)\\)와 길이 \\(M\\in\\mathbb{N}\\)의 \\(Y:=(y_1,y_2,\\ldots,y_M)\\)가 주어져있을 때, 동적 시간 워핑 (dynamic time warping, DTW) 의 목적은 특정 제약 조건에서 최적으로 두 시퀀스를 시간적으로 정렬하는 것이다.\n시퀀스는 이산 신호, 피쳐 시퀀스, 문자 시퀀스 또는 모든 종류의 시계열일 수 있다. 종종 시퀀스의 인덱스는 균일한 시간 간격으로 간격을 둔 연속적인 시간 점에 해당한다. 다음 그림은 길이 \\(N=9\\)의 시퀀스 \\(X\\)와 길이 \\(M=7\\)의 시퀀스 \\(Y\\) 사이의 정렬(빨간색 양방향 화살표로 표시됨)을 보여준다.\n\n\nImage('../img/4.music_synchronization/FMP_C3_F12.png', width=600)\n\n\n\n\n\n각각의 빨간색 양방향 화살표는 \\(n\\in[1:N]\\) 및 \\(m\\in[1:M]\\)에 대한 두 요소 \\(x_n\\) 및 \\(y_m\\) 간의 대응 관계를 인코딩한다. 이러한 로컬 대응은 인덱스 쌍 \\((n,m)\\)로 모델링할 수 있다. 위 그림의 오른쪽은 왼쪽에 표시된 정렬이 일련의 인덱스 쌍으로 인코딩되는 방식을 보여준다."
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#워핑-경로-warping-path",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#워핑-경로-warping-path",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "워핑 경로 (Warping Path)",
    "text": "워핑 경로 (Warping Path)\n\n시퀀스 \\(X\\) 및 \\(Y\\)의 원소 간 전역(global) 정렬을 모델링하려면, 특정 제약 조건을 충족하는 인덱스 쌍(pairs) 시퀀스를 고려하는 것이 좋다.\n이것은 워핑 경로라는 개념으로 이어진다. 정의에 따르면 \\(L\\in\\mathbb{N}\\) 길이의 \\((N,M)\\)-워핑 경로는 다음의 시퀀스와 같다.\n\n\\(P=(p_1,\\ldots,p_L)\\)\nwith \\(p_\\ell=(n_\\ell,m_\\ell)\\in[1:N]\\times [1:M]\\) for \\(\\ell\\in[1:L]\\)\n\n그리고 다음의 세가지 조건을 만족한다.\n\n경계 조건(Boundary condition): \\(p_1= (1,1)\\) and \\(p_L=(N,M)\\)\n단조 조건(Monotonicity condition): \\(n_1\\leq n_2\\leq\\ldots \\leq n_L\\) and \\(m_1\\leq m_2\\leq \\ldots \\leq m_L\\)\n단계-크기 조건(Step-size condition): \\(p_{\\ell+1}-p_\\ell\\in \\Sigma:=\\{(1,0),(0,1),(1,1)\\}\\) for \\(\\ell\\in[1:L]\\)\n\n\\((N,M)\\)-워핑 경로 \\(P=(p_1,\\ldots,p_L)\\)는 두 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\) 사이의 정렬을 \\(X\\)의 원소 \\(x_{n_\\ell}\\)를 \\(Y\\)의 원소 \\(y_{m_\\ell}\\)에 할당하여 정의한다.\n경계 조건(boundary condition)은 \\(X\\) 및 \\(Y\\)의 첫 번째 원소와 \\(X\\) 및 \\(Y\\)의 마지막 원소가 서로 정렬되도록 강제한다.\n단조 조건(monotonicity condition)은 정확한 타이밍의 요구 사항을 반영한다. \\(X\\)의 원소가 \\(X\\)의 두 번째 원소 앞에 오는 경우 \\(Y\\)의 해당 원소에도 적용되어야 하며 그 반대의 경우도 마찬가지이다.\n마지막으로 집합 \\(\\Sigma\\)에 대한 단계-크기 조건(step-size condition)은 일종의 연속성 조건을 나타낸다. \\(X\\) 및 \\(Y\\)의 원소는 생략할 수 없으며 정렬에 복제가 없다.\n다음 그림은 세 조건(boundary, monotonicity, step-size)이 위반되는 예를 각각 보여준다.\n\n\nImage(\"../img/4.music_synchronization/FMP_C3_F13.png\", width=600)"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#비용행렬과-최적성-cost-matrix-and-optimality",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#비용행렬과-최적성-cost-matrix-and-optimality",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "비용행렬과 최적성 (Cost Matrix and Optimality)",
    "text": "비용행렬과 최적성 (Cost Matrix and Optimality)\n\n다음으로, 워핑 경로의 quality 에 대해 알려주는 개념을 살펴본다. 이를 위해서는 피쳐 시퀀스 \\(X\\)와 \\(Y\\)의 원소를 수치적으로 비교할 수 있는 방법이 필요하다. \\(\\mathcal{F}\\)를 피쳐 공간으로 두고, \\(x_n,y_m\\in\\mathcal{F}\\) for \\(n\\in[1:N]\\) and \\(m\\in[1:M]\\)를 가정한다.\n두 가지 다른 피쳐를 비교하려면 다음과 같은 함수로 정의된 로컬 비용 측정 (local cost measure)이 필요하다. \\[c:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\]\n일반적으로 \\(x\\)와 \\(y\\)가 서로 유사하면 \\(c(x,y)\\)는 작고(저비용), 그렇지 않으면 \\(c(x,y)\\)가 크다(고비용). 시퀀스 \\(X\\) 및 \\(Y\\)의 각 원소 쌍에 대한 로컬 비용 측정값을 평가하면 다음과 같이 정의된 비용 매트릭스 (cost matirx) \\(C\\in\\mathbb{R}^{N\\times M}\\)를 얻는다. \\[C(n,m):=c(x_n,y_m)\\] for \\(n\\in[1:N]\\) and \\(m\\in[1:M]\\).\n행렬 \\(C\\)의 항목을 나타내는 튜플 \\((n,m)\\)은 행렬의 셀 (cell) 이라고 한다. 로컬 비용 측정 \\(c\\)와 관련하여 두 시퀀스 \\(X\\) 및 \\(Y\\) 사이의 워핑 경로 \\(P\\)의 총 비용 \\(c_P(X,Y)\\)는 다음과 같이 정의된다. \\[ c_P:=\\sum_{\\ell=1}^L c(x_{n_\\ell},y_{m_\\ell}) = \\sum_{\\ell=1}^L C(n_\\ell,m_\\ell)\\]\n위 정의는 워핑 경로가 통과하는 모든 셀의 비용을 누적한다. 워핑 경로는 총 비용이 낮으면 “좋은 것”이고 총 비용이 높으면 “나쁜 것”이다.\n이제 \\(X\\)와 \\(Y\\) 사이의 최적 워핑 경로 (optimal warping path) 를 보자. 이는 가능한 모든 워핑 경로 중에서 총 비용이 최소인 워핑 경로 \\(P^\\ast\\)로 정의된다.\n이 워핑 경로의 셀은 두 시퀀스의 원소 간의 전반적인 최적 정렬을 인코딩한다. 여기서 워핑 경로 조건은 \\(X\\) 시퀀스의 각 원소가 \\(Y\\)의 적어도 하나의 원소에 할당되며 그 반대의 경우도 마찬가지다.\n이것은 길이 \\(N\\)의 \\(X\\)와 길이 \\(M\\)의 \\(Y\\) 사이에서 \\(\\mathrm{DTW}(X,Y)\\)로 표시되는 DTW 거리(distance) 의 정의로 이어진다. 이는 최적의 \\((N,M)\\)-워핑 경로 \\(P^\\ast\\)의 총 비용으로 정의된다. \\[\\mathrm{DTW}(X,Y) :=c_{P^\\ast}(X,Y) = \\min\\{c_P(X,Y)\\mid P \\mbox{ is an $(N,M)$-warping path} \\}\\]"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#예시",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#예시",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "예시",
    "text": "예시\n\n# 길이가 다른 두 시퀀스\nX =  [1, 3, 9, 2, 1] #시퀀스\nY = [2, 0, 0, 8, 7, 2] #시퀀스\nN = len(X)\nM = len(Y)\n\nplt.figure(figsize=(6, 2))\nplt.plot(X, c='k', label='$X$')\nplt.plot(Y, c='b', label='$Y$')\nplt.legend()\nplt.tight_layout()\n\n\n\n\n\ndef compute_cost_matrix(X, Y, metric='euclidean'):\n    \"\"\"Compute the cost matrix of two feature sequences\n\n    Args:\n        X (np.ndarray): Sequence 1\n        Y (np.ndarray): Sequence 2\n        metric (str): Cost metric, a valid strings for scipy.spatial.distance.cdist (Default value = 'euclidean')\n\n    Returns:\n        C (np.ndarray): Cost matrix\n    \"\"\"\n    X, Y = np.atleast_2d(X, Y)\n    C = scipy.spatial.distance.cdist(X.T, Y.T, metric=metric)\n    return C\n\n\n# 비용행렬 계산\nC =  compute_cost_matrix(X, Y, metric='euclidean')\nprint('Cost matrix C =', C, sep='\\n')\n\nCost matrix C =\n[[1. 1. 1. 7. 6. 1.]\n [1. 3. 3. 5. 4. 1.]\n [7. 9. 9. 1. 2. 7.]\n [0. 2. 2. 6. 5. 0.]\n [1. 1. 1. 7. 6. 1.]]\n\n\n\ndef compute_accumulated_cost_matrix(C):\n    \"\"\"Compute the accumulated cost matrix given the cost matrix\n\n    Args:\n        C (np.ndarray): Cost matrix\n\n    Returns:\n        D (np.ndarray): Accumulated cost matrix\n    \"\"\"\n    N = C.shape[0]\n    M = C.shape[1]\n    D = np.zeros((N, M))\n    D[0, 0] = C[0, 0]\n    for n in range(1, N):\n        D[n, 0] = D[n-1, 0] + C[n, 0]\n    for m in range(1, M):\n        D[0, m] = D[0, m-1] + C[0, m]\n    for n in range(1, N):\n        for m in range(1, M):\n            D[n, m] = C[n, m] + min(D[n-1, m], D[n, m-1], D[n-1, m-1])\n    return D\n\n\n# 누적 비용행렬 계산\nD =  compute_accumulated_cost_matrix(C)\nprint('Accumulated cost matrix D =', D, sep='\\n')\nprint('DTW distance DTW(X, Y) =', D[-1, -1])\n\nAccumulated cost matrix D =\n[[ 1.  2.  3. 10. 16. 17.]\n [ 2.  4.  5.  8. 12. 13.]\n [ 9. 11. 13.  6.  8. 15.]\n [ 9. 11. 13. 12. 11.  8.]\n [10. 10. 11. 18. 17.  9.]]\nDTW distance DTW(X, Y) = 9.0\n\n\n\ndef compute_optimal_warping_path(D):\n    \"\"\"Compute the warping path given an accumulated cost matrix\n\n    Args:\n        D (np.ndarray): Accumulated cost matrix\n\n    Returns:\n        P (np.ndarray): Optimal warping path\n    \"\"\"\n    N = D.shape[0]\n    M = D.shape[1]\n    n = N - 1\n    m = M - 1\n    P = [(n, m)]\n    while n > 0 or m > 0:\n        if n == 0:\n            cell = (0, m - 1)\n        elif m == 0:\n            cell = (n - 1, 0)\n        else:\n            val = min(D[n-1, m-1], D[n-1, m], D[n, m-1])\n            if val == D[n-1, m-1]:\n                cell = (n-1, m-1)\n            elif val == D[n-1, m]:\n                cell = (n-1, m)\n            else:\n                cell = (n, m-1)\n        P.append(cell)\n        (n, m) = cell\n    P.reverse()\n    return np.array(P)\n\n\n# 최적 warping path 계산\nP = compute_optimal_warping_path(D)\nprint('Optimal warping path P =', P.tolist())\n\nOptimal warping path P = [[0, 0], [0, 1], [1, 2], [2, 3], [2, 4], [3, 5], [4, 5]]\n\n\n\nc_P = sum(C[n, m] for (n, m) in P)\nprint('Total cost of optimal warping path:', c_P)\nprint('DTW distance DTW(X, Y) =', D[-1, -1])\n\nTotal cost of optimal warping path: 9.0\nDTW distance DTW(X, Y) = 9.0\n\n\n\nP = np.array(P) \nplt.figure(figsize=(8, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(C, cmap='gray_r', origin='lower', aspect='equal')\nplt.plot(P[:, 1], P[:, 0], marker='o', color='r')\nplt.clim([0, np.max(C)])\nplt.colorbar()\nplt.title('$C$ with optimal warping path')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.subplot(1, 2, 2)\nplt.imshow(D, cmap='gray_r', origin='lower', aspect='equal')\nplt.plot(P[:, 1], P[:, 0], marker='o', color='r')\nplt.clim([0, np.max(D)])\nplt.colorbar()\nplt.title('$D$ with optimal warping path')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.tight_layout()\n\n\n\n\n\n## librosa example\n\nD, P = librosa.sequence.dtw(X, Y, metric='euclidean', \n                            step_sizes_sigma=np.array([[1, 1], [0, 1], [1, 0]]),\n                            weights_add=np.array([0, 0, 0]), weights_mul=np.array([1, 1, 1]))\n\nplt.figure(figsize=(8, 3))\nax = plt.subplot(1,1,1)\nplot_matrix_with_points(D, P, linestyle='-', \n    ax=[ax], aspect='equal', clim=[0, np.max(D)],\n    title='$D$ with optimal warping path', xlabel='Sequence Y', ylabel='Sequence X');\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#단계-크기-조건-step-size-condition",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#단계-크기-조건-step-size-condition",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "단계 크기 조건 (Step-size Condition)",
    "text": "단계 크기 조건 (Step-size Condition)\n\n기존 DTW에서 step-size 조건은 \\(\\Sigma=\\{(1,0),(0,1),(1,1)\\}\\) 집합으로 표현된다. 일종의 local 연속성 조건을 도입하는 이 조건은 워핑 경로가 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)의 각 원소를 \\(Y=(y_1,y_2,\\ldots, y_M)\\)의 원소로 할당하도록 하며 그 반대도 마찬가지이다.\n이 조건의 한 가지 단점은 한 시퀀스의 단일 원소가 다른 시퀀스의 많은 연속된 원소에 할당되어 워핑 경로에서 수직 및 수평 섹션으로 이어질 수 있다는 것이다.\n직관적으로 이러한 경우 워핑 경로는 시퀀스 중 하나의 시퀀스에서는 특정 위치에 고정되고 다른 시퀀스에서는 이동한다. 물리적 시간 측면에서 이 상황은 두 시계열의 정렬에서 강한 시간적 변형(temporal deformation)에 해당된다. 이러한 변성을 피하기 위해 허용 가능한 워핑 경로의 기울기를 제한하여 step-size 조건을 수정할 수 있다.\n이는 집합 \\(\\Sigma\\)를 교체하여 수행할 수 있다.\n\n\nImage(\"../img/4.music_synchronization/FMP_C3_F17.png\", width=600)\n\n\n\n\n\n첫 번째 경우 원래의 집합 \\(\\Sigma=\\{(1,0),(0,1),(1,1)\\}\\)가 사용된다(워핑 경로의 degeneration 관찰).\n이 집합을 \\(\\Sigma = \\{(2,1),(1,2),(1,1)\\}\\)과 같이 교체하면 워핑 경로가 \\(1/2\\) 및 \\(2\\) 범위 내에서 local 기울기를 갖는 워핑 경로로 이어진다.\n새로운 step size 제약을 만족하는 최적 워핑 경로를 계산하기 위해 원래의 DTW 알고리즘을 조금만 변형하면 된다. 누적 비용행렬 \\(\\mathbf{D}\\)을 계산하기 위해 다음의 recursion을 사용한다. \\[\\mathbf{D}(n,m)= \\mathbf{C}(n,m) + \\min\\left\\{\n           \\begin{array}{l}\\mathbf{D}(n-1,m-1),\\\\ \\mathbf{D}(n-2,m-1),\\\\\\mathbf{D}(n-1,m-2) \\end{array}\\right.\\] for \\(n\\in [1:N]\\) and \\(m\\in [1:N]\\) with \\((n,m)\\not=(1,1)\\).\n초기화를 위해 \\(\\mathbf{D}\\)를 두 개의 추가 행과 열(\\(-1\\) 및 \\(0\\)로 인덱싱됨), 그리고 집합 \\(\\mathbf{D}(1,1):=\\mathbf{C}(1,1)\\), \\(\\mathbf{D}(n,-1):=\\mathbf{D}(n,0):=\\infty\\) for \\(n\\in [-1:N]\\)와 \\(\\mathbf{D}(-1,m):=\\mathbf{D}(0,m):=\\infty\\) for \\(m\\in [-1:M]\\) 로 확장하는 트릭을 사용할 수 있다.\n수정된 step-size 조건과 관련하여 두 시퀀스 \\(X\\)와 \\(Y\\) 사이에 유한의 총 비용 워핑 경로가 있다. 또한 \\(X\\)의 모든 원소를 \\(Y\\)의 일부 원소에 할당할 필요는 없으며 그 반대도 마찬가지다(위 그림 참조). 위 그림의 세 번째 경우는 워핑 경로의 기울기에 제약을 가하면서 이러한 누락을 피하는 step-size 조건을 보여준다.\n\n\ndef compute_accumulated_cost_matrix_21(C):\n    \"\"\"Compute the accumulated cost matrix given the cost matrix\n\n    Args:\n        C (np.ndarray): Cost matrix\n\n    Returns:\n        D (np.ndarray): Accumulated cost matrix\n    \"\"\"\n    N = C.shape[0]\n    M = C.shape[1]\n    D = np.zeros((N + 2, M + 2))\n    D[:, 0:2] = np.inf\n    D[0:2, :] = np.inf\n    D[2, 2] = C[0, 0]\n\n    for n in range(N):\n        for m in range(M):\n            if n == 0 and m == 0:\n                continue\n            D[n+2, m+2] = C[n, m] + min(D[n-1+2, m-1+2], D[n-2+2, m-1+2], D[n-1+2, m-2+2])\n    D = D[2:, 2:]\n    return D\n\n\ndef compute_optimal_warping_path_21(D):\n    \"\"\"Compute the warping path given an accumulated cost matrix\n\n    Args:\n        D (np.ndarray): Accumulated cost matrix\n\n    Returns:\n        P (np.ndarray): Optimal warping path\n    \"\"\"\n    N = D.shape[0]\n    M = D.shape[1]\n    n = N - 1\n    m = M - 1\n    P = [(n, m)]\n    while n > 0 or m > 0:\n        if n == 0:\n            cell = (0, m - 1)\n        elif m == 0:\n            cell = (n - 1, 0)\n        else:\n            val = min(D[n-1, m-1], D[n-2, m-1], D[n-1, m-2])\n            if val == D[n-1, m-1]:\n                cell = (n-1, m-1)\n            elif val == D[n-2, m-1]:\n                cell = (n-2, m-1)\n            else:\n                cell = (n-1, m-2)\n        P.append(cell)\n        (n, m) = cell\n    P.reverse()\n    P = np.array(P)\n    return P\n\n\n# Sequences\nX = [1, 3, 9, 2, 1]\nY = [2, 0, 0, 8, 7, 2]\nN, M = len(X), len(Y)\n\nC = compute_cost_matrix(X, Y, metric='euclidean')\nD = compute_accumulated_cost_matrix_21(C)\nP = compute_optimal_warping_path_21(D)  \n    \nplt.figure(figsize=(6, 2))\nplt.plot(X, c='k', label='X')\nplt.plot(Y, c='b', label='Y')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figsize=(9, 3))\nax = plt.subplot(1, 2, 1)\nplot_matrix_with_points(C, P, linestyle='-', \n    ax=[ax], aspect='equal', clim=[0, np.max(C)],\n    title='$C$ with optimal warping path', xlabel='Sequence Y', ylabel='Sequence X');\n\nax = plt.subplot(1, 2, 2)\nD_max = np.nanmax(D[D != np.inf])\nplot_matrix_with_points(D, P, linestyle='-', \n    ax=[ax], aspect='equal', clim=[0, D_max],\n    title='$D$ with optimal warping path', xlabel='Sequence Y', ylabel='Sequence X');\nfor x, y in zip(*np.where(np.isinf(D))):\n    plt.text(y, x, '$\\infty$', horizontalalignment='center', verticalalignment='center')\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\n## librosa 예\n\ndef compute_plot_D_P(X, Y, ax, step_sizes_sigma=np.array([[1, 1], [0, 1], [1, 0]]),\n                     weights_mul=np.array([1, 1, 1]), title='',\n                     global_constraints=False, band_rad=0.25):\n    D, P = librosa.sequence.dtw(X, Y, metric='euclidean', weights_mul=weights_mul,\n                    step_sizes_sigma=step_sizes_sigma, \n                    global_constraints=global_constraints, band_rad=band_rad)\n    D_max = np.nanmax(D[D != np.inf])\n    plot_matrix_with_points(D, P, linestyle='-', \n        ax=[ax], aspect='equal', clim=[0, D_max],\n        title= title, xlabel='Sequence Y', ylabel='Sequence X');\n    for x, y in zip(*np.where(np.isinf(D))):\n        plt.text(y, x, '$\\infty$', horizontalalignment='center', verticalalignment='center')\n    \nX = [1, 3, 9, 2, 1, 3, 9, 9]\nY = [2, 0, 0, 9, 1, 7]  \n\nprint('다양한 step-size 조건에 대한 누적 비용 행렬 및 최적 워핑 경로:')\nplt.figure(figsize=(10, 3))\n\nax = plt.subplot(1, 3, 1)\nstep_sizes_sigma = np.array([[1, 0], [0, 1], [1, 1]])\ntitle='Step sizes:'+''.join(str(s) for s in step_sizes_sigma)\ncompute_plot_D_P(X, Y, ax=ax, step_sizes_sigma=step_sizes_sigma, title=title)\n\nax = plt.subplot(1, 3, 2)\nstep_sizes_sigma = np.array([[1, 1], [2, 1], [1, 2]])\ntitle='Step sizes:'+''.join(str(s) for s in step_sizes_sigma)\ncompute_plot_D_P(X, Y, ax=ax, step_sizes_sigma=step_sizes_sigma, title=title)\n\nax = plt.subplot(1, 3, 3)\nstep_sizes_sigma = np.array([[1, 1], [3, 1], [1, 3]])\ntitle='Step sizes:'+''.join(str(s) for s in step_sizes_sigma)                                                       \ncompute_plot_D_P(X, Y, ax=ax, step_sizes_sigma=step_sizes_sigma, title=title)\n\nplt.tight_layout()\n\n다양한 step-size 조건에 대한 누적 비용 행렬 및 최적 워핑 경로:"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#로컬-가중치-local-weights",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#로컬-가중치-local-weights",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "로컬 가중치 (Local Weights)",
    "text": "로컬 가중치 (Local Weights)\n\n정렬에 수직, 수평 또는 대각선 방향 등 특정 방향을 선호한다면, 추가 로컬 가중치 (local weights) \\(w_{\\mathrm{d}},w_{\\mathrm{h}},w_{\\mathrm{v}}\\in\\mathbb{R}\\)를 도입할 수 있다. 누적 비용 행렬 \\(\\mathbf{D}\\)를 계산하기 위해 다음 initialization 및 recursion을 사용한다.\n\n\\[\\mathbf{D}(1,1):=\\mathbf{C}(1,1)\\] \\[\\mathbf{D}(n,1)=\\sum_{k=1}^{n} w_{\\mathrm{h}}\\cdot \\mathbf{C}(k,1) \\,\\,\\mbox{for}\\,\\, n\\in [2:N]\\] \\[\\mathbf{D}(1,m)=\\sum_{k=1}^{m} w_{\\mathrm{v}}\\cdot \\mathbf{C}(1,k) \\,\\,\\mbox{for}\\,\\, m\\in [2:M]\\] \\[\\mathbf{D}(n,m)=\\min \\left\\{\n   \\begin{array}{l}\n    \\mathbf{D}(n-1,m-1) + w_{\\mathrm{d}}\\cdot \\mathbf{C}(n,m) \\\\\n    \\mathbf{D}(n-1,m)   + w_{\\mathrm{v}}\\cdot \\mathbf{C}(n,m) \\\\\n    \\mathbf{D}(n,m-1)   + w_{\\mathrm{h}}\\cdot \\mathbf{C}(n,m)\n   \\end{array}\n   \\right.\\]\nfor \\(n\\in[2:N]\\) and \\(m\\in[2:M]\\).\n\n\\(w_{\\mathrm{d}}=w_{\\mathrm{h}}=w_{\\mathrm{v}}=1\\)이면 기존 DTW로 축소된다.\n기본적으로 하나의 대각선 단계(셀 하나의 비용)가 하나의 수평 단계와 하나의 수직 단계(두 셀의 비용)의 조합에 해당하기 때문에, 대각선 정렬 방향을 선호한다. 이러한 선호의 균형을 맞추기 위해 종종 \\(w_{\\mathrm{d}}=2\\) 및 \\(w_{\\mathrm{h}}=w_{\\mathrm{v}}=1\\)를 선택한다.\n마찬가지로 다른 step-size 조건에 대한 가중치를 도입할 수 있다.\n다음 코드 셀에서 다른 가중치 설정으로 librosa.sequence.dtw 함수를 호출한다.\n\n\nX = [1, 1, 1, 1, 2, 1, 1, 6, 6]\nY = [0, 3, 3, 3, 9, 9, 7, 7]\n\nprint(r'다양한 local 가중치에 따른 누적 비용 행렬과 최적 warping 경로:')\nplt.figure(figsize=(10, 3))\n\nax = plt.subplot(1, 3, 1)\nweights_mul = np.array([1, 1, 1])\ntitle='Weights: '+'[ '+''.join(str(s)+' ' for s in weights_mul)+']'\ncompute_plot_D_P(X, Y, ax=ax, weights_mul=weights_mul, title=title)\n\nax = plt.subplot(1, 3, 2)\nweights_mul = np.array([2, 1, 1])\ntitle='Weights: '+'[ '+''.join(str(s)+' ' for s in weights_mul)+']'\ncompute_plot_D_P(X, Y, ax=ax, weights_mul=weights_mul, title=title)\n\nax = plt.subplot(1, 3, 3)\nweights_mul = np.array([1, 3, 3])\ntitle='Weights: '+'[ '+''.join(str(s)+' ' for s in weights_mul)+']'                                                      \ncompute_plot_D_P(X, Y, ax=ax, weights_mul=weights_mul, title=title)\n\nplt.tight_layout()\n\n다양한 local 가중치에 따른 누적 비용 행렬과 최적 warping 경로:"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#전역-제약-global-constraints",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#전역-제약-global-constraints",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "전역 제약 (Global Constraints)",
    "text": "전역 제약 (Global Constraints)\n\n일반적인 DTW 변형 중 하나는 허용 가능한 워핑 경로에 전역 제약(global constraint)을 부과하는 것이다.\n이러한 제약 조건은 DTW 계산 속도를 높일 뿐만 아니라 워핑 경로의 전체 과정을 전체적으로 제어하여 “pathological” 정렬을 방지한다.\n보다 정확하게는 \\(R\\subseteq [1:N]\\times[1:M]\\)를 전역적 제약 영역이라고 하는 하위 집합이라고 하자. 그러면 \\(R\\)과 연관된(relative to) 워핑 경로 는 \\(R\\) 지역 내에서 완전히 실행되는 워핑 경로이다.\n\\(P^\\ast_{R}\\)로 표시되는 \\(R\\)과 연관된 최적 워핑 경로 는 \\(R\\)에 대한 모든 워핑 경로 중에서 비용을 최소화하는 워핑 경로이다.\n다음 그림은 Sakoe–Chiba 밴드 및 Itakura 평행사변형으로 알려진 두 개의 전역 제약 영역을 보여준다. 셀의 정렬은 각 음영 영역에서만 선택할 수 있다.\n\n\nImage(\"../img/4.music_synchronization/FMP_C3_F18_text.png\", width=600)\n\n\n\n\n\n일반적으로 제약 영역 \\(R\\)의 경우, 경로 \\(P^\\ast_{R}\\)는 \\(\\mathbf{C}(n,m):=\\infty\\) for all \\((n,m)\\in [1:N]\\times[1:M]\\setminus R\\)로 세팅하여 제약 없는 경우와 유사하게 계산할 수 있다.\n따라서 \\(P^\\ast_{R}\\) 계산에서 \\(R\\)에 있는 셀만 평가하면 된다. 이렇게 하면 DTW 계산 속도가 상당히 빨라질 수 있다. 그러나 제약되지 않은 최적 워핑 경로 \\(P^\\ast\\)가 지정된 제약 영역 외부의 셀을 통과할 수 있기 때문에 전역 제약 영역의 사용 또한 문제가 된다. 이 경우 제한된 최적 워핑 경로 \\(P^\\ast_{R}\\)는 \\(P^\\ast\\)와 일치하지 않는다(위 그림의 마지막 경우 참조).\n다음 코드 셀에서 Sakoe-Chiba 밴드에 의해 결정된 다양한 제약 영역으로 librosa.sequence.dtw 함수를 사용해본다.\n다음의 예에서 워핑 경로는 \\((1, 1)\\)에서 시작하지 않는데, 이는 librosa의 역추적 버그 때문이다.(차후 수정될 수 있음)\n\n\nX = [1, 1, 1, 1, 2, 1, 1, 6, 6]\nY = [0, 3, 3, 3, 9, 9, 7, 7]\n\nprint(r'Accumulated cost matrix and optimal warping path for different constraint regions:')\nplt.figure(figsize=(10, 3))\nglobal_constraints = True\n\nax = plt.subplot(1, 3, 1)\nband_rad = 1\ntitle='Sakao-Chiba band (rad = %.2f)'%band_rad\ncompute_plot_D_P(X, Y, ax=ax, global_constraints=global_constraints, band_rad=band_rad,title=title)\n\nax = plt.subplot(1, 3, 2)\nband_rad = 0.5\ntitle='Sakao-Chiba band (rad = %.2f)'%band_rad\ncompute_plot_D_P(X, Y, ax=ax, global_constraints=global_constraints, band_rad=band_rad,title=title)\n\nax = plt.subplot(1, 3, 3)\nband_rad = 0.25\ntitle='Sakao-Chiba band (rad = %.2f)'%band_rad\ncompute_plot_D_P(X, Y, ax=ax, global_constraints=global_constraints, band_rad=band_rad,title=title)\n\nplt.tight_layout()\n\nAccumulated cost matrix and optimal warping path for different constraint regions:"
  },
  {
    "objectID": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#멀티스케일-multiscale-dtw",
    "href": "posts/4. Music Synchronization/4.2.Dynamic_Time_Warping.html#멀티스케일-multiscale-dtw",
    "title": "4.2. 동적 시간 워핑 (DTW)",
    "section": "멀티스케일 (Multiscale) DTW",
    "text": "멀티스케일 (Multiscale) DTW\n\nglobal 제약 영역의 개념을 사용할 때, 계산할 최적 워핑 경로가 실제로 이 영역 내에 있는지 확인해야 한다. 이 경로는 선험적으로 알려져 있지 않기 때문에, 제한 영역을 가능한 한 작게 선택(계산 속도를 높이기 위해)하지만 원하는 경로를 포함할 만큼 충분히 크게 선택하는 것 사이에서 적절한 절충점을 찾기가 어렵다.\n“올바른” 경로를 찾을 확률을 높이는 한 가지 가능한 전략은 데이터 독립적인 고정 제약 조건 영역 대신 데이터 종속 제약 조건 영역을 사용하는 것이다. 이 아이디어는 DTW에 대한 멀티스케일 접근 방식 (multiscale approach) 에 의해 실현될 수 있다.\n여기서 일반적인 전략은 재귀적으로 거친 해상도 수준에서 계산된 최적 워핑 경로를 다음 상위 수준으로 투영한 다음, 투영된 경로를 미세 조정하는 것이다. 다음 그림은 이러한 접근 방식의 주요 단계를 요약한 것이다. 거친 해상도 수준에서 계산된 최적 워핑은 더 미세한 해상도 수준으로 투영된다. 작은 이웃과 함께(전체 절차의 견고성을 높이기 위해) 더 미세한 해상도 수준에서 워핑 경로를 계산하는 데 사용되는 제약 조건 영역을 정의한다.\n\n\nImage(\"../img/4.music_synchronization/FMP_C3_F19.png\", width=600)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html",
    "title": "5.1. 음악 구조와 분할",
    "section": "",
    "text": "주어진 음악 녹음에 대한 구조적(structural) 분석과 시간적 분할(segmentation)를 소개하고, 반복성(repitition), 동질성(homogeneity) 및 새로움(novelty)을 기반으로 하는 기본 분할 원칙에 대해 논의한다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#분할과-구조-분석-segmentation-and-structure-analysis",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#분할과-구조-분석-segmentation-and-structure-analysis",
    "title": "5.1. 음악 구조와 분할",
    "section": "분할과 구조 분석 (Segmentation and Structure Analysis)",
    "text": "분할과 구조 분석 (Segmentation and Structure Analysis)\n\n분할/세분화(Segmentation) 은 일반적으로 원본보다 더 의미 있고 분석하기 쉬운 것으로 표시를 단순화하기 위해 지정된 문서를 여러 세그먼트로 분할하는 과정을 말한다. 예를 들어 이미지 처리에서 목표는 주어진 이미지를 영역 집합으로 분할하여 각 영역이 색상, 강도 또는 질감과 같은 일부 특성과 관련하여 유사하도록 하는 것이다. 영역 경계는 종종 이미지 밝기 또는 기타 속성이 급격하게 변하고 불연속성을 나타내는 등고선이나 가장자리로 설명될 수 있다.\n음악에서 분할 작업은 주어진 오디오 스트림을 음향적으로 의미 있는 섹션으로 분해하는 것이다. 각각은 시작 및 종료 경계(boundary) 로 지정된 연속 시간 간격에 해당한다.\n미세한 수준에서의 분할은 개별 음 사이의 경계를 찾거나 비트 위치에 의해 지정된 비트 간격을 찾는 것을 목표로 할 수 있다. 대략적인 수준에서의 목표는 악기나 화음의 변화를 감지하거나 절(verse)과 합창 부분 사이의 경계를 찾는 것일 수 있다. 또한 침묵, 말, 음악을 구별하거나 실제 음악 녹음의 시작 부분을 찾거나 공연이 끝난 후 박수를 분리하는 것이 전형적인 분할 작업이다.\n구조 분석(structural analysis) 의 목표는 단순한 분할을 넘어 세그먼트 간의 관계를 찾고 이해하는 것이다.\n예를 들어, 특정 세그먼트는 악기에 의해 특징지어질 수 있다. 현악기로만 연주되는 구간이 있을 수 있으며, 전체 오케스트라가 연주하는 섹션 다음에 솔로 섹션이 이어질 수 있다. 노래하는 목소리가 있는 절 부분은 순전히 기악 부분으로 대체될 수 있다. 또는 부드럽고 느린 도입부 부분이 훨씬 빠른 템포로 연주되는 메인 테마 앞에 나올 수 있다.\n또한 섹션이 자주 반복된다. 대부분의 음악 관련 이벤트는 어떤 식으로든 반복된다. 그러나 반복은 원래 섹션과 거의 동일하지 않으며 가사, 악기 또는 멜로디와 같은 측면에서 조금씩 수정된다. 구조 분석의 주요 작업 중 하나는 주어진 음악 녹음을 분할하는 것뿐만 아니라 세그먼트를 음악적으로 의미 있는 범주(예: 인트로, 코러스, 절, 아웃트로)로 그룹화하는 것이다.\n전산 음악 구조 분석의 문제는 음악의 구조가 반복, 대조, 변형 및 동질성을 비롯한 다양한 종류의 관계에서 발생한다는 것이다. 음악 구조에 결정적으로 영향을 미치는 다양한 원리를 고려하여 음악 구조 분석에 대한 다양한 접근 방식이 많이 개발되었다. 세 가지 방법 분류를 대략적으로 구분해보자.\n\n첫째, 반복(repetition) 기반 방법은 반복 패턴을 식별하는 데 사용된다.\n둘째, 새로움(novelty) 기반 방법을 사용하여 대조되는 부분 간의 전환을 감지한다.\n셋째, 동질성(homogeneity) 기반 방법은 일부 음악적 속성과 관련하여 일관된 구절을 결정하는 데 사용된다.\n\n새로움 기반 및 동질성 기반 접근 방식은 동전의 양면 같다. 새로움 감지는 더 동질적인 세그먼트 이후의 놀라운 이벤트 또는 변화를 관찰하는 것을 기반으로 한다. 새로움 감지의 목적은 변경 사항의 시간 위치를 찾는 것이지만 동질성 분석의 초점은 일부 음악적 속성과 관련하여 일관성 있는 더 긴 악절을 식별하는 데 있다.\n다음 그림은 유사한 분할 및 구조화 원칙이 이미지 및 3D 데이터와 같은 다른 도메인에 적용됨을 보여준다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F03_text.png\", width=600)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#음악-구조-musical-structure",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#음악-구조-musical-structure",
    "title": "5.1. 음악 구조와 분할",
    "section": "음악 구조 (Musical Structure)",
    "text": "음악 구조 (Musical Structure)\n\n음악 구조를 구체화하기 위해 몇 가지 용어를 소개한다. 먼저 음악 작품(a piece of music)(다소 추상적인 의미)과 특정 오디오 녹음(audio recording)(실제 연주)을 구분해야 한다. 파트(part) 라는 용어는 추상적인 음악 영역에서 사용되는 반면 세그먼트(segment) 라는 용어는 오디오 영역에서 사용된다.\n음악적 파트는 일반적으로 처음 나타나는 순서대로 대문자 \\(A,B,C,\\ldots\\)로 표시되며 여기서 숫자(종종 아래 첨자로 표시됨)는 반복되는 순서를 나타낸다.\n예를 들어 요하네스 브람스의 헝가리 무곡 5번을 생각해보자. 이 춤은 피아노 버전에서 풀 오케스트라 버전에 이르기까지 다양한 악기와 앙상블을 위해 편곡되었다. 다음 그림은 전체 오케스트라를 위한 편곡의 바이올린 음성에 대한 악보 표현을 보여준다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F05_Sibelius_annotated.png\", width=600)\n\n\n\n\n\n음악적 구조는 \\(A_1A_2B_1B_2CA_3B_3B_4D\\)로 \\(A\\) 파트 3개, \\(B\\) 파트 4개, \\(C\\) 파트 1개, 닫는 \\(D\\) 파트 1개로 구성되어 있다. \\(A\\) 부분에는 두 개 이상 반복되는 하위 부분으로 구성된 하위 구조가 있다. 또한 악보를 보면 알 수 있듯이 중간 \\(C\\) 부분은 \\(d_1d_2e_1e_2e_3e_4\\)로 설명할 수 있는 하위 구조로 더 분할될 수 있다. 악보에서 이러한 하위 부분은 종종 소문자 \\(a,b,c,\\ldots\\)를 사용하여 표시된다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F28.png\", width=400)\n\n\n\n\n\ndef convert_structure_annotation(ann, Fs=1, remove_digits=False, index=False):\n    \"\"\"Convert structure annotations\n\n    Args:\n        ann (list): Structure annotions\n        Fs (scalar): Sampling rate (Default value = 1)\n        remove_digits (bool): Remove digits from labels (Default value = False)\n        index (bool): Round to nearest integer (Default value = False)\n\n    Returns:\n        ann_converted (list): Converted annotation\n    \"\"\"\n    ann_converted = []\n    for r in ann:\n        s = r[0] * Fs\n        t = r[1] * Fs\n        if index:\n            s = int(np.round(s))\n            t = int(np.round(t))\n        if remove_digits:\n            label = ''.join([i for i in r[2] if not i.isdigit()])\n        else:\n            label = r[2]\n        ann_converted = ann_converted + [[s, t, label]]\n    return ann_converted\n\n\ndef read_structure_annotation(fn_ann, fn_ann_color, Fs=1, remove_digits=False, index=False):\n    \"\"\"Read and convert structure annotation and colors\n\n    Args:\n        fn_ann (str): Path and filename for structure annotions\n        fn_ann_color (str): Filename used to identify colors (Default value = '')\n        Fs (scalar): Sampling rate (Default value = 1)\n        remove_digits (bool): Remove digits from labels (Default value = False)\n        index (bool): Round to nearest integer (Default value = False)\n\n    Returns:\n        ann (list): Annotations\n        color_ann (dict): Color scheme\n    \"\"\"\n    df = pd.read_csv(fn_ann, sep=';', keep_default_na=False, header=0)\n    ann = [(start, end, label) for i, (start, end, label) in df.iterrows()]\n    ann = convert_structure_annotation(ann, Fs=Fs, remove_digits=remove_digits, index=index)\n    color_ann = {}\n    if len(fn_ann_color) > 0:\n        color_ann = fn_ann_color\n        if remove_digits:\n            color_ann_reduced = {}\n            for key, value in color_ann.items():\n                key_new = ''.join([i for i in key if not i.isdigit()])\n                color_ann_reduced[key_new] = value\n            color_ann = color_ann_reduced\n    return ann, color_ann\n\n\nann_color = {'A1': [1, 0, 0, 0.2], 'A2': [1, 0, 0, 0.2], 'A3': [1, 0, 0, 0.2],\n                     'B1': [0, 1, 0, 0.2], 'B2': [0, 1, 0, 0.2], 'B3': [0, 1, 0, 0.2],\n                     'B4': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0]}\n\n\n# Annotation file\nfn_ann = \"../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv\"\n\n# Read annotations\nann, color_ann = read_structure_annotation(fn_ann, fn_ann_color=fn_ann_color)\nprint('Original annotations with time specified in seconds')\nprint('Annotations:', ann)\nprint('Colors:', color_ann)\nfig, ax = plot_segments(ann, figsize=(8, 1.2), colors=color_ann, time_label='Time (seconds)')\nplt.show()\n\n# Read and convert annotations\nFs = 2\nann, color_ann = read_structure_annotation(fn_ann, fn_ann_color=fn_ann_color, Fs=Fs, remove_digits=True, index=True)\nprint('Converted annotations (Fs = %d) with reduced labels (removing digits)'%Fs)\nprint('Annotations:', ann)\nprint('Colors:', color_ann)\nfig, ax = plot_segments(ann, figsize=(8, 1.2), colors=color_ann, time_label='Time (frames)')\nplt.show()\n\nOriginal annotations with time specified in seconds\nAnnotations: [[0.0, 1.01, ''], [1.01, 22.11, 'A1'], [22.11, 43.06, 'A2'], [43.06, 69.42, 'B1'], [69.42, 89.57, 'B2'], [89.57, 131.64, 'C'], [131.64, 150.84, 'A3'], [150.84, 176.96, 'B3'], [176.96, 196.9, 'B4'], [196.9, 199.64, '']]\nColors: {'A1': [1, 0, 0, 0.2], 'A2': [1, 0, 0, 0.2], 'A3': [1, 0, 0, 0.2], 'B1': [0, 1, 0, 0.2], 'B2': [0, 1, 0, 0.2], 'B3': [0, 1, 0, 0.2], 'B4': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0]}\n\n\n\n\n\nConverted annotations (Fs = 2) with reduced labels (removing digits)\nAnnotations: [[0, 2, ''], [2, 44, 'A'], [44, 86, 'A'], [86, 139, 'B'], [139, 179, 'B'], [179, 263, 'C'], [263, 302, 'A'], [302, 354, 'B'], [354, 394, 'B'], [394, 399, '']]\nColors: {'A': [1, 0, 0, 0.2], 'B': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0]}"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#크로마그램",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#크로마그램",
    "title": "5.1. 음악 구조와 분할",
    "section": "크로마그램",
    "text": "크로마그램\n\n첫째, 크로마 기반 표현은 음악 녹음의 화성(harmonic) 및 멜로디(melodic) 속성과 관련된다.\n크로마그램에서 볼 수 있는 패턴은 중요한 구조 정보를 나타낸다.\n\n\n# Chromagram\nN, H = 4096, 2048\nchromagram = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\n\nfilt_len = 41\ndown_sampling = 10\nfilt_kernel = np.ones([1,filt_len])\nchromagram_smooth =  signal.convolve(chromagram, filt_kernel, mode='same')/filt_len\nchromagram_smooth = chromagram_smooth[:,::down_sampling]\nchromagram_smooth, Fs_smooth = smooth_downsample_feature_sequence(chromagram, \n                        Fs/H, filt_len=filt_len, down_sampling=down_sampling)\n\n# Visualization\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 2, 0.5]}, figsize=(9, 5))       \nplot_chromagram(chromagram, Fs=Fs/H, ax=[ax[0,0], ax[0,1]],  \n                         chroma_yticks = [0,4,7,11], \n                         title='Chromagram (resolution %0.1f Hz)'%(Fs/H), \n                         ylabel='Chroma', colorbar=True);\nplot_chromagram(chromagram_smooth, Fs_smooth, ax=[ax[1,0], ax[1,1]],  \n                         chroma_yticks = [0,4,7,11], \n                         title='Smoothed chromagram (resolution %0.1f Hz)'%Fs_smooth, \n                         ylabel='Chroma', colorbar=True);\nplot_segments(ann, ax=ax[2,0], time_max=x_dur, \n                       colors=color_ann, time_label='Time (seconds)')\nax[2,1].axis('off')\n\nplt.tight_layout()\n\n\n\n\n\nAABBCABB\n4개의 반복되는 \\(B\\) 부분 세그먼트는 크로마그램에서 4개의 유사한 특성 하위 시퀀스로 명확하게 표시된다.\n또한 \\(C\\) 부분 세그먼트는 전체 섹션에서 높은 수준의 동질성을 보여줌으로써 크로마그램에서 두드러진다. 실제로 이 세그먼트의 모든 크로마 기능에 대해 대부분의 신호 에너지는 \\(\\mathrm{G}\\)-, \\(\\mathrm{B}\\)- 및 \\(\\mathrm{D}\\)-대역에 포함된다(이는 \\(C\\) 부분이 \\(\\mathrm{G}\\) major에 있기 때문에 놀라운 일은 아니다).\n대조적으로, \\(A\\) 부분 세그먼트의 경우 많은 크로마 벡터가 \\(\\mathrm{G}\\)-, \\(\\mathrm{B}^\\flat\\)- 및 \\(\\mathrm{D}\\)에 지배적으로 분포한다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#mfcc-표현",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#mfcc-표현",
    "title": "5.1. 음악 구조와 분할",
    "section": "MFCC 표현",
    "text": "MFCC 표현\n\n멜로디와 하모니 외에도 악기와 음색(timbre)의 특성은 음악 구조에 대한 인간의 인식에 매우 중요하다.\n음색 기반 구조 분석의 맥락에서 원래 자동 음성 인식용으로 개발된 MFCC(mel-frequency cepstral coefficients) 를 자주 사용한다.\n음악의 경우 MFFC 기반 특징은 악기 및 음색과 같은 측면과 상관관계가 있는 중간 수준(mid-level)의 표현이다.\n노래하는 목소리가 있는 섹션과 악기 섹션이 번갈아 나타나는 팝송과 같은 많은 음악 녹음의 경우 MFCC 기반 특징 표현은 새로움(novelty) 기반 및 동질성 기반 분할에 매우 적합하다.\n\n\n# MFCC\nN, H = 4096, 2048\nX_MFCC = librosa.feature.mfcc(y=x, sr=Fs, hop_length=H, n_fft=N)\ncoef = np.arange(4,15)\nX_MFCC_upper = X_MFCC[coef,:]\n\n# Visualization\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 2, 0.5]}, figsize=(9, 5))       \nplot_matrix(X_MFCC, Fs=Fs/H, ax=[ax[0,0], ax[0,1]], \n                     title='MFCC (coefficents 0 to 19)', ylabel='', colorbar=True);\nax[0,0].set_yticks([0, 10, 19])\nplot_matrix(X_MFCC_upper, Fs=Fs/H, ax=[ax[1,0], ax[1,1]], \n                     title='MFFC (coefficents 4 to 14)', ylabel='', colorbar=True);\nax[1,0].set_yticks([0, 5, 10])\nax[1,0].set_yticklabels(coef[0] + [0, 5, 10])\nplot_segments(ann, ax=ax[2,0], time_max=x_dur, \n                       colors=color_ann, time_label='Time (seconds)');  \nax[2,1].axis('off')\nplt.tight_layout()\n\n\n\n\n\n\\(A\\) 부분 세그먼트 내의 MFCC 기능이 \\(B\\) 부분 및 \\(C\\) 부분 세그먼트의 기능과 다르다는 것을 인식할 수 있다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#템포그램-tempogram-표현",
    "href": "posts/5. Music Structure Analysis/5.1.Music_Structure_and_Segmentation.html#템포그램-tempogram-표현",
    "title": "5.1. 음악 구조와 분할",
    "section": "템포그램 (Tempogram) 표현",
    "text": "템포그램 (Tempogram) 표현\n\n음악 구조 분석에서 템포 및 비트 정보는 동질성 기반 분할 접근 방식과 함께 사용될 수도 있다. 이러한 정보를 명시적으로 추출하는 대신, 템포 및 리듬과 관련된 중간-수준의 특징 표현으로 더 높은 구조 수준에서 의미 있는 분할을 유도하는 데 충분할 수 있다.\n로컬 템포 정보를 인코딩하는 중간-수준 표현인 템포그램을 사용한다.\n템포그램의 순환 변형이 표시되며 여기서 2의 거듭제곱 차이가 나는 템포가 식별된다. 옥타브별로 다른 피치가 식별되는 순환 크로마 특징과 유사하다. 템포그램 표현을 살펴보면 다양한 음악 파트가 다른 템포로 연주된다는 것을 알 수 있다(단, 표현이 정확한 템포를 나타내지는 않지만). 또한 템포그램 특징에 지배적인 항목이 없는 섹션이 있는데, 이는 녹음에 템포에 대한 명확한 개념이 없음을 나타낼 수 있다.\n\n(자세한 것은 템포에 대해 다루는 챕터에서 보기로 한다.)\n\nimport utils.tempo_tools as tmpo\n\n\n# Tempogram\nnov, Fs_nov = tmpo.compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\nnov, Fs_nov = tmpo.resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nX, T_coef, F_coef_BPM = tmpo.compute_tempogram_fourier(nov, Fs_nov, N=1000, H=100, Theta=np.arange(30, 601))\n\noctave_bin = 30\ntempogram_F = np.abs(X)\noutput = tmpo.compute_cyclic_tempogram(tempogram_F, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_F = output[0]\nF_coef_scale = output[1]\ntempogram_cyclic_F = normalize_feature_sequence(tempogram_cyclic_F, norm='max')\n\noutput = tmpo.compute_tempogram_autocorr(nov, Fs_nov, N=500, H=100, norm_sum=False,\n                                              Theta=np.arange(30, 601))\ntempogram_A = output[0]\nT_coef = output[1]\nF_coef_BPM = output[2]\n\noutput = tmpo.compute_cyclic_tempogram(tempogram_A, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_A = output[0]\nF_coef_scale = output[1]\ntempogram_cyclic_A = normalize_feature_sequence(tempogram_cyclic_A, norm='max')\n\n# Visualization\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 2, 0.5]}, figsize=(9, 5))       \n\nplot_matrix(tempogram_cyclic_F, T_coef=T_coef, ax=[ax[0,0], ax[0,1]], \n                     title='Fourier-based cyclic tempogram', ylabel='Scaling',\n                     colorbar=True, clim=[0, 1])\ntmpo.set_yticks_tempogram_cyclic(ax[0,0], octave_bin, F_coef_scale, num_tick=5)\n\nplot_matrix(tempogram_cyclic_A, T_coef=T_coef, ax=[ax[1,0], ax[1,1]], \n                     title='Autocorrelation-based cyclic tempogram', ylabel='Scaling',\n                     colorbar=True, clim=[0, 1])\ntmpo.set_yticks_tempogram_cyclic(ax[1,0], octave_bin, F_coef_scale, num_tick=5)\n\nplot_segments(ann, ax=ax[2,0], time_max=x_dur, \n                       colors=color_ann, time_label='Time (seconds)')\nax[2,1].axis('off')\n\nplt.tight_layout()\n\n\n\n\n\n다양한 음악적 차원 외에도 적절한 특징 표현을 찾을 때 염두에 두어야 할 또 다른 측면이 있다. 바로 시간적(temporal) 차원이다. 위에서 언급한 모든 특징 표현에서 분석 창(winodw)은 음악 신호 위로 이동한다.\n분명히 분석 윈도우의 길이와 홉 크기 매개변수는 특징 표현의 품질에 결정적인 영향을 미친다. 예를 들어 긴 윈도우 크기와 큰 홉 크기는 동질성 기반 분할에서 종종 원하는 속성인 관련 없는 로컬 변형을 부드럽게 하는 데 도움이 될 수 있다. 단점은 시간적 해상도가 감소하고 중요한 세부 정보가 손실될 수 있어 정확한 분할 경계를 찾을 때 문제가 발생할 수 있다는 것이다.\n요약하면 특징 표현 및 매개변수 설정의 적절한 선택은 응용 맥락에 따라 크게 달라진다. 음악 구조의 풍부함과 다양성은 구조 분석을 어렵게 할 수 있다.\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S1_MusicStructureGeneral.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "",
    "text": "음악 구조를 분석하기 위한 기술적 도구인 자기 유사성 행렬(self similarity matrix)의 개념에 대해 상세히 다루고, 그 구조적 속성에 대해 논의한다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#기본-정의",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#기본-정의",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "기본 정의",
    "text": "기본 정의\n\n\\(\\mathcal{F}\\)를 특징(feature) 공간이라고 하고 \\(s:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\)를 두 요소 \\(x,y\\in\\mathcal{F}\\)를 비교할 수 있는 유사성(similarity) 척도라고 하자. 일반적으로 \\(s(x,y)\\) 값은 요소 \\(x,y\\in\\mathcal{F}\\)가 비슷하면 높고 그렇지 않으면 작다.\n특징 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)가 주어지면 시퀀스의 모든 요소를 서로 비교할 수 있다.\n결과적으로 \\(N\\)-square 자기 유사성 행렬(self-similarity matrix) \\(\\mathbf{S}\\in\\mathbb{R}^{N\\times N}\\)이 아래와 같이 정의된다.\n\n\\[\\mathbf{S}(n,m):=s(x_n,x_m),\\] where \\(x_n,x_m\\in\\mathcal{F}\\) for \\(n,m\\in[1:N]\\)\n\n다음에서 \\((n,m)\\in[1:N]\\times[1:N]\\) 튜플은 \\(\\mathbf{S}\\)의 셀(cell)이라고도 하며 값 \\(\\mathbf{S}(n,m)\\)는 셀 \\((n,m)\\)의 점수(score)라고 한다.\n응용의 맥락 및 데이터를 비교하는 데 사용되는 개념에 따라 반복 플롯(recurrence plot), 비용 매트릭스(cost matrix) 또는 자기-거리 매트릭스(self-distance matrix) 와 같은 다른 이름으로 알려진 관련된 개념이 많다.\n종종 특징 공간이 차원 \\(K\\in\\mathbb{N}\\)의 유클리드 공간 \\(\\mathcal{F}=\\mathbb{R}^K\\)라고 가정한다. 간단한 유사성 측정 \\(s\\)는 예를 들어 다음과 같이 정의된 내적이다.\n\n\\[s(x,y) := \\langle x,y\\rangle\\] for two vectors \\(x,y\\in\\mathcal{F}\\)\n\n이 유사성 측정을 사용하면 직교(orthogonal)하는 두 특징(feature) 벡터 사이의 점수는 0이고, 그렇지 않으면 0이 아니다. 특징 벡터가 유클리드 노름(norm)에 대해 정규화되는 경우, 유사성 값 \\(s(x,y)\\)는 \\([-1,1]\\) 구간에 있다. 이 경우 정규화된 피쳐의 시퀀스 \\(X=x_1,x_2,\\ldots,x_N)\\)가 주어지면, \\(s(x_n,x_n)=1\\) for all \\(n\\in[1:N]\\)인 경우 SSM의 최대값이 가정된다. 따라서 결과 SSM에는 값이 큰 대각선이 있다. 더 일반적으로 말하면, 주어진 특징 시퀀스의 반복 패턴은 유사성 값이 큰 구조의 형태로 SSM에서 볼 수 있다.\n다음 예에서는 정규화된(normalized) 특징 벡터의 합성(synthetic) 특징 시퀀스를 생성한다. 특징 벡터의 차원은 \\(K=4\\)이고 시퀀스의 길이는 \\(N=500\\)이다. 그림은 특징 시퀀스와 결과 SSM을 보여준다.\n중요 사항:\n\nSSM을 시각화할 때 컬러맵 ’cmap’의 선택은 그림의 전체적인 모습에 상당한 영향을 미칠 수 있다. 적합한 컬러맵을 선택하면 SSM의 특정 속성을 시각적으로 강조하는 데 도움이 될 수 있다.\n위에서 설명한 정규화된 특징의 특징 시퀀스와 유사도 측정을 사용하면 간단한 행렬-행렬 곱으로 SSM을 계산할 수 있다. 보다 정확하게는 \\(K\\times N\\)-matrix \\(X\\)에 의해 특성 시퀀스가 구현되는 경우, SSM \\(S\\)는 \\(S=X^\\top X\\)로 주어진다. \n또한, 특징 벡터의 모든 항목이 양수라고 종종 가정한다. 이 경우 \\(s(x_n,x_m)\\) 값은 양수이며 \\([0,1]\\) 간격에 있다.\n\n\n\n# Generate normalized feature sequence\nK = 4\nM = 100\nr = np.arange(M)\nb1 = np.zeros((K,M))\nb1[0,:] = r\nb1[1,:] = M-r\nb2 = np.ones((K,M))\nX = np.concatenate(( b1, b1, np.roll(b1, 2, axis=0), b2, b1 ), axis=1)\nX_norm = normalize_feature_sequence(X, norm='2', threshold=0.001)\n\n# Compute SSM\nS = np.dot(np.transpose(X_norm), X_norm)\n\n# Visualization\ncmap = 'gray_r'\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [0.3, 1]}, figsize=(4, 5))\nplot_matrix(X_norm, Fs=1, ax=[ax[0,0], ax[0,1]], cmap=cmap,\n            xlabel='Time (frames)', ylabel='', title='Feature sequence')\nplot_matrix(S, Fs=1, ax=[ax[1,0], ax[1,1]], cmap=cmap,\n            title='SSM', xlabel='Time (frames)', ylabel='Time (frames)', colorbar=True);\nplt.tight_layout()\n\n\n\n\n\n컬러맵을 적절하게 조정하여 시각적 모양을 변경할 수 있다. 예를 들어 색상 분포를 더 밝은 색상으로 이동하면 시각화의 경로 구조가 향상된다.\n\n\ncmap = compressed_gray_cmap(alpha=-1000)\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [0.3, 1]}, figsize=(4,5))\nplot_matrix(X, Fs=1, ax=[ax[0,0], ax[0,1]], cmap=cmap,\n            xlabel='Time (frames)', ylabel='', title='Feature sequence')\nplot_matrix(S, Fs=1, ax=[ax[1,0], ax[1,1]], cmap=cmap,\n            title='SSM', xlabel='Time (frames)', ylabel='Time (frames)', colorbar=True);\nplt.tight_layout()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#블록과-경로구조-block-and-path-structures",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#블록과-경로구조-block-and-path-structures",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "블록과 경로구조 (Block and Path Structures)",
    "text": "블록과 경로구조 (Block and Path Structures)\n\nSSM에서 가장 눈에 띄는 두 가지 구조는 이미 앞의 예에서 볼 수 있듯이 블록(block) 및 경로(paths) 라고 한다.\n특징 시퀀스가 전체 음악의 지속 기간(duration) 동안 다소 일정하게 유지되는 음악 속성을 캡처하는 경우, 각 특징 벡터는 이 세그먼트 내의 다른 모든 특징 벡터와 유사함을 의미한다. 결과적으로 큰 값의 전체 블록이 SSM에 나타난다.\n\n즉, 동질성(homogeneity) 속성은 블록 같은(block-like) 구조에 해당한다.\n\n특징 시퀀스가 2개의 반복 하위시퀀스(예를 들어, 동일한 멜로디에 대응하는 2개의 세그먼트)를 포함하는 경우, 두 하위시퀀스의 대응하는 요소는 서로 유사하다. 결과적으로 주 대각선과 평행하게 실행되는 유사성이 높은 경로(또는 스트라이프(stripe))가 SSM에 표시된다.\n\n즉, 반복(repetitive) 속성은 경로 같은(path-like) 구조에 해당한다.\n\n예로, 다음 그림은 음악적 구조가 \\(A_1A_2B_1B_2CA_3B_3B_4D\\)인 Brahms의 헝가리 무곡 5번에 대한 이상적인 SSM을 보여준다. 세 개의 반복되는 \\(A\\) 부분 세그먼트가 동종이라고 가정하면 SSM에는 \\(A_1A_2\\)에 해당하는 세그먼트를 자신과 관련시키는 2차 블록과 \\(A_3\\) 부분 세그먼트를 자신과 관련시키는 또 다른 2차 블록이 있다. 또한 두 개의 직사각형 블록이 있는데, 하나는 \\(A_1A_2\\) 부분 세그먼트를 \\(A_3\\) 부분 세그먼트에 연결하고 다른 하나는 \\(A_3\\) 부분 세그먼트를 \\(A_1A_2\\) 부분 세그먼트에 연결한다. 3개의 반복되는 \\(A\\) 부분 세그먼트가 동질적이지 않은 경우 SSM은 주 대각선에 평행하게 실행되는 경로 구조를 나타낸다. 예를 들어 \\(A_1\\)와 \\(A_2\\)의 유사도가 큰 경로와 \\(A_1\\)와 \\(A_3\\)의 경로가 있다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F07a.png\", width=300)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#크로마그램-기반-ssm",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#크로마그램-기반-ssm",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "크로마그램 기반 SSM",
    "text": "크로마그램 기반 SSM\n\n다음 코드는 크로마그램(chromagram) 을 특징 표현으로 사용하여 Brahms의 헝가리 댄스 녹음에서 SSM을 생성한다. 시각화에서 \\(\\mathbf{S}\\)의 큰 값은 어두운 회색으로 표시되고 작은 값은 밝은 회색으로 표시된다.\n실제로 이 경우 얻은 SSM은 이상적인 SSM과 상당 부분 유사하다. \\(A\\) 부분 세그먼트에 해당하는 block-like 구조는 이러한 세그먼트가 화음과 관련하여 상당히 동질적임을 나타낸다. \\(C\\) 부분 세그먼트도 마찬가지다. 또한, \\(C\\) 부분 블록 외부의 작은 유사성 값(즉, \\(C\\) 부분 프레임을 다른 세그먼트의 프레임에 연결하는 모든 셀)은 \\(C\\) 부분 세그먼트가 화성적으로 모든 부분과 거의 관련이 없음을 보여준다. \\(B\\) 부분 세그먼트의 경우 path-like 구조가 있고 block-like 구조는 없다. 이는 \\(B\\) 파트 세그먼트가 동일한 화성 진행(즉, 화성에 대한 반복)을 공유하지만 화성에 대해 동질적이지 않음을 보여준다.\n흥미로운 점은 반복하더라도 \\(B\\) 부분 세그먼트가 다른 템포로 재생되므로 길이가 다르다는 것이다. 예를 들어 짧은 \\(B_2\\) 섹션은 \\(B_1\\) 섹션보다 빠르게 재생된다. 결과적으로 해당 경로는 정확히 병렬로 실행되지 않는다.\n\n\ndef compute_sm_dot(X, Y):\n    \"\"\"Computes similarty matrix from feature sequences using dot (inner) product\n\n    Args:\n        X (np.ndarray): First sequence\n        Y (np.ndarray): Second Sequence\n\n    Returns:\n        S (float): Dot product\n    \"\"\"\n    S = np.dot(np.transpose(X), Y)\n    return S\n\n\ndef plot_feature_ssm(X, Fs_X, S, Fs_S, ann, duration, color_ann=None,\n                     title='', label='Time (seconds)', time=True,\n                     figsize=(5, 6), fontsize=10, clim_X=None, clim=None):\n    \"\"\"Plot SSM along with feature representation and annotations (standard setting is time in seconds)\n\n    Args:\n        X: Feature representation\n        Fs_X: Feature rate of ``X``\n        S: Similarity matrix (SM)\n        Fs_S: Feature rate of ``S``\n        ann: Annotaions\n        duration: Duration\n        color_ann: Color annotations (see :func:`libfmp.b.b_plot.plot_segments`) (Default value = None)\n        title: Figure title (Default value = '')\n        label: Label for time axes (Default value = 'Time (seconds)')\n        time: Display time axis ticks or not (Default value = True)\n        figsize: Figure size (Default value = (5, 6))\n        fontsize: Font size (Default value = 10)\n        clim_X: Color limits for matrix X (Default value = None)\n        clim: Color limits for matrix ``S`` (Default value = None)\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n    \"\"\"\n    cmap = compressed_gray_cmap(alpha=-10)\n    fig, ax = plt.subplots(3, 3, gridspec_kw={'width_ratios': [0.1, 1, 0.05],\n                                              'wspace': 0.2,\n                                              'height_ratios': [0.3, 1, 0.1]},\n                           figsize=figsize)\n    plot_matrix(X, Fs=Fs_X, ax=[ax[0, 1], ax[0, 2]], clim=clim_X,\n                         xlabel='', ylabel='', title=title)\n    ax[0, 0].axis('off')\n    plot_matrix(S, Fs=Fs_S, ax=[ax[1, 1], ax[1, 2]], cmap=cmap, clim=clim,\n                         title='', xlabel='', ylabel='', colorbar=True)\n    ax[1, 1].set_xticks([])\n    ax[1, 1].set_yticks([])\n    plot_segments(ann, ax=ax[2, 1], time_axis=time, fontsize=fontsize,\n                           colors=color_ann,\n                           time_label=label, time_max=duration*Fs_X)\n    ax[2, 2].axis('off')\n    ax[2, 0].axis('off')\n    plot_segments(ann, ax=ax[1, 0], time_axis=time, fontsize=fontsize,\n                           direction='vertical', colors=color_ann,\n                           time_label=label, time_max=duration*Fs_X)\n    return fig, ax\n\n\nfn_wav = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.wav'\nx, Fs = librosa.load(fn_wav)\nipd.Audio(x,rate=Fs)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nx_duration = (x.shape[0])/Fs\n\n# Chroma Feature Sequence\nN, H = 4096, 1024\nchromagram = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=2, hop_length=H, n_fft=N)\nX, Fs_X = smooth_downsample_feature_sequence(chromagram, Fs/H, filt_len=41, down_sampling=10) #smoothed\n\n# Annotation\nfn_ann = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv'\nann, color_ann = read_structure_annotation(fn_ann)\nann_frames = convert_structure_annotation(ann, Fs=Fs_X) \n\n# SSM \nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\nS = compute_sm_dot(X,X)\n\n\nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X, color_ann=color_ann,\n                           clim_X=[0,1], clim=[0,1], label='Time (frames)',\n                           title='Chroma feature (Fs=%0.2f)'%Fs_X)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#mfcc-기반-ssm",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#mfcc-기반-ssm",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "MFCC 기반 SSM",
    "text": "MFCC 기반 SSM\n\n다음으로 MFCC 특징을 기반으로 SSM을 계산해본다. 이 표현의 모든 \\(K(=20)\\)개의 MFCC 계수를 사용하면 주로 block-like 구조를 가지는 SSM이 생성된다. 특히 \\(A\\) 부분과 \\(C\\) 부분에 대략적으로 대응하는 블록을 관찰할 수 있다. 실제로 블록 구조는 MFCC 특징의 처음 두 개(낮은) 계수에 의해 지배된다. 계수 \\(4\\) ~ \\(14\\)만 고려하면 더 미세한 블록 구조를 갖고 경로와 같은 구조를 나타내는 SSM이 생성된다.\n중요 사항:\n\nSSM을 계산하기 전에, 다운샘플링과 함께 평균화 필터를 적용하여 스무딩을 적용한다. 스무딩은 피처 시퀀스의 작은 로컬 변동을 억제하고 결과 SSM에 상당한 영향을 미칠 수 있다.\n특징 시퀀스를 \\(H\\)만큼 다운샘플링하면 SSM \\(\\mathbf{S}\\)를 계산할 때 \\(H^2\\)만큼 효율성이 증가한다.\nMFCC 특징의 값이 음수일 수 있으므로 SSM 값도 음수일 수 있다(MFCC 기능을 정규화한 후 \\([1,-1]\\) 구간에서).\n\n\n\n# MFCC-based feature sequence\nN, H = 2048, 1024\nX_MFCC = librosa.feature.mfcc(y=x, sr=Fs, hop_length=H, n_fft=N)\n\ncoef = np.arange(0,20)\nX_MFCC_upper = X_MFCC[coef,:]\nX, Fs_X = smooth_downsample_feature_sequence(X_MFCC_upper, Fs/H, filt_len=41, down_sampling=10)\nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\nS = compute_sm_dot(X,X)\n\nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X, color_ann=color_ann,\n    title='MFCC (20 coefficients, Fs=%0.2f)'%Fs_X, label='Time (frames)')\n\n\n# MFCC-based feature sequence only using coefficients 4 to 14\ncoef = np.arange(4,15)\nX_MFCC_upper = X_MFCC[coef,:]\nX, Fs_X = smooth_downsample_feature_sequence(X_MFCC_upper, Fs/H, filt_len=41, down_sampling=10)\nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\nS = compute_sm_dot(X,X)\nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X, \n                           color_ann=color_ann, label='Time (frames)',\n                           title='MFCC (coefficients 4 to 14, Fs=%0.2f)'%Fs_X)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#템포그램-기반-ssm",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#템포그램-기반-ssm",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "템포그램 기반 SSM",
    "text": "템포그램 기반 SSM\n\n마지막으로 순환 푸리에 템포그램(cyclic Fourier tempogram) 을 기본 특징(feature) 표현으로 사용하여 SSM을 계산한다. 크로마 기반의 SSM에 비해 템포그램 기반의 SSM은 구조가 명확하지 않다.\n적어도 \\(C\\) 부분 세그먼트에 해당하는 블록을 관찰할 수 있으므로 대조되는 역할을 강조한다. 또한 SSM은 음악 녹음에서 발생하는 많은 템포 변화를 나타낸다.\n\n\n# Tempogram feature sequence\nnov, Fs_nov = tmpo.compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\nnov, Fs_nov = tmpo.resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\n\nN, H = 1000, 100\nX, T_coef, F_coef_BPM = tmpo.compute_tempogram_fourier(nov, Fs_nov, N=N, H=H, Theta=np.arange(30, 601))\noctave_bin = 12\ntempogram_F = np.abs(X)\noutput = tmpo.compute_cyclic_tempogram(tempogram_F, F_coef_BPM, octave_bin=octave_bin)\nX = output[0]\nF_coef_scale = output[1]\nFs_X = Fs_nov/H\nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\nS = compute_sm_dot(X,X)\n\n\nann_frames = convert_structure_annotation(ann, Fs=Fs_X)\n\nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X, color_ann=color_ann,\n    title='Tempogram (Fs=%0.2f)'%Fs_X, label='Time (frames)')"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#경로와-블록의-정의",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#경로와-블록의-정의",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "경로와 블록의 정의",
    "text": "경로와 블록의 정의\n\n세그먼트(segment)를 시작점 \\(s\\)와 끝점 \\(t\\)로 지정된 집합 \\(\\alpha=[s:t]\\subseteq [1:N]\\)로 정의한다. (특징 인덱스로 주어짐).\n\\(|\\alpha|:=t-s+1\\)가 \\(\\alpha\\)의 길이를 나타낸다고 하자.\n다음으로, 길이 \\(L\\)의 \\(\\alpha\\) 의 경로(path)는 다음의 시퀀스와 같다. \\[P=((n_1,m_1), \\ldots,(n_L,m_L))\\]\n\n\\((n_\\ell,m_\\ell)\\in[1:N]^2\\), \\(\\ell\\in[1:L]\\)\n\\(m_1=s\\) 및 \\(m_L=t\\)를 만족 (boundary condition)\n\\((n_{\\ell+1},m_{\\ell+1}) -(n_\\ell,m_\\ell)\\in \\Sigma\\) (step size condition), 여기서 \\(\\Sigma\\)는 허용 가능한 step size 집합을 나타냄\n\n이 정의는 워핑 경로(warping path)의 정의와 비슷하다. \\(\\Sigma = \\{(1,1)\\}\\)의 경우, 경로는 “strictly diagonal”하다.\n다음에서 집합 \\(\\Sigma = \\{(2,1),(1,2),(1,1)\\}\\)을 사용한다. 경로 \\(P\\)에 대해, 각각 \\(\\pi_1(P):=[n_1:n_L]\\)와 \\(\\pi_2(P):=[m_1:m_L]\\) 투영(projection)으로 정의된 두 세그먼트를 연결할 수 있다.\n경계 조건에 따라 \\(\\pi_2(P)=\\alpha\\)이 적용된다. 다른 세그먼트 \\(\\pi_1(P)\\)는 유도된(induced) 세그먼트라고 한다.\n\\(P\\)의 점수 \\(\\sigma(P)\\)는 다음과 같이 정의된다. \\[\\sigma(P) := \\sum_{\\ell=1}^L \\mathbf{S}(n_\\ell,m_\\ell)\\]\n\\(\\alpha\\) 세그먼트의 각 경로는 \\(\\alpha\\)와 유도된 세그먼트 사이의 관계를 인코딩하며, 이 때 \\(\\sigma(P)\\) 점수는 이 관계에 대한 퀄리티 척도(quality measure)를 산출한다.\n세그먼트 \\(\\alpha=[s:t]\\) 위의 블록은 다음의 하위 집합이다.. \\[B=\\alpha' \\times \\alpha \\subseteq [1:N]\\times [1:N]\\] for some segment \\(\\alpha'=[s':t']\\)\n경로와 마찬가지로 블록 \\(B\\)에 대한 두 개의 투영 \\(\\pi_1(B)=\\alpha'\\) 및 \\(\\pi_2(B)=\\alpha\\)를 정의하고 \\(\\alpha'\\)를 유도된 세그먼트라고 부른다.\n또한 블록 \\(B\\)의 점수를 다음과 같이 정의한다.\n\n\\(\\sigma(B)=\\sum_{(n,m)\\in B}\\mathbf{S}(n,m)\\)\n\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F08.png\", width=600)\n\n\n\n\n\n경로와 블록을 기반으로 세그먼트 간의 서로 다른 종류의 유사 관계를 고려할 수 있다.\n\n\\(\\pi_1(P)=\\alpha_1\\) 및 \\(\\pi_2(P)=\\alpha_2\\)의 높은 점수의 경로 \\(P\\)가 있는 경우, \\(\\alpha_1\\) 세그먼트는 \\(\\alpha_2\\) 세그먼트와 경로 유사(path-similar) 하다고 한다.\n\\(\\pi_1(B)=\\alpha_1\\) 및 \\(\\pi_2(B)=\\alpha_2\\)의 높은 점수의 블록 \\(B\\)가 있는 경우, \\(\\alpha_1\\)는 \\(\\alpha_2\\)와 블록 유사(block-similar) 하다.\n\n유사성 측정 \\(s\\)가 대칭인 경우, 자기 유사성 행렬 \\(\\mathbf{S}\\)와 위에서 정의한 세그먼트 간의 유사성 관계도 대칭적이다.\n유사성 관계의 또 다른 중요한 속성은 전이성(transitivity)이다. 즉 \\(\\alpha_1\\) 세그먼트가 \\(\\alpha_2\\) 세그먼트와 유사하고 \\(\\alpha_2\\) 세그먼트가 \\(\\alpha_3\\) 세그먼트와 유사하면 \\(\\alpha_1\\)도 \\(\\alpha_3\\)와 유사해야 한다.\n또한 유사성 척도 \\(s\\)에 전이성이 있는 경우 경로 및 블록 유사성에 대해서도 전이성은 유지된다. 결과적으로 경로 및 블록 구조는 (적어도 이상적인 경우에는) 특정 대칭 및 전이 속성을 충족하는 그룹으로 나타나는 경우가 많다.\n음악 구조 분석에 대한 대부분의 계산적 접근은 SSM의 경로- 및 블록-like 구조를 어떤 식으로든 활용하며, 전체 알고리즘 파이프라인에는 일반적으로 다음과 같은 일반적인 단계가 포함된다.\n\n음악 신호는 적절한 특징 시퀀스(feature sequence)로 변환된다.\n자기 유사성 행렬(SSM)은 유사성 측정을 기반으로 특징 시퀀스에서 계산된다.\n전체 점수가 높은 블록과 경로는 SSM에서 구해진다. 각 블록 또는 경로는 유사한 세그먼트 쌍을 정의한다.\n상호 유사한 세그먼트의 전체 그룹은 클러스터링 단계를 적용하여 쌍별(pairwise) 관계에서 형성된다.\n\n마지막 단계는 블록 및 경로 구조에 의해 유도된 쌍별 세그먼트 관계의 일종의 전이적 폐쇄(transitive closure)를 형성하는 것으로 간주할 수 있다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/f.4.9.PNG\")"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#대각-스무딩-diagonal-smoothing",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#대각-스무딩-diagonal-smoothing",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "대각 스무딩 (Diagonal Smoothing)",
    "text": "대각 스무딩 (Diagonal Smoothing)\n\n유사성 행렬의 중요한 속성 중 하나는 주 대각선과 평행한 높은 유사성의 경로 모양이다.\n이러한 각 경로는 각각 가로 및 세로 축에 경로를 투영하여 얻은 두 세그먼트의 유사성을 인코딩한다. 이러한 경로의 식별과 추출은 많은 음악 분석의 응용 단계이다. 그러나 음악적 및 음향적 변화로 인해 경로 구조는 종종 매우 노이지하며 추출하기 어려워 진다.\n이러한 노이즈는 피쳐 계산 단계에서 더 긴 분석 윈도우를 사용하고 피쳐 레이트를 조정함으로써 어느 정도 줄일 수 있다.\n경로 구조를 더욱 향상시키기 위한 일반적인 전략 중 하나는 주 대각선 방향을 따라 일종의 스무딩 필터를 적용하여 \\(\\mathbf{S}\\)의 대각선 정보를 강조하고 다른 구조의 노이즈를 제거하는 것이다.\n대각 스무딩(diagonal smoothing)이라고도 하는 이러한 필터링 과정은 동적 시스템(dynamical system) 분석에 널리 사용되는 시간 지연 임베딩(time-delay embedding) 개념과 밀접한 관련이 있다.\n\\(\\mathbf{S}\\)를 \\(N\\times N\\) 크기의 SSM이라고 하고 \\(L\\in\\mathbb{N}\\)를 스무딩 필터의 길이(length)라고 하자. 평활화된 SSM \\(\\mathbf{S}_L\\)을 다음과 같이 정의한다. \\[\\mathbf{S}_L(n,m) := \\frac{1}{L} \\sum_{\\ell=0}^{L-1} \\mathbf{S}(n+\\ell,m+\\ell)\\] for \\(n,m\\in[1:N-L+1]\\)\n\\(\\mathbf{S}\\)를 적절하게 확장함으로써(예: 제로 패딩으로 0열과 행이 추가됨) \\(n,m\\in[1:N]\\)에 대해 \\(\\mathbf{S}_L(n,m)\\)이 정의된다.\n효율성 문제를 생각했을 때, 행렬 기반 작업을 사용하여 스무딩 절차를 구현한다. 구현의 주요 아이디어는 다음과 같다.\n\n\\(\\ell\\in[0:L-1]\\)에 대해 \\(\\mathbf{S}\\)의 \\(\\ell\\)-shifted 버전을 생성한다. 이를 위해 주 대각선을 따라 \\(\\ell\\) 위치만큼 \\(\\mathbf{S}\\)를 이동시킨다.\n그런 다음 \\(\\ell\\in[0:L-1]\\)에서 \\(\\ell\\)-이동된 버전을 합산한다. 결과 행렬을 \\(L\\)로 나누면 \\(\\mathbf{S}_L\\) 행렬이 된다.\n경계 효과를 처리하기 위해 \\((N+L)\\) 사각 행렬(all zero)로 시작하는 제로 패딩을 사용한다.\n\n\n\ndef filter_diag_sm(S, L):\n    \"\"\"Path smoothing of similarity matrix by forward filtering along main diagonal\n\n    Args:\n        S (np.ndarray): Similarity matrix (SM)\n        L (int): Length of filter\n\n    Returns:\n        S_L (np.ndarray): Smoothed SM\n    \"\"\"\n    N = S.shape[0]\n    M = S.shape[1]\n    S_L = np.zeros((N, M))\n    S_extend_L = np.zeros((N + L, M + L))\n    S_extend_L[0:N, 0:M] = S\n    for pos in range(0, L):\n        S_L = S_L + S_extend_L[pos:(N + pos), pos:(M + pos)]\n    S_L = S_L / L\n    return S_L\n\n\ndef subplot_matrix_colorbar(S, fig, ax, title='', Fs=1,\n                            xlabel='Time (seconds)', ylabel='Time (seconds)',\n                            clim=None, xlim=None, ylim=None, cmap=None, interpolation='nearest'):\n    \"\"\"Visualization function for showing zoomed sections of matrices\n\n    Args:\n        S: Similarity matrix (SM)\n        fig: Figure handle\n        ax: Axes handle\n        title: Title for figure (Default value = '')\n        Fs: Feature rate (Default value = 1)\n        xlabel: Label for x-axis (Default value = 'Time (seconds)')\n        ylabel: Label for y-axis (Default value = 'Time (seconds)')\n        clim: Color limits (Default value = None)\n        xlim: Limits for x-axis (Default value = None)\n        ylim: Limits for x-axis (Default value = None)\n        cmap: Colormap for imshow (Default value = None)\n        interpolation: Interpolation value for imshow (Default value = 'nearest')\n\n    Returns:\n        im: Imshow handle\n    \"\"\"\n    if cmap is None:\n        cmap = compressed_gray_cmap(alpha=-100)\n    len_sec = S.shape[0] / Fs\n    extent = [0, len_sec, 0, len_sec]\n    im = ax.imshow(S, aspect='auto', extent=extent, cmap=cmap,  origin='lower', interpolation=interpolation)\n    fig.sca(ax)\n    fig.colorbar(im)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    if ylim is not None:\n        ax.set_ylim(ylim)\n    if clim is not None:\n        im.set_clim(clim)\n    return im\n\n\nL, H = 21, 5\nX, Fs_X = smooth_downsample_feature_sequence(C, Fs_C, \n                        filt_len=L, down_sampling=H)\n\nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\nS = compute_sm_dot(X,X)\n\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 3))\nsubplot_matrix_colorbar(S, fig, ax[0], clim=[0,1], ylabel='Time (frames)', xlabel='Time (frames)',\n                     title=r'Original SSM $\\mathbf{S}$')\nL = 20\nS_L = filter_diag_sm(S, L)\nsubplot_matrix_colorbar(S_L, fig, ax[1], clim=[0,1], ylabel='Time (frames)', xlabel='Time (frames)',\n                     title=r'Smoothed SSM $\\mathbf{S}_{L}$ (L = %d)'%L)\nL_long = 40\nS_L_long = filter_diag_sm(S, L_long)\nsubplot_matrix_colorbar(S_L_long, fig, ax[2], clim=[0,1], ylabel='Time (frames)', xlabel='Time (frames)',\n                     title=r'Smoothed SSM $\\mathbf{S}_{L}$ (L = %d)'%L_long)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n비교할 세그먼트 간에 상대적인 템포 차이가 없는 경우 주 대각선을 따라 간단한 필터링이 잘 작동한다. 그러나 이 가정은 파트가 더 빠르거나 더 느린 템포로 반복될 때는 위반된다.\nBrahms 예의 B 섹션에서 짧은 B2 섹션은 B1 섹션보다 훨씬 빠르게 재생된다. B2 섹션의 시작 부분만 B1 섹션의 시작 부분보다 훨씬 빠르게 재생되는 반면, 두 섹션은 파트의 끝 부분에서 거의 동일한 템포를 가진다.\n이로 인해 경로가 주 대각선(특히 시작 부분)과 정확히 평행하지 않으므로 주 대각선 방향으로 평균화 필터를 적용하면 일부 경로 구조가 파괴된다. 이것은 필터 길이 L이 큰 경우에 발생한다.\n\n\nxlim = [75, 150]\nylim = [125, 200]\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 3.5))\nsubplot_matrix_colorbar(S, fig, ax[0], clim=[0,1], xlim=xlim, ylim=ylim,\n                        title=r'Original SSM $\\mathbf{S}$', \n                        xlabel='Time (frames)', ylabel='Time (frames)')\nsubplot_matrix_colorbar(S_L, fig, ax[1], clim=[0,1], xlim=xlim, ylim=ylim,\n                        title=r'Smoothed SSM $\\mathbf{S}_{L}$ (L = %d)'%L, \n                        xlabel='Time (frames)', ylabel='Time (frames)')\nsubplot_matrix_colorbar(S_L_long, fig, ax[2], clim=[0,1], xlim=xlim, ylim=ylim,\n                        title=r'Smoothed SSM $\\mathbf{S}_{L}$ (L = %d)'%L_long, \n                        xlabel='Time (frames)', ylabel='Time (frames)')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#다중-필터링-multiple-filtering",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#다중-필터링-multiple-filtering",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "다중 필터링 (Multiple Filtering)",
    "text": "다중 필터링 (Multiple Filtering)\n\n상대적인 템포 차이로 인한 비대각선 경로 구조를 처리하기 위해, 특정 가정 하에 이러한 구조를 보존하는 다중 필터링 접근 방식을 도입한다.\n이 접근법의 주요 아이디어는 주 대각선으로 정의된 방향의 이웃에 있는 다양한 방향을 따라 유사성 행렬을 필터링하는 것이다. 이러한 각 방향은 템포 차이에 해당하며, 별도로 필터링된 유사성 매트릭스가 생성된다. 최종 유사성 행렬은 이러한 모든 행렬에 대해 셀별 최대값을 취하여 얻는다.\n이러한 방식으로 경로 구조는 로컬 템포 변형이 있는 경우에도 향상된다.\n템포 차이가 실수 \\(\\theta>0\\)로 주어진다고 가정하자. \\((1,\\theta)\\) 방향으로 평활화된 자기 유사성 행렬을 다음과 같이 정의한다. \\[\\mathbf{S}_{L,\\theta}(n,m) := \\frac{1}{L} \\sum_{\\ell=0}^{L-1} \\mathbf{S}(n+\\ell,m+[\\ell\\cdot\\theta])\\]\n\n\\([\\ell\\cdot\\theta]\\)는 실수 \\(\\ell\\cdot\\theta\\)에 가장 가까운 정수\n\n다시, 행렬 \\(\\mathbf{S}\\)를 적절히 제로-패딩함으로써 \\(\\mathbf{S}_{L,\\theta}\\)가 \\(n,m\\in[1:N]\\)에 대해 정의되었다고 가정할 수 있다.\n실제로는 주어진 음악 녹음에서 발생할 수 있는 로컬 템포 차이를 알 수 없다. 또한 두 반복 섹션 간의 상대적인 템포 차이는 시간이 지남에 따라 변경될 수 있다. 따라서 다른 상대적 템포 차이에 대한 템포 매개변수 \\(\\theta\\in\\Theta\\)로 구성된 (유한) 집합 \\(\\Theta\\)를 고려한다. 그런 다음 각 \\(\\theta\\)에 대해 행렬 \\(\\mathbf{S}_{L,\\theta}\\)를 계산하고 모든 \\(\\theta\\in\\Theta\\)에 대한 셀별 최대화로 최종 행렬 \\(\\mathbf{S}_{L,\\Theta}\\)를 얻는다. \\[\\mathbf{S}_{L,\\Theta}(n,m) := \\max_{\\theta\\in\\Theta} \\mathbf{S}_{L,\\theta}(n,m)\\]\n\\(\\Theta=\\{1\\}\\)를 선택하면 \\(\\mathbf{S}_{L,\\Theta}=\\mathbf{S}_{L}\\)의 경우로 축소된다.\n실제로는 \\(\\Theta\\) 집합을 결정하기 위해 예상되는 상대 템포 차이에 대한 사전 정보를 사용할 수 있다.\n\n예를 들어, 반복되는 세그먼트 간의 상대적 템포 차이가 \\(50\\)퍼센트보다 큰 경우는 거의 발생하지 않으므로, \\(\\Theta\\)를 대략 \\(-50\\)에서 \\(+50\\)퍼센트까지의 템포 변화를 처리할 수 있도록 선택할 수 있다.\n\n또한 실제로는 상대적으로 적은 수의 템포 매개변수만 고려하면 템포 범위를 잘 커버할 수 있다.\n\n예를 들어, 집합 \\(\\Theta=\\{0.66,0.81,1.00,1.22,1.50\\}\\) (로그 간격의 템포 매개변수 포함)은 대략 \\(-50\\)에서 \\(+50\\) 퍼센트의 템포 변화를 다룬다.\n\n\n\ndef compute_tempo_rel_set(tempo_rel_min, tempo_rel_max, num):\n    \"\"\"Compute logarithmically spaced relative tempo values\n\n    Args:\n        tempo_rel_min (float): Minimum relative tempo\n        tempo_rel_max (float): Maximum relative tempo\n        num (int): Number of relative tempo values (inlcuding the min and max)\n\n    Returns:\n        tempo_rel_set (np.ndarray): Set of relative tempo values\n    \"\"\"\n    tempo_rel_set = np.exp(np.linspace(np.log(tempo_rel_min), np.log(tempo_rel_max), num))\n    return tempo_rel_set\n\n\ntempo_rel_min = 0.66\ntempo_rel_max = 1.5\nnum = 5\ntempo_rel_set = compute_tempo_rel_set(tempo_rel_min=tempo_rel_min, tempo_rel_max=tempo_rel_max, num=num) \nprint(tempo_rel_set)\n\n[0.66       0.81036517 0.99498744 1.22167146 1.5       ]\n\n\n\n스무딩 길이 매개변수 \\(L\\in\\mathbb{N}\\) 및 상대적 템포 차이의 이산 집합 \\(\\Theta\\)는 스무딩 품질을 제어하기 위한 두 가지 주요 매개변수이다.\n대각선 스무딩과 마찬가지로 다중 필터링 접근 방식의 구현은 효율적으로 계산할 수 있는 전체 행렬 연산으로 표현된다. 구현의 주요 아이디어는 다음과 같다.\n\n각 템포 매개변수 \\(\\theta\\in\\Theta\\)에 대해 \\(\\theta\\)에 의해 결정된 계수로 한 축을 따라 SSM을 리샘플링한다. 리샘플링은 \\(\\theta\\)로 지정된 상대 템포 차이를 시뮬레이션한다.\n리샘플링된 각 행렬에 대해 길이 \\(L\\)의 대각선 필터링을 적용한다.\n필터링된 각 행렬은 (역)리샘플링을 적용하여 원래 형식으로 다시 변환된다.\n\\(\\theta\\in\\Theta\\)에 대한 결과 행렬의 셀별 최대값을 취하면 행렬 \\(\\mathbf{S}_{L,\\Theta}\\)를 얻을 수 있다.\n\n\n\ndef filter_diag_mult_sm(S, L=1, tempo_rel_set=np.asarray([1]), direction=0):\n    \"\"\"Path smoothing of similarity matrix by filtering in forward or backward direction\n    along various directions around main diagonal.\n    Note: Directions are simulated by resampling one axis using relative tempo values\n\n    Args:\n        S (np.ndarray): Self-similarity matrix (SSM)\n        L (int): Length of filter (Default value = 1)\n        tempo_rel_set (np.ndarray): Set of relative tempo values (Default value = np.asarray([1]))\n        direction (int): Direction of smoothing (0: forward; 1: backward) (Default value = 0)\n\n    Returns:\n        S_L_final (np.ndarray): Smoothed SM\n    \"\"\"\n    N = S.shape[0]\n    M = S.shape[1]\n    num = len(tempo_rel_set)\n    S_L_final = np.zeros((N, M))\n\n    for s in range(0, num):\n        M_ceil = int(np.ceil(M / tempo_rel_set[s]))\n        resample = np.multiply(np.divide(np.arange(1, M_ceil+1), M_ceil), M)\n        np.around(resample, 0, resample)\n        resample = resample - 1\n        index_resample = np.maximum(resample, np.zeros(len(resample))).astype(np.int64)\n        S_resample = S[:, index_resample]\n\n        S_L = np.zeros((N, M_ceil))\n        S_extend_L = np.zeros((N + L, M_ceil + L))\n\n        # Forward direction\n        if direction == 0:\n            S_extend_L[0:N, 0:M_ceil] = S_resample\n            for pos in range(0, L):\n                S_L = S_L + S_extend_L[pos:(N + pos), pos:(M_ceil + pos)]\n\n        # Backward direction\n        if direction == 1:\n            S_extend_L[L:(N+L), L:(M_ceil+L)] = S_resample\n            for pos in range(0, L):\n                S_L = S_L + S_extend_L[(L-pos):(N + L - pos), (L-pos):(M_ceil + L - pos)]\n\n        S_L = S_L / L\n        resample = np.multiply(np.divide(np.arange(1, M+1), M), M_ceil)\n        np.around(resample, 0, resample)\n        resample = resample - 1\n        index_resample = np.maximum(resample, np.zeros(len(resample))).astype(np.int64)\n\n        S_resample_inv = S_L[:, index_resample]\n        S_L_final = np.maximum(S_L_final, S_resample_inv)\n\n    return S_L_final\n\n\ntempo_rel_min = 0.66\ntempo_rel_max = 1.5\nnum = 5\ntempo_rel_set = compute_tempo_rel_set(tempo_rel_min=tempo_rel_min, tempo_rel_max=tempo_rel_max, num=num) \nL = 20\nS_L = filter_diag_sm(S, L)\nS_L_mult = filter_diag_mult_sm(S, L, tempo_rel_set)\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 3))\nsubplot_matrix_colorbar(S, fig, ax[0], clim=[0,1], xlabel='Time (frames)', ylabel='Time (frames)',\n                        title=r'Original SSM $\\mathbf{S}$')\nsubplot_matrix_colorbar(S_L, fig, ax[1], clim=[0,1], xlabel='Time (frames)', ylabel='Time (frames)',\n                        title=r'Smoothed SSM $\\mathbf{S}_{L}$ (L = %d)'%L)\nsubplot_matrix_colorbar(S_L_mult, fig, ax[2], clim=[0,1], xlabel='Time (frames)', ylabel='Time (frames)',\n                        title=r'Multiple filtering SSM $\\mathbf{S}_{L,\\theta}$ (L = %d)'%L)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#forward-backward-스무딩",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#forward-backward-스무딩",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "Forward-Backward 스무딩",
    "text": "Forward-Backward 스무딩\n\n이전 그림은 특히 경로 구조의 끝에 페이딩 아티팩트(fading artifact)가 있음을 보여준다. 이 아티팩트의 이유는 지금까지 설명한 스무딩 절차가 순방향으로 작동하기 때문이다. 페이딩 아티팩트를 방지하기 위한 방법으로 역방향으로 평균화 필터를 추가로 적용할 수 있다. 최종 자기 유사성 행렬은 forward-smoothed 및 backward-smoothed 행렬에 대해 셀별 최대값을 취하여 얻는다.\n\n\ntempo_rel_min = 0.66\ntempo_rel_max = 1.5\nnum = 5\ntempo_rel_set = compute_tempo_rel_set(tempo_rel_min=tempo_rel_min, tempo_rel_max=tempo_rel_max, num=num) \nL = 20\nS_forward = filter_diag_mult_sm(S, L, tempo_rel_set, direction=0)\nS_backward = filter_diag_mult_sm(S, L, tempo_rel_set, direction=1)\nS_final = np.maximum(S_forward, S_backward)\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 3))\nsubplot_matrix_colorbar(S, fig, ax[0], clim=[0,1], \n                        title=r'Original SSM', xlabel='Time (frames)', ylabel='Time (frames)')\nsubplot_matrix_colorbar(S_forward, fig, ax[1], clim=[0,1],\n                        title=r'Forward SSM', xlabel='Time (frames)', ylabel='Time (frames)')\nsubplot_matrix_colorbar(S_final, fig, ax[2], clim=[0,1], \n                        title=r'Forward-backward SSM', xlabel='Time (frames)', ylabel='Time (frames)')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n위의 리샘플 기반(resample-based) 접근법의 대안은 Librosa에서 구현된 것처럼 여러 2D 컨볼루션 커널(각 방향에 대해 하나의 커널)을 적용하는 것이다. 또 다른 수정 방법은 페이딩 부산물을 방지하기 위해 중앙값 필터링(평균 필터링 대신)을 사용하는 것이다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#조옮김된transposed-ssm",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#조옮김된transposed-ssm",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "조옮김된(transposed) SSM",
    "text": "조옮김된(transposed) SSM\n순환 시프트 연산자 (Cyclic Shift Operator)\n\n이미 조옮김(transposition)이 크로마 특징을 주기적으로 이동하여 시뮬레이션할 수 있음을 확인한 바 있다(4.1. 오디오 동기화 피쳐). \\([0:11]\\) 집합으로 12개 크로마 값을 식별하면 순환 이동은 순환 이동 연산자 \\(\\rho:\\mathbb{R}^{12} \\to \\mathbb{R}^{12}\\)에 의해 모델링된다. \\[\\rho(x):=(x(11),x(0),x(1),\\ldots,x(10))^\\top\\] for \\(x=(x(0),x(1),\\ldots,x(10),x(11))^\\top\\in\\mathbb{R}^{12}\\).\n\\(i\\)의 반음 위 순환 이동을 정의하는 \\(\\rho^i:=\\rho\\circ\\rho^{i-1}\\) for \\(i\\in\\mathbb{N}\\) 를 얻기 위해 순환 이동 연산자가 연속적으로 적용될 수 있다.\n\\(\\rho^{12}(x) = x\\)는 크로마 벡터를 12반음(1옥타브) 위로 주기적으로 이동하여 원래 벡터를 복구한다. 크로마그램의 모든 프레임에 순환 이동 연산자를 동시에 적용하면 전체 크로마그램이 수직 방향으로 순환 이동하게 된다.\n\\(X=(x_1,x_2,\\ldots,x_N)\\)를 특징 시퀀스라고 하자. 그러면 \\(i\\)-조옮김(-transposed) 특징 행렬 은 다음과 같이 주어진다. \\[\\rho^i(X)=(\\rho^i(x_1),\\rho^i(x_2),\\ldots,\\rho^i(x_N))\\]\n\n\ndef shift_cyc_matrix(X, shift=0):\n    \"\"\"Cyclic shift of features matrix along first dimension\n\n    Args:\n        X (np.ndarray): Feature respresentation\n        shift (int): Number of bins to be shifted (Default value = 0)\n\n    Returns:\n        X_cyc (np.ndarray): Cyclically shifted feature matrix\n    \"\"\"\n    # Note: X_cyc = np.roll(X, shift=shift, axis=0) does to work for jit\n    K, N = X.shape\n    shift = np.mod(shift, K)\n    X_cyc = np.zeros((K, N))\n    X_cyc[shift:K, :] = X[0:K-shift, :]\n    X_cyc[0:shift, :] = X[K-shift:K, :]\n    return X_cyc\n\n\nshift_set = [0,1,2]\nshift_num = len(shift_set)\n\nfig, ax = plt.subplots(shift_num, 2, gridspec_kw={'width_ratios': [1, 0.02]}, figsize=(6, 6))   \n                                          \nfor m in range(shift_num):\n    shift = shift_set[m]\n    X_cyc = shift_cyc_matrix(X, shift)    \n    plot_chromagram(X_cyc, Fs=Fs_X, ax=[ax[m,0], ax[m,1]], chroma_yticks=[0,2,4,5,7,9,11],\n                     title=r'$%d$-transposed chromgram'%shift, ylabel='Chroma', colorbar=True);\n\nplt.tight_layout()\nplt.show() \n\n\n\n\n\n동일한 멜로디 또는 조화 진행을 공유하지만 하나 또는 여러 개의 반음이 조옮김된 유사한 세그먼트를 감지해보자.\n주어진 특징 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)에 대해, \\(i\\)-transposed self-similarity matrix \\(\\rho^i(\\mathrm{S})\\)를 다음과 같이 정의한다. \\[\\rho^i(\\mathrm{S})(n,m):=s(\\rho^i(x_n),x_m)\\]\n\nfor \\(n,m\\in[1:N]\\) and \\(i\\in\\mathbb{Z}\\)\n\\(\\rho^{12}(\\mathrm{S})=\\mathrm{S}\\)\n\n직관적으로 \\(\\rho^i(\\mathrm{S})\\)는 원본 음악 녹음(\\(X=(x_1,x_2,\\ldots,x_N)\\)로 표현됨)과 \\(i\\) 반음 위쪽(\\(i\\)-transposed 기능 시퀀스로 표시됨)으로 조옮김된 음악 녹음 간의 유사성 관계를 설명한다.\n\n\nL = 8\ntempo_rel_set = np.asarray([1])\nshift_set = np.asarray([0,1,2])\nshift_num = len(shift_set)\n\nfig, ax = plt.subplots(1, shift_num, figsize=(10, 3))\nfor m in range(shift_num):\n    shift = shift_set[m]\n    X_cyc = shift_cyc_matrix(X, shift)    \n    S = compute_sm_dot(X,X_cyc)\n    S_forward = filter_diag_mult_sm(S, L, tempo_rel_set=tempo_rel_set, direction=0)\n    S_backward = filter_diag_mult_sm(S, L, tempo_rel_set=tempo_rel_set, direction=1)\n    S_final = np.maximum(S_forward, S_backward)\n    subplot_matrix_colorbar(S_final, fig, ax[m], clim=[0.5,1], \n                        title=r'$%5d$-transposed SSM'%shift)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#조옮김-불변transposition-invariant-ssm",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#조옮김-불변transposition-invariant-ssm",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "조옮김-불변(Transposition-Invariant) SSM",
    "text": "조옮김-불변(Transposition-Invariant) SSM\n\n일반적으로 음악 녹음에서 발생하는 조옮김의 종류를 모르기 때문에 상대적인 템포 편차(relative tempo deviations)를 처리할 때 이전과 유사한 전략을 적용한다. 12개의 서로 다른 순환 이동에 대해 셀별 최대값을 취하면 다음과 같이 정의된 단일 조옮김-불변(transposition-invariant) 자기 유사성 행렬 \\(\\mathrm{S}^\\mathrm{TI}\\)를 얻는다.\n\n\\[\\mathrm{S}^\\mathrm{TI}(n,m):=\\max_{i\\in [0:11]} \\,\\rho^i(\\mathrm{S})(n,m)\\]\n\n또한 추가 \\(N\\)-정사각 행렬 \\(\\mathrm{I}\\)에 최대화 이동(shift) 지수를 저장하며 이를 전치 인덱스 행렬이라고 하고 다음과 같다. \\[\\mathrm{I}(n,m):= \\underset{i\\in [0:11]}{\\mathrm{argmax}} \\,\\,\\, \\rho^i(\\mathrm{S})(n,m)\\]\n다음의 코드 셀에서는 조옮김-불변 SSM과 조옮김 인덱스 행렬을 계산하기 위한 구현을 제공한다.\n다음 사항에 유의해야 한다.\n\n이 함수는 X와 Y 두 가지 특징 시퀀스를 허용하여 보다 일반적이다. ’X = Y’의 경우는 이 노트북에서 고려된 SSM 케이스로 축소된다.\n조옮김된 각 SSM에 대해 평활화(정방향, 역방향, 조합) 및 정규화가 적용된다.\n마지막에 (조옮김된 12개의 모든 SSM에 대해)단일 최대화를 갖는 대신, 최대화는 이동 인덱스 루프(shift index loop) 내에서 업데이트된다. 동시에 두 개의 행렬만 저장하면 되므로 메모리 요구 사항이 줄어든다.\n\n\n\ndef compute_sm_ti(X, Y, L=1, tempo_rel_set=np.asarray([1]), shift_set=np.asarray([0]), direction=2):\n    \"\"\"Compute enhanced similaity matrix by applying path smoothing and transpositions\n\n    Args:\n        X (np.ndarray): First feature sequence\n        Y (np.ndarray): Second feature sequence\n        L (int): Length of filter (Default value = 1)\n        tempo_rel_set (np.ndarray): Set of relative tempo values (Default value = np.asarray([1]))\n        shift_set (np.ndarray): Set of shift indices (Default value = np.asarray([0]))\n        direction (int): Direction of smoothing (0: forward; 1: backward; 2: both directions) (Default value = 2)\n\n    Returns:\n        S_TI (np.ndarray): Transposition-invariant SM\n        I_TI (np.ndarray): Transposition index matrix\n    \"\"\"\n    for shift in shift_set:\n        Y_cyc = shift_cyc_matrix(Y, shift)\n        S_cyc = compute_sm_dot(X, Y_cyc)\n\n        if direction == 0:\n            S_cyc = filter_diag_mult_sm(S_cyc, L, tempo_rel_set, direction=0)\n        if direction == 1:\n            S_cyc = filter_diag_mult_sm(S_cyc, L, tempo_rel_set, direction=1)\n        if direction == 2:\n            S_forward = filter_diag_mult_sm(S_cyc, L, tempo_rel_set=tempo_rel_set, direction=0)\n            S_backward = filter_diag_mult_sm(S_cyc, L, tempo_rel_set=tempo_rel_set, direction=1)\n            S_cyc = np.maximum(S_forward, S_backward)\n        if shift == shift_set[0]:\n            S_TI = S_cyc\n            I_TI = np.ones((S_cyc.shape[0], S_cyc.shape[1])) * shift\n        else:\n            # jit does not like the following lines\n            # I_greater = np.greater(S_cyc, S_TI)\n            # I_greater = (S_cyc > S_TI)\n            I_TI[S_cyc > S_TI] = shift\n            S_TI = np.maximum(S_cyc, S_TI)\n\n    return S_TI, I_TI\n\n\ndef subplot_matrix_ti_colorbar(S, fig, ax, title='', Fs=1, xlabel='Time (seconds)', ylabel='Time (seconds)',\n                               clim=None, xlim=None, ylim=None, cmap=None, alpha=1, interpolation='nearest',\n                               ind_zero=False):\n    \"\"\"Visualization function for showing transposition index matrix\n\n    Args:\n        S: Self-similarity matrix (SSM)\n        fig: Figure handle\n        ax: Axes handle\n        title: Title for figure (Default value = '')\n        Fs: Feature rate (Default value = 1)\n        xlabel: Label for x-axis (Default value = 'Time (seconds)')\n        ylabel: Label for y-axis (Default value = 'Time (seconds)')\n        clim: Color limits (Default value = None)\n        xlim: Limits for x-axis (Default value = None)\n        ylim: Limits for y-axis (Default value = None)\n        cmap: Color map (Default value = None)\n        alpha: Alpha value for imshow (Default value = 1)\n        interpolation: Interpolation value for imshow (Default value = 'nearest')\n        ind_zero: Use white (True) or black (False) color for index zero (Default value = False)\n\n    Returns:\n        im: Imshow handle\n    \"\"\"\n    if cmap is None:\n        color_ind_zero = np.array([0, 0, 0, 1])\n        if ind_zero == 0:\n            color_ind_zero = np.array([0, 0, 0, 1])\n        else:\n            color_ind_zero = np.array([1, 1, 1, 1])\n        colorList = np.array([color_ind_zero, [1, 1, 0, 1],  [0, 0.7, 0, 1],  [1, 0, 1, 1],  [0, 0, 1, 1],\n                             [1, 0, 0, 1], [0, 0, 0, 0.5], [1, 0, 0, 0.3], [0, 0, 1, 0.3], [1, 0, 1, 0.3],\n                             [0, 0.7, 0, 0.3], [1, 1, 0, 0.3]])\n        cmap = ListedColormap(colorList)\n    len_sec = S.shape[0] / Fs\n    extent = [0, len_sec, 0, len_sec]\n    im = ax.imshow(S, aspect='auto', extent=extent, cmap=cmap,  origin='lower', alpha=alpha,\n                   interpolation=interpolation)\n    if clim is None:\n        im.set_clim(vmin=-0.5, vmax=11.5)\n    fig.sca(ax)\n    ax_cb = fig.colorbar(im)\n    ax_cb.set_ticks(np.arange(0, 12, 1))\n    ax_cb.set_ticklabels(np.arange(0, 12, 1))\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    if xlim is not None:\n        ax.set_xlim(xlim)\n    if ylim is not None:\n        ax.set_ylim(ylim)\n    return im\n\n\nL = 8\ntempo_rel_set = np.asarray([1])\nshift_set = np.array(range(12))\nS_TI, I_TI = compute_sm_ti(X, X, L=L, tempo_rel_set=tempo_rel_set, \n                           shift_set=shift_set, direction=2)\n\nfig, ax = plt.subplots(1, 2, figsize=(8, 3.5))\nsubplot_matrix_colorbar(S_TI, fig, ax[0], clim=[0.5,1], \n                                  title='Transposition-invariant SSM')\nsubplot_matrix_ti_colorbar(I_TI, fig, ax[1], ind_zero=True,\n                                  title='Transposition index matrix')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n오른쪽의 조옮김 인덱스 행렬(transposition index matrix)을 자세히 살펴보자.\n먼저 \\(\\mathrm{I}\\) 행렬이 \\(i=0\\)(흰색) 값을 가정하는 경우,\n\n\\((n,m)\\) 셀의 \\(i=0\\) 값은 \\(s(\\rho^{i}(x_n),x_m)\\)가 \\(i=0\\)에 대해 최대값을 가정함을 나타낸다.\n즉, 크로마 벡터 \\(x_m\\)은 \\(x_n\\)의 다른 이동된 버전보다 \\(x_n\\)에 더 가깝다.그러나 이것이 반드시 \\(x_m\\)이 절대적으로 \\(x_n\\)에 가깝다는 것을 의미하지는 않는다.\n최대화 지수는 원래 SSM이 저비용 경로를 나타내는 모든 위치에서 \\(i=0\\)이다.\n\n다음으로 \\(\\mathrm{I}\\) 행렬이 \\(i=1\\)(노란색 ) 값을 가정하는 경우를 고려해보자.\n\n\\((n,m)\\) 셀의 \\(i=1\\) 값은 반음 위로 이동했을 때 \\(x_n\\)이 \\(x_m\\)과 가장 비슷해짐을 나타낸다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#매개변수에-따른-의존도",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#매개변수에-따른-의존도",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "매개변수에 따른 의존도",
    "text": "매개변수에 따른 의존도\n\n샘플링 속도(rate) \\(F_\\mathrm{s}=22050~\\mathrm{Hz}\\)의 오디오 신호로 시작하여 먼저 윈도우 및 홉 크기 매개변수가 필요한 STFT를 계산한다. 예를 들어 \\(N=4410\\)의 윈도우 크기와 \\(H=2050\\)의 홉 크기를 사용하면 특징 해상도가 \\(10~\\mathrm{Hz}\\)가 된다.\n둘째, 크로마그램을 도출하기 위해 스무딩 및 다운샘플링 전략을 적용한다. \\(41\\) 특징(약 \\(4\\)초 커버)의 스무딩 윈도우와 \\(10\\)의 홉 크기를 사용하면 \\(1~\\mathrm{Hz}\\)의 크로마 해상도가 생성된다.\n셋째, SSM을 계산할 때 경로 향상(path enhancement)을 적용한다. 이것은 다시 길이 매개변수가 필요하다. 예를 들어 \\(20\\) 길이를 사용하면 \\(20\\)초의 오디오에 해당한다. 순방향 및 역방향 스무딩을 적용하면 “번짐(smearing)”이 더욱 증가한다.\n마지막으로, 다중 필터링을 적용하면 “번짐” 효과가 또 발생한다.\n다음 그림은 이러한 매개변수의 선택이 최종 결과에 결정적인 영향을 미친다는 것을 보여준다. SSM 계산에서 경로 향상을 사용하지 않으면 노이즈가 많은 SSM이 발생하고 “scattered”된 조옮김 인덱스 분포가 발생한다. 경로 향상을 도입하면 구조적 속성이 향상되지만 과도한 평활화로 중요한 세부 사항의 손실이 발생할 수도 있다.\n\n\nC = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=2, hop_length=2205, n_fft=4410)\nFs_C = Fs/2205\n\nL_feature = 41\nH_feature = 10\nX, Fs_X = smooth_downsample_feature_sequence(C, Fs_C, \n                                    filt_len=L_feature, down_sampling=H_feature)\nX = normalize_feature_sequence(X, norm='2', threshold=0.001)\n\ntempo_rel_min = 0.66\ntempo_rel_max = 1.5\nnum = 5\ntempo_rel_set = compute_tempo_rel_set(tempo_rel_min=tempo_rel_min, tempo_rel_max=tempo_rel_max, num=num) \n\nshift_set = np.array(range(12))\n\nL_set = [1, 20]\nL_num = len(L_set)\ntitle_set = ['Transposition-invariant SSM', 'Smoothed transposition-invariant SSM']\n\nfig, ax = plt.subplots(L_num, 2, figsize=(8, 7))\nfor m in range(L_num):\n    L = L_set[m]\n    S_TI, I_TI = compute_sm_ti(X, X, L=L, tempo_rel_set=tempo_rel_set, shift_set=shift_set, direction=2)\n    subplot_matrix_colorbar(S_TI, fig, ax[m,0], clim=[0.5,1], \n                                  title=title_set[m])\n    subplot_matrix_ti_colorbar(I_TI, fig, ax[m,1], ind_zero=True,\n                                  title='Transposition index matrix')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#전역-임계값-global-thresholding",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#전역-임계값-global-thresholding",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "전역 임계값 (Global Thresholding)",
    "text": "전역 임계값 (Global Thresholding)\n\n가장 간단한 방법은 전역 임계값 global thresholding을 적용하는 것이다. 여기서 주어진 임계값 매개변수 \\(\\tau>0\\) 보다 작은 행렬 \\(\\mathbf{S}\\in\\mathbb{R}_{\\geq 0}^{N\\times M}\\)의 모든 값 \\(\\mathbf{S}(n,m)\\)은 0으로 설정된다.\n\n\\[\\mathbf{S}_\\tau(n,m):=\\left\\{\\begin{array}{ll}\n    \\mathbf{S}(n,m) &\\quad \\mbox{if $\\mathbf{S}(n,m)\\geq\\tau$,}\\\\\n    0,&\\quad\\mbox{otherwise.}\n\\end{array}\\right.\\]\n\n또한 임계값 이상의 모든 값을 1로 설정하고 다른 모든 값을 0으로 설정하여 행렬의 이진화(binarization)를 적용할 수 있다.\n\n\nS_global = np.copy(S)\n\nthresh = 6\nS_global = np.copy(S)\nS_global[S_global < thresh] = 0\nS_binary = np.copy(S_global)\nS_binary[S_global >= thresh] = 1\n\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 2))\nplot_matrix(S, ax=[ax[0]], xlabel='', ylabel='',\n                     title=r'Original matrix')\nplot_matrix(S_global, ax=[ax[1]], xlabel='', ylabel='',\n                     title=r'Global thresholding ($\\tau = %0.2f$)'%thresh)\nplot_matrix(S_binary, ax=[ax[2]], xlabel='', ylabel='', \n                     title=r'Binarization')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#스케일링과-페널티-scaling-and-penalty",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#스케일링과-페널티-scaling-and-penalty",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "스케일링과 페널티 (Scaling and Penalty)",
    "text": "스케일링과 페널티 (Scaling and Penalty)\n\n이진화 대신, 범위 \\([\\tau,\\mu]\\)를 \\([0,1]\\)로 선형 스케일링 할 수 있다.(단, \\(\\mu:=\\max_{n,m}\\{\\mathbf{S}(n,m)\\}>\\tau\\)의 경우, 그렇지 않으면 모든 항목 0으로 설정)\n경우에 따라 추가 페널티(penalty) 매개변수 \\(\\delta\\leq 0\\)를 도입하여, 임계값 미만의 모든 원래 값을 \\(\\delta\\) 값으로 설정하는 것이 도움이 될 수도 있다.\n다음 그림은 스케일링 전과 후의 결과를 보여준다. 또한 페널티 매개변수 \\(\\delta=-2\\)를 적용한다.\n\n\nmin_value = thresh\nmax_value = np.max(S_global) \nif max_value > min_value:\n    S_scale = np.divide((S_global - min_value), (max_value -  min_value)) \n    S_scale[S_global<thresh] = 0\nelse:\n    raise Exception('Scaling not possible: max > min is violated')    \n        \npenalty = -2\nS_penalty = np.divide((S_global - min_value), (max_value -  min_value)) \nS_penalty[S_global<thresh] = penalty\n\nfig, ax = plt.subplots(1, 3, figsize=(10,2))\nplot_matrix(S_global, ax=[ax[0]], xlabel='', ylabel='', \n                     title=r'Global thresholding ($\\tau = %0.2f$)'%thresh)\nplot_matrix(S_scale, ax=[ax[1]], xlabel='', ylabel='', \n                     title=r'Scaling')\nplot_matrix(S_penalty, ax=[ax[2]], xlabel='', ylabel='', \n                     title=r'Penalty ($\\rho = %0.2f$)'%penalty)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#상대-임계값-relative-thresholding",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#상대-임계값-relative-thresholding",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "상대 임계값 (Relative Thresholding)",
    "text": "상대 임계값 (Relative Thresholding)\n\n전역 임계값 \\(\\tau\\)는 상대적(relative) 방식으로도 선택할 수 있다. 상대 임계값(relative threshold) 매개변수 \\(\\rho\\in [0,1]\\)가 주어지면 가장 높은 값을 가진 셀의 \\(\\rho\\cdot 100\\%\\)를 유지한다.\n이를 위해 행렬 항목을 값에 따라 정렬해야 한다. 정렬된 목록에서 \\(\\rho\\)에 따라 값을 분할하는 전역 임계값을 결정할 수 있다. 이 상대 임계값 전략은 다른 상대 임계값 매개변수를 사용한다.\n\n반올림 문제와 서로 다른 행렬 항목이 동일한 값을 가질 수 있다는 문제 때문에 \\(\\rho\\)로 지정된 분할이 정확하지 않을 수 있다.\n\n\n\ndef threshold_matrix_relative(S, thresh_rel=0.2, details=False):\n    \"\"\"Treshold matrix in a relative fashion\n\n    Args:\n        S (np.ndarray): Input matrix\n        thresh_rel (float): Relative treshold (Default value = 0.2)\n        details (bool): Print details on thresholding procedure (Default value = False)\n\n    Returns:\n        S_thresh (np.ndarray): Thresholded matrix\n        thresh_abs (float): Absolute threshold used for thresholding\n    \"\"\"\n    S_thresh = np.copy(S)\n    num_cells_below_thresh = int(np.round(S_thresh.size*(1-thresh_rel)))\n    values_sorted = np.sort(S_thresh.flatten('F'))\n    thresh_abs = values_sorted[num_cells_below_thresh]\n    S_thresh[S_thresh < thresh_abs] = 0\n    if details:\n        print('thresh_rel=%0.2f, thresh_abs=%d, total_num_cells=%d, num_cells_below_thresh=%d, ' %\n              (thresh_rel, thresh_abs, S_thresh.size, num_cells_below_thresh))\n    return S_thresh, thresh_abs\n\n\nthresh_rel_set = [0.5, 0.3, 0.1]\nnum = len(thresh_rel_set)\nfig, ax = plt.subplots(1, num, figsize=(10,2))\nfor m in range(num):\n    thresh_rel = thresh_rel_set[m]\n    S_relative, thresh_abs = threshold_matrix_relative(S, thresh_rel, details=True)\n    plot_matrix(S_relative, ax=[ax[m]], xlabel='', ylabel='', \n                         title=r'Relative thresholding ($\\rho = %0.2f$)'%thresh_rel)\n\nplt.tight_layout()\nplt.show()\n\nthresh_rel=0.50, thresh_abs=5, total_num_cells=24, num_cells_below_thresh=12, \nthresh_rel=0.30, thresh_abs=6, total_num_cells=24, num_cells_below_thresh=17, \nthresh_rel=0.10, thresh_abs=8, total_num_cells=24, num_cells_below_thresh=22,"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#로컬-임계값-local-thresholding",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#로컬-임계값-local-thresholding",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "로컬 임계값 (Local Thresholding)",
    "text": "로컬 임계값 (Local Thresholding)\n\n마지막으로, 임계값은 row-wise 및 column-wise으로 임계값을 지정하여 더 많은 로컬 전략을 사용하여 수행할 수도 있다.\n\\(\\rho_1\\in[0,1]\\)를 행(row)의 상대 임계값으로 하고 \\(\\rho_2\\in[0,1]\\)를 열(column)의 상대 임계값으로 설정한다.\n각 셀 \\((n,m)\\)에 대해 \\(\\mathbf{S}(n,m)\\) 값은 행 \\(n\\)에서 가장 큰 셀의 \\(\\rho_1\\cdot 100\\%\\)에 있으면 유지된다. 동시에 \\(m\\) 열에서 가장 큰 셀의 \\(\\rho_2\\cdot 100\\%\\) 중에 있으면 다른 모든 값은 0으로 설정된다.\n다음 그림에서는 셀이 각각 행 또는 열 중에서 가장 큰지 여부와 최종 결과를 나타내는 두 개의 이진 마스크(binary masks)(중간 결과)를 보여준다.\n\n\nthresh_rel_row=0.8\nthresh_rel_col=0.5\n\nS_binary_row = np.ones([N,M])\nnum_cells_row_below_thresh = int(np.round(M*(1-thresh_rel_row)))\nfor n in range(N):\n    row = S[n,:]\n    values_sorted = np.sort(row)\n    thresh_abs = values_sorted[num_cells_row_below_thresh]\n    S_binary_row[n,:] = (row>=thresh_abs)\n\nS_binary_col = np.ones([N,M])\nnum_cells_col_below_thresh = int(np.round(N*(1-thresh_rel_col)))  \nfor m in range(M):\n    col = S[:,m]\n    values_sorted = np.sort(col)\n    thresh_abs = values_sorted[num_cells_col_below_thresh]\n    S_binary_col[:,m] = (col>=thresh_abs)\n    \nS_local =  S * S_binary_row * S_binary_col\n    \nfig, ax = plt.subplots(1, 3, figsize=(10,2))\nplot_matrix(S_binary_row, ax=[ax[0]], xlabel='', ylabel='', \n                     title=r'Binary mask for rows ($\\rho_1 = %0.2f$)'%thresh_rel_row)\nplot_matrix(S_binary_col, ax=[ax[1]], xlabel='', ylabel='', \n                     title=r'Binary mask for columns ($\\rho_2 = %0.2f$)'%thresh_rel_col)\nplot_matrix(S_local, ax=[ax[2]], xlabel='', ylabel='', \n                     title=r'Local thresholding ($\\rho_1 = %0.2f$, $\\rho_2 = %0.2f$)'%(thresh_rel_row,thresh_rel_col))\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#구현",
    "href": "posts/5. Music Structure Analysis/5.2.Self_Similarity_Matrix.html#구현",
    "title": "5.2. 자기 유사성 행렬 (SSM)",
    "section": "구현",
    "text": "구현\n\ndef threshold_matrix(S, thresh, strategy='absolute', scale=False, penalty=0.0, binarize=False):\n    \"\"\"Treshold matrix in a relative fashion\n\n    Args:\n        S (np.ndarray): Input matrix\n        thresh (float or list): Treshold (meaning depends on strategy)\n        strategy (str): Thresholding strategy ('absolute', 'relative', 'local') (Default value = 'absolute')\n        scale (bool): If scale=True, then scaling of positive values to range [0,1] (Default value = False)\n        penalty (float): Set values below treshold to value specified (Default value = 0.0)\n        binarize (bool): Binarizes final matrix (positive: 1; otherwise: 0) (Default value = False)\n\n    Returns:\n        S_thresh (np.ndarray): Thresholded matrix\n    \"\"\"\n    if np.min(S) < 0:\n        raise Exception('All entries of the input matrix must be nonnegative')\n\n    S_thresh = np.copy(S)\n    N, M = S.shape\n    num_cells = N * M\n\n    if strategy == 'absolute':\n        thresh_abs = thresh\n        S_thresh[S_thresh < thresh] = 0\n\n    if strategy == 'relative':\n        thresh_rel = thresh\n        num_cells_below_thresh = int(np.round(S_thresh.size*(1-thresh_rel)))\n        if num_cells_below_thresh < num_cells:\n            values_sorted = np.sort(S_thresh.flatten('F'))\n            thresh_abs = values_sorted[num_cells_below_thresh]\n            S_thresh[S_thresh < thresh_abs] = 0\n        else:\n            S_thresh = np.zeros([N, M])\n\n    if strategy == 'local':\n        thresh_rel_row = thresh[0]\n        thresh_rel_col = thresh[1]\n        S_binary_row = np.zeros([N, M])\n        num_cells_row_below_thresh = int(np.round(M * (1-thresh_rel_row)))\n        for n in range(N):\n            row = S[n, :]\n            values_sorted = np.sort(row)\n            if num_cells_row_below_thresh < M:\n                thresh_abs = values_sorted[num_cells_row_below_thresh]\n                S_binary_row[n, :] = (row >= thresh_abs)\n        S_binary_col = np.zeros([N, M])\n        num_cells_col_below_thresh = int(np.round(N * (1-thresh_rel_col)))\n        for m in range(M):\n            col = S[:, m]\n            values_sorted = np.sort(col)\n            if num_cells_col_below_thresh < N:\n                thresh_abs = values_sorted[num_cells_col_below_thresh]\n                S_binary_col[:, m] = (col >= thresh_abs)\n        S_thresh = S * S_binary_row * S_binary_col\n\n    if scale:\n        cell_val_zero = np.where(S_thresh == 0)\n        cell_val_pos = np.where(S_thresh > 0)\n        if len(cell_val_pos[0]) == 0:\n            min_value = 0\n        else:\n            min_value = np.min(S_thresh[cell_val_pos])\n        max_value = np.max(S_thresh)\n        # print('min_value = ', min_value, ', max_value = ', max_value)\n        if max_value > min_value:\n            S_thresh = np.divide((S_thresh - min_value), (max_value - min_value))\n            if len(cell_val_zero[0]) > 0:\n                S_thresh[cell_val_zero] = penalty\n        else:\n            print('Condition max_value > min_value is voliated: output zero matrix')\n\n    if binarize:\n        S_thresh[S_thresh > 0] = 1\n        S_thresh[S_thresh < 0] = 0\n    return S_thresh\n\n\nfigsize=(11, 4)\nfig, ax = plt.subplots(2, 3, figsize=figsize)\nplot_matrix(S, ax=[ax[0,0]], xlabel='', ylabel='', title=r'Original matrix')\n\nstrategy = 'relative'\nthresh = 0.7\nS_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy, scale=0, penalty=0, binarize=0)\nplot_matrix(S_thresh, ax=[ax[0,1]], xlabel='', ylabel='', \n                     title=[strategy, thresh])\n\nstrategy = 'relative'\nthresh = 0.7\nS_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy, scale=1, penalty=-2, binarize=0)\nplot_matrix(S_thresh, ax=[ax[0,2]], xlabel='', ylabel='', \n                     title=[strategy, thresh, \"scale\", \"-2\"])\n\nstrategy = 'relative'\nthresh = 0.7\nS_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy, scale=1, penalty=-2, binarize=1)\nplot_matrix(S_thresh, ax=[ax[1,0]], xlabel='', ylabel='', \n                     title=[strategy, thresh, \"scale\", \"-2\", \"binarize\"])\n\nstrategy = 'local'\nthresh = [0.7, 0.7]\nS_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy, scale=0, penalty=0, binarize=0)\nplot_matrix(S_thresh, ax=[ax[1,1]], xlabel='', ylabel='', \n                     title=[strategy, thresh])\n\nstrategy = 'local'\nthresh = [0.7, 0.7]\nS_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy, scale=1, penalty=-2, binarize=0)\nplot_matrix(S_thresh, ax=[ax[1,2]], xlabel='', ylabel='', \n                     title=[strategy, thresh, \"scale\", \"-2\"])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html",
    "title": "5.3. 오디오 썸네일",
    "section": "",
    "text": "음악 녹음의 가장 대표적인 섹션을 자동으로 결정하는 오디오 썸네일(thumbnail)에 대해 다룬다. 세그먼트의 반복 기반 접근 방식의 최적화 방법을 알아본다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#ssm의-필요조건",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#ssm의-필요조건",
    "title": "5.3. 오디오 썸네일",
    "section": "SSM의 필요조건",
    "text": "SSM의 필요조건\n\n적합도(fitness) 측정의 아이디어는 주어진 세그먼트와 그것의 대략적인 반복 간의 모든 관계를 동시에 설정하는 것이다. 이를 위해서는 자기 유사성 행렬(SSM) 이 필요하다. 적합도 측정에 대한 다음 설명은 일부 기본 정규화 속성(normalization properties) 만 충족하는 일반 자기 유사성 행렬과 함께 작동한다는 점에서 일반적이다. \\[\\mathbf{S}(n,m)\\leq 1 \\text{ and } \\mathbf{S}(n,n)=1 \\text{ for all } n,m\\in[1:N]\\]\n5.2. 자기 유사성 행렬에서 사용한 브람스 예시를 계속하여 보면, 향상된(enhanced) SSM에서 시작하여 정규화 속성을 만족시키는 임계값 절차(thresholding procedure)를 거친다. 결과적으로 SSM의 모든 상관있는(relevant) 성분은 0과 1 사이(주대각선 성분은 1)가 되며, 상관없는 성분은 \\(\\delta=-2\\)의 음수 점수를 가진다.\n\n\ndef colormap_penalty(penalty=-2, cmap=compressed_gray_cmap(alpha=5)):\n    \"\"\"Extend colormap with white color between the penalty value and zero\n\n    Args:\n        penalty (float): Negative number (Default value = -2.0)\n        cmap (mpl.colors.Colormap): Original colormap (Default value = libfmp.b.compressed_gray_cmap(alpha=5))\n\n    Returns:\n        cmap_penalty (mpl.colors.Colormap): Extended colormap\n    \"\"\"\n    if isinstance(cmap, str):\n        cmap = matplotlib.cm.get_cmap(cmap, 128)\n    cmap_matrix = cmap(np.linspace(0, 1, 128))[:, :3]\n    num_row = int(np.abs(penalty)*128)\n    # cmap_penalty = np.flip(np.concatenate((cmap_matrix, np.ones((num_row, 3))), axis=0), axis=0)\n    cmap_penalty = np.concatenate((np.ones((num_row, 3)), cmap_matrix), axis=0)\n    cmap_penalty = ListedColormap(cmap_penalty)\n\n    return cmap_penalty\n\n\ndef normalization_properties_ssm(S):\n    \"\"\"Normalizes self-similartiy matrix to fulfill S(n,n)=1.\n    Yields a warning if max(S)<=1 is not fulfilled\n\n    Args:\n        S (np.ndarray): Self-similarity matrix (SSM)\n\n    Returns:\n        S_normalized (np.ndarray): Normalized self-similarity matrix\n    \"\"\"\n    S_normalized = S.copy()\n    N = S_normalized.shape[0]\n    for n in range(N):\n        S_normalized[n, n] = 1\n        max_S = np.max(S_normalized)\n    if max_S > 1:\n        print('Normalization condition for SSM not fulfill (max > 1)')\n    return S_normalized\n\n\ndef plot_ssm_ann(S, ann, Fs=1, cmap='gray_r', color_ann=[], ann_x=True, ann_y=True,\n                 fontsize=12, figsize=(5, 4.5), xlabel='', ylabel='', title=''):\n    \"\"\"Plot SSM and annotations (horizontal and vertical as overlay)\n\n    Args:\n        S: Self-similarity matrix\n        ann: Annotations\n        Fs: Feature rate of path_family (Default value = 1)\n        cmap: Color map for S (Default value = 'gray_r')\n        color_ann: color scheme used for annotations (see :func:`libfmp.b.b_plot.plot_segments`)\n            (Default value = [])\n        ann_x: Plot annotations on x-axis (Default value = True)\n        ann_y: Plot annotations on y-axis (Default value = True)\n        fontsize: Font size used for annotation labels (Default value = 12)\n        figsize: Size of figure (Default value = (5, 4.5))\n        xlabel: Label for x-axis (Default value = '')\n        ylabel: Label for y-axis (Default value = '')\n        title: Figure size (Default value = '')\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n        im: Handle for imshow\n    \"\"\"\n    fig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05],\n                                              'height_ratios': [1, 0.1]}, figsize=figsize)\n\n    fig_im, ax_im, im = plot_matrix(S, Fs=Fs, Fs_F=Fs,\n                                             ax=[ax[0, 0], ax[0, 1]], cmap=cmap,\n                                             xlabel='', ylabel='', title='')\n    ax[0, 0].set_ylabel(ylabel)\n    ax[0, 0].set_xlabel(xlabel)\n    ax[0, 0].set_title(title)\n    if ann_y:\n        plot_segments_overlay(ann, ax=ax_im[0], direction='vertical',\n                                       time_max=S.shape[0]/Fs, print_labels=False,\n                                       colors=color_ann, alpha=0.05)\n    if ann_x:\n        plot_segments(ann, ax=ax[1, 0], time_max=S.shape[0]/Fs, colors=color_ann,\n                               time_axis=False, fontsize=fontsize)\n    else:\n        ax[1, 0].axis('off')\n    ax[1, 1].axis('off')\n    plt.tight_layout()\n    return fig, ax, im\n\n\nfn_wav = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.wav'\n\ntempo_rel_set = compute_tempo_rel_set(0.66, 1.5, 5)\npenalty = -2\nx, x_duration, X, Fs_feature, S, I = compute_sm_from_filename(fn_wav, L=21, H=5, \n                        L_smooth=12, tempo_rel_set=tempo_rel_set, penalty=penalty, thresh=0.15)\nS = normalization_properties_ssm(S)\n \nfn_ann = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv'\nann_frames, color_ann = read_structure_annotation(fn_ann, Fs=Fs_feature)\n\ncmap_penalty = colormap_penalty(penalty=penalty)\nfig, ax, im = plot_ssm_ann(S, ann_frames, Fs=1, color_ann=color_ann, cmap=cmap_penalty, \n                       xlabel='Time (frames)', ylabel='Time (frames)')\n\n\n\n\n\nS.shape\n\n(409, 409)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#경로군-path-family",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#경로군-path-family",
    "title": "5.3. 오디오 썸네일",
    "section": "경로군 (Path Family)",
    "text": "경로군 (Path Family)\n\n\\(P=((n_1,m_1), (n_2,m_2), \\ldots,(n_L,m_L))\\)를 주어진 세그먼트 \\(\\alpha=[s:t]\\subseteq [ 1:N]\\)의 경로(path)라고 하고, \\(\\Sigma\\)를 기본 허용 가능한 step size 집합을 나타낸다고 하자. 그러면 정의에 따라 \\(m_1=s\\) 및 \\(m_L=t\\) 이고, 경로 \\(P\\)는 \\(\\alpha=\\pi_2(P)\\)와 유도된(induced) 세그먼트 \\(\\pi_1(P)\\) 사이의 관계를 인코딩한다.\n경로의 개념을 확장하여 이제 음악 녹음에서 \\(\\alpha\\)와 다른 여러 세그먼트 간의 관계를 캡처할 수 있는 경로군(path family)의 개념을 소개한다. 이를 위해 먼저 \\(K\\) 크기의 세그먼트 군(segment family)을 다음의 집합으로 정의한다. \\[\\mathcal{A}:=\\{\\alpha_1,\\alpha_2,\\ldots,\\alpha_K\\}\\]\n즉, \\(k\\not= j\\)인 모든 \\(k,j\\in[1:K]\\)에 대해 \\(\\alpha_k\\cap\\alpha_j=\\emptyset\\)이다. \\(\\alpha\\) 위의 경로군는 다음의 집합으로 정의된다. \\[\\mathcal{P}:=\\{P_1,P_2,\\ldots,P_K\\}\\]\n\nof size \\(K\\),\nconsisting of paths \\(P_k\\) over \\(\\alpha\\) for \\(k\\in[1:K]\\)\n\n또한 추가 조건으로 유도된 세그먼트가 “pairwise disjoint”이어야 한다. 즉, \\(\\{\\pi_1(P_1),\\ldots,\\pi_1(P_K)\\}\\) 집합이 세그먼트 군이 되어야 한다.\n이러한 정의는 다음의 그림으로 설명된다.\n\n(왼쪽부터) 세그먼트 \\(\\alpha\\)가 있는 SSM / 세 개의 경로로 구성된 \\(\\alpha\\) 위에 경로군가 있이 SSM / 경로군이 아닌 경로들(유도된 세그먼트가 겹치기 때문에) / 두 개의 경로로 구성된 경로군\n\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F16.png\", width=1000)\n\n\n\n\n\ndef plot_path_family(ax, path_family, Fs=1, x_offset=0, y_offset=0, proj_x=True, w_x=7, proj_y=True, w_y=7):\n    \"\"\"Plot path family into a given axis\n\n    Args:\n        ax: Axis of plot\n        path_family: Path family\n        Fs: Feature rate of path_family (Default value = 1)\n        x_offset: Offset x-axis (Default value = 0)\n        y_offset: Yffset x-axis (Default value = 0)\n        proj_x: Display projection on x-axis (Default value = True)\n        w_x: Width used for projection on x-axis (Default value = 7)\n        proj_y: Display projection on y-axis (Default value = True)\n        w_y: Width used for projection on y-axis (Default value = 7)\n    \"\"\"\n    for path in path_family:\n        y = [(path[i][0] + y_offset)/Fs for i in range(len(path))]\n        x = [(path[i][1] + x_offset)/Fs for i in range(len(path))]\n        ax.plot(x, y, \"o\", color=[0, 0, 0], linewidth=3, markersize=5)\n        ax.plot(x, y, '.', color=[0.7, 1, 1], linewidth=2, markersize=6)\n    if proj_y:\n        for path in path_family:\n            y1 = path[0][0]/Fs\n            y2 = path[-1][0]/Fs\n            ax.add_patch(plt.Rectangle((0, y1), w_y, y2-y1, linewidth=1,\n                                       facecolor=[0, 1, 0], edgecolor=[0, 0, 0]))\n            # ax.plot([0, 0], [y1, y2], linewidth=8, color=[0, 1, 0])\n    if proj_x:\n        for path in path_family:\n            x1 = (path[0][1] + x_offset)/Fs\n            x2 = (path[-1][1] + x_offset)/Fs\n            ax.add_patch(plt.Rectangle((x1, 0), x2-x1, w_x, linewidth=1,\n                                       facecolor=[0, 0, 1], edgecolor=[0, 0, 0]))\n            # ax.plot([x1, x2], [0, 0], linewidth=8, color=[0, 0, 1])                 \n\n\n# Manually defined path family\n# For implementation reasons, the seconds components are of the paths \n# start with index 0 (corresponding to seg[0])\nseg_sec = [20, 80]\nseg = [int(seg_sec[0]*Fs_feature), int(seg_sec[1]*Fs_feature)]\npath_1 = [np.array([i+seg[0],i]) for i in range(0, seg[-1]-seg[0]+1)]\npath_2 = [np.array([int(i+130*Fs_feature),i]) for i in range(0, seg[-1]-seg[0]+1)]\npath_family = [path_1, path_2]\nprint('Segment:', seg)\nfig, ax, im = plot_ssm_ann(S, ann_frames, Fs=1, color_ann=color_ann, cmap=cmap_penalty, \n                       xlabel='Time (frames)', ylabel='Time (frames)') \nplot_path_family(ax[0,0], path_family, Fs=1, x_offset=seg[0])\n\nSegment: [20, 80]"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#커버리지-coverage",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#커버리지-coverage",
    "title": "5.3. 오디오 썸네일",
    "section": "커버리지 (Coverage)",
    "text": "커버리지 (Coverage)\n\n이제 경로군 \\(\\mathcal{P}\\)의 특정 속성을 설명하는 몇 가지 측정값을 정의한다.\n\\(\\mathcal{A}:=\\{\\alpha_1,\\alpha_2,\\ldots,\\alpha_K\\}\\)를 세그먼트 군이라고 하자.\n\\(\\mathcal{A}\\)의 커버리지/반영범위(coverage) \\(\\gamma(\\mathcal{A})\\)는 다음과 같이 정의된다. \\[\\gamma(\\mathcal{A}):=\\sum_{k=1}^K|\\alpha_k|\\]\n또한, 경로군 \\(\\mathcal{P}=\\{P_1,P_2,\\ldots,P_K\\}\\)의 범위 \\(\\gamma(\\mathcal{P})\\)는 유도된 세그먼트 군 \\(\\{\\pi_1(P_1),\\ldots,\\pi_1(P_K)\\}\\)의 커버리지로 정의된다.\n다음 코드에서 유도된 세그먼트군과 주어진 경로군에 대한 커버리지를 유도하는 함수를 제공한다. 이 함수는 위에서 생성한 경로군에 적용된다.\n\n\ndef compute_induced_segment_family_coverage(path_family):\n    \"\"\"Compute induced segment family and coverage from path family\n\n    Args:\n        path_family (list): Path family\n\n    Returns:\n        segment_family (np.ndarray): Induced segment family\n        coverage (float): Coverage of path family\n    \"\"\"\n    num_path = len(path_family)\n    coverage = 0\n    if num_path > 0:\n        segment_family = np.zeros((num_path, 2), dtype=int)\n        for n in range(num_path):\n            segment_family[n, 0] = path_family[n][0][0]\n            segment_family[n, 1] = path_family[n][-1][0]\n            coverage = coverage + segment_family[n, 1] - segment_family[n, 0] + 1\n    else:\n        segment_family = np.empty\n\n    return segment_family, coverage\n\n\nsegment_family, coverage = compute_induced_segment_family_coverage(path_family)\nprint('Segment (alpha):', seg)\nprint('Induced segment family:')\nprint(segment_family)\nprint('Coverage: %d'%coverage)    \n\nSegment (alpha): [20, 80]\nInduced segment family:\n[[ 20  80]\n [130 190]]\nCoverage: 122"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#점수와-최적-score-and-optimality",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#점수와-최적-score-and-optimality",
    "title": "5.3. 오디오 썸네일",
    "section": "점수와 최적 (Score and Optimality)",
    "text": "점수와 최적 (Score and Optimality)\n\n다음으로 점수(score)의 개념을 경로에서 경로군로 전환한다. 주어진 SSM \\(\\mathbf{S}\\)에서 경로 \\(P\\)의 점수 \\(\\sigma(P)\\)는 다음과 같이 정의된다. \\[\\sigma(P)=\\sum_{\\ell=1}^L \\mathbf{S}(n_\\ell,m_\\ell).\\]\n이 값은 세그먼트 \\(\\pi_1(P)\\)와 \\(\\pi_2(P)\\) 사이의 유사성 관계에 대한 품질 척도로 생각할 수 있다. 경로군 \\(\\mathcal{P}\\)의 경우 점수 \\(\\sigma(\\mathcal{P})\\)는 다음과 같이 정의된다. \\[\\sigma(\\mathcal{P}) := \\sum_{k=1}^{K} \\sigma(P_k).\\]\n큰 점수 \\(\\sigma(\\mathcal{P})\\)는 \\(\\mathcal{P}\\)의 경로 구성 요소가 해당 세그먼트 간에 강력한 유사성 관계를 표현함을 나타낸다.\n일반적으로 많은 수의 \\(\\alpha\\) 위로의 가능한 경로군가 있다. 이러한 경로군 중에서 \\(\\mathcal{P}^\\ast := \\underset{\\mathcal{P}}{\\mathrm{argmax}} \\,\\,\\, \\sigma(\\mathcal{P})\\)를 최대 점수의 최적 경로군을 나타낸다고 하자.\n다음에서는 \\(\\mathcal{P}^\\ast\\) 경로에 의해 유도된(induced) 세그먼트로 구성된 것을 유도된(induced) 세그먼트 군 (of \\(\\mathcal{P}^\\ast\\) 또는 간단히 \\(\\alpha\\))이라고 한다. 직관적으로, 유도된 세그먼트 군은 세그먼트 \\(\\alpha\\)의 (겹치지 않는) 반복을 포함한다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#최적-경로군-계산-computation-of-optimal-path-families",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#최적-경로군-계산-computation-of-optimal-path-families",
    "title": "5.3. 오디오 썸네일",
    "section": "최적 경로군 계산 (Computation of Optimal Path Families)",
    "text": "최적 경로군 계산 (Computation of Optimal Path Families)\n\n최적 워핑 경로를 계산하는 DTW(dynamic time warping)과 유사하게 동적(dynamic) 프로그래밍을 사용하여 최적 경로군 \\(\\mathcal{P}^\\ast\\)를 계산하는 효율적인 알고리즘이 있다.\n다음 코드에서 허용 가능한 step size의 \\(\\Sigma = \\{(2,1),(1,2),(1,1)\\}\\) 집합을 사용하여 이 알고리즘의 구현한다.\n\\(\\mathbf{S}\\)를 필요한 정규화 속성(점수 매트릭스라고도 함)과 세그먼트 \\(\\alpha=[s:t]\\subseteq [ 1:N]\\)와 \\(M:=|\\alpha|\\)을 충족하는 \\(N\\)-square SSM이라고 하자.\n알고리즘의 입력(input)은 \\(\\mathbf{S}\\)의 열 \\(s\\)에서 \\(t\\)로 구성된 \\(N\\times M\\) 부분행렬(submatrix) \\(\\mathcal{S}^{\\alpha}\\)이다.\n먼저 동적 프로그래밍을 사용하여 누적 점수 매트릭스 \\(D\\)를 계산한다.\n\n\ndef compute_accumulated_score_matrix(S_seg):\n    \"\"\"Compute the accumulated score matrix\n\n    Args:\n        S_seg (np.ndarray): Submatrix of an enhanced and normalized SSM ``S``.\n            Note: ``S`` must satisfy ``S(n,m) <= 1 and S(n,n) = 1``\n\n    Returns:\n        D (np.ndarray): Accumulated score matrix\n        score (float): Score of optimal path family\n    \"\"\"\n    inf = math.inf\n    N = S_seg.shape[0]\n    M = S_seg.shape[1]+1\n\n    # Iinitializing score matrix\n    D = -inf * np.ones((N, M), dtype=np.float64)\n    D[0, 0] = 0.\n    D[0, 1] = D[0, 0] + S_seg[0, 0]\n\n    # Dynamic programming\n    for n in range(1, N):\n        D[n, 0] = max(D[n-1, 0], D[n-1, -1])\n        D[n, 1] = D[n, 0] + S_seg[n, 0]\n        for m in range(2, M):\n            D[n, m] = S_seg[n, m-1] + max(D[n-1, m-1], D[n-1, m-2], D[n-2, m-1])\n\n    # Score of optimal path family\n    score = np.maximum(D[N-1, 0], D[N-1, M-1])\n\n    return D, score\n\n\n다음으로, 역추적(backtracking)을 이용하여 최적 경로군 \\(\\mathcal{P}^\\ast\\)를 도출한다.\n\n\ndef compute_optimal_path_family(D):\n    \"\"\"Compute an optimal path family given an accumulated score matrix\n\n    Args:\n        D (np.ndarray): Accumulated score matrix\n\n    Returns:\n        path_family (list): Optimal path family consisting of list of paths\n            (each path being a list of index pairs)\n    \"\"\"\n    # Initialization\n    inf = math.inf\n    N = int(D.shape[0])\n    M = int(D.shape[1])\n\n    path_family = []\n    path = []\n\n    n = N - 1\n    if(D[n, M-1] < D[n, 0]):\n        m = 0\n    else:\n        m = M-1\n        path_point = (N-1, M-2)\n        path.append(path_point)\n\n    # Backtracking\n    while n > 0 or m > 0:\n\n        # obtaining the set of possible predecesors given our current position\n        if(n <= 2 and m <= 2):\n            predecessors = [(n-1, m-1)]\n        elif(n <= 2 and m > 2):\n            predecessors = [(n-1, m-1), (n-1, m-2)]\n        elif(n > 2 and m <= 2):\n            predecessors = [(n-1, m-1), (n-2, m-1)]\n        else:\n            predecessors = [(n-1, m-1), (n-2, m-1), (n-1, m-2)]\n\n        # case for the first row. Only horizontal movements allowed\n        if n == 0:\n            cell = (0, m-1)\n        # case for the elevator column: we can keep going down the column or jumping to the end of the next row\n        elif m == 0:\n            if D[n-1, M-1] > D[n-1, 0]:\n                cell = (n-1, M-1)\n                path_point = (n-1, M-2)\n                if(len(path) > 0):\n                    path.reverse()\n                    path_family.append(path)\n                path = [path_point]\n            else:\n                cell = (n-1, 0)\n        # case for m=1, only horizontal steps to the elevator column are allowed\n        elif m == 1:\n            cell = (n, 0)\n        # regular case\n        else:\n\n            # obtaining the best of the possible predecesors\n            max_val = -inf\n            for i, cur_predecessor in enumerate(predecessors):\n                if(max_val < D[cur_predecessor[0], cur_predecessor[1]]):\n                    max_val = D[cur_predecessor[0], cur_predecessor[1]]\n                    cell = cur_predecessor\n\n            # saving the point in the current path\n            path_point = (cell[0], cell[1]-1)\n            path.append(path_point)\n\n        (n, m) = cell\n\n    # adding last path to the path family\n    path.reverse()\n    path_family.append(path)\n    path_family.reverse()\n\n    return path_family\n\n\n다음 그림에서 부분 행렬 \\(\\mathcal{S}^{\\alpha}\\)와 세그먼트 \\(\\alpha=[83:137]\\) (seg = [82, 136])에 대한 누적 점수 행렬 \\(D\\)가 나타나 있다.\n\\(2~\\mathrm{Hz}\\)의 프레임 속도(frame rate)를 사용하여 이 세그먼트는 \\(41\\)에서 \\(68\\)초 범위의 간격에 해당하며 대략적으로 Brahms 레코딩의 \\(B_1\\) 부분이다. 다음 사항에 유의하자.\n\n부분 행렬 \\(\\mathcal{S}^{\\alpha}\\)의 열 수는 \\(M=55\\)이다.\nPython 구현에서 \\(\\mathcal{S}^{\\alpha}\\)의 열은 인덱스 0부터 인덱스 M-1까지 열거된다.\n행렬 \\(D\\)에는 소위 엘리베이터(elevator) 컬럼을 포함하는 \\(M+1\\) 컬럼이 있다.\n단계 크기(step size) 조건 \\(\\Sigma = \\{(2,1),(1,2),(1,1)\\}\\)를 사용하면, 경로가 로컬 템포 차이에 적응할 수 있다.\n최적의 경로군은 4개의 경로로 구성된다. 유도된(induced) 세그먼트는 Brahms 녹음의 4개의 \\(B\\) 파트 섹션에 대략 해당된다.\n\n\n\ndef plot_matrix_seg(ax, M, seg, title='', xlabel='', cmap='gray_r'):\n    plot_matrix(M, Fs=1, ax=[ax], cmap=cmap, \n        xlabel=xlabel, ylabel='Time (frames)', title=title)\n    ax.set_xticks([0, M.shape[1]-1])    \n    \nseg_sec = [41,68]\nseg = [int(seg_sec[0]*Fs_feature), int(seg_sec[1]*Fs_feature)]\nprint('Segment:', seg)\nprint('Length of segment: M = ', seg[1]-seg[0]+1)\nS_seg = S[:,seg[0]:seg[1]+1]\nD, score = compute_accumulated_score_matrix(S_seg)\npath_family = compute_optimal_path_family(D)\n\nplt.figure(figsize=(8, 8))\nax = plt.subplot(2, 2, 1)\ntitle = r'$\\mathcal{S}^{\\alpha}$'\nplot_matrix_seg(ax, S_seg, seg, title=title, xlabel='', cmap=cmap_penalty)\n\nax = plt.subplot(2, 2, 2)\ntitle = r'$D$'\nplot_matrix_seg(ax, D, seg, title=title)\n\nax = plt.subplot(2, 2, 3)\ntitle = r'$\\mathcal{S}^{\\alpha}$ with optimal path family'\nplot_matrix_seg(ax, S_seg, seg, title=title, xlabel='', cmap=cmap_penalty)\nplot_path_family(ax, path_family, x_offset=0, w_y=1)\n\nax = plt.subplot(2, 2, 4)\ntitle = r'$D$ with optimal path family'\nplot_matrix_seg(ax, D, seg, title=title)\nplot_path_family(ax, path_family, x_offset=1, w_y=1)\n\nplt.tight_layout()\n\nSegment: [41, 68]\nLength of segment: M =  28"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#적합도-측정-fitness-measure",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#적합도-측정-fitness-measure",
    "title": "5.3. 오디오 썸네일",
    "section": "적합도 측정 (Fitness Measure)",
    "text": "적합도 측정 (Fitness Measure)\n\n주어진 세그먼트 \\(\\alpha\\)에 대해, \\(\\mathcal{P}^\\ast=\\{P_1,\\ldots,P_K\\}\\)를 최적 경로군이라고 하자. 이제 \\(\\sigma(\\mathcal{P}^\\ast)\\) 점수와 \\(\\mathcal{P}^\\ast\\)의 유도된 세그먼트 군에서 적합도(fitness) 측정을 유도하는 방법을 설명한다.\n첫 번째, 총 점수 \\(\\sigma(\\mathcal{P}^\\ast)\\)를 \\(\\alpha\\)의 적합도 값으로 사용한다. 그러나 이 측정은 \\(\\alpha\\)의 길이와 경로에 의존할 뿐만 아니라 당연한 자체-설명(self-explanation)도 캡처하기 때문에 아직 원하는 속성을 갖지 않는다.\n\n예를 들어 \\(\\alpha=[1:N]\\) 세그먼트는 전체 시퀀스 \\(X\\)를 완벽하게 설명하며 이는 당연한 사실이다. 좀 더 일반적으로 각 세그먼트 \\(\\alpha\\)는 자기 유사성 행렬의 주 대각선으로 인코딩된 정보를 완벽하게 설명한다.\n\n따라서 적합도 측정을 정의할 때의 한 가지 방법은 이러한 당연한 자기-설명(trivial self-explanation)을 무시하는 것이다.\n\n정규화 속성 \\(\\mathbf{S}(n,m)\\leq 1\\) 및 \\(\\mathbf{S}(n,n)=1\\)를 가정하면, \\(\\sigma(\\mathcal{P}^\\ast)\\) 점수에서 \\(|\\alpha|\\) 길이를 빼는 방법을 사용할 수 있다.\n\n또한 최적 경로군 \\(\\mathcal{P}^\\ast\\)에 포함된 경로 \\(P_k\\)의 길이 \\(L_k:=|P_k|\\)에 대한 점수를 정규화한다.\n\n다음과 같이 정의된 정규화 점수(normalized score) \\(\\bar{\\sigma}(\\alpha)\\)를 산출한다. \\[\\bar{\\sigma}(\\alpha) := \\frac{\\sigma(\\mathcal{P}^\\ast) - |\\alpha|}{\\sum_{k=1}^{K} L_k}\\]\n\n직관적으로 \\(\\bar{\\sigma}(\\alpha)\\) 값은 최적 경로군 \\(\\mathcal{P}^\\ast\\)의 평균 점수를 나타낸다.\n정규화 점수는 주어진 세그먼트가 다른 세그먼트를 얼마나 잘 설명하는지를 나타내며, 여기서 정규화는 세그먼트 길이의 영향을 제거한다. 이렇게 하면 길이가 다른 세그먼트를 비교할 때 정규화된 점수가 공정한 측정이 될 수 있다.\n반복성 외에 또 다른 문제는 썸네일 및 썸네일 관련 세그먼트가 기본 음악 녹음을 얼마나 많이 (how much) 포함하는지이다.\n이 속성을 포착하기 위해 주어진 \\(\\alpha\\)에 대한 커버리지(coverage) 측정을 정의한다.\n이를 위해 \\(\\mathcal{A}^\\ast:=\\{\\pi_1(P_1),\\ldots,\\pi_1(P_K)\\}\\)를 최적 경로군 \\(\\mathcal{P}^\\ast\\)에 의해 유도된 세그먼트 군이라고 하고, \\(\\gamma(\\mathcal{A}^\\ast)\\)를 그것의 커버리지라고 하자.\n그런 다음 정규화 커버리지 \\(\\bar{\\gamma}(\\alpha)\\)를 다음과 같이 정의한다. \\[\\bar{\\gamma}(\\alpha) := \\frac{\\gamma(\\mathcal{A}^\\ast) - |\\alpha|}{N}.\\]\n위와 같이 사소한(trivial) 커버리지에 대해 \\(|\\alpha|\\) 길이를 뺀다.\n\\(\\bar{\\gamma}(\\alpha)\\) 값은 \\(\\alpha\\)의 유도된 세그먼트의 합집합과 원본 녹음의 전체 길이 사이의 비율을 나타낸다(자기-설명의 비율을 뺀 값).\n높은 평균 점수와 높은 커버리지는 썸네일 세그먼트를 정의하는 데 바람직한 속성이다. 그러나 이 두 가지 속성을 동시에 만족시키기 어려운 경우가 있다.\n세그먼트가 짧을수록 평균 점수는 더 높지만 커버리지는 더 낮고, 길이가 더 긴 세그먼트는 평균 점수가 더 낮지만 커버리지가 더 높은 경향이 있다.\n이 두 추세의 균형을 맞추기 위해 적절한 평균을 취하여 점수와 커버리지 측정을 결합한다.\n특히 세그먼트 \\(\\alpha\\)의 적합도(fitness) \\(\\varphi(\\alpha)\\)를 정규화된 점수와 정규화된 커버리지 사이의 조화 평균 (harmonic mean) 을 \\[\\varphi(\\alpha) := 2\\cdot \\frac{\\bar{\\sigma}(\\alpha) \\cdot \\bar{\\gamma}(\\alpha)}{\\bar{\\sigma}(\\alpha)+\\bar{\\gamma}(\\alpha )}\\] 라고 정의한다.\n정규화된 값 \\(\\bar{\\sigma}(\\alpha)\\) 및 \\(\\bar{\\gamma}(\\alpha)\\)를 기반으로, \\(\\varphi(\\alpha)\\leq 1-|\\alpha |/N\\) 임을 보일 수 있다.\n다음 코드 셀에서는 이전 코드 셀에서 고려한 경로군에 대해 이러한 다양한 측정값을 계산한다.\n\n\ndef compute_fitness(path_family, score, N):\n    \"\"\"Compute fitness measure and other metrics from path family\n\n    Args:\n        path_family (list): Path family\n        score (float): Score\n        N (int): Length of feature sequence\n\n    Returns:\n        fitness (float): Fitness\n        score (float): Score\n        score_n (float): Normalized score\n        coverage (float): Coverage\n        coverage_n (float): Normalized coverage\n        path_family_length (int): Length of path family (total number of cells)\n    \"\"\"\n    eps = 1e-16\n    num_path = len(path_family)\n    M = path_family[0][-1][1] + 1\n\n    # Normalized score\n    path_family_length = 0\n    for n in range(num_path):\n        path_family_length = path_family_length + len(path_family[n])\n    score_n = (score - M) / (path_family_length + eps)\n\n    # Normalized coverage\n    segment_family, coverage = compute_induced_segment_family_coverage(path_family)\n    coverage_n = (coverage - M) / (N + eps)\n\n    # Fitness measure\n    fitness = 2 * score_n * coverage_n / (score_n + coverage_n + eps)\n\n    return fitness, score, score_n, coverage, coverage_n, path_family_length\n\n\nN = S.shape[0]\n\nsegment_family, coverage = compute_induced_segment_family_coverage(path_family)\nfitness, score, score_n, coverage, coverage_n, path_family_length = compute_fitness(\n    path_family, score, N)\n\nprint('Segment (alpha):', seg)\nprint('Length of segment:', seg[-1]-seg[0]+1)\nprint('Length of feature sequence:', N)\nprint('Induced segment path family:\\n', segment_family)\nprint('Fitness: %0.3f'%fitness) \nprint('Score: %0.3f'%score) \nprint('Normalized score: %0.3f'%score_n) \nprint('Coverage: %d'%coverage) \nprint('Normalized coverage: %0.3f'%coverage_n) \nprint('Length of all paths of family: %d'%path_family_length)\n\nSegment (alpha): [41, 68]\nLength of segment: 28\nLength of feature sequence: 205\nInduced segment path family:\n [[ 41  67]\n [ 68  88]\n [149 174]\n [175 196]]\nFitness: 0.392\nScore: 73.857\nNormalized score: 0.478\nCoverage: 96\nNormalized coverage: 0.332\nLength of all paths of family: 96"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#썸네일-선정-thumbnail-selection",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#썸네일-선정-thumbnail-selection",
    "title": "5.3. 오디오 썸네일",
    "section": "썸네일 선정 (Thumbnail Selection)",
    "text": "썸네일 선정 (Thumbnail Selection)\n\n적합도 측정을 기반으로 구한 오디오 썸네일을 최대 적합도(maximizing-fitness) 세그먼트로 정의한다. \\[\\alpha^\\ast := \\underset{\\alpha}{\\mathrm{argmax}} \\,\\, \\varphi(\\alpha)\\]\n적합도 측정으로 이 세그먼트는 오디오 녹음의 “가능한 많은 부분을 포함하는 겹치지 않는 반복”을 가지게 된다. 또한 이러한 반복은 오디오 녹음을 “pairwise disjoint” 세그먼트로 분할하는 \\(\\alpha^\\ast\\)의 최적 경로군으로 얻은 유도된 세그먼트에 의해 주어진다.\n사전 지식을 설명하고 잘못된 추정을 제거하기 위해 썸네일 솔루션에 추가 요구 사항을 부과할 수 있다. 가능한 최소 썸네일 길이에 대한 하한 \\(\\theta\\)를 도입하면 자기 유사성 행렬(SSM)에 분산된 노이즈의 영향을 줄일 수 있다. 위의 정의를 확장하여 \\(\\alpha^\\ast_\\theta := \\underset{\\alpha, |\\alpha|\\geq \\theta}{\\mathrm{argmax}} \\,\\, \\varphi(\\alpha)\\)를 정의한다.\n오디오 썸네일 계산에는 가능한 모든 세그먼트 \\(\\alpha\\)에 대한 적합도 측정 계산이 포함된다.\n적합도 최대화 세그먼트를 선택하는 것 외에도 모든 세그먼트에 대한 적합도 값의 시각화는 매우 유익하며 음악 녹음의 구조적 속성에 대한 음악적 통찰력을 제공한다. 그 중 scape plot에 대해 뒤에서 다룰 것이며, 여기서 적합도 측정의 추가 속성에 대해서도 논의하기로 한다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#세그먼트의-삼각형-표현triangular-representation",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#세그먼트의-삼각형-표현triangular-representation",
    "title": "5.3. 오디오 썸네일",
    "section": "세그먼트의 삼각형 표현(Triangular Representation)",
    "text": "세그먼트의 삼각형 표현(Triangular Representation)\n\n위에서 오디오 썸네일링의 맥락에서 세그먼트 특정의 속성을 표현하는 적합도(fitness) 값을 가능한 각 세그먼트에 할당하도록 적합도 측정을 계산했다.\n이제 세그먼트 의존적(segment-dependent) 속성을 간결하고(compact) 계층적인(hierarchical) 방식으로 시각화할 수 있는 표현을 소개한다.\n세그먼트 \\(\\alpha=[s:t]\\subseteq [1:N]\\)는 시작점 \\(s\\)와 끝점 \\(t\\)에 의해 고유하게 결정된다. \\(s\\leq t\\)인 임의의 두 숫자 \\(s,t\\in[1:N]\\)가 세그먼트를 정의하므로, \\((N+1)N/2\\)개의 다른 세그먼트가 있다고 볼 수 있다.\n시작점과 끝점을 고려하는 대신 각 세그먼트를 그것의 중심(center) \\(c(\\alpha):=(s+t)/2\\)과 길이 \\(|\\alpha|\\)로 고유하게 설명할 수도 있다. 중심을 사용하여 수평 축을 매개변수화하고, 길이를 사용하여 높이를 매개변수화하면 각 세그먼트는 삼각형 표현(triangular representation) 의 점으로 표시될 수 있다.\n이렇게 하면 세그먼트 집합이 길이에 따라 계층적 방식으로 아래에서 위로 정렬된다. 특히, 이 삼각형의 상단은 최대 길이 \\(N\\)의 고유 세그먼트에 해당하고, 삼각형의 하단 점은 길이가 1인 \\(N\\) 세그먼트에 해당한다(시작점과 끝점이 일치함). 또한 주어진 세그먼트 \\(\\alpha\\)에 포함된 모든 세그먼트 \\(\\alpha'\\subseteq\\alpha\\)는 \\(\\alpha\\)로 주어진 점 아래의 하위 삼각형에 있는 삼각형 표현의 점에 해당한다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F19.png\", width=500)\n\n\n\n\n\n\\([1:N]\\) 내의 모든 세그먼트를 삼각형으로 표현한 경우, 다음 예는 다음 세그먼트 집합을 시각적으로 나타낸다.\n\n\n주어진 임계값 \\(\\theta\\geq 0\\) 이상의 최소 길이를 갖는 모든 세그먼트\n\n\n주어진 세그먼트 \\(\\alpha\\)를 포함하는 모든 세그먼트\n\n\n주어진 세그먼트 \\(\\alpha\\)와 분리된 모든 세그먼트\n\n\n주어진 세그먼트 \\(\\alpha\\)의 중심 \\(c(\\alpha)\\)를 포함하는 모든 세그먼트\n\n\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_E12.png\", width=600)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#scape-plot",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#scape-plot",
    "title": "5.3. 오디오 썸네일",
    "section": "Scape Plot",
    "text": "Scape Plot\n\n삼각형 표현은 모든 세그먼트 \\(\\alpha\\)에 대해 계산할 수 있는 특정 숫자 속성 \\(\\varphi(\\alpha)\\in\\mathbb{R}\\)를 시각화하기 위한 그리드로 사용할 수 있다. 예를 들어 이 속성은 오디오 썸네일에 사용되는 적합성 값(fitness value)일 수 있다.\n이러한 시각적 표현은 속성의 “scape plot” 표현이라고도 한다.\n보다 정확하게는 다음을 설정하여 scape plot \\(\\Delta\\)를 정의한다. \\[\\label{eq:AudioStru:Thumb:SPfitness} \\Delta(c(\\alpha),|\\alpha|):=\\varphi(\\alpha)\\]\n쉬운 예제로 \\(\\alpha=[s:t]\\)에 대해 \\(\\varphi(\\alpha):= (t-s+1)/N\\)로 정의된 \\(\\varphi\\) 함수를 고려해보자. 총 길이 \\(N\\)에 대한 세그먼트 길이를 인코딩한다.\n주의: 이 구현에서는 세그먼트 종속 속성 \\(\\varphi(\\alpha)\\in\\mathbb{R}\\)을 저장하기 위한 데이터 구조로 N-스퀘어 행렬 SP를 사용한다. 길이를 인코딩하기 위해 SP의 첫 번째 차원을 사용하고 중심을 인코딩하기 위해 두 번째 차원을 사용한다. Python의 인덱싱은 인덱스 0부터 시작하므로 길이 차원을 해석할 때 주의해야 한다. 특히, 엔트리 SP[length_minus_one, start]는 length_minus_one = 0, ..., N-1에 대해 length_minus_one + 1의 길이를 갖는 세그먼트에 대한 정보를 포함한다. 또한 SP는 왼쪽 위 부분(대각선 포함)만 사용한다는 점에 유의해야 한다.\n\n\ndef visualize_scape_plot(SP, Fs=1, ax=None, figsize=(4, 3), title='',\n                         xlabel='Center (seconds)', ylabel='Length (seconds)', interpolation='nearest'):\n    \"\"\"Visualize scape plot\n\n    Args:\n        SP: Scape plot data (encodes as start-duration matrix)\n        Fs: Sampling rate (Default value = 1)\n        ax: Used axes (Default value = None)\n        figsize: Figure size (Default value = (4, 3))\n        title: Title of figure (Default value = '')\n        xlabel: Label for x-axis (Default value = 'Center (seconds)')\n        ylabel: Label for y-axis (Default value = 'Length (seconds)')\n        interpolation: Interpolation value for imshow (Default value = 'nearest')\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n        im: Handle for imshow\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig = plt.figure(figsize=figsize)\n        ax = plt.gca()\n    N = SP.shape[0]\n    SP_vis = np.zeros((N, N))\n    for length_minus_one in range(N):\n        for start in range(N-length_minus_one):\n            center = start + length_minus_one//2\n            SP_vis[length_minus_one, center] = SP[length_minus_one, start]\n\n    extent = np.array([-0.5, (N-1)+0.5, -0.5, (N-1)+0.5]) / Fs\n    im = plt.imshow(SP_vis, cmap='hot_r', aspect='auto', origin='lower', extent=extent, interpolation=interpolation)\n    x = np.asarray(range(N))\n    x_half_lower = x/2\n    x_half_upper = x/2 + N/2 - 1/2\n    plt.plot(x_half_lower/Fs, x/Fs, '-', linewidth=3, color='black')\n    plt.plot(x_half_upper/Fs, np.flip(x, axis=0)/Fs, '-', linewidth=3, color='black')\n    plt.plot(x/Fs, np.zeros(N)/Fs, '-', linewidth=3, color='black')\n    plt.xlim([0, (N-1) / Fs])\n    plt.ylim([0, (N-1) / Fs])\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    plt.tight_layout()\n    plt.colorbar(im, ax=ax)\n    return fig, ax, im\n\n\nN = 9\nSP = np.zeros((N,N))\nfor k in range(N):\n    for s in range(N-k):\n        length = k + 1\n        SP[k, s]= length/N  \n\nplt.figure(figsize=(7,3))\nax = plt.subplot(121)\nplt.imshow(SP, cmap='hot_r', aspect='auto') \nax.set_title('Data structure (N = %d)'%N)\nax.set_xlabel('Segment start (samples)')\nax.set_ylabel('Length minus one (samples)')\nplt.colorbar()  \n\nax = plt.subplot(122)\nfig, ax, im = visualize_scape_plot(SP, Fs=1, ax=ax, title='Scape plot visualization', \n                xlabel='Segment center (samples)', ylabel='Length minus one (samples)')"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#적합도-scape-plot",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#적합도-scape-plot",
    "title": "5.3. 오디오 썸네일",
    "section": "적합도 Scape Plot",
    "text": "적합도 Scape Plot\n\nscape plot 표현을 사용해 모든 세그먼트에 대한 적합도 측정을 시각화한다.\n\n\nfn_wav = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.wav'\n\ntempo_rel_set = compute_tempo_rel_set(0.66, 1.5, 5)\npenalty = -2\nx, x_duration, X, Fs_feature, S, I = compute_sm_from_filename(fn_wav, L=41, H=10, \n                        L_smooth=8, tempo_rel_set=tempo_rel_set, penalty=penalty, thresh= 0.15)\nS = normalization_properties_ssm(S)\n \nfn_ann = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv'\nann_frames, color_ann = read_structure_annotation(fn_ann, Fs=Fs_feature)\n\ncmap_penalty = colormap_penalty(penalty=penalty)\nfig, ax, im = plot_ssm_ann(S, ann_frames, Fs=1, color_ann=color_ann, cmap=cmap_penalty, \n                       xlabel='Time (frames)', ylabel='Time (frames)')\n\n\n\n\n\n다음 코드는 세그먼트 \\(\\alpha\\)에 대한 적합도 척도 \\(\\varphi(\\alpha)\\in\\mathbb{R}\\) (및 점수 \\(\\sigma(\\alpha)\\), 정규화된 점수 \\(\\bar{\\sigma}(\\alpha)\\), 커버리지(coverage) \\(\\gamma(\\alpha)\\), 정규화된 커버리지 \\(\\bar{\\gamma}(\\alpha)\\))를 계산한다.\n\n\ndef compute_fitness_scape_plot(S):\n    \"\"\"Compute scape plot for fitness and other measures\n\n    Args:\n        S (np.ndarray): Self-similarity matrix\n\n    Returns:\n        SP_all (np.ndarray): Vector containing five different scape plots for five measures\n            (fitness, score, normalized score, coverage, normlized coverage)\n    \"\"\"\n    N = S.shape[0]\n    SP_fitness = np.zeros((N, N))\n    SP_score = np.zeros((N, N))\n    SP_score_n = np.zeros((N, N))\n    SP_coverage = np.zeros((N, N))\n    SP_coverage_n = np.zeros((N, N))\n\n    for length_minus_one in range(N):\n        for start in range(N-length_minus_one):\n            S_seg = S[:, start:start+length_minus_one+1]\n            D, score = compute_accumulated_score_matrix(S_seg)\n            path_family = compute_optimal_path_family(D)\n            fitness, score, score_n, coverage, coverage_n, path_family_length = compute_fitness(\n                path_family, score, N)\n            SP_fitness[length_minus_one, start] = fitness\n            SP_score[length_minus_one, start] = score\n            SP_score_n[length_minus_one, start] = score_n\n            SP_coverage[length_minus_one, start] = coverage\n            SP_coverage_n[length_minus_one, start] = coverage_n\n    SP_all = [SP_fitness, SP_score, SP_score_n, SP_coverage, SP_coverage_n]\n    return SP_all\n\n\nSP_all = compute_fitness_scape_plot(S)\n\n\n다음으로 적합도 scape plot라고도 하는 scape plot 표현을 사용하여 적합성 값 \\(\\varphi(\\alpha)\\)를 시각화한다.\n또한 경로군 및 유도된 세그먼트에 걸쳐 적합도-최대화 세그먼트 또는 오디오 썸네일 \\(\\alpha^\\ast := \\underset{\\alpha}{\\mathrm{argmax}} \\,\\, \\varphi(\\alpha)\\)를 그린다.\n썸네일과 유도된 세그먼트는 scape plot 표현에서 점(각각 파란색과 녹색 점)으로 표시된다.\n\n\ndef seg_max_sp(SP):\n    \"\"\"Return segment with maximal value in SP\n\n    Args:\n        SP (np.ndarray): Scape plot\n\n    Returns:\n        seg (tuple): Segment ``(start_index, end_index)``\n    \"\"\"\n    N = SP.shape[0]\n    # value_max = np.max(SP)\n    arg_max = np.argmax(SP)\n    ind_max = np.unravel_index(arg_max, [N, N])\n    seg = [ind_max[1], ind_max[1]+ind_max[0]]\n    return seg\n\n\ndef plot_seg_in_sp(ax, seg, S=None, Fs=1):\n    \"\"\"Plot segment and induced segements as points in SP visualization\n\n    Args:\n        ax: Axis for image\n        seg: Segment ``(start_index, end_index)``\n        S: Self-similarity matrix (Default value = None)\n        Fs: Sampling rate (Default value = 1)\n    \"\"\"\n    if S is not None:\n        S_seg = S[:, seg[0]:seg[1]+1]\n        D, score = compute_accumulated_score_matrix(S_seg)\n        path_family = compute_optimal_path_family(D)\n        segment_family, coverage = compute_induced_segment_family_coverage(path_family)\n        length = segment_family[:, 1] - segment_family[:, 0] + 1\n        center = segment_family[:, 0] + length//2\n        ax.scatter(center/Fs, length/Fs, s=64, c='white', zorder=9999)\n        ax.scatter(center/Fs, length/Fs, s=16, c='lime', zorder=9999)\n    length = seg[1] - seg[0] + 1\n    center = seg[0] + length//2\n    ax.scatter(center/Fs, length/Fs, s=64, c='white', zorder=9999)\n    ax.scatter(center/Fs, length/Fs, s=16, c='blue', zorder=9999)\n\n    \ndef plot_sp_ssm(SP, seg, S, ann, color_ann=[], title='', figsize=(5, 4)):\n    \"\"\"Visulization of SP and SSM\n\n    Args:\n        SP: Scape plot\n        seg: Segment ``(start_index, end_index)``\n        S: Self-similarity matrix\n        ann: Annotation\n        color_ann: color scheme used for annotations (Default value = [])\n        title: Title of figure (Default value = '')\n        figsize: Figure size (Default value = (5, 4))\n    \"\"\"\n    fig, ax, im = visualize_scape_plot(SP, figsize=figsize, title=title,\n                                       xlabel='Center (frames)', ylabel='Length (frames)')\n    plot_seg_in_sp(ax, seg, S)\n\n    penalty = np.min(S)\n    cmap_penalty = colormap_penalty(penalty=penalty)\n    fig, ax, im = plot_ssm_ann_optimal_path_family(\n        S, ann, seg, color_ann=color_ann, fontsize=8, cmap=cmap_penalty, figsize=(4, 4),\n        ylabel='Time (frames)')\n\n    \n    \ndef check_segment(seg, S):\n    \"\"\"Prints properties of segments with regard to SSM ``S``\n\n    Args:\n        seg (tuple): Segment ``(start_index, end_index)``\n        S (np.ndarray): Self-similarity matrix\n\n    Returns:\n         path_family (list): Optimal path family\n    \"\"\"\n    N = S.shape[0]\n    S_seg = S[:, seg[0]:seg[1]+1]\n    D, score = compute_accumulated_score_matrix(S_seg)\n    path_family = compute_optimal_path_family(D)\n    fitness, score, score_n, coverage, coverage_n, path_family_length = compute_fitness(\n                path_family, score, N)\n    segment_family, coverage2 = compute_induced_segment_family_coverage(path_family)\n    print('Segment (alpha):', seg)\n    print('Length of segment:', seg[-1]-seg[0]+1)\n    print('Length of feature sequence:', N)\n    print('Induced segment path family:\\n', segment_family)\n    print('Fitness: %0.10f' % fitness)\n    print('Score: %0.10f' % score)\n    print('Normalized score: %0.10f' % score_n)\n    print('Coverage: %d, %d' % (coverage, coverage2))\n    print('Normalized coverage: %0.10f' % coverage_n)\n    print('Length of all paths of family: %d' % path_family_length)\n    return path_family\n\n\nfigsize=(5,4)\nSP = SP_all[0]\nseg = seg_max_sp(SP)\nplot_sp_ssm(SP=SP, seg=seg, S=S, ann=ann_frames, color_ann=color_ann, \n            title='Scape plot: Fitness', figsize=figsize)\nplt.show()\npath_family = check_segment(seg, S)\n\n\n\n\n\n\n\nSegment (alpha): [175, 197]\nLength of segment: 23\nLength of feature sequence: 205\nInduced segment path family:\n [[ 41  67]\n [ 68  90]\n [150 175]\n [176 197]]\nFitness: 0.4286698291\nScore: 68.0249475309\nNormalized score: 0.5175281325\nCoverage: 98, 98\nNormalized coverage: 0.3658536585\nLength of all paths of family: 87\n\n\n\n결과의 적합도 scape plot는 계층적 방식으로 음악적 구조를 반영한다.\n썸네일 세그먼트는 \\(\\alpha^\\ast=[175:197]\\)이며 음악적으로 \\(B_4\\) 부분에 해당한다.\nscape plot의 좌표는 중심 \\(c(\\alpha)=186\\) 및 길이 \\(|\\alpha|=23\\)로 지정된다.\n유도된 세그먼트 군은 4개의 \\(B\\) 부분으로 구성된다. 네 개의 \\(B\\) 부분 세그먼트는 모두 거의 동일한 적합성을 가지며 거의 동일한 세그먼트 군으로 이어진다.\n적합도 측정은 짧은 세그먼트를 조금 더 선호한다. 따라서 이 녹음에서 \\(B_4\\)-파트가 \\(B_1\\)-파트보다 빠르게 재생되기 때문에 적합도 측정은 \\(B_1\\)-파트 세그먼트보다 \\(B_4\\)-파트 세그먼트를 선호한다. 즉, 이 절차는 가장 짧은 가장 대표적인 세그먼트를 썸네일로 선택하게 된다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#정규화된-점수와-커버리지",
    "href": "posts/5. Music Structure Analysis/5.3.Audio_Thumbnail.html#정규화된-점수와-커버리지",
    "title": "5.3. 오디오 썸네일",
    "section": "정규화된 점수와 커버리지",
    "text": "정규화된 점수와 커버리지\n\n다음으로 적합도 측정의 정의에서 점수(score)와 커버리지(coverage)의 정규화(normalization)와 두 측정의 조합(조화 평균)이 매우 중요함을 설명한다.\n이를 위해 다양한 척도(및 척도-최대화(measure-maximizing) 세그먼트)의 scape plot를 개별적으로 살펴보자.\n점수 척도 \\(\\sigma\\)를 시작한다. 점수 극대화 구간은 전체 녹음인 \\(\\alpha=[1:N]\\) 이다.\n\n\nSP = SP_all[1]\nseg = seg_max_sp(SP)\nplot_sp_ssm(SP=SP, seg=seg, S=S, ann=ann_frames, color_ann=color_ann, \n            title='Scape plot: Score', figsize=figsize)\npath_family = check_segment(seg, S)\n\nSegment (alpha): [0, 204]\nLength of segment: 205\nLength of feature sequence: 205\nInduced segment path family:\n [[  0 204]]\nFitness: 0.0000000000\nScore: 205.0000000000\nNormalized score: 0.0000000000\nCoverage: 205, 205\nNormalized coverage: 0.0000000000\nLength of all paths of family: 205\n\n\n\n\n\n\n\n\n\n사소한(trivial) 자기-설명을 빼고 최적 경로군의 길이에 대해 정규화하면 정규화 점수 \\(\\bar{\\sigma}\\)가 된다.\n이 척도는 오디오 자료가 실제로 얼마나 커버되는지를 나타내지 않고 경로군의 평균 점수를 나타내므로, 많은 작은 세그먼트가 상대적으로 높은 점수를 얻는다. 이러한 측정을 사용하면 일반적으로 짧은 길이의 false-positive 세그먼트가 생성된다.\n이는 다음과 같은 scape plot와 \\(\\bar{\\sigma}\\)-최대화 경로 군으로도 증명된다.\n\n\nSP = SP_all[2]\nseg = seg_max_sp(SP)\nplot_sp_ssm(SP=SP, seg=seg, S=S, ann=ann_frames, color_ann=color_ann, \n            title='Scape plot: Normalized score', figsize=figsize)\npath_family = check_segment(seg, S)\n\nSegment (alpha): [183, 188]\nLength of segment: 6\nLength of feature sequence: 205\nInduced segment path family:\n [[ 54  61]\n [ 76  81]\n [163 168]\n [183 188]]\nFitness: 0.1680842515\nScore: 20.5560875887\nNormalized score: 0.6065036495\nCoverage: 26, 26\nNormalized coverage: 0.0975609756\nLength of all paths of family: 24\n\n\n\n\n\n\n\n\n\n다음 그림은 커버리지 측정 \\(\\gamma\\)에 대한 scape plot를 보여준다. 악보의 경우 커버리지-최대화 세그먼트는 \\(\\alpha=[1:N]\\)로 전체 녹음이다.\n\n\nSP = SP_all[3]\nseg = seg_max_sp(SP)\nplot_sp_ssm(SP=SP, seg=seg, S=S, ann=ann_frames, color_ann=color_ann, \n            title='Scape plot: Coverage', figsize=figsize)\npath_family = check_segment(seg, S)\n\nSegment (alpha): [91, 204]\nLength of segment: 114\nLength of feature sequence: 205\nInduced segment path family:\n [[  0  92]\n [ 93 204]]\nFitness: 0.1205736958\nScore: 128.2312820570\nNormalized score: 0.0697611866\nCoverage: 205, 205\nNormalized coverage: 0.4439024390\nLength of all paths of family: 204\n\n\n\n\n\n\n\n\n\ntrivial한 자기-설명을 빼고 길이 \\(N\\)에 대해 정규화하면 정규화 커버리지 \\(\\bar{\\gamma}\\)가 된다.\n\\(\\bar{\\gamma}\\)-최대화 세그먼트와 함께 다음 scape plot에서 볼 수 있듯이, 커버리지(coverage)는 점수(score)와는 개념적으로 다른 속성을 측정한다. 정규화된 점수와 달리, 정규화된 커버리지는 일반적으로 입력 시퀀스의 많은 부분을 차지하는 세그먼트군을 유도하는 세그먼트를 선호한다.\n\n\nSP = SP_all[4]\nseg = seg_max_sp(SP)\nplot_sp_ssm(SP=SP, seg=seg, S=S, ann=ann_frames, color_ann=color_ann, \n            title='Scape plot: Normalized coverage', figsize=figsize)\npath_family = check_segment(seg, S)\n\nSegment (alpha): [56, 76]\nLength of segment: 21\nLength of feature sequence: 205\nInduced segment path family:\n [[ 32  55]\n [ 56  76]\n [ 77  94]\n [140 163]\n [164 184]\n [185 204]]\nFitness: 0.3370126389\nScore: 50.3634037795\nNormalized score: 0.2488424049\nCoverage: 128, 128\nNormalized coverage: 0.5219512195\nLength of all paths of family: 118"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html",
    "title": "5.4. 노벨티 기반 분할",
    "section": "",
    "text": "음악 구조 섹션 사이의 전환을 표시할 수 있는 노벨티(novelty) 기반 분할을 소개하며, 노벨티를 감지하는 방법을 알아본다.\n주요 참고자료: Jonathan Foote: Automatic audio segmentation using a measure of audio novelty. Proceedings of the IEEE International Conference on Multimedia and Expo (ICME), New York, NY, USA, 2000, pp. 452–455."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#체커보드-커널-checkerboard-kernel-박스box",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#체커보드-커널-checkerboard-kernel-박스box",
    "title": "5.4. 노벨티 기반 분할",
    "section": "체커보드 커널 (Checkerboard Kernel): 박스(Box)",
    "text": "체커보드 커널 (Checkerboard Kernel): 박스(Box)\n\n\\(X=(x_1,x_2,\\ldots x_N)\\)을 특징 시퀀스라고 하고 \\(\\mathbf{S}\\)를 \\(X\\)에서 파생된 \\(N\\times N\\) 크기의 SSM이라고 하자.\n먼저 두 개의 동질적이지만 대조되는 섹션으로 구성된 오디오 녹음을 고려해 보기로 한다. 시각화하면 SSM 결과는 \\(2\\times 2\\) 체커보드처럼 보인다.\n\n\nipd.Image('../img/5.music_structure_analysis/FMP_C4_F23a-b.png', width= 500)\n\n\n\n\n\n주대각선에 있는 두 개의 어두운 블록은 두 섹션 내에서 유사도가 높은 영역에 해당한다. 대조적으로, 이 블록 외부의 밝은 영역은 섹션 간의 낮은 교차 유사성이 있음을 나타낸다. 따라서 두 부분의 경계를 찾기 위해서는 바둑판의 크럭스를 식별해야 한다.\n이는 \\(\\mathbf{S}\\)를 체커보드처럼 보이는 커널과 연관시켜 수행할 수 있다. 가장 간단한 커널은 다음과 같이 정의된 \\((2\\times 2)\\) 단위 커널이다.\n\n\\[\\mathbf{K} = \\left[\\begin{array}{rr} -1 & 1\\\\ 1 & -1 \\end{array}\\right]\n        = \\left[\\begin{array}{rr} 0 & \\,\\,\\,\\,\\,1\\\\ 1 & 0 \\end{array}\\right] -\n          \\left[\\begin{array}{rr} 1 & \\,\\,\\,\\,\\,0\\\\ 0 & 1 \\end{array}\\right]\\]\n\n이 커널은 “coherence” 커널과 “anti-coherence” 커널의 차이로 쓸 수 있다.\n첫 번째 커널은 중심점 양쪽의 자기 유사성(self-similarity)을 측정하며 두 영역이 각각 동질일 때 높을 것이다.\n두 번째 커널은 두 영역 간의 교차 유사성(cross-similarity)을 측정하며 중심점에서 차이가 거의 없을 때 높을 것이다.\n두 값의 차이는 중심점에서 특징 시퀀스의 새로움/노벨티(novelty) 을 추정한다. 두 영역이 자기-유사하지만 서로 다를 때 노벨티가 높다.\n일반적으로 더 큰 시간 척도의 변화에 관심이 있는 오디오 구조 분석에서는 더 큰 크기의 커널이 사용된다.\n물리적 시간 위치가 윈도우 또는 커널의 중심과 연관되는 중심 보기(centered view)를 채택하면, 어떤 \\(L\\in\\mathbb{N}\\)에 대해 \\(M=2L+1\\)로 주어진 커널의 크기가 홀수라고 가정한다.\n\\(M\\) 크기의 상자 모양(box-like)의 체커보드 커널은 \\([-L:L]\\times[-L:L]\\)로 인덱싱되는 \\((M\\times M)\\) 행렬 \\(\\mathbf{K}_\\mathrm{Box}\\)이다.\n행렬은 다음과 같이 정의된다. \\[\\mathbf{K}_\\mathrm{Box} = \\mathrm{sgn}(k)\\cdot \\mathrm{sgn}(\\ell),\\] \\(k,\\ell\\in[-L:L]\\), “\\(\\mathrm{sgn}\\)”는 부호 함수(\\(-1\\)는 음수, \\(0\\)는 0, \\(1\\)는 양수)\n예를 들어, \\(L=2\\)인 경우 다음과 같다. \\[\\mathbf{K}_\\mathrm{Box} = \\left[\\begin{array}{rrrrr}\n-1 & -1 & \\,\\,\\,\\,\\,0 & 1 & 1 \\\\\n-1 & -1 & 0 &1 & 1 \\\\\n0 &  0  & 0 & 0 &0 \\\\\n1 & 1  & 0 & -1 & -1 \\\\\n1 & 1  & 0 & -1 & -1       \n\\end{array}\\right]\\]\n커널 행렬의 대칭성을 확보하기 위해 이론적인 이유로 중간에 0 행과 0 열을 더 많이 도입한다.\n다음 코드 셀에서는 상자 모양의 바둑판 커널을 구현하고 시각화한다.\n\n\ndef compute_kernel_checkerboard_box(L):\n    \"\"\"Compute box-like checkerboard kernel [FMP, Section 4.4.1]\n\n    Args:\n        L (int): Parameter specifying the kernel size 2*L+1\n\n    Returns:\n        kernel (np.ndarray): Kernel matrix of size (2*L+1) x (2*L+1)\n    \"\"\"\n    axis = np.arange(-L, L+1)\n    kernel = np.outer(np.sign(axis), np.sign(axis))\n    return kernel\n\n\nL = 10\nkernel = compute_kernel_checkerboard_box(L)\nplt.figure(figsize=(4,3))\nplt.imshow(kernel, aspect='auto', origin='lower', \n           extent=[-L-0.5,L+0.5,-L-0.5,L+0.5], cmap='seismic')\nplt.colorbar()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#체커보드-커널-가우시안-gaussian",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#체커보드-커널-가우시안-gaussian",
    "title": "5.4. 노벨티 기반 분할",
    "section": "체커보드 커널: 가우시안 (Gaussian)",
    "text": "체커보드 커널: 가우시안 (Gaussian)\n\n가장자리 효과(edge effect)를 피하기 위해 가장자리에서 0을 향해 점점 가늘어지는 윈도우를 사용하여 바둑판 커널을 부드럽게 할 수 있다.\n이를 위해 다음과 같이 정의된 방사형(radially) 대칭 가우시안(Gaussian) 함수 \\(\\phi:\\mathbb{R}^2\\to \\mathbb{R}\\)를 사용할 수 있다. \\[\\phi(s,t) = \\mathrm{exp}(-\\varepsilon^2(s^2+t^2)),\\] 이때 \\(\\varepsilon>0\\) 매개변수를 사용하면 테이퍼링 정도를 조정할 수 있다.\n그런 다음 행렬 \\(\\mathbf{K}_\\mathrm{Gauss}\\)에 의해 주어진 가우시안 체커보드 커널은 pointwise 곱셈으로 얻는다. \\[\\mathbf{K}_\\mathrm{Gauss}(k,\\ell) = \\phi(k,\\ell) \\cdot \\mathbf{K}_\\mathrm{Box}(k,\\ell),\\] for \\(k,\\ell\\in[-L:L]\\)\n\n\nipd.Image('../img/5.music_structure_analysis/FMP_C4_F23c-d.png',width=500)\n\n\n\n\n\n실제 커널 크기와 테이퍼링의 영향을 보상하기 위해 커널을 정규화할 수 있다.\n커널을 커널 행렬의 절대값 합계로 나누어 할 수 있다. \\[\\mathbf{K}_\\mathrm{norm}(k,\\ell) = \\frac{\\mathbf{K}_\\mathrm{Gauss}(k,\\ell)}{\\sum_{k,\\ell\\in[-L:L]}|\\mathbf{K}_\\mathrm{Gauss}(k,\\ell)|}\\]\n서로 다른 크기의 커널에서 얻은 노벨티 정보를 결합하고 융합할 때 정규화가 중요해진다.\n다음 구현에서 테이퍼 매개변수 \\(\\varepsilon\\)은 분산 \\(\\sigma\\)(커널 크기 \\(M=2L+1\\)에 대해 정규화됨)로 지정된다.\n\n\ndef compute_kernel_checkerboard_gaussian(L, var=1, normalize=True):\n    \"\"\"Compute Guassian-like checkerboard kernel [FMP, Section 4.4.1].\n    See also: https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/\n\n    Args:\n        L (int): Parameter specifying the kernel size M=2*L+1\n        var (float): Variance parameter determing the tapering (epsilon) (Default value = 1.0)\n        normalize (bool): Normalize kernel (Default value = True)\n\n    Returns:\n        kernel (np.ndarray): Kernel matrix of size M x M\n    \"\"\"\n    taper = np.sqrt(1/2) / (L * var)\n    axis = np.arange(-L, L+1)\n    gaussian1D = np.exp(-taper**2 * (axis**2))\n    gaussian2D = np.outer(gaussian1D, gaussian1D)\n    kernel_box = np.outer(np.sign(axis), np.sign(axis))\n    kernel = kernel_box * gaussian2D\n    if normalize:\n        kernel = kernel / np.sum(np.abs(kernel))\n    return kernel\n\n\nL = 10\nvar = 0.5\nkernel = compute_kernel_checkerboard_gaussian(L, var)\nplt.figure(figsize=(4,3))\nplt.imshow(kernel, aspect='auto', origin='lower', \n           extent=[-L-0.5,L+0.5,-L-0.5,L+0.5], cmap='seismic')\nplt.colorbar()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#노벨티-함수",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#노벨티-함수",
    "title": "5.4. 노벨티 기반 분할",
    "section": "노벨티 함수",
    "text": "노벨티 함수\n\n인접한 블록 사이의 2D 코너 포인트를 감지하기 위해 SSM을 바둑판 커널과 로컬에서 비교할 수 있다.\n이를 위해 SSM의 주 대각선을 따라 적절한 체커보드 커널 \\(\\mathbf{K}\\)를 슬라이드하고 \\(\\mathbf{K}\\)와 \\(\\mathbf{S}\\)의 요소별 곱을 합산한다. \\[\\Delta_\\mathrm{Kernel}(n) := \\sum_{k,\\ell\\in[-L:L]} \\mathbf{K}(k,\\ell) \\mathbf{S}(n+k,n+\\ell)\\] for \\(n\\in[L+1:N-L]\\)\n제로-패딩으로 경계에서 행렬 \\(\\mathbf{S}\\)을 확장하면(즉, \\((k,\\ell)\\in\\mathbb{Z}\\times\\mathbb{Z} \\setminus [1:N]\\times[1:N]\\)에 대해 \\(\\mathbf{S}(k,\\ell)=0\\) 설정), \\(n\\in[1:N]\\)로 가정할 수 있다.\n이는 \\(\\Delta_\\mathrm{Kernel}:[1:N]\\to\\mathbb{R}\\) 함수를 정의하며, 노벨티 함수라고도 하며, 이는 특징 시퀀스의 각 인덱스 \\(n\\in[1:N]\\)에 대해 새로움의 척도 \\(\\Delta_\\mathrm{Kernel}(n)\\)를 구체화한다.\n\\(\\mathbf{K}\\) 커널이 \\(\\mathbf{S}\\)의 비교적 균일한(uniform) 영역에 위치할 때 그 곱의 양수 값과 음수 값의 합은 0이 되고 \\(\\Delta_\\mathrm{Kernel}( n)\\)이 작아진다.\n반대로 \\(\\mathbf{K}\\) 커널이 \\(\\mathbf{S}\\)의 체커보드와 같은 구조의 꼭지점에 정확히 위치하면 곱의 값은 모두 양수이고 합계는 큰 \\(\\Delta_\\mathrm{Kernel}( n)\\)이 된다.\n\n\nipd.Image(\"../img/5.music_structure_analysis/FMP_C4_F24_color.png\", width=400)\n\n\n\n\n\ndef compute_novelty_ssm(S, kernel=None, L=10, var=0.5, exclude=False):\n    \"\"\"Compute novelty function from SSM [FMP, Section 4.4.1]\n\n    Args:\n        S (np.ndarray): SSM\n        kernel (np.ndarray): Checkerboard kernel (if kernel==None, it will be computed) (Default value = None)\n        L (int): Parameter specifying the kernel size M=2*L+1 (Default value = 10)\n        var (float): Variance parameter determing the tapering (epsilon) (Default value = 0.5)\n        exclude (bool): Sets the first L and last L values of novelty function to zero (Default value = False)\n\n    Returns:\n        nov (np.ndarray): Novelty function\n    \"\"\"\n    if kernel is None:\n        kernel = compute_kernel_checkerboard_gaussian(L=L, var=var)\n    N = S.shape[0]\n    M = 2*L + 1\n    nov = np.zeros(N)\n    # np.pad does not work with numba/jit\n    S_padded = np.pad(S, L, mode='constant')\n\n    for n in range(N):\n        # Does not work with numba/jit\n        nov[n] = np.sum(S_padded[n:n+M, n:n+M] * kernel)\n    if exclude:\n        right = np.min([L, N])\n        left = np.max([0, N-L])\n        nov[0:right] = 0\n        nov[left:N] = 0\n\n    return nov\n\n\nL_kernel = 20\nnov = compute_novelty_ssm(S, L=L_kernel, exclude=False)   \nfig, ax, line = plot_signal(nov, Fs = Fs_X, color='k')    \n\n\n\n\n\n위는 Brahms 예의 크로마 기반 SSM에 대해 노벨티 곡선을 계산했다.\n우선, 노벨티 함수는 시작과 끝에서 큰 값을 가진다는 것을 알 수 있다. SSM의 제로 패딩으로 인한 이 아티팩트(부산물)는 첫 번째와 마지막 \\(L\\) 프레임에 대해 노벨티 곡선 \\(\\Delta_\\mathrm{Kernel}\\)을 0으로 설정하여 억제할 수 있다.\n다음 그림은 세그먼트 주석(annotation)으로 오버레이된 결과 노벨티 곡선을 보여준다. 그림에서 알 수 있듯이, 노벨티 함수의 local maxima는 하모니의 변화를 잘 나타내고 있으며, 특히 서로 다른 음악적 부분에 해당하는 세그먼트 간의 경계에서 발생한다.\n\n\nL_kernel = 20\nnov = compute_novelty_ssm(S, L=L_kernel, exclude=True)        \nfig, ax, line = plot_signal(nov, Fs = Fs_X, color='k') \nplot_segments_overlay(ann, ax=ax, colors=color_ann, alpha=0.1, edgecolor='k', print_labels=False)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#커널-크기-kernel-size",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#커널-크기-kernel-size",
    "title": "5.4. 노벨티 기반 분할",
    "section": "커널 크기 (Kernel Size)",
    "text": "커널 크기 (Kernel Size)\n\n시작하는 SSM의 quality 외에도 커널의 크기는 노벨티 함수의 속성에 중요한 영향을 미친다.\n작은 커널은 짧은 시간 척도에서 새로움을 감지하는 데 적합할 수 있는 반면, 큰 커널은 거친 구조 섹션 간의 경계 및 전환을 감지하는 데 적합하다.\n주어진 커널의 적합성은 각 응용과 기본이 되는 SSM의 속성에 따라 크게 달라진다.\n다음 예는 다른 특징 표현(다른 특징 속도 포함)을 기반으로 다른 크기와 SSM을 사용하는 노벨티 함수를 보여준다. 작은 커널 크기를 사용하면 거짓 피크와 함께 노이즈가 많은 노벨티 함수가 발생할 수 있다. 이것은 기본 SSM이 블록뿐만 아니라 경로와 같은 구조를 포함할 때 특히 그렇다.\n더 큰 커널을 사용하면 로컬 변동을 평균화하고 더 부드러운 노벨티 함수가 생성된다.\nSSM 스무딩(smoothing)으로 유사한 효과를 얻을 수 있으며, 이는 종종 블록 구조의 향상과 경로 구조의 감쇠로 이어진다.\nSSM 속성과 커널 크기 사이의 상호 작용은 다음 그림에 설명되어 있다.\n\n\n\nS_dict = {}\nFs_dict = {}\nx, x_duration, X, Fs_X, S, I = compute_sm_from_filename(fn_wav, \n                                                L=11, H=5, L_smooth=1, thresh=1)\nS_dict[0], Fs_dict[0] = S, Fs_X\nann_frames = convert_structure_annotation(ann, Fs=Fs_X) \nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X,\n            label='Time (frames)', color_ann=color_ann, clim_X=[0,1], clim=[0,1], \n            title='Feature rate: %0.0f Hz'%(Fs_X), figsize=(4.5, 5.5))\n\n\nx, x_duration, X, Fs_X, S, I = compute_sm_from_filename(fn_wav, \n                                                L=41, H=10, L_smooth=1, thresh=1)\nS_dict[1], Fs_dict[1] = S, Fs_X\nann_frames = convert_structure_annotation(ann, Fs=Fs_X) \nfig, ax = plot_feature_ssm(X, 1, S, 1, ann_frames, x_duration*Fs_X,\n            label='Time (frames)', color_ann=color_ann, clim_X=[0,1], clim=[0,1], \n            title='Feature rate: %0.0f Hz'%(Fs_X), figsize=(4.5, 5.5))\n\n\n\n\n\n\n\n\nfigsize=(8,6)\nL_kernel_set = [5, 10, 20, 40]\nnum_kernel = len(L_kernel_set)\nnum_SSM = len(S_dict)\n\nfig, ax = plt.subplots(num_kernel, num_SSM, figsize=figsize)\nfor s in range(num_SSM):\n    for t in range(num_kernel):\n        L_kernel = L_kernel_set[t]\n        S = S_dict[s]\n        nov = compute_novelty_ssm(S, L=L_kernel, exclude=True)        \n        fig_nov, ax_nov, line_nov = plot_signal(nov, Fs = Fs_dict[s], \n                color='k', ax=ax[t,s], figsize=figsize, \n                title='Feature rate = %0.0f Hz, $L_\\mathrm{kernel}$ = %d'%(Fs_dict[s],L_kernel)) \n        plot_segments_overlay(ann, ax=ax_nov, colors=color_ann, alpha=0.1, \n                                       edgecolor='k', print_labels=False)\nplt.tight_layout()\nplt.show()  \n\n\n\n\n\n종종 해당 값이 노벨티 함수의 로컬 평균을 초과할 때만 피크가 선택되는 적응적(adaptive) 임계값 전략이 적용되기도 한다.\n거짓 피크의 수를 더 줄이기 위한 또 다른 전략으로 두 개의 후속 피크 위치 사이의 최소 거리에 제약을 가하는 방법도 있다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#시차time-lag-표현",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#시차time-lag-표현",
    "title": "5.4. 노벨티 기반 분할",
    "section": "시차(Time-lag) 표현",
    "text": "시차(Time-lag) 표현\n\n구조 특징을 정의하기 위한 주요 기술 요소인 시차 행렬(time-lag matrix) 의 개념을 소개한다.\n\\(\\mathbf{S}\\)를 특징 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)에서 파생된 자기 유사성 행렬(SSM)이라고 하자.\n\\(\\alpha_1=[s_1:t_1]\\) 및 \\(\\alpha_2=[s_2:t_2]\\)와 같은 두 개의 반복 세그먼트는 \\((s_1,s_2)\\)에서 시작하여 \\((t_1,t_2)\\)에서 끝나는 \\(\\mathbf{S}\\)의 높은 유사성 경로이다. 또한 두 세그먼트 사이에 상대적인 템포 차이가 없으면 경로는 기본 대각선과 정확히 평행하다.\n\\(\\ell=s_2-s_1\\) 프레임에 해당하는 시차가 지나면 \\(\\alpha_1\\) 세그먼트가 반복된다는 것으로 이 속성을 표현할 수도 있다. 이것은 하나의 시간(time) 축이 지연(lag) 축으로 대체되는 SSM의 시차 표현이라는 개념으로 이어진다.\n표기를 단순화하기 위해 다음에서 프레임이 인덱스 \\(n=0\\)부터 시작하여 인덱싱된다고 가정한다. 따라서 \\(X=(x_0,x_1,\\ldots,x_{N-1})\\)와 자기 유사성 행렬 \\(\\mathbf{S}\\)는 \\([0:N-1]\\times[0 :N-1]\\)으로 인덱싱된다.\n\\(\\mathbf{S}\\)의 시차 표현은 다음과 같이 정의된다. \\[\\mathrm{L}(\\ell,n)=\\mathbf{S}(n+\\ell,n)\\] for \\(n\\in[0:N-1]\\) and \\(\\ell\\in[-n:N-1-n]\\)\n시차 매개변수 \\(\\ell\\)의 범위는 시간 매개변수 \\(n\\)에 따라 달라진다. \\(n+\\ell\\) 합계가 \\([0:N-1]\\) 범위에 있도록 시차 지수를 선택해야 한다. 예를 들어 시간 인덱스 \\(n=0\\)의 경우 \\(\\ell\\in[0:N-1]\\)로만 미래를 볼 수 있지만 시간 인덱스 \\(n=N-1\\)의 경우 \\(\\ell\\in[-N+1:0]\\)로 과거만을 볼 수 있다.\n표기법을 단순화하기 위해 정의를 통해 순환(circular) 시차 표현 \\(\\mathrm{L}^{\\!\\circ}\\)을 도입한다. \\[\\mathrm{L}^{\\!\\circ}(\\ell,n)=\\mathbf{S}\\big( (n+\\ell) \\mod N, n \\big)\\] for \\(n\\in[0:N-1]\\) and \\(\\ell\\in[0:N-1]\\)\n\n\n구현과 예제\n\n이제 음악 형식 \\(A_1B_1A_2B_2B_3A_3\\)에 해당하는 종합적으로 생성된 SSM을 고려하여 정의를 설명해보자\n다음 그림은 입력 SSM \\(\\mathbf{S}\\), 시차 표현 결과 \\(\\mathrm{L}\\) 및 순환 버전 \\(\\mathrm{L}^{\\!\\circ}\\)를 보여준다.\n\\(\\mathrm{L}\\) 표현은 원본 행렬 \\(\\mathbf{S}\\)를 가로축에 평행하게 잘라 얻는다.\n결과적으로 \\(\\mathbf{S}\\)에서 주대각선과 평행한 선은 \\(\\mathrm{L}\\)에서 수평선이 된다. 즉, 대각선 구조가 수평 구조로 변형된다.\n순환 버전에서 \\(\\mathrm{L}\\)에서 사용되는 음의 시차 매개변수 \\(\\ell\\in[-n:-1]\\)는 \\(\\mathrm{L}^{\\!\\circ}\\)에서 \\(\\ell+N\\)로 식별된다.\n이렇게 하면 시차 표현 \\(\\mathrm{L}^{\\!\\circ}\\)는 다시 \\([0:N-1]\\times[0:N-1]\\)로 인덱싱된 행렬이 되며, 행렬 \\(\\mathbf{S}\\)에 대해서도 그렇다.\n\n\nipd.Image('../img/5.music_structure_analysis/FMP_C4_F26.png', width=600)\n\n\n\n\n\ndef compute_time_lag_representation(S, circular=True):\n    \"\"\"Computation of (circular) time-lag representation\n\n    Args:\n        S (np.ndarray): Self-similarity matrix\n        circular (bool): Computes circular version (Default value = True)\n\n    Returns:\n        L (np.ndarray): (Circular) time-lag representation of S\n    \"\"\"\n    N = S.shape[0]\n    if circular:\n        L = np.zeros((N, N))\n        for n in range(N):\n            L[:, n] = np.roll(S[:, n], -n)\n    else:\n        L = np.zeros((2*N-1, N))\n        for n in range(N):\n            L[((N-1)-n):((2*N)-1-n), n] = S[:, n]\n    return L\n\n\nann = [[0, 9, 'A'], [10, 19, 'B'], [20, 29, 'A'], [30, 39, 'B'], [40, 49, 'B'], [50, 59, 'A']]\nS = generate_ssm_from_annotation(ann, score_path=1, score_block=0.3)\nN = S.shape[0]\nL = compute_time_lag_representation(S, circular=False)\nL_circ = compute_time_lag_representation(S, circular=True)\n\nplt.figure(figsize=(5, 5))\n\nax1 = plt.axes([0, 0.6, 0.4, 0.4]) \nax2 = plt.axes([0, 0, 0.4, 0.4])\nax3 = plt.axes([0.6, 0.0, 0.4, 0.8]) \n\nfig, ax, im = plot_matrix(S, ax=[ax1], title='SSM', \n                                   xlabel='Time (frames)', ylabel='Time (frames)', colorbar=None)\n\nfig, ax, im = plot_matrix(L_circ, ax=[ax2], title='Circular time-lag matrix', \n                                   xlabel='Time (frames)', ylabel='Lag (frames)', colorbar=None)\n\nfig, ax, im = plot_matrix(L, ax=[ax3], extent=[0, N-1, -(N-1), N-1], title='Time-lag matrix', \n                                   xlabel='Time (frames)', ylabel='Lag (frames)', colorbar=None)\n\n\n\n\n\n\\(A\\) 파트가 \\(B_1\\) 파트 세그먼트의 길이와 같고 \\(B_2\\) 파트 세그먼트의 길이가 두 배(\\(B_1\\)의 템포의 절반으로 연주됨)인 \\(AB_1B_2\\) 구조의 음악 작품을 보자.\n주 대각선과 평행하지 않은 \\(\\mathbf{S}\\)에서의 경로 구조는 결과 원형 시차 표현 \\(\\mathrm{L}^{\\!\\circ}\\)에서 해석하기 어려워진다.\n\n\nann = [[0, 9, 'A'], [10, 19, 'B'], [20, 39, 'B']]\nS = generate_ssm_from_annotation(ann, score_path=1, score_block=0.3)\nN = S.shape[0]\nL = compute_time_lag_representation(S, circular=False)\nL_circ = compute_time_lag_representation(S, circular=True)\n\nplt.figure(figsize=(5, 2.5))\nax = plt.subplot(121)\nfig, ax, im = plot_matrix(S, ax=[ax], title='SSM', \n                                   xlabel='Time (frames)', ylabel='Time (frames)', colorbar=None)\nax = plt.subplot(122)\nfig, ax, im = plot_matrix(L_circ, ax=[ax], title='Circular time-lag matrix', \n                                   xlabel='Time (frames)', ylabel='Lag (frames)', colorbar=None)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#구조-특징과-노벨티-함수",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#구조-특징과-노벨티-함수",
    "title": "5.4. 노벨티 기반 분할",
    "section": "구조 특징과 노벨티 함수",
    "text": "구조 특징과 노벨티 함수\n\n다음에서 \\(\\mathbf{S}^{[n]}\\)는 주어진 시간 프레임 \\(n\\in[0:N-1]\\)에 대해 \\(\\mathbf{S}\\)의 \\(n^{\\mathrm{th}}\\) 열을 나타낸다.\n\\(\\mathbf{S}^{[n]}\\in\\mathbb{R}^N\\) 벡터는 \\(n\\) 시간 프레임에 존재하는 관계의 종류를 나타낸다. \\(\\mathbf{S}^{[n]}(m)\\) (for \\(m\\in[0:N-1]\\))가 큰 경우, \\(n\\) 시간 프레임은 \\(m\\) 시간 프레임과 관련된다. 값이 작은 경우, 두 프레임은 관련이 없다.\n즉, \\(\\mathbf{S}^{[n]}\\)는 프레임 \\(n\\)의 전역적인 구조 관계를 드러낸다.\n시차 행렬 \\({\\mathrm{L}^{\\!\\circ}}^{[n]}\\)의 \\(n^{\\mathrm{th}}\\) 열에 대해서도 동일한 해석이 적용된다.\n그러나 \\(\\mathbf{S}\\)와 \\(\\mathrm{L}^{\\!\\circ}\\) 사이에는 결정적인 차이가 있다. 연속되는 두 프레임 \\(n\\)과 \\(n+1\\)이 동일한 구조적 속성을 갖는 경우, 두 벡터 \\(\\mathbf{S}^{[n]}\\)와 \\(\\mathbf{S}^{[n+ 1]}\\)는 서로 순환 이동(cyclically shifted) 버전인 반면, 두 벡터 \\({\\mathrm{L}^{\\!\\circ}}^{[n]}\\) 및 \\({\\mathrm{L} ^{\\!\\circ}}^{[n+1]}\\)는 동일(identical) 하다.\n이 관찰을 기반으로 구조 특징(structure feature) 을 \\(y_n:={\\mathrm{L}^{\\!\\circ}}^{[n]}\\in\\mathbb{R}^N\\) for \\(\\mathrm{L}^{\\!\\circ}\\), \\(n\\in[0:N-1]\\) 열로 정의한다.\n이 과정을 통해 지역적(음향, 음악적) 특성을 포착한 \\(x_n\\)의 원래 시퀀스 \\(X=(x_0,x_1,\\ldots,x_{N-1})\\)를 전역(구조) 특성을 캡처하는 피처 \\(y_n\\)의 시퀀스 \\(Y=(y_0,y_1,\\ldots,y_{N-1})\\)로 변환했다. 결과적으로 전역 구조 부분의 경계는 특징 시퀀스 \\(Y\\)의 지역적 변화를 찾아 식별할 수 있다.\n이러한 지역적 변화를 포착하는 방법에는 여러 가지가 있다. 간단한 전략은 적절한 거리 함수를 기반으로 연속 구조 특징 간의 차이를 계산하는 것이다. 예를 들어 \\(\\mathbb{R}^N\\)의 유클리드 norm을 사용하면 노벨티 함수를 얻는다. \\[\\Delta_\\mathrm{Structure}(n) := \\| y_{n+1}-y_n \\|= \\|{\\mathrm{L}^{\\!\\circ}}^{[n+1]}-{\\mathrm{L}^{\\!\\circ}}^{[n]}\\|\\] for \\(n\\in[0:N-2]\\).\n다시 제로 패딩으로 \\(n\\in[0:N-1]\\)를 가정할 수 있다. 이 함수의 로컬 최대값 또는 피크의 위치는 구조적 경계에 대한 후보를 선정한다.\n전체 절차는 원래 시퀀스 \\(X\\)에 사용된 특징 유형 또는 \\(\\mathbf{S}\\)가 계산되는 방식을 포함하여 많은 설계 선택 및 매개변수 설정에 따라 달라진다.\n또한 실제로는 더 많은 관련 도함수 연산자를 사용하고 적절한 전처리 단계(예: 행렬 \\(\\mathrm{L}^{\\!\\circ}\\) 추가 향상) 및 후처리 단계(예: novelty 함수 \\(\\Delta_\\mathrm{Structure}\\) 정규화)를 적용하는 경우가 많다.\n마지막으로 피크 선택 전략은 최종 결과에 결정적인 영향을 미칠 수 있다.\n\n\n구현과 예제\n\n다음 코드 셀에서는 구조 특징을 기반으로 novelty 함수를 계산한다. 위에서 소개한 합성 예제 \\(A_1B_1A_2B_2B_3A_3\\)에 대한 결과를 표시한다.\n이 예에서 생성되는 novelty 함수의 피크 위치는 경로 구성 요소의 (조인트) 시작 및 끝 위치와 일치하며, 이는 차례로 음악 섹션의 경계와 일치한다.\n\n\ndef novelty_structure_feature(L, padding=True):\n    \"\"\"Computation of the novelty function from a circular time-lag representation\n\n    Args:\n        L (np.ndarray): Circular time-lag representation\n        padding (bool): Padding the result with the value zero (Default value = True)\n\n    Returns:\n        nov (np.ndarray): Novelty function\n    \"\"\"\n    N = L.shape[0]\n    if padding:\n        nov = np.zeros(N)\n    else:\n        nov = np.zeros(N-1)\n    for n in range(N-1):\n        nov[n] = np.linalg.norm(L[:, n+1] - L[:, n])\n    return nov\n\n\ndef plot_ssm_structure_feature_nov(S, L, nov, Fs=1, figsize=(10, 3), ann=[], color_ann=[]):\n    \"\"\"Plotting an SSM, structure features, and a novelty function\n\n    Args:\n        S: SSM\n        L: Circular time-lag representation\n        nov: Novelty function\n        Fs: Feature rate (indicated in title of SSM) (Default value = 1)\n        figsize: Figure size (Default value = (10, 3))\n        ann: Annotations (Default value = [])\n        color_ann: Colors used for annotations (see :func:`libfmp.b.b_plot.plot_segments`) (Default value = [])\n\n    Returns:\n        ax1: First subplot\n        ax2: Second subplot\n        ax3: Third subplot\n    \"\"\"\n    plt.figure(figsize=figsize)\n    ax1 = plt.subplot(131)\n    if Fs == 1:\n        title = 'SSM'\n    else:\n        title = 'SSM (Fs = %d)' % Fs\n    fig_im, ax_im, im = plot_matrix(S, ax=[ax1], title=title,\n                                             xlabel='Time (frames)', ylabel='Time (frames)')\n    if ann:\n        plot_segments_overlay(ann, ax=ax_im[0], edgecolor='k',\n                                       print_labels=False, colors=color_ann, alpha=0.05)\n\n    ax2 = plt.subplot(132)\n    fig_im, ax_im, im = plot_matrix(L, ax=[ax2], title='Structure features',\n                                             xlabel='Time (frames)', ylabel='Lag (frames)', colorbar=True)\n    if ann:\n        plot_segments_overlay(ann, ax=ax_im[0], edgecolor='k', ylim=False,\n                                       print_labels=False, colors=color_ann, alpha=0.05)\n\n    ax3 = plt.subplot(133)\n    fig, ax, im = plot_signal(nov, ax=ax3, title='Novelty function',\n                                       xlabel='Time (frames)', color='k')\n    if ann:\n        plot_segments_overlay(ann, ax=ax, edgecolor='k', colors=color_ann, alpha=0.05)\n    plt.tight_layout()\n    return ax1, ax2, ax3\n\n\ncolor_ann = {'A': [1, 0, 0, 0.2], 'B': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0.2]}\nann = [[0, 9, 'A'], [10, 19, 'B'], [20, 29, 'A'], [30, 39, 'B'], [40, 49, 'B'], [50, 59, 'A']]\nS = generate_ssm_from_annotation(ann, score_path=1, score_block=0)\nL = compute_time_lag_representation(S, circular=True)\nnov = novelty_structure_feature(L)\nax = plot_ssm_structure_feature_nov(S, L, nov, ann=ann, color_ann=color_ann)\n\n\n\n\n\n구조 특징은 두 가지 이유로 이 예에서 특히 잘 작동한다.\n\n첫째, 반복되는 부분이 많아 경로 구조가 풍부하다.\n둘째, 두 개의 반복되는 음악 부분이 서로 다른 순으로 발생하여, 구조 특징에 잘 포착되는 특성적인 경로 단절이 발생한다.\n\n즉, \\(A_1A_2A_3A_4\\) 또는 \\(A_1B_1A_2B_2\\)의 음악적 구조를 가진 작품에서는 구조 기반 참신성 감지가 작동하지 않지만 \\(A_1B_1A_2A_3\\) 또는 \\(A_1A_2B_1B_2\\)의 음악적 구조를 가진 작품에서는 잘 작동한다.\n다음 그림으로 그 사실을 볼 수 있다.\n\n\ncolor_ann = {'A': [1, 0, 0, 0.2], 'B': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0.2]}\nfigsize = (8,2.5)\n\nann_set = [[[0, 9, 'A'], [10, 19, 'A'], [20, 29, 'A'], [30, 39, 'A']],\n       [[0, 9, 'A'], [10, 19, 'B'], [20, 29, 'A'], [30, 39, 'B']],\n       [[0, 9, 'A'], [10, 19, 'B'], [20, 29, 'A'], [30, 39, 'A']],\n       [[0, 9, 'A'], [10, 19, 'A'], [20, 29, 'B'], [30, 39, 'B']]]\n\nfor ann in ann_set:\n    S = generate_ssm_from_annotation(ann, score_path=1, score_block=0)\n    L = compute_time_lag_representation(S, circular=True)\n    nov = novelty_structure_feature(L)\n    ax = plot_ssm_structure_feature_nov(S, L, nov, figsize=figsize, ann=ann, color_ann=color_ann)"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#example-chopin",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#example-chopin",
    "title": "5.4. 노벨티 기반 분할",
    "section": "Example: Chopin",
    "text": "Example: Chopin\n\n구조-기반 노벨티 감지(novelty detection)의 전체적인 과정을 설명하기 위해 쇼팽의 Prelude Op.28, No. 11 (구조 \\(A_1A_2BA_3A_4CD\\))를 보자.\n다음 그림은 크로마-기반의 특징 표현으로부터 계산된 (경로-향상과 임계값-설정된) SSM이다.\n또한, 그림은 시차(time-lag) 행렬 \\(\\mathrm{L}^{\\!\\circ}\\)과 참신성 함수 \\(\\Delta_\\mathrm{Structure}\\)를 보여준다.\n녹음과 세그먼트가 짧기 때문에, 상대적으로 높은 피쳐 레이트를 사용한다.\n\n\nfn_wav = '../data_FMP/FMP_C4_Audio_Chopin_Op028-11_003_20100611-SMD.wav'\nfn_ann = '../data_FMP/FMP_C4_Audio_Chopin_Op028-11_003_20100611-SMD.csv'\nx, x_duration, X, Fs_feature, S, I = compute_sm_from_filename(fn_wav, L=9, H=2, L_smooth=11, thresh= 0.1)\n\nann_frames, color_ann = read_structure_annotation(fn_ann, Fs=Fs_feature, remove_digits=True, index=True)\ncolor_ann = {'A': [1, 0, 0, 0.2], 'B': [0, 1, 0, 0.2], 'C': [0, 0, 1, 0.2], '': [1, 1, 1, 0.2]}\n\nL = compute_time_lag_representation(S, circular=True)\nnov = novelty_structure_feature(L)\nax = plot_ssm_structure_feature_nov(S, L, nov, Fs=Fs_feature, ann=ann_frames, color_ann=color_ann)\n\n\n\n\n\n하지만 노벨티 곡선이 상당히 노이즈해 보인다. 세그먼트 경계 중 일부(예: \\(A_2\\)에서 \\(B\\) 또는 \\(A_4\\)에서 \\(C\\))만 피크 위치로 잘 표시된다.\n최종 결과에 상당한 영향을 미칠 수 있는 많은 매개 변수(예: 특징 표현 또는 시간적 해상도)가 있다.\n이 예에서 \\(\\mathrm{L}^{\\!\\circ}\\)로 정의된 구조 특징은 잡음이 많고, 최종 참신성 함수를 손상시키는 많은 작은 조각을 포함하고 있다.\n이러한 이상값을 제거하고 노이즈를 억제하는 좋은 방법은 가로 방향으로 중앙값 필터를 적용하는 것이다. 또한, 참신함 계산의 미분을 작은 편차에 덜 취약하게 만들기 위해 가우시안 커널과 함께 컨볼루션(convolution) 을 적용하여 시차 행렬을 더 매끄럽게 할 수 있다.\n이 두 가지 후처리 전략의 효과는 다음 그림에 설명되어 있다.\n\n\nprint('Application of median filter')\nL_filter = ndimage.median_filter(L, (3,11))\nnov = novelty_structure_feature(L_filter)\nax = plot_ssm_structure_feature_nov(S, L_filter, nov, ann=ann_frames, color_ann=color_ann)\nplt.show()\n\nprint('Application of convolution filter')\nL_filter = ndimage.gaussian_filter(L_filter, 4)\nnov = novelty_structure_feature(L_filter)\nax = plot_ssm_structure_feature_nov(S, L_filter, nov, ann=ann_frames, color_ann=color_ann)\n\nApplication of median filter\n\n\n\n\n\nApplication of convolution filter"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#example-brahms",
    "href": "posts/5. Music Structure Analysis/5.4.Novelty-Based_Segmentation.html#example-brahms",
    "title": "5.4. 노벨티 기반 분할",
    "section": "Example: Brahms",
    "text": "Example: Brahms\n\n다음으로 브람스의 헝가리 무곡 예(\\(A_1A_2B_1B_2CA_3B_3B_4D\\))도 적용해보자.\nB파트 세그먼트 사이의 상대적 템포 차이 때문에, 시차 표현을 쓰는 것에 문제가 생기는 것을 볼 수 있다.\n\n\nfn_wav = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.wav'\nfn_ann = '../data_FMP/FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv'\n\ntempo_rel_set = np.array([0.8, 1, 1.25])\nx, x_duration, X, Fs_feature, S, I = compute_sm_from_filename(fn_wav, L=21, H=5, \n                                    L_smooth=11, tempo_rel_set=tempo_rel_set, thresh= 0.1)\n\nann_frames, color_ann = read_structure_annotation(fn_ann, Fs=Fs_feature, remove_digits=True, index=True)\n\nL = compute_time_lag_representation(S, circular=True)\nnov = novelty_structure_feature(L)\nax = plot_ssm_structure_feature_nov(S, L, nov, Fs=Fs_feature, ann=ann_frames, color_ann=color_ann)\n\n\n\n\n\n노벨티 함수의 문제는 입력 SSM에서 이미 볼 수 있는 잡음이 많은 경로 관계 때문이며, 또한 상대적인 템포 차이로 인한 비대각선 경로 구조 때문이기도 하다.\n다시 말하지만, 노벨티 함수를 계산하기 전에 일부 후처리(중앙값 필터링, 가우시안 커널과의 컨볼루션)를 적용하면 일부 문제가 완화된다.\n\n\nL_filter = ndimage.median_filter(L, (3,21))\nL_filter = ndimage.gaussian_filter(L_filter, 6)\nnov = novelty_structure_feature(L_filter)\nax = plot_ssm_structure_feature_nov(S, L_filter, nov, Fs=Fs_feature, ann=ann_frames, color_ann=color_ann)\n\n\n\n\n\n입력된 SSM의 품질은 의미 있는 노벨티를 얻기 위해 매우 중요하다. 스무딩 기술(중앙값, 컨볼루션)을 적용하여 시차 행렬을 후처리하는 것은 전체 절차의 견고성을 높이는 중요한 단계이다.\n음악적으로 정보를 얻은 방식(예: 세그먼트의 예상 최소 길이, 음악 녹음 기간)으로 절차의 매개변수(예: 기능 해상도, SSM 매개변수, SSM 임계값, 필터 매개변수)를 조정하면 최종 결과를 크게 향상시킬 수 있다.\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S4_NoveltySegmentation.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C4/C4S4_StructureFeature.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.5.Evaluation.html",
    "href": "posts/5. Music Structure Analysis/5.5.Evaluation.html",
    "title": "5.5. 음악 처리의 평가 방법",
    "section": "",
    "text": "음악 구조에 대한 처리(예: 분할)가 얼마나 잘 되었는지를 평가하는 방법에 대해 알아본다."
  },
  {
    "objectID": "posts/5. Music Structure Analysis/5.5.Evaluation.html#쌍별pair-wise-평가-척도",
    "href": "posts/5. Music Structure Analysis/5.5.Evaluation.html#쌍별pair-wise-평가-척도",
    "title": "5.5. 음악 처리의 평가 방법",
    "section": "쌍별(pair-wise) 평가 척도",
    "text": "쌍별(pair-wise) 평가 척도\n\n이제 pair-wise precision, pair-wise recall 및 pair-wise F-measure라고 하는 몇 가지 프레임 기반 평가 측정값을 소개한다.\n\\(\\varphi^\\mathrm{Ref}\\) 및 \\(\\varphi^\\mathrm{Est}\\)를 각각 참조 및 추정 구조 주석에 대한 레이블 함수로 둔다.\n실제 레이블 네이밍에서 독립적이기 위해서는, 레이블을 직접 보지 않고 레이블 동시 발생을 찾는 것이 주요 아이디어다.\n이를 위해 동일한 레이블에 할당된 프레임 쌍(pair)을 고려한다.\n\n\\[\\mathcal{I}=\\{(n,m)\\in[1:N]\\times[1:N]\\mid m<n\\},\\]\n\n참조 및 추정 주석에 대해 양의 아이템을 다음과 같이 정의한다. \\[\\mathcal{I}^\\mathrm{Ref}_+=\\{(n,m)\\in\\mathcal{I}\\mid \\varphi^\\mathrm{Ref}(n)=\\varphi^\\mathrm{Ref}(m)\\},\\\\\n\\mathcal{I}^\\mathrm{Est}_+=\\{(n,m)\\in\\mathcal{I}\\mid \\varphi^\\mathrm{Est}(n)=\\varphi^\\mathrm{Est}(m)\\},\\] whereas \\(\\mathcal{I}^\\mathrm{Ref}_-=\\mathcal{I}\\setminus\\mathcal{I}^\\mathrm{Ref}_+\\) and \\(\\mathcal{I}^\\mathrm{Est}_-=\\mathcal{I}\\setminus\\mathcal{I}^\\mathrm{Est}_+\\)\n즉, \\((n,m)\\) 항목은 프레임 \\(n\\)과 \\(m\\)이 동일한 레이블을 갖는 경우 주석에 대해 양(positive) 으로 간주된다.\n이제 pairwise precision는 이 이진 분류 체계의 precision으로 정의된다. 유사하게 pairwise recall와 pairwise F-measure은 위의 체계의 recall 및 F-measure이다.\n다음 두 예에서 샘플링된 시간 간격은 \\(N=10\\) 샘플로 구성된다.\n\n\ndef convert_ann_to_seq_label(ann):\n    \"\"\"Convert structure annotation with integer time positions (given in indices)\n    into label sequence\n\n    Args:\n        ann (list): Annotation (list ``[[s, t, 'label'], ...]``, with ``s``, ``t`` being integers)\n\n    Returns:\n        X (list): Sequencs of labels\n    \"\"\"\n    X = []\n    for seg in ann:\n        K = seg[1] - seg[0]\n        for k in range(K):\n            X.append(seg[2])\n    return X\n\n    \ndef plot_seq_label(ax, X, Fs=1, color_label=[], direction='horizontal',\n                   fontsize=10, time_axis=False, print_labels=True):\n    \"\"\"Plot label sequence in the style of annotations\n\n    Args:\n        ax: Axis used for plotting\n        X: Label sequence\n        Fs: Sampling rate (Default value = 1)\n        color_label: List of colors for labels (Default value = [])\n        direction: Parameter used for :func:`libfmp.b.b_plot.plot_segments` (Default value = 'horizontal')\n        fontsize: Parameter used for :func:`libfmp.b.b_plot.plot_segments` (Default value = 10)\n        time_axis: Parameter used for :func:`libfmp.b.b_plot.plot_segments` (Default value = False)\n        print_labels: Parameter used for :func:`libfmp.b.b_plot.plot_segments` (Default value = True)\n\n    Returns:\n         ann_X: Structure annotation for label sequence\n    \"\"\"\n    ann_X = []\n    for m, cur_x in enumerate(X):\n        ann_X.append([(m-0.5)/Fs, (m+0.5)/Fs, cur_x])\n    plot_segments(ann_X, ax=ax, time_axis=time_axis, fontsize=fontsize,\n                           direction=direction, colors=color_label, print_labels=print_labels)\n    return ann_X\n\n\ncolor_label = {'A': [1, 0, 0, 0.2], 'B': [0, 0, 1, 0.2], 'C': [0, 1, 0, 0.2],\n             'X': [1, 0, 0, 0.2], 'Y': [0, 0, 1, 0.2], 'Z': [0, 1, 0, 0.2]}\n\nann_ref = [[0, 4, 'A'], [4, 7, 'B'], [7, 10, 'A']]\nann_est = [[0, 1, 'X'], [1, 3, 'Y'], [3, 7, 'Z'], [7, 9, 'Y'], [9, 10, 'X']]\n\nX_ref = convert_ann_to_seq_label(ann_ref)\nX_est = convert_ann_to_seq_label(ann_est)\n\nprint('Segment-based structure annotation:')\nplt.figure(figsize=(5,2))\nax = plt.subplot(211)\nplot_segments(ann_ref, ax=ax, colors=color_label); \n#ax.set_xticks([])\nax = plt.subplot(212)\nplot_segments(ann_est, ax=ax, colors=color_label); \n#ax.set_xticks([])\nplt.tight_layout()\nplt.show()\n\nprint('Frame-based label sequence:')\nplt.figure(figsize=(5,2))\nax = plt.subplot(211)\nplot_seq_label(ax, X_ref, color_label=color_label, time_axis=True);\nax = plt.subplot(212)\nplot_seq_label(ax, X_est, color_label=color_label, time_axis=True);\nplt.tight_layout()\n\nSegment-based structure annotation:\n\n\n\n\n\nFrame-based label sequence:\n\n\n\n\n\n\n첫 번째 시퀀스를 참조 \\(\\varphi^\\mathrm{Est}\\)로 사용하고, 두 번째 시퀀스를 추정 \\(\\varphi^\\mathrm{Est}\\)로 사용하여 이제 pairwise precision, recall, F-measure를 설명한다.\n특히 참조 및 추정 주석의 양의 항목(회색 항목으로 표시)을 표시한다. 또한 true positive (TP), false negative (FN), false positive (FP) 항목을 시각화한다.\n\n\ndef compare_pairwise(X):\n    \"\"\"Compute set of positive items from label sequence [FMP, Section 4.5.3]\n\n    Args:\n        X (list or np.ndarray): Label sequence\n\n    Returns:\n        I_pos (np.ndarray): Set of positive items\n    \"\"\"\n    N = len(X)\n    I_pos = np.zeros((N, N))\n    for n in range(1, N):\n        for m in range(n):\n            if X[n] is X[m]:\n                I_pos[n, m] = 1\n    return I_pos\n\n\ndef evaluate_pairwise(I_ref_pos, I_est_pos):\n    \"\"\"Compute pairwise evaluation measures [FMP, Section 4.5.3]\n\n    Args:\n        I_ref_pos (np.ndarray): Referenence set of positive items\n        I_est_pos (np.ndarray): Set of items being estimated as positive\n\n    Returns:\n        P (float): Precision\n        R (float): Recall\n        F (float): F-measure\n        num_TP (int): Number of true positives\n        num_FN (int): Number of false negatives\n        num_FP (int): Number of false positives\n        I_eval (np.ndarray): Data structure encoding TP, FN, FP\n    \"\"\"\n    I_eval = np.zeros(I_ref_pos.shape)\n    TP = (I_ref_pos + I_est_pos) > 1\n    FN = (I_ref_pos - I_est_pos) > 0\n    FP = (I_ref_pos - I_est_pos) < 0\n    I_eval[TP] = 1\n    I_eval[FN] = 2\n    I_eval[FP] = 3\n    num_TP = np.sum(TP)\n    num_FN = np.sum(FN)\n    num_FP = np.sum(FP)\n    P, R, F = measure_prf(num_TP, num_FN, num_FP)\n    return P, R, F, num_TP, num_FN, num_FP, I_eval\n\n\ndef plot_matrix_label(M, X, color_label=None, figsize=(3, 3), cmap='gray_r', fontsize=8, print_labels=True):\n    \"\"\"Plot matrix and label sequence\n\n    Args:\n        M: Matrix\n        X: Label sequence\n        color_label: List of colors for labels (Default value = None)\n        figsize: Figure size (Default value = (3, 3))\n        cmap: Colormap for imshow (Default value = 'gray_r')\n        fontsize: Font size (Default value = 8)\n        print_labels: Display labels inside Rectangles (Default value = True)\n\n    Returns:\n        fig: Handle for figure\n        ax: Handle for axes\n    \"\"\"\n    fig, ax = plt.subplots(2, 3, gridspec_kw={'width_ratios': [0.1, 1, 0.05],\n                                              'wspace': 0.2, 'height_ratios': [1, 0.1]},\n                           figsize=figsize)\n\n    colorList = np.array([[1, 1, 1, 1],  [0, 0, 0, 0.7]])\n    cmap = ListedColormap(colorList)\n    im = ax[0, 1].imshow(M, aspect='auto', cmap=cmap,  origin='lower', interpolation='nearest')\n    im.set_clim(vmin=-0.5, vmax=1.5)\n    ax_cb = plt.colorbar(im, cax=ax[0, 2])\n    ax_cb.set_ticks(np.arange(0, 2, 1))\n    ax_cb.set_ticklabels(np.arange(0, 2, 1))\n    ax[0, 1].set_xticks([])\n    ax[0, 1].set_yticks([])\n    plot_seq_label(ax[1, 1], X, color_label=color_label, fontsize=fontsize, print_labels=print_labels)\n    ax[1, 2].axis('off')\n    ax[1, 0].axis('off')\n    plot_seq_label(ax[0, 0], X, color_label=color_label, fontsize=fontsize, print_labels=print_labels,\n                   direction='vertical')\n    return fig, ax\n\n\ndef plot_matrix_pairwise(I_eval, figsize=(3, 2.5)):\n    \"\"\"Plot matrix I_eval encoding TP, FN, FP (see :func:`libfmp.c4.c4s5_evaluation.evaluate_pairwise`)\n\n   Args:\n        I_eval: Data structure encoding TP, FN, FP\n        figsize: Figure size (Default value = (3, 2.5))\n\n    Returns:\n        fig: Handle for figure\n        im: Handle for imshow\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    colorList = np.array([[1, 1, 1, 1], [0, 0.7, 0, 1], [1, 0, 0, 1], [1, 0.5, 0.5, 1]])\n    cmap = ListedColormap(colorList)\n    im = plt.imshow(I_eval, aspect='auto', cmap=cmap,  origin='lower', interpolation='nearest')\n    im.set_clim(vmin=-0.5, vmax=3.5)\n    plt.xticks([])\n    plt.yticks([])\n    ax_cb = plt.colorbar(im)\n    ax_cb.set_ticks(np.arange(0, 4, 1))\n    ax_cb.set_ticklabels(['', 'TP', 'FN', 'FP'])\n    return fig, im\n\n\nI_ref_pos = compare_pairwise(X_ref)\nfig, ax = plot_matrix_label(I_ref_pos, X_ref, color_label=color_label)\nax[0,1].set_title('Reference')\n\nI_est_pos = compare_pairwise(X_est)\nfig, ax = plot_matrix_label(I_est_pos, X_est, color_label=color_label)\nax[0,1].set_title('Estimation')\n\nP, R, F, num_TP, num_FN, num_FP, I_eval =  evaluate_pairwise(I_ref_pos, I_est_pos)\nfig, im = plot_matrix_pairwise(I_eval)\nplt.title('Pairwise TP, FN, FP')\n\nplt.show()\n\nprint('#TP = ', num_TP, '\\n#FN = ', num_FN, '\\n#FP = ', num_FP)\nprint('precision = %0.4f\\nrecall = %0.4f\\nF-measure = %0.4f' % (P, R, F))\n\n\n\n\n\n\n\n\n\n\n#TP =  10 \n#FN =  14 \n#FP =  3\nprecision = 0.7692\nrecall = 0.4167\nF-measure = 0.5405\n\n\n\n첫 번째 그림에 참조 주석이 표시된다. \\(45\\) 아이템 중 \\(24\\)는 이 주석과 관련하여 positive(회색 상자로 표시됨)이다.\n추정된 주석을 나타내는 두 번째 그림에는 \\(13\\)의 positive 항목이 있다(회색 상자로 표시됨).\n세 번째 그림에서 true positive(\\(\\#\\mathrm{TP}=10\\)), false positive (\\(\\#\\mathrm{FP}=3\\)) 및 false negative(\\(\\#\\mathrm{FN) }=14\\))가 표시된다.\n이로부터 다음을 얻을 수 있다. \\[\\begin{eqnarray}\n\\mathrm{P} &=& \\#\\mathrm{TP}/(\\#\\mathrm{TP}+\\#\\mathrm{FP})=10/13\\approx 0.769,\\\\\n\\mathrm{R} &=& \\#\\mathrm{TP}/(\\#\\mathrm{TP}+\\#\\mathrm{FN})=10/24\\approx 0.417,\\\\\n\\mathrm{F} &=& 2\\mathrm{P}\\mathrm{R}/(\\mathrm{P} + \\mathrm{R})\\approx 0.541.\n\\end{eqnarray}\\]\n이 예에서 거의 \\(77\\%\\)의 precision은 상대적으로 높은 반면 \\(42\\%\\)의 recall은 상대적으로 낮다.\nF-measure은 이 두 값 사이이며 더 작은 쪽으로 편향된다.\n추가 예시로 브람스 주석을 보자.\n\n\nX_0 = convert_ann_to_seq_label(ann_Brahms[0])\nX_1 = convert_ann_to_seq_label(ann_Brahms[1])\nX_2 = convert_ann_to_seq_label(ann_Brahms[2])\n\nX_set = [X_0, X_1, X_2]\ncombinations = [(0,1), (0,2), (1,2)]\ncase_label = ['Coarse', 'Medium', 'Fine']\n\nfor c in combinations:\n    X_ref = X_set[c[0]]\n    X_est = X_set[c[1]]\n    \n    I_ref_pos = compare_pairwise(X_ref)\n    fig, ax = plot_matrix_label(I_ref_pos, X_ref, color_label=color_ann_Brahms, print_labels=False)\n    ax[0,1].set_title('Reference: '+case_label[c[0]])\n    \n    I_est_pos = compare_pairwise(X_est)\n    fig, ax = plot_matrix_label(I_est_pos, X_est, color_label=color_ann_Brahms, print_labels=False)\n    ax[0,1].set_title('Estimation: '+case_label[c[1]])\n\n    P, R, F, num_TP, num_FN, num_FP, I_eval =  evaluate_pairwise(I_ref_pos, I_est_pos)\n    fig, im = plot_matrix_pairwise(I_eval)\n    plt.title('Pairwise TP, FN, FP')\n    \n    plt.show()\n    \n    print('#TP = ', num_TP, ';  #FN = ', num_FN, ';  #FP = ', num_FP)\n    print('P = %0.3f;  R = %0.3f;  F = %0.3f' % (P, R, F))\n    print()\n\n\n\n\n\n\n\n\n\n\n#TP =  226 ;  #FN =  218 ;  #FP =  12\nP = 0.950;  R = 0.509;  F = 0.663\n\n\n\n\n\n\n\n\n\n\n\n\n#TP =  143 ;  #FN =  301 ;  #FP =  4\nP = 0.973;  R = 0.322;  F = 0.484\n\n\n\n\n\n\n\n\n\n\n\n\n#TP =  136 ;  #FN =  102 ;  #FP =  11\nP = 0.925;  R = 0.571;  F = 0.706"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html",
    "title": "6.1. 화성의 기본 이론",
    "section": "",
    "text": "화음 인식에 대해 본격적으로 다루기 전에, 화성의 기본 개념을 알아본다. 음정(interval), 화음(chord), 음계(scale) 등에 대해 설명한다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#반음-차이-semitone-differences",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#반음-차이-semitone-differences",
    "title": "6.1. 화성의 기본 이론",
    "section": "반음 차이 (Semitone Differences)",
    "text": "반음 차이 (Semitone Differences)\n\n12 평균율 음계를 가정하면 한 옥타브는 로그 주파수 축에서 등간격으로 배치된 12개의 음계 단계로 세분화 된다.\n이 음계에서 가능한 가장 작은 간격을 반음(semitone) 이라고 하며, 이는 두 개의 연이은 음계 단계 간의 차이이다.\n주파수 측면에서 반음은 (차이가 아닌) 비율을 나타내며 센트(cents) 라는 로그 단위를 사용하여 지정할 수 있다.\n\\(\\omega_1\\)와 \\(\\omega_2\\)라고 하는 두 주파수 간의 센트 차이는 다음과 같다. \\[\\log_2\\left(\\frac{\\omega_1}{\\omega_2}\\right)\\cdot 1200\\]\n따라서 평균율 음계에서 반음은 \\(100\\) 센트에 해당한다.\n반음의 개념을 바탕으로 서양 음악 이론에서 사용되는 서로 다른 음정을 지정할 수 있다. 이러한 음정에 대한 명명은 과거의 관행을 기반으로 한다. 특히, “음정 이름”은 낮은 음과 높은 음 사이의 반음 차이를 설명할 뿐만 아니라 악보 표기법에서 음정을 지정하는 방법도 설명할 수 있다.\n서로 다른 음표 기호가 동일한 음을 나타낼 수도 있으며, 이는 이명동음적 상등(enharmonic equivalence) 으로 알려져 있다. 유사하게, 반음으로 측정될 때(동일 평균율 음계로 가정) 동일한 거리에 해당하는 음악적으로 다른 음정(다른 음정 이름으로 지칭됨) 사이에는 “enharmonic equivalence”이 존재한다. 예를 들어, “augmented unison”과 “minor second”라는 두 개의 음정은 모두 한 반음의 차이를 표현하지만 악보 표기를 사용할 때는 다르게 지정된다.\n여기서는 음정을 지나치게 단순화되더라도 악보를 무시하고 음정 사이의 거리(반음 단위)로 생각하기로 하자.\n다음 그림은 가장 일반적인 음정 이름(C4를 낮은 음으로 사용)과 그 의미를 반음 차이(\\(\\Delta\\)로 표시)로 나타낸 것이다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F02b.png\", width=600)"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#주파수-비율frequency-ratios",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#주파수-비율frequency-ratios",
    "title": "6.1. 화성의 기본 이론",
    "section": "주파수 비율(Frequency Ratios)",
    "text": "주파수 비율(Frequency Ratios)\n\n음정의 개념은 피치의 부분음들 사이에서 자연적으로 발생하는 주파수 관계를 고려하여 물리적인 관점에서 접근할 수 있다. 고조파는 톤의 배음열을 형성하는 기본 주파수의 정수배이다.\n물리적 접근에서는 동일한 고조파 계열 내에서 발생하는 부분음 간의 주파수 관계에서 음정을 도출한다. 다음 그림은 음표 C2의 고조파 시리즈를 보여준다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F04.png\", width=600)\n\n\n\n\n\n예를 들어, 옥타브(octave)는 처음 두 부분음 사이의 음정(interval)이고, 5도(fifth)는 두 번째와 세 번째 부분음 사이의 음정이고, 4도(fourth)는 세 번째와 네 번째 부분음 사이의 음정으로 발생하는 식으로 이어진다.\n이 관찰은 작은 정수의 비율을 기반으로 하는 음정의 정의로 이어진다. 이렇게 정의된 간격을 순수(pure) 또는 단순(just) 음정이라고도 한다. 마찬가지로 고조파에 기반한 음악 조율은 순정률(pure (or just) intonation) 으로 알려져 있다.\n순정률 외에도, 주파수 비율 측면에서 음정을 정의하는 데 사용할 수 있는 더 많은 조율 시스템이 있다.\n알려진 가장 오래된 튜닝 시스템은 그리스 철학자이자 수학자 피타고라스(기원전 6세기)에 의해 소개되었다. 기하학적 동기가 부여된 피타고라스 튜닝은 옥타브의 주파수 비율 \\(1:2\\)와 5도의 비율 \\(2:3\\)에만 기반한다. 다른 모든 음정은 5도와 옥타브를 적절하게 더하거나 빼서 그 비율에서 얻는다. 이로 인해 2의 거듭제곱 또는 3의 거듭제곱만 포함하는 주파수 비율로 표현할 수 있는 음정이 생성된다.\n다음 그림에서는 간격의 다양한 정의와 그 관계를 보여준다. 왼쪽 열부터 보면 반음의 차이(\\(\\Delta\\)), 음정의 이름, C4를 근음(root note)으로 가정한 음정, 순정률에 대한 비율(JI), 피타고라스 비율(Pyt)이 나와있다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F03.png\", width=400)\n\n\n\n\n\n다음 코드 셀에서는 평균율(ET), 순음률(JI) 및 피타고라스 튜닝(Pyt)과 관련하여 다양한 음정의 정현파(sinusodial) 음향화(sonification)를 제공한다.\n또한 ET 기반 및 JI 기반 간격(JI Dev)과 ET 기반 및 Pyt 기반 간격(Pyt Dev) 간의 편차(센트 단위)를 지정한다.\n\n\ndef f_pitch(p):\n    frequency = 2 ** ((p - 69) / 12) * 440\n    return frequency\n\ndiff_semitones = ['0','1','2','3','4','5','6','7','8','9','10','11','12',]\n\nJI_frac = ['$1:1$', '$15:16$', '$8:9$', '$5:6$', '$4:5$', '$3:4$', '$32:45$', \n           '$2:3$', '$5:8$', '$3:5$', '$5:9$', '$8:15$', '$1:2$']\nJI_ratio = np.asarray([1, 15/16, 8/9, 5/6, 4/5, 3/4, 32/45, 2/3, 5/8, 3/5, 5/9, 8/15, 1/2])\n\npyt_frac = ['$1:1$', '$3^5:2^8$', '$2^3:3^2$', '$3^3:2^5$', '$2^6:3^4$', '$3:2^2$', '$2^9:3^6$', \n            '$2:3$', '$3^4:2^7$', '$2^4:3^3$', '$3^2:2^4$', '$2^7:3^5$', '$1:2$']\npyt_ratio = np.asarray([1, 243/256, 8/9, 27/32, 64/81, 3/4, 512/729, 2/3, 81/128, 16/27, 9/16, 128/243, 1/2])\n\np = 60\nomega = f_pitch(p)\nfreq_JI = omega / JI_ratio \nfreq_pyt = omega / pyt_ratio\nnotes = np.asarray(range(p, p + 13))\nfreq_center = f_pitch(notes)\nfreq_deviation_cents_JI =  np.log2(freq_JI / freq_center) * 1200\nfreq_deviation_cents_pyt =  np.log2(freq_pyt / freq_center) * 1200\n\nduration = 1\nFs = 4000\nN = int(duration * Fs)\nt = np.arange(0, N) / Fs\n\ndef generate_sinusoid(omega, t):\n    return np.sin(2 * np.pi * omega * t)\n\ndef generate_sinusoid_list(freq_list, x_ref=[]):\n    sinusoid_list = []\n    for f in freq_list:\n        s = generate_sinusoid(f, t)\n        x = np.concatenate((x_ref, s))\n        sinusoid_list.append(x)\n    return sinusoid_list\n\nx_ref = generate_sinusoid(omega, t)\nsinusoid_center = generate_sinusoid_list(freq_center, x_ref)\nsinusoid_JI = generate_sinusoid_list(freq_JI, x_ref)\nsinusoid_pyt = generate_sinusoid_list(freq_pyt, x_ref) \n\n# Generation of html table\ndef generate_audio_tag_html_list(sinusoid_list, Fs):\n    audio_tag_html_list = []\n    for i in range(len(sinusoid_list)):\n        audio_tag = ipd.Audio( sinusoid_list[i], rate=Fs)\n        audio_tag_html = audio_tag._repr_html_().replace('\\n', '').strip()\n        audio_tag_html = audio_tag_html.replace('<audio ', \n                                                '<audio style=\"width: 110px; height: 30px;\"')  \n        audio_tag_html_list.append(audio_tag_html)\n    return audio_tag_html_list\n\naudio_tag_html_center = generate_audio_tag_html_list(sinusoid_center, Fs=Fs)\naudio_tag_html_JI = generate_audio_tag_html_list(sinusoid_JI, Fs=Fs)\naudio_tag_html_pyt = generate_audio_tag_html_list(sinusoid_pyt, Fs=Fs)\n\n\npd.options.display.float_format = '{:,.1f}'.format    \npd.set_option('display.max_colwidth', None)    \ndf = pd.DataFrame(OrderedDict([\n    ('$\\Delta$', diff_semitones),\n    ('Interval name', ['(Perfect) unison','Minor second','Major second','Minor Third',\n                       'Major Third','(Perfect) fourth', 'Tritone','(Perfect) fifth',\n                       'Minor sixth','Major sixth','Minor seventh','Major seventh','(Perfect) octave']),\n    ('&emsp;Interval', ['C4&ndash;C4','C4&ndash;C$^\\sharp$4','C4&ndash;D4',\n                        'C4&ndash;D$^\\sharp$4','C4&ndash;E4','C4&ndash;F4',\n                        'C4&ndash;F$^\\sharp$4','C4&ndash;G4','C4&ndash;G$^\\sharp$4',\n                        'C4&ndash;A4','C4&ndash;A$^\\sharp$4', 'C4&ndash;B4','C4&ndash;C4']),\n    ('ET Sinusoid', audio_tag_html_center), \n    ('&emsp;&emsp; JI Ratio', JI_frac),     \n    ('JI Sinusoid', audio_tag_html_JI),\n    ('JI Dev', freq_deviation_cents_JI),\n    ('&emsp;&emsp; Pyt Ratio', pyt_frac),    \n    ('Pyt Sinusoid', audio_tag_html_pyt),                                   \n    ('Pyt Dev', freq_deviation_cents_pyt)]))\n\ndf.index = np.arange(1, len(df) + 1)\nipd.HTML(df.to_html(escape=False, index=False))\n\n\n\n  \n    \n      $\\Delta$\n      Interval name\n       Interval\n      ET Sinusoid\n         JI Ratio\n      JI Sinusoid\n      JI Dev\n         Pyt Ratio\n      Pyt Sinusoid\n      Pyt Dev\n    \n  \n  \n    \n      0\n      (Perfect) unison\n      C4–C4\n                                              Your browser does not support the audio element.                \n      $1:1$\n                                              Your browser does not support the audio element.                \n      0.0\n      $1:1$\n                                              Your browser does not support the audio element.                \n      0.0\n    \n    \n      1\n      Minor second\n      C4–C$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $15:16$\n                                              Your browser does not support the audio element.                \n      11.7\n      $3^5:2^8$\n                                              Your browser does not support the audio element.                \n      -9.8\n    \n    \n      2\n      Major second\n      C4–D4\n                                              Your browser does not support the audio element.                \n      $8:9$\n                                              Your browser does not support the audio element.                \n      3.9\n      $2^3:3^2$\n                                              Your browser does not support the audio element.                \n      3.9\n    \n    \n      3\n      Minor Third\n      C4–D$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:6$\n                                              Your browser does not support the audio element.                \n      15.6\n      $3^3:2^5$\n                                              Your browser does not support the audio element.                \n      -5.9\n    \n    \n      4\n      Major Third\n      C4–E4\n                                              Your browser does not support the audio element.                \n      $4:5$\n                                              Your browser does not support the audio element.                \n      -13.7\n      $2^6:3^4$\n                                              Your browser does not support the audio element.                \n      7.8\n    \n    \n      5\n      (Perfect) fourth\n      C4–F4\n                                              Your browser does not support the audio element.                \n      $3:4$\n                                              Your browser does not support the audio element.                \n      -2.0\n      $3:2^2$\n                                              Your browser does not support the audio element.                \n      -2.0\n    \n    \n      6\n      Tritone\n      C4–F$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $32:45$\n                                              Your browser does not support the audio element.                \n      -9.8\n      $2^9:3^6$\n                                              Your browser does not support the audio element.                \n      11.7\n    \n    \n      7\n      (Perfect) fifth\n      C4–G4\n                                              Your browser does not support the audio element.                \n      $2:3$\n                                              Your browser does not support the audio element.                \n      2.0\n      $2:3$\n                                              Your browser does not support the audio element.                \n      2.0\n    \n    \n      8\n      Minor sixth\n      C4–G$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:8$\n                                              Your browser does not support the audio element.                \n      13.7\n      $3^4:2^7$\n                                              Your browser does not support the audio element.                \n      -7.8\n    \n    \n      9\n      Major sixth\n      C4–A4\n                                              Your browser does not support the audio element.                \n      $3:5$\n                                              Your browser does not support the audio element.                \n      -15.6\n      $2^4:3^3$\n                                              Your browser does not support the audio element.                \n      5.9\n    \n    \n      10\n      Minor seventh\n      C4–A$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:9$\n                                              Your browser does not support the audio element.                \n      17.6\n      $3^2:2^4$\n                                              Your browser does not support the audio element.                \n      -3.9\n    \n    \n      11\n      Major seventh\n      C4–B4\n                                              Your browser does not support the audio element.                \n      $8:15$\n                                              Your browser does not support the audio element.                \n      -11.7\n      $2^7:3^5$\n                                              Your browser does not support the audio element.                \n      9.8\n    \n    \n      12\n      (Perfect) octave\n      C4–C4\n                                              Your browser does not support the audio element.                \n      $1:2$\n                                              Your browser does not support the audio element.                \n      0.0\n      $1:2$\n                                              Your browser does not support the audio element.                \n      0.0"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#협화음과-불협화음-consonance-and-dissonance",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#협화음과-불협화음-consonance-and-dissonance",
    "title": "6.1. 화성의 기본 이론",
    "section": "협화음과 불협화음 (Consonance and Dissonance)",
    "text": "협화음과 불협화음 (Consonance and Dissonance)\n\n조화 부분음(harmonic partials)에 기초한 순정률을 사용하여, \\(1:1\\)(1도; unison), \\(1:2\\)(옥타브; octave), \\(2:3\\)(5도), 또는 \\(3:4\\)(4도)와 같은 작은 정수 비로 특정 음정이 설명될 수 있음을 보았다.\n이러한 음정은 고조파 시리즈의 배음에서 자연스럽게 나타나며 대체로 일관되고 만족스러운(pleasant) 것으로 인식된다.\n보다 일반적으로 사용되는 협화음(consonance) 이라는 용어는 동시에 연주될 때 대부분의 사람들에게 듣기 좋게 들리는 음의 조합을 의미한다. 반대로 불협화음(dissonance) 이라는 용어는 거칠거나 불쾌하게 들리는 음의 조합을 가리키는 데 사용된다. 협화음은 불안정한 것으로 간주되는 불협화음(전이 특성을 가짐)과 달리 안정적인 것으로 간주된다.\n협화음과 불협화음의 정의는 음악적, 물리적, 지각적 기준에 따라 다양하다.\n음정의 경우, 1도, 옥타브, 5도, 4도는 일반적으로 완전(perfect) 협화음으로 간주된다. 따라서 이러한 음정을 완전음정(perfect interval) 이라고도 한다.\n장3도(major thirds)와 단3도(minor thirds), 장6도(major sixths)와 단6도(minor sixth)는 여전히 협화음으로 인식되지만 그 정도는 낮다 (불완전 협화음; imperfect consonance).\n다른 음정은 일반적으로 불협화음으로 간주된다. 특히 가장 불협화음 음정인 트라이톤 음정(tritone interval) 이 있다. 마지막 절에서 보듯이 순정률에서 트라이톤(tritone)의 주파수 비율은 가장 큰 정수를 포함한다.\n음을 동시에 연주할 때 협화음의 정도는 연주한 음의 화성이 일치하는 정도와 관련이 있다. 이러한 관점에서 볼 때, 협화음은 두 음 사이의 음정 크기뿐만 아니라 결과 소리의 결합된 스펙트럼 분포에 따라 달라진다.\n다음 코드 예에서 동시 음정(harmonic 음정이라고도 함)은 서로 다른 조음 체계에 따라 정현파로 생성된다.\n\n\nduration = 3\nFs = 4000\nN = int(duration * Fs)\nt = np.arange(0, N) / Fs\n\ndef generate_sinusoid_interval_list(freq_list, x_ref=[]):\n    sinusoid_list = []\n    for f in freq_list:\n        s = generate_sinusoid(f, t)\n        x = x_ref + s\n        sinusoid_list.append(x)\n    return sinusoid_list\n\nx_ref = generate_sinusoid(omega, t)\nsinusoid_center = generate_sinusoid_interval_list(freq_center, x_ref)\nsinusoid_JI = generate_sinusoid_interval_list(freq_JI, x_ref)\nsinusoid_pyt = generate_sinusoid_interval_list(freq_pyt, x_ref) \nsinusoid_sum = list(range(len(freq_center)))\nfor i in range(len(freq_center)):\n    sinusoid_sum[i] = (sinusoid_center[i] + sinusoid_JI[i] + sinusoid_pyt[i]) / 3\n\n# Generation of html table\ndef generate_audio_tag_html_list(sinusoid_freq_list, Fs):\n    audio_tag_html_list = []\n    for i in range(len(sinusoid_freq_list)):\n        audio_tag = ipd.Audio( sinusoid_freq_list[i], rate=Fs)\n        audio_tag_html = audio_tag._repr_html_().replace('\\n', '').strip()\n        audio_tag_html = audio_tag_html.replace('<audio ', \n                                                '<audio style=\"width: 110px; height: 30px;\"')  \n        audio_tag_html_list.append(audio_tag_html)\n    return audio_tag_html_list\n\naudio_tag_html_center = generate_audio_tag_html_list(sinusoid_center, Fs=Fs)\naudio_tag_html_JI = generate_audio_tag_html_list(sinusoid_JI, Fs=Fs)\naudio_tag_html_pyt = generate_audio_tag_html_list(sinusoid_pyt, Fs=Fs)\n\npd.options.display.float_format = '{:,.1f}'.format    \npd.set_option('display.max_colwidth', None)    \ndf = pd.DataFrame(OrderedDict([\n    ('Delta', diff_semitones),\n    ('Interval name', ['(Perfect) unison','Minor second','Major second','Minor Third',\n                       'Major Third','(Perfect) fourth', 'Tritone','(Perfect) fifth',\n                       'Minor sixth','Major sixth','Minor seventh','Major seventh','(Perfect) octave']),\n    ('&emsp;Interval', ['C4&ndash;C4','C4&ndash;C$^\\sharp$4','C4&ndash;D4',\n                        'C4&ndash;D$^\\sharp$4','C4&ndash;E4','C4&ndash;F4',\n                        'C4&ndash;F$^\\sharp$4','C4&ndash;G4','C4&ndash;G$^\\sharp$4',\n                        'C4&ndash;A4','C4&ndash;A$^\\sharp$4', 'C4&ndash;B4','C4&ndash;C4']),\n    ('ET Sinusoid', audio_tag_html_center), \n    ('&emsp;&emsp; JI Ratio ', JI_frac),     \n    ('JI Sinusoid', audio_tag_html_JI),\n    ('JI Dev.', freq_deviation_cents_JI),\n    ('&emsp;&emsp; Pyt Ratio ', pyt_frac),    \n    ('Pyt Sinusoid', audio_tag_html_pyt),                                   \n    ('Pyt Dev.', freq_deviation_cents_pyt)]))\n\ndf.index = np.arange(1, len(df) + 1)\nipd.HTML(df.to_html(escape=False, justify='center', index=False))\n\n\n\n  \n    \n      Delta\n      Interval name\n       Interval\n      ET Sinusoid\n         JI Ratio\n      JI Sinusoid\n      JI Dev.\n         Pyt Ratio\n      Pyt Sinusoid\n      Pyt Dev.\n    \n  \n  \n    \n      0\n      (Perfect) unison\n      C4–C4\n                                              Your browser does not support the audio element.                \n      $1:1$\n                                              Your browser does not support the audio element.                \n      0.0\n      $1:1$\n                                              Your browser does not support the audio element.                \n      0.0\n    \n    \n      1\n      Minor second\n      C4–C$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $15:16$\n                                              Your browser does not support the audio element.                \n      11.7\n      $3^5:2^8$\n                                              Your browser does not support the audio element.                \n      -9.8\n    \n    \n      2\n      Major second\n      C4–D4\n                                              Your browser does not support the audio element.                \n      $8:9$\n                                              Your browser does not support the audio element.                \n      3.9\n      $2^3:3^2$\n                                              Your browser does not support the audio element.                \n      3.9\n    \n    \n      3\n      Minor Third\n      C4–D$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:6$\n                                              Your browser does not support the audio element.                \n      15.6\n      $3^3:2^5$\n                                              Your browser does not support the audio element.                \n      -5.9\n    \n    \n      4\n      Major Third\n      C4–E4\n                                              Your browser does not support the audio element.                \n      $4:5$\n                                              Your browser does not support the audio element.                \n      -13.7\n      $2^6:3^4$\n                                              Your browser does not support the audio element.                \n      7.8\n    \n    \n      5\n      (Perfect) fourth\n      C4–F4\n                                              Your browser does not support the audio element.                \n      $3:4$\n                                              Your browser does not support the audio element.                \n      -2.0\n      $3:2^2$\n                                              Your browser does not support the audio element.                \n      -2.0\n    \n    \n      6\n      Tritone\n      C4–F$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $32:45$\n                                              Your browser does not support the audio element.                \n      -9.8\n      $2^9:3^6$\n                                              Your browser does not support the audio element.                \n      11.7\n    \n    \n      7\n      (Perfect) fifth\n      C4–G4\n                                              Your browser does not support the audio element.                \n      $2:3$\n                                              Your browser does not support the audio element.                \n      2.0\n      $2:3$\n                                              Your browser does not support the audio element.                \n      2.0\n    \n    \n      8\n      Minor sixth\n      C4–G$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:8$\n                                              Your browser does not support the audio element.                \n      13.7\n      $3^4:2^7$\n                                              Your browser does not support the audio element.                \n      -7.8\n    \n    \n      9\n      Major sixth\n      C4–A4\n                                              Your browser does not support the audio element.                \n      $3:5$\n                                              Your browser does not support the audio element.                \n      -15.6\n      $2^4:3^3$\n                                              Your browser does not support the audio element.                \n      5.9\n    \n    \n      10\n      Minor seventh\n      C4–A$^\\sharp$4\n                                              Your browser does not support the audio element.                \n      $5:9$\n                                              Your browser does not support the audio element.                \n      17.6\n      $3^2:2^4$\n                                              Your browser does not support the audio element.                \n      -3.9\n    \n    \n      11\n      Major seventh\n      C4–B4\n                                              Your browser does not support the audio element.                \n      $8:15$\n                                              Your browser does not support the audio element.                \n      -11.7\n      $2^7:3^5$\n                                              Your browser does not support the audio element.                \n      9.8\n    \n    \n      12\n      (Perfect) octave\n      C4–C4\n                                              Your browser does not support the audio element.                \n      $1:2$\n                                              Your browser does not support the audio element.                \n      0.0\n      $1:2$\n                                              Your browser does not support the audio element.                \n      0.0"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#화음-triads",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#화음-triads",
    "title": "6.1. 화성의 기본 이론",
    "section": "3화음 (Triads)",
    "text": "3화음 (Triads)\n\n다음과 같은 소수의 화음으로 제한하여 살펴보자.\n서양 음악에서 가장 중요한 3화음은 3도(thirds)로 쌓일 수 있는 3개의 음으로 구성된다. 3도를 쌓을 때 가장 낮은 음을 근음(root note) 이라고 한다.\n단3도(반음 3개)와 장3도(반음 4개)가 있으므로, 다음의 4개의 3화음을 구분할 수 있다: 장3화음(major triad), 단3화음(minor triad), 감3화음(diminished triad), 증3화음(augmented triad).\n다음 그림은 근음 C4에 대한 이러한 유형의 3화음을 보여준다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F05_triad.png\",width=600)\n\n\n\n\n\n각 화음 유형은 서로 다른 근음에 따라 연주될 수 있다.\n주어진 근음에 관계없이 4개의 3화음 형식은 청취자가 인식하는 방식에 따라 질이 다르다.\n장3도와 완전5도로 구성된 장(major)3화음은 협화음정을 구성한다. 또한 세 구성음의 부분음 사이에 일치도가 높다. 그 결과 장3화음의 소리는 종종 일관되고 유쾌하며 행복한 것으로 묘사된다.\n두 번째 음의 피치를 반음 낮추면 장3화음이 단(minor)3화음으로 바뀐다. 여전히 협화음이 있고 일관성 있는 것으로 간주되지만 단화음은 종종 슬프고 우울하거나 침울한 것으로 인식된다.\n감(diminished)3화음 및 증(augmented)3화음의 경우 구성음의 부분음이 거의 겹치지 않으며 이러한 3화음은 일반적으로 불협화음 및 불안정한 것으로 인식된다. 종종 감 및 증3화음은 장화음과 단화음을 기반으로 보다 안정적인 화성 사이를 이동하기 위해 전환 악절에서 사용된다.\n다음은 피아노로 연주되는 \\(\\mathrm{C}\\)-major 및 \\(\\mathrm{C}\\)-minor 화음의 사운드 예이다.\n정현파를 사용하여 네 가지 화음 유형 각각의 합성(sythetic) 변형을 생성하자.\n\n\nprint(\"Major triad\")\nipd.display(ipd.Audio(filename=\"../data_FMP/FMP_C5_F05a_C-Major-Triad_AudioLabs.mp3\"))\n\nprint(\"Minor triad\")\nipd.display(ipd.Audio(filename=\"../data_FMP/FMP_C5_F05b_C-Minor-Triad_AudioLabs.mp3\"))\n\nMajor triad\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMinor triad\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef generate_sinusoid_chord(pitches=[69], duration=1, Fs=4000, amplitude_max=0.5):\n    \"\"\"Generate synthetic sound of chord using sinusoids\n\n    Args:\n        pitches (list): List of pitches (MIDI note numbers) (Default value = [69])\n        duration (float): Duration (seconds) (Default value = 1)\n        Fs (scalar): Sampling rate (Default value = 4000)\n        amplitude_max (float): Amplitude (Default value = 0.5)\n\n    Returns:\n        x (np.ndarray): Synthesized signal\n    \"\"\"\n    N = int(duration * Fs)\n    t = np.arange(0, N) / Fs\n    x = np.zeros(N)\n    for p in pitches:\n        omega = 2 ** ((p - 69) / 12) * 440\n        x = x + np.sin(2 * np.pi * omega * t)\n    x = amplitude_max * x / np.max(x)\n    return x\n\n\nduration = 2\nFs = 4000\n\npitches = [60, 64, 67]\nx = generate_sinusoid_chord(pitches=pitches, duration=duration, Fs=Fs)\nprint('Major chord', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\npitches = [60, 63, 67]\nx = generate_sinusoid_chord(pitches=pitches, duration=duration, Fs=Fs)\nprint('Minor chord', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\npitches = [60, 63, 66]\nx = generate_sinusoid_chord(pitches=pitches, duration=duration, Fs=Fs)\nprint('Diminished chord', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\npitches = [60, 64, 68]\nx = generate_sinusoid_chord(pitches=pitches, duration=duration, Fs=Fs)\nprint('Augmented chord', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nMajor chord\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMinor chord\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nDiminished chord\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nAugmented chord\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#장화음-단화음-major-and-minor-chords",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#장화음-단화음-major-and-minor-chords",
    "title": "6.1. 화성의 기본 이론",
    "section": "장화음, 단화음 (Major and Minor Chords)",
    "text": "장화음, 단화음 (Major and Minor Chords)\n\n12개의 서로 다른 근음이 있기 때문에 기본적으로 12개의 장3화음과 12개의 단3화음을 형성할 수 있다.\n다음은 각 화음의 가장 낮은 음이 근음인 3화음의 악보 표현을 보여준다. 또한 피아노 녹음과 각 3화음의 합성 버전을 들어보자.\n\n\nipd.display(ipd.Image('../img/6.chord_recognition/FMP_C5_F06_major.png'))\nipd.display(ipd.Audio('../data_FMP/FMP_C5_F06_Major-Triads_AudioLabs_NoPedal.mp3'))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nipd.display(ipd.Image('../img/6.chord_recognition/FMP_C5_F06_minor.png'))\nipd.display(ipd.Audio('../data_FMP/FMP_C5_F06_Minor-Triads_AudioLabs_NoPedal.mp3'))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nduration = 1\nFs = 4000\n\nx_major = []\npitches = np.array([60, 64, 67])\nfor i in range(12):\n    x = generate_sinusoid_chord(pitches=pitches+i, duration=duration, Fs=Fs)\n    x_major = np.append(x_major, x)\n\nx_minor = []\npitches = np.array([60, 63, 67])\nfor i in range(12):\n    x = generate_sinusoid_chord(pitches=pitches+i, duration=duration, Fs=Fs)\n    x_minor = np.append(x_minor, x)\n\nprint('Major chords', flush=True)\nipd.display(ipd.Audio(data=x_major, rate=Fs))    \n    \nprint('Minor chords', flush=True)\nipd.display(ipd.Audio(data=x_minor, rate=Fs))\n\nMajor chords\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMinor chords\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n음 수준에서는 일반적으로 주어진 화음를 실현하기 위한 많은 대안이 있다.\n화음의 가장 낮은 음(베이스 음)이 근음일 때 화음는 근음 위치(root position) 또는 정상 형식(normal form) 에 있다고 한다. 근음이 화음에서 연주되는 가장 낮은 음이 아닌 경우 반전(inverted) 이라고 한다.\n다음 그림은 정규 형식, 첫 번째 반전(inversion), 두 번째 반전, 옥타브 더블링(doubling)이 있는 화음 및 깨진 화음 (broken chord) (코드의 음을 차례로 연주할 수 있음)을 보여준다.\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F07.png\", width=\"400\"))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F07_C-Major-Variants_AudioLabs_NoPedal.mp3\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nFs = 4000\nduration = 2\nx = []\nx = generate_sinusoid_chord(pitches=[60, 64, 67], duration=duration, Fs=Fs)\nx = np.append(x,generate_sinusoid_chord(pitches=[64, 67, 72], duration=duration, Fs=Fs))\nx = np.append(x,generate_sinusoid_chord(pitches=[67, 72, 76], duration=duration, Fs=Fs))\nx = np.append(x,generate_sinusoid_chord(pitches=[60, 64, 67, 72, 76], duration=duration, Fs=Fs))\nduration = duration / 8\nbroken_chord = [60, 64, 67, 72, 76, 67, 72, 76]\nfor p in broken_chord:\n    x = np.append(x,generate_sinusoid_chord(pitches=[p], duration=duration, Fs=Fs))\n\nipd.display(ipd.Audio(data=x, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n장(major) 화음은 일반적으로 근음의 피치 클래스에 사용되는 것과 동일한 기호로 표시된다.\n다음부터는 굵은 글씨를 사용하여 화음의 개념과 피치 클래스 또는 음의 개념을 구분한다.\n\n단(minor) 화음의 경우 “minor”를 나타내는 \\(\\mathbf{m}\\) 문자를 추가하는 것을 제외하고는 메이저 화음과 동일한 표기법을 사용하는 경우가 많다.\n예를 들어, \\(\\mathbf{C}\\)로 표시되는 \\(\\mathrm{C}\\)-major 화음은 피치 클래스 \\(\\mathrm{C}\\), \\(\\mathrm{E}\\) 및 \\(\\mathrm{G}\\)가 있는 세 개의 음으로 구성된다. 반면, \\(\\mathbf{Cm}\\)로 표시되는 \\(\\mathrm{C}\\)-minor 화음은 \\(\\mathrm{C}\\), \\(\\mathrm{E}^\\flat\\) 및 \\(\\mathrm{G}\\)로 구성된다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#수학적-모형",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#수학적-모형",
    "title": "6.1. 화성의 기본 이론",
    "section": "수학적 모형",
    "text": "수학적 모형\n\n구성 음의 피치 클래스 또는 크로마 값에 의해 장 또는 단 화음이 결정되는 다소 단순한 관점을 채택하면, 각 3화음을 12개의 크로마 속성으로 구성된 \\(\\{\\mathrm{C},\\mathrm{C}^\\sharp,\\mathrm{D},\\ldots\\mathrm{B}\\}\\)로 생각할 수 있다.\n동등하게, 3개 원소의 하위 집합은 하위 집합에 의해 인코딩된 크로마 위치에서 \\(1\\) 값의 3개 항목이 있는 이진 크로마 벡터로 간주될 수 있다.\n이 수학적 모형에 기초하여 장3화음 \\(\\mathbf{C}\\)를 12가지 다른 방식으로 순환 이동함으로써 12개의 장화음을 얻을 수 있다. 마찬가지로 \\(\\mathbf{Cm}\\)에서 12개의 단화음을 얻는다.\n각 \\(24\\)개의 장 및 단 3화음은 서로 다른 3개 요소 하위 집합으로 이어진다. 즉, 피치 클래스 수준에서 장화음과 단화음은 고유하게 정의된다.\n\\(24\\)개의 장, 단화음의 크로마 패턴 결과는 다음 그림과 같다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F06.png\", width=800)"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#도화음-등-seventh-chords-and-beyond",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#도화음-등-seventh-chords-and-beyond",
    "title": "6.1. 화성의 기본 이론",
    "section": "7도화음 등 (Seventh Chords and Beyond)",
    "text": "7도화음 등 (Seventh Chords and Beyond)\n\n3화음은 4개, 5개 또는 그 이상의 음으로 구성된 보다 복잡한 화음의 하위 집합으로도 나타난다.\n3화음에 3도 음정을 더 쌓으면 7도 화음이 된다.\n7화음에는 여러 종류가 있다.\n\n장7도 화음은 장3화음 위에 장3도를 쌓아 만든 것으로 주로 팝, 재즈 음악에 사용된다.\ndominant 7도 화음 (장3화음 + 단3도)는 다음 화음으로 리졸브(resolve)되는 경향이 강하다.\n단7도 화음 (단3화음 + 단3도)은 더 안정적이며 팝/록/재즈와 클래식 음악 모두에 나타난다.\n반-감(half-diminished)7도 화음 (감3화음 + 장3도)은 낭만주의 음악(예: Richard Wagner)과 재즈 음악에서 광범위하게 사용된다.\n감7화음 (감3화음 + 단3도)은 3개의 연결된 단3도음으로 구성되어 바로크 음악에서 다른 키로 변조하는 데 사용되었다.\n\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_SeventhChords.png\", width=800))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F07_Seventh-Chords_AudioLabs_NoPedal.mp3\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#크로마틱-스케일-반음계-chromatic-scale",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#크로마틱-스케일-반음계-chromatic-scale",
    "title": "6.1. 화성의 기본 이론",
    "section": "크로마틱 스케일 / 반음계 (Chromatic Scale)",
    "text": "크로마틱 스케일 / 반음계 (Chromatic Scale)\n\n첫 번째 예로, 한 옥타브가 12개의 음계 단계로 세분되는 12음 평균율 음계를 고려해보자. 이 음계는 반음계(chromatic scale)라고도 한다.\n이 경우 모든 음계 단계는 1반음(또는 \\(100\\) cents) 크기의 동일한 음정에 해당하며, 다음 그림과 같다(enharmonic equivalence로 두 가지 방법으로 표시). 또한 피아노 녹음과 반음계의 합성 버전(다음 코드 셀)을 들어보자.\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F08.png\", width=500))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F08_Chromatic-Scale_AudioLabs.mp3\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef generate_sinusoid_scale(pitches=[69], duration=0.5, Fs=4000, amplitude_max=0.5):\n    \"\"\"Generate synthetic sound of scale using sinusoids\n\n    Args:\n        pitches (list): List of pitchs (MIDI note numbers) (Default value = [69])\n        duration (float): Duration (seconds) (Default value = 0.5)\n        Fs (scalar): Sampling rate (Default value = 4000)\n        amplitude_max (float): Amplitude (Default value = 0.5)\n\n    Returns:\n        x (np.ndarray): Synthesized signal\n    \"\"\"\n    N = int(duration * Fs)\n    t = np.arange(0, N) / Fs\n    x = []\n    for p in pitches:\n        omega = 2 ** ((p - 69) / 12) * 440\n        x = np.append(x, np.sin(2 * np.pi * omega * t))\n    x = amplitude_max * x / np.max(x)\n    return x\n\n\nduration = 0.2\nFs = 4000\npitches = [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72]\nx = generate_sinusoid_scale(pitches=pitches, duration=duration, Fs=Fs)\nprint('Chromatic scale', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nChromatic scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#장음계-단음계-major-and-minor-scale",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#장음계-단음계-major-and-minor-scale",
    "title": "6.1. 화성의 기본 이론",
    "section": "장음계, 단음계 (Major and Minor Scale)",
    "text": "장음계, 단음계 (Major and Minor Scale)\n\n다음에서는 음계 단계를 반음 단위로 지정할 수 있는 반음계의 하위 집합인 음계만 고려한다. 음계에서 단초(1반음)는 half step, 장초(2반음)는 whole step이라고도 한다.\n화음과 마찬가지로 서양 음악 이론에서 특히 중요한 두 가지 음계 유형이 있다.\n첫 번째 음계 유형은 장음계로 알려져 있으며 7개의 음과 반복되는 옥타브로 구성된다. 장음계의 첫 번째 음은 해당 스케일의 으뜸음 (key note) 라고 한다. 으뜸음에서 시작해 장음계의 이어지는 음 간의 음정의 시퀀스는 다음과 같다(C4를 으뜸음으로 사용).\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F09a.png\", width=500))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F09a_C-Major-Scale_AudioLabs.mp3\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n으뜸음의 크로마 이름은 음계의 이름도 결정한다. 예를 들어, \\(\\mathrm{C}\\)로 시작하는 장음계를 \\(\\mathrm{C}\\)-major scale이라고 한다. 때로는 굵은 글씨의 \\(\\mathbf{C}\\)도 음계를 나타내는 약어로 사용된다.\n\\(\\mathrm{C}\\)-major scale을 순환 이동하여 다른 장음계를 얻을 수 있다.\n장음계의 음에는 으뜸음에 상대적인 위치를 지정하기 위해 음계도(scale degrees)라고도 하는 이름이 지정된다.\n으뜸음는 “tonic” 이라고도 하며 음계의 메인 음이다. 음계의 네 번째 음은 “subdominant” 라고 하고 다섯 번째 음은 “dominant” 라고 한다. 나머지 이름은 다음 그림에 나와있다.\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F09b.png\", width=500))\n\n\n\n\n\n두 번째 음계 유형은 단음계이다. 장음계와 유사하게 단음계는 7개의 음과 반복되는 옥타브로 구성된다. 그러나 이번에는 음 사이의 음정 순서는 다음과 같다(C4를 으뜸음으로 사용).\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F09b.png\", width=500))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F09c_C-Minor-Scale_AudioLabs.mp3\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n단음계와 유사한 명명 규칙을 가진 12개의 단음계가 있다. 장음계와 단음계 모두 온음계(diatonic scale) 라는 일반 용어에 포함될 수 있다. 이것은 (정의상) 각 옥타브에 대해 5개의 whole-step과 2개의 half-step이 있는 7음 음계이며, 여기서 두 개의 half-step은 두 개 또는 세 개의 whole-step으로 각각 분리된다.\n\n\nduration = 0.5\n\nx_maj = generate_sinusoid_scale(pitches=[60, 62, 64, 65, 67, 69, 71, 72], duration=duration, Fs=Fs)\nx_min = generate_sinusoid_scale(pitches=[60, 62, 63, 65, 67, 68, 70, 72], duration=duration, Fs=Fs)\n\nprint('C-major scale', flush=True)\nipd.display(ipd.Audio(data=x_maj, rate=Fs))\nprint('C-minor scale (natural)', flush=True)\nipd.display(ipd.Audio(data=x_min, rate=Fs))\n\nC-major scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nC-minor scale (natural)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#기타-음계-further-scales",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#기타-음계-further-scales",
    "title": "6.1. 화성의 기본 이론",
    "section": "기타 음계 (Further Scales)",
    "text": "기타 음계 (Further Scales)\n\n위에서 소개한 단음계(natural minor scale이라고도 함) 외에도 harmonic minor 및 melodic minor scale라는 다른 종류의 단음계들이 있다.\nharmonic minor scale의 음은 natural minor scale와 같지만 7도를 반음 올려 6도와 7도 사이에서 2도가 증가한다.\n또한 5음으로 구성된 펜타토닉 음계(pentatonic scale), 6음으로 구성된 온음 음계(whole tone scale), 8음으로 구성된 옥타토닉 음계(octatonic scale)(감음 음계) 등이 있다.\n\n\nprint('Harmonic minor scale (7 pitches + octave)', flush=True)\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_FurtherScales_HarmonicMinor.png\", width=400))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5S1_Scale-Harmonic-Minor_AudioLabs.mp3\"))\n\nprint('Pentatonic scale (5 pitches + octave)', flush=True)\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_FurtherScales_Pentatonic.png\", width=400))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5S1_Scale-Pentatonic_AudioLabs.mp3\"))\n\nprint('Whole tone scale (6 pitches + octave)', flush=True)\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_FurtherScales_WholeTone.png\", width=400))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5S1_Scale-Whole-Tone_AudioLabs.mp3\"))\n\nprint('Octatonic scale (8 pitches + octave)', flush=True)\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_FurtherScales_Octatonic.png\", width=400))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5S1_Scale-Octatonic_AudioLabs.mp3\"))\n\nHarmonic minor scale (7 pitches + octave)\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPentatonic scale (5 pitches + octave)\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWhole tone scale (6 pitches + octave)\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nOctatonic scale (8 pitches + octave)\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nduration = 0.5\n\nx_mh = generate_sinusoid_scale(pitches=[60, 62, 63, 65, 67, 68, 71, 72], duration=duration, Fs=Fs)\nx_p = generate_sinusoid_scale(pitches=[60, 62, 64, 67, 69, 72], duration=duration, Fs=Fs)\nx_w = generate_sinusoid_scale(pitches=[60, 62, 64, 66, 68, 70, 72], duration=duration, Fs=Fs)\nx_o = generate_sinusoid_scale(pitches=[60, 61, 63, 64, 66, 67, 69, 70, 72], duration=duration, Fs=Fs)\n\nprint('Harmonic minor scale (7 pitches + octave)', flush=True)\nipd.display(ipd.Audio(data=x_mh, rate=Fs))\nprint('Pentatonic scale (5 pitches + octave)', flush=True)\nipd.display(ipd.Audio(data=x_p, rate=Fs))\nprint('Whole tone scale (6 pitches + octave)', flush=True)\nipd.display(ipd.Audio(data=x_w, rate=Fs))\nprint('Octatonic scale (8 pitches + octave)', flush=True)\nipd.display(ipd.Audio(data=x_o, rate=Fs))\n\nHarmonic minor scale (7 pitches + octave)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPentatonic scale (5 pitches + octave)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWhole tone scale (6 pitches + octave)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nOctatonic scale (8 pitches + octave)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#오도권-circle-of-fifths",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#오도권-circle-of-fifths",
    "title": "6.1. 화성의 기본 이론",
    "section": "오도권 (Circle of Fifths)",
    "text": "오도권 (Circle of Fifths)\n\n온음계(diatonic scales)의 특징 중 하나는 6개의 연속적인 완전5도 음정의 체인으로부터 얻을 수 있다는 것이다.\n예를 들어, \\(\\mathrm{C}\\)-major 음계는 \\(\\mathrm{F}\\)로 시작하는 6개의 완전5도의 오름차순 체인에서 얻는다.\n\\[\\mathrm{F}-\\mathrm{C}-\\mathrm{G}-\\mathrm{D}-\\mathrm{A}-\\mathrm{E}-\\mathrm{B}\\]\n\n\nipd.Audio(\"../data_FMP/FMP_C5S1_Scale-Diatonic-Fifths.mp3\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n옥타브가 아닌 가장 협화음인 5도 음정은 음, 화음, 음계를 연결할 때 특히 중요한 역할을 한다. 5도와 관련된 모든 음은 온음계에 어느 정도 일관성과 균형을 부여하며 전체 음계를 연관시키는 것도 가능하게 한다.\n반음계의 12개 음과 관련 장음계와 단음계 사이의 관계를 시각적으로 표현한 유명한 5도권(circle of fifths)이 있다. 직관적으로 5도권은 서로 다른 음계 사이의 “음악적” 유사성의 정도를 반영한다. 두 음계가 원에 가까울수록 음조(tonal) 측면에서 더 많이 공유한다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F10.png\", width=400)\n\n\n\n\n\n다음 녹음에서 각 음계의 으뜸음(key note)은 시계 방향으로 5도의 원을 그리며 연주된다.\n\\[\\mathrm{C}-\\mathrm{G}-\\mathrm{D}-\\mathrm{A}-\\mathrm{E}-\\mathrm{B}-\\mathrm{G}^\\flat-\\mathrm{D}^\\flat-\\mathrm{A}^\\flat-\\mathrm{E}^\\flat-\\mathrm{B}^\\flat-\\mathrm{F}\\]\n\n\nipd.Audio(\"../data_FMP/FMP_C5_F10_Circle-Fifths.mp3\")\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#음조-musical-keys",
    "href": "posts/6. Chord Recognition/6.1.Basic_Theory_of_Harmony.html#음조-musical-keys",
    "title": "6.1. 화성의 기본 이론",
    "section": "음조 (Musical Keys)",
    "text": "음조 (Musical Keys)\n\n좀 더 공식적으로 5도권은 음조(key) 간의 관계를 나타낸다. 이 개념은 장음계와 단음계와 밀접하게 연결되어 있다.\n음계는 일반적으로 음조에 사용되는 정렬된 음의 집합이지만 음조는 특정 코드 진행에 의해 설정된 중심이다.\n5도권의 상단에는 \\(\\mathrm{C}\\)-major key(C장조)가 있다. 피아노 건반의 흰색 건반에 해당하는 해당 \\(\\mathrm{C}\\)-major scale의 음은 서양 음악 표기법에서 임시표(accidental)(\\(\\sharp\\), \\(\\flat\\))가 필요하지 않다. 결과적으로 \\(\\mathrm{C}\\) major의 조표에는 플랫이나 샤프가 없다.\n\\(\\mathrm{A}\\)단조는 해당 음계가 동일한 7개의 음으로 구성되며, \\(\\mathrm{C}\\)-major와 동일한 음조 시그니쳐를 공유한다.\n일반적으로 각 장조에는 해당 음계가 동일한 음을 공유하는 단조가 있다. 이 관계를 상대관계(relative relationship) 라고 한다. 위에 표시된 5도권에서 장조는 원 외부에 표시되고 해당 상대적 단조는 원 안에 표시된다.\n원의 상단에 있는 \\(\\mathrm{C}\\)장조부터 시작하여 시계 방향으로 오름차순 5도까지 다른 음조를 얻는다.\n다음 음조는 \\(\\mathrm{G}\\)장조이며 해당 음계는 \\(\\mathrm{C}\\)-major 스케일과 6개의 음을 공유한다. \\(\\mathrm{C}\\)-major의 \\(\\mathrm{F}\\)만 \\(\\mathrm{G}\\)-major의 \\(\\mathrm{F}^\\sharp\\)가 된다. 즉 \\(\\mathrm{G}\\)장조의 조표에 샤프를 도입한다.\n동일한 종류의 관계가 5도권을 따라 두 개의 후속 음조 또는 음계 사이에 유지된다. 5도를 위로 진행하면 음계의 1음이 변경되고 조표에 1개의 샤프가 추가된다. 이 과정을 평균율에서 12번 반복하는 경우, 원래의 \\(\\mathrm{C}\\)-major로 돌아가서 원을 닫게 된다.\n마찬가지로 오름차순 4도에 해당하는 내림차순 5도로 시계 반대 방향으로 원을 따라 이동할 수 있다. 4도권으로 진행하면 결과 조표에 플랫이 추가된다.\n5도권을 따라 정렬된 모든 \\(24\\) 장단음계의 합성 버전을 들어보자.\n\n\nduration = 0.25\n\nscale_major = np.array([60, 62, 64, 65, 67, 69, 71, 72])\nscale_minor = np.array([57, 59, 60, 62, 64, 65, 67, 69])\nscale_major_name = ['C','G','D','A','E','B','F$^\\sharp$',\n                    'D$^\\\\flat$','A$^\\\\flat$','E$^\\\\flat$','B$^\\\\flat$','F','C',]\nscale_minor_name = ['Am','Em','Bm','F$^\\sharp$m','C$^\\sharp$m','G$^\\sharp$m','D$^\\sharp$m',\n                    'B$^\\\\flat$m','Fm','Cm','Gm','Dm','Am',]\n\nscale_major_list = []\nfor i in range(13):\n    x = generate_sinusoid_scale(pitches=scale_major, duration=duration, Fs=Fs)\n    scale_major_list.append(x)\n    scale_major += 7\n    if scale_major[-1] > 80:\n        scale_major -= 12\n        \nscale_minor_list = []\nfor i in range(13):\n    x = generate_sinusoid_scale(pitches=scale_minor, duration=duration, Fs=Fs)\n    scale_minor_list.append(x)\n    scale_minor += 7\n    if scale_minor[-1] > 80:\n        scale_minor -= 12\n        \n        \naudio_tag_html_list_major = []\nfor i in range(13):\n    audio_tag = ipd.Audio(scale_major_list[i], rate=Fs)\n    audio_tag_html = audio_tag._repr_html_().replace('\\n', '').strip()\n    audio_tag_html = audio_tag_html.replace('<audio ', \n                                            '<audio style=\"width: 200px; height: 30px;\"')  \n    audio_tag_html_list_major.append(audio_tag_html)        \n\naudio_tag_html_list_minor = []\nfor i in range(13):\n    audio_tag = ipd.Audio(scale_minor_list[i], rate=Fs)\n    audio_tag_html = audio_tag._repr_html_().replace('\\n', '').strip()\n    audio_tag_html = audio_tag_html.replace('<audio ', \n                                            '<audio style=\"width: 200px; height: 30px;\"')  \n    audio_tag_html_list_minor.append(audio_tag_html)    \n    \npd.options.display.float_format = '{:,.1f}'.format    \npd.set_option('display.max_colwidth', None)    \ndf = pd.DataFrame(OrderedDict([\n    ('Major', scale_major_name),\n    (' ', audio_tag_html_list_major),\n    ('Minor', scale_minor_name),\n    ('  ', audio_tag_html_list_minor)]))\n\n#df.index = np.arange(0, len(df))\n#df = df.T\nipd.HTML(df.to_html(escape=False, justify='center', index=True, header=True))\n\n\n\n  \n    \n      \n      Major\n      \n      Minor\n      \n    \n  \n  \n    \n      0\n      C\n                                              Your browser does not support the audio element.                \n      Am\n                                              Your browser does not support the audio element.                \n    \n    \n      1\n      G\n                                              Your browser does not support the audio element.                \n      Em\n                                              Your browser does not support the audio element.                \n    \n    \n      2\n      D\n                                              Your browser does not support the audio element.                \n      Bm\n                                              Your browser does not support the audio element.                \n    \n    \n      3\n      A\n                                              Your browser does not support the audio element.                \n      F$^\\sharp$m\n                                              Your browser does not support the audio element.                \n    \n    \n      4\n      E\n                                              Your browser does not support the audio element.                \n      C$^\\sharp$m\n                                              Your browser does not support the audio element.                \n    \n    \n      5\n      B\n                                              Your browser does not support the audio element.                \n      G$^\\sharp$m\n                                              Your browser does not support the audio element.                \n    \n    \n      6\n      F$^\\sharp$\n                                              Your browser does not support the audio element.                \n      D$^\\sharp$m\n                                              Your browser does not support the audio element.                \n    \n    \n      7\n      D$^\\flat$\n                                              Your browser does not support the audio element.                \n      B$^\\flat$m\n                                              Your browser does not support the audio element.                \n    \n    \n      8\n      A$^\\flat$\n                                              Your browser does not support the audio element.                \n      Fm\n                                              Your browser does not support the audio element.                \n    \n    \n      9\n      E$^\\flat$\n                                              Your browser does not support the audio element.                \n      Cm\n                                              Your browser does not support the audio element.                \n    \n    \n      10\n      B$^\\flat$\n                                              Your browser does not support the audio element.                \n      Gm\n                                              Your browser does not support the audio element.                \n    \n    \n      11\n      F\n                                              Your browser does not support the audio element.                \n      Dm\n                                              Your browser does not support the audio element.                \n    \n    \n      12\n      C\n                                              Your browser does not support the audio element.                \n      Am\n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S1_Intervals.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S1_Chords.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S1_Scales_CircleFifth.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "",
    "text": "크로마 기반 특징 표현과 템플릿 기반 패턴 매칭을 통한 화음 인식 방법을 소개하고, 화음 인식 결과를 평가하는 방법에 대해 설명한다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#크로마-기반-특징-표현-chroma-based-feature-representation",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#크로마-기반-특징-표현-chroma-based-feature-representation",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "크로마-기반 특징 표현 (Chroma-Based Feature Representation)",
    "text": "크로마-기반 특징 표현 (Chroma-Based Feature Representation)\n\n음악의 오디오 녹음이 주어지면 첫 번째 단계는 녹음을 특징 벡터 \\(x_n\\in\\mathcal{F}\\), \\(n\\in[1:N]\\)의 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)로 바꾸는 것이다.\n위에서 언급한 것처럼 거의 모든 전통적인 화음 인식 절차는 크로마 기반 특징 표현 유형에 의존한다.\n이는 크로마 기반 특징이 신호의 단시간의 톤 내용을 캡처하기 때문이며, 이는 기본 곡의 화성 진행과 밀접하게 관련되어 있다.\n평균율 음계를 가정하면 크로마 값은 집합 \\(\\{\\mathrm{C},\\mathrm{C}^\\sharp,\\mathrm{D},\\ldots,\\mathrm{B}\\}\\)에 해당하며 집합 \\([0:11]\\)로 식별한다.\n크로마 특징은 \\(12\\)차원 벡터로 표현될 수 있다. \\[x=(x(0),x(1),\\ldots,x(11))^\\top\\in\\mathcal{F}=\\mathbb{R}^{12}\\]\n크로마 특징을 계산하는 여러 많은 방법이 있다. 또한 그 속성은 적당한 후처리(postprocessing) 과정(예: 로그 컴프레션, 정규화, 스무딩)을 통해 조정될 수 있다.\n다음에서 세개의 크로마 특징 방법을 보기로 한다.\n\nSTFT-based chroma features (librosa.feature.chroma_stft).\nFilter-bank decomposition using IIR elliptic filters (librosa.iirt), logarithmic compression, and chroma binning.\nCQT-based chroma features (librosa.feature.chroma_cqt)\n\n각 변형에 대해 동일한 윈도우(window) 길이(N=4096)와 홉(hop) 크기(H=2048)를 사용한다. 또한 각 변형에서 유클리드 norm (\\(\\ell^2\\)-norm)에 대해 크로마 벡터를 정규화한다.\n\n\ndef compute_chromagram_from_filename(fn_wav, Fs=22050, N=4096, H=2048, gamma=None, version='STFT', norm='2'):\n    \"\"\"Compute chromagram for WAV file specified by filename\n\n    Args:\n        fn_wav (str): Filenname of WAV\n        Fs (scalar): Sampling rate (Default value = 22050)\n        N (int): Window size (Default value = 4096)\n        H (int): Hop size (Default value = 2048)\n        gamma (float): Constant for logarithmic compression (Default value = None)\n        version (str): Technique used for front-end decomposition ('STFT', 'IIS', 'CQT') (Default value = 'STFT')\n        norm (str): If not 'None', chroma vectors are normalized by norm as specified ('1', '2', 'max')\n            (Default value = '2')\n\n    Returns:\n        X (np.ndarray): Chromagram\n        Fs_X (scalar): Feature reate of chromagram\n        x (np.ndarray): Audio signal\n        Fs (scalar): Sampling rate of audio signal\n        x_dur (float): Duration (seconds) of audio signal\n    \"\"\"\n    x, Fs = librosa.load(fn_wav, sr=Fs)\n    x_dur = x.shape[0] / Fs\n    if version == 'STFT':\n        # Compute chroma features with STFT\n        X = librosa.stft(x, n_fft=N, hop_length=H, pad_mode='constant', center=True)\n        if gamma is not None:\n            X = np.log(1 + gamma * np.abs(X) ** 2)\n        else:\n            X = np.abs(X) ** 2\n        X = librosa.feature.chroma_stft(S=X, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\n    if version == 'CQT':\n        # Compute chroma features with CQT decomposition\n        X = librosa.feature.chroma_cqt(y=x, sr=Fs, hop_length=H, norm=None)\n    if version == 'IIR':\n        # Compute chroma features with filter bank (using IIR elliptic filter)\n        X = librosa.iirt(y=x, sr=Fs, win_length=N, hop_length=H, center=True, tuning=0.0)\n        if gamma is not None:\n            X = np.log(1.0 + gamma * X)\n        X = librosa.feature.chroma_cqt(C=X, bins_per_octave=12, n_octaves=7,\n                                       fmin=librosa.midi_to_hz(24), norm=None)\n    if norm is not None:\n        X = normalize_feature_sequence(X, norm='2')\n    Fs_X = Fs / H\n    return X, Fs_X, x, Fs, x_dur\n\n\ndef plot_chromagram_annotation(ax, X, Fs_X, ann, color_ann, x_dur, cmap='gray_r', title=''):\n    \"\"\"Plot chromagram and annotation\n\n    Args:\n        ax: Axes handle\n        X: Feature representation\n        Fs_X: Feature rate\n        ann: Annotations\n        color_ann: Color for annotations\n        x_dur: Duration of feature representation\n        cmap: Color map for imshow (Default value = 'gray_r')\n        title: Title for figure (Default value = '')\n    \"\"\"\n    plot_chromagram(X, Fs=Fs_X, ax=ax,\n                             chroma_yticks=[0, 2, 4, 7, 9, 11], clim=[0, 1], cmap=cmap,\n                             title=title, ylabel='Chroma', colorbar=True)\n    plot_segments_overlay(ann, ax=ax[0], time_max=x_dur,\n                                   print_labels=False, colors=color_ann, alpha=0.1)\n\n\n# Compute chroma features\nfn_wav = '../data_FMP/FMP_C5_F01_Beatles_LetItBe-mm1-4_Original.wav'\nN = 4096\nH = 2048\nX_STFT, Fs_X, x, Fs, x_dur = compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=0.1, version='STFT')\nX_IIR, Fs_X, x, Fs, x_dur = compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=100, version='IIR')\nX_CQT, Fs_X, x, Fs, x_dur = compute_chromagram_from_filename(fn_wav, N=N, H=H, version='CQT')\n\n# Annotations\nfn_ann = '../data_FMP/FMP_C5_F01_Beatles_LetItBe-mm1-4_Original_Chords_simplified.csv'\nann, _ = read_structure_annotation(fn_ann)\ncolor_ann = {'N': [1, 1, 1, 1], 'C': [1, 0.5, 0, 1], 'G': [0, 1, 0, 1], \n             'Am': [1, 0, 0, 1], 'F': [0, 0, 1, 1]}\n\n\n# Plot\ncmap = compressed_gray_cmap(alpha=1, reverse=False)\nfig, ax = plt.subplots(5, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1, 2, 2, 2, 0.5]}, figsize=(7, 8))\nplot_signal(x, Fs, ax=ax[0,0], title='Waveform of audio signal')\nplot_segments_overlay(ann, ax=ax[0,0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\nax[0,1].axis('off')\n\ntitle = 'STFT-based chromagram (feature rate = %0.1f Hz, N = %d)'%(Fs_X, X_STFT.shape[1])\nplot_chromagram_annotation([ax[1, 0], ax[1, 1]], X_STFT, Fs_X, ann, color_ann, x_dur, title=title)\n\ntitle = 'IIR-based chromagram (feature rate = %0.1f Hz, N = %d)'%(Fs_X, X_IIR.shape[1])\nplot_chromagram_annotation([ax[2, 0], ax[2, 1]], X_IIR, Fs_X, ann, color_ann, x_dur, title=title)\n\ntitle = 'CQT-based chromagram (feature rate = %0.1f Hz, N = %d)'%(Fs_X, X_CQT.shape[1])\nplot_chromagram_annotation([ax[3, 0], ax[3, 1]], X_CQT, Fs_X, ann, color_ann, x_dur, title=title)\n\nplot_segments(ann, ax=ax[4, 0], time_max=x_dur, time_label='Time (seconds)',\n              colors=color_ann,  alpha=0.3)\nax[4,1].axis('off')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#템플릿-기반-패턴-매칭-template-based-pattern-matching",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#템플릿-기반-패턴-매칭-template-based-pattern-matching",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "템플릿-기반 패턴 매칭 (Template-Based Pattern Matching)",
    "text": "템플릿-기반 패턴 매칭 (Template-Based Pattern Matching)\n\n크로마 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 가능한 화음 라벨(chord label)의 집합 \\(\\Lambda\\)가 주어지면, 다음 단계의 목적은 각 크로마 벡터 \\(x_n\\in\\mathbb{R}^{12}\\)을 화음 라벨 \\(\\lambda_{n} \\in \\Lambda\\), \\(n\\in[1:N]\\)에 매핑하는 것이다.\n예를 들어 12개의 장3화음과 12개의 단3화음으로 구성된 집합을 생각할 수 있다. \\[\\Lambda = \\{\\mathbf{C},\\mathbf{C}^\\sharp,\\ldots,\\mathbf{B},\\mathbf{Cm},\\mathbf{C^\\sharp m},\\ldots,\\mathbf{Bm}\\}\\]\n이 경우 각 프레임 \\(n\\in[1:N]\\)은 \\(\\lambda_{n}\\)로 지정된 장화음 또는 단화음에 할당된다. 패턴 매칭 단계를 위해 이제 간단한 템플릿 기반 접근 방식을 도입해보자.\n집합 \\(\\mathcal{T}\\subset\\mathcal{F}=\\mathbb{R}^{12}\\)를 미리 계산한다. (\\(\\mathbf{t}_\\lambda\\in\\mathcal{T}\\), \\(\\lambda\\in\\Lambda\\))\n직관적으로 각 템플릿은 특정 화음을 나타내는 원형(prototypical) 크로마 벡터로 생각할 수 있다.\n또한 서로 다른 크로마 벡터를 비교할 수 있도록 다음의 유사성 측정을 고정한다. \\[s:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\]\n그 다음 해당 템플릿과 주어진 특징 벡터 \\(x_n\\) 사이의 유사성을 최대화하는 코드 라벨을 할당한다. \\[\\lambda_{n} := \\underset{\\lambda \\in \\Lambda}{\\mathrm{argmax}}\n       \\,\\, s( \\mathbf{t}_\\lambda , x_n )\\]\n이 절차에는 화음 인식의 성능에 결정적으로 영향을 미치는 많은 선택 사항이 있다.\n\n\\(\\mathcal{T}\\)에서 어떤 화음을 고려해야 하는가?\n화음 템플릿은 어떻게 정의되는가?\n특징 벡터와 화음 템플릿을 비교하기 위한 적절한 유사도 측정은 무엇인가?\n\n첫 번째 간단한 화음 인식 시스템을 얻기 위해 다음과 같은 설계 선택을 한다.\n화음 라벨의 집합 \\(\\Lambda\\)에 대해 12개의 장3화음과 12개의 단3화음을 선택한다. 이 선택은 음악적 관점에서 문제가 있을 수 있으나 편리하고 유익하다. 각 3화음은 \\([0:11]\\)의 3개 요소 하위 집합으로 인코딩될 수 있다.\n예를 들어, \\(\\mathrm{C}\\)-major 화음 \\(\\mathbf{C}\\)는 하위집합 \\(\\{0,4,7\\}\\)에 해당한다. 차례로 각 하위 집합은 이진 12차원 크로마 벡터 \\(x=(x(0),x(1),\\ldots,x(11))^\\top\\)로 식별할 수 있다. (여기서 \\(i\\in[0:11]\\)이 화음에 포함하면 \\(x(i) =1\\); iff)\n\\(\\mathrm{C}\\)-major 화음 \\(\\mathbf{C}\\)의 경우 결과 크로마 벡터는 다음과 같다. \\[\\label{eq:ChordReco:Template:Basic:ChromaVectC}\n   \\mathbf{t}_{\\mathbf{C}}{} := x =(1,0,0,0,1,0,0,1,0,0,0,0)^\\top\\]\n크로마 기반 인코딩을 사용하여 \\(\\mathrm{C}\\)-major 및 \\(\\mathrm{C}\\)-minor 3화음에 대한 이진 벡터를 각각 순환 이동(cyclically shitfting)하여 12개의 장화음 및 12개의 단화음을 얻을 수 있다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F06.png\", width=800)\n\n\n\n\n\n크로마 특징과 화음 템플릿을 비교하기 위해 정규화된 벡터의 내적을 사용한 다음과 같은 간단한 유사성 측정을 사용한다. \\[s(x,y)= \\frac{\\langle x,y\\rangle}{\\|x\\|\\cdot\\|y\\|}\\] for \\(x,y\\in\\mathcal{F}\\) with \\(\\|x\\|\\not= 0\\) and \\(\\|y\\|\\not= 0\\)\n\\(\\|x\\|=0\\) 또는 \\(\\|y\\|=0\\)인 경우, \\(s(x,y)=0\\)로 설정한다. 이 측정값은 항상 \\(s(x,y)\\in[-1,1]\\) 값을 산출하며, 벡터 \\(x\\) 및 \\(y\\)에 양수 항목만 있는 경우 하나는 \\(s(x,y)\\in[0,1]\\)이다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#구현",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#구현",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "구현",
    "text": "구현\n\n다음 코드 셀에서 앞에서 설명한 템플릿 기반 화음 인식 절차를 구현한다.\n\n먼저 오디오 녹음이 크로마 표현으로 변환된다. 예를 들어 STFT 변형을 사용한다.\n둘째, 각 크로마 벡터는 각 \\(24\\) 이진 화음 템플릿과 비교되어 프레임당 \\(24\\) 유사성 값을 생성한다. 이러한 유사성 값은 시간-화음(time-chord) 표현의 형태로 시각화된다.\n셋째, 모든 \\(24\\)개 화음 템플릿에 대해 유사성 값을 최대화하는 템플릿의 화음 라벨 \\(\\lambda_{n}\\)를 각 프레임에 대해 선택한다. 이것은 이진 시간-화음 표현의 형태로 표시되는 최종 화음 인식 결과를 산출한다.\n넷째, 수동으로 생성된 화음 주석(annotation)이 시각화된다.\n\n\n\ndef get_chord_labels(ext_minor='m', nonchord=False):\n    \"\"\"Generate chord labels for major and minor triads (and possibly nonchord label)\n\n    Args:\n        ext_minor (str): Extension for minor chords (Default value = 'm')\n        nonchord (bool): If \"True\" then add nonchord label (Default value = False)\n\n    Returns:\n        chord_labels (list): List of chord labels\n    \"\"\"\n    chroma_labels = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n    chord_labels_maj = chroma_labels\n    chord_labels_min = [s + ext_minor for s in chroma_labels]\n    chord_labels = chord_labels_maj + chord_labels_min\n    if nonchord is True:\n        chord_labels = chord_labels + ['N']\n    return chord_labels\n\n\ndef generate_chord_templates(nonchord=False):\n    \"\"\"Generate chord templates of major and minor triads (and possibly nonchord)\n\n    Args:\n        nonchord (bool): If \"True\" then add nonchord template (Default value = False)\n\n    Returns:\n        chord_templates (np.ndarray): Matrix containing chord_templates as columns\n    \"\"\"\n    template_cmaj = np.array([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]).T\n    template_cmin = np.array([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]).T\n    num_chord = 24\n    if nonchord:\n        num_chord = 25\n    chord_templates = np.ones((12, num_chord))\n    for shift in range(12):\n        chord_templates[:, shift] = np.roll(template_cmaj, shift)\n        chord_templates[:, shift+12] = np.roll(template_cmin, shift)\n    return chord_templates\n\n\ndef chord_recognition_template(X, norm_sim='1', nonchord=False):\n    \"\"\"Conducts template-based chord recognition\n    with major and minor triads (and possibly nonchord)\n\n    Args:\n        X (np.ndarray): Chromagram\n        norm_sim (str): Specifies norm used for normalizing chord similarity matrix (Default value = '1')\n        nonchord (bool): If \"True\" then add nonchord template (Default value = False)\n\n    Returns:\n        chord_sim (np.ndarray): Chord similarity matrix\n        chord_max (np.ndarray): Binarized chord similarity matrix only containing maximizing chord\n    \"\"\"\n    chord_templates = generate_chord_templates(nonchord=nonchord)\n    X_norm = normalize_feature_sequence(X, norm='2')\n    chord_templates_norm = normalize_feature_sequence(chord_templates, norm='2')\n    chord_sim = np.matmul(chord_templates_norm.T, X_norm)\n    if norm_sim is not None:\n        chord_sim = normalize_feature_sequence(chord_sim, norm=norm_sim)\n    # chord_max = (chord_sim == chord_sim.max(axis=0)).astype(int)\n    chord_max_index = np.argmax(chord_sim, axis=0)\n    chord_max = np.zeros(chord_sim.shape).astype(np.int32)\n    for n in range(chord_sim.shape[1]):\n        chord_max[chord_max_index[n], n] = 1\n\n    return chord_sim, chord_max\n\n\n# Chord recognition\nX = X_STFT\nchord_sim, chord_max = chord_recognition_template(X, norm_sim='max')\nchord_labels = get_chord_labels(nonchord=False)\n\n\n# Plot\ncmap = compressed_gray_cmap(alpha=1, reverse=False)\nfig, ax = plt.subplots(4, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1.5, 3, 3, 0.3]}, figsize=(8, 10))\nplot_chromagram(X, ax=[ax[0,0], ax[0,1]], Fs=Fs_X, clim=[0, 1], xlabel='',\n                         title='STFT-based chromagram (feature rate = %0.1f Hz)' % (Fs_X))\nplot_segments_overlay(ann, ax=ax[0,0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_matrix(chord_sim, ax=[ax[1, 0], ax[1, 1]], Fs=Fs_X, \n                     title='Time–chord representation of chord similarity matrix',\n                     ylabel='Chord', xlabel='')\nax[1, 0].set_yticks(np.arange( len(chord_labels) ))\nax[1, 0].set_yticklabels(chord_labels)\nplot_segments_overlay(ann, ax=ax[1, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_matrix(chord_max, ax=[ax[2, 0], ax[2, 1]], Fs=Fs_X, \n                     title='Time–chord representation of chord recognition result',\n                     ylabel='Chord', xlabel='')\nax[2, 0].set_yticks(np.arange( len(chord_labels) ))\nax[2, 0].set_yticklabels(chord_labels)\nax[2, 0].grid()\nplot_segments_overlay(ann, ax=ax[2, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_segments(ann, ax=ax[3, 0], time_max=x_dur, time_label='Time (seconds)',\n                       colors=color_ann,  alpha=0.3)\nax[3, 1].axis('off')\nplt.tight_layout()\n\n\n\n\n\n시간-화음 표현 형태로 표시된 화음 유사도 값은 각 크로마 벡터에 대해 가능한 \\(24\\)개 화음에 대한 일종의 우도(likelihood)를 나타낸다.\n예를 들어 노래의 시작 부분에 있는 크로마 벡터가 \\(\\mathrm{C}\\)-major 화음 \\(\\mathbf{C}\\)의 템플릿과 가장 유사하다는 것을 보여준다. 또한 \\(\\mathbf{C}\\), \\(\\mathbf{Em}\\) 및 \\(\\mathbf{Am}\\)에 대한 템플릿과의 유사성도 더 높다.\n최종 화음 인식 결과를 참조 주석과 비교하면 자동화된 절차에서 얻은 결과가 대부분의 프레임에 대한 참조 라벨과 일치함을 관찰할 수 있다.\n마지막으로 각 프레임에 대한 유사성-최대화 화음 템플릿을 살펴본다. 이렇게 하면 원래 입력 크로마그램과 비교할 수 있는 시간-크로마 표현이 생성된다. 어떤 면에서는 화음 템플릿의 시퀀스는 입력 크로마 표현의 음악적 정보를 바탕으로 한 양자화(quantization)로 생각할 수 있다.\n\n\nchord_templates = generate_chord_templates() \nX_chord = np.matmul(chord_templates, chord_max)\n\n# Plot\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1, 1, 0.2]}, figsize=(8, 5))\n\nplot_chromagram(X, ax=[ax[0, 0], ax[0, 1]], Fs=Fs_X, clim=[0, 1], xlabel='',\n                         title='STFT-based chromagram (feature rate = %0.1f Hz)' % (Fs_X))\nplot_segments_overlay(ann, ax=ax[0, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_chromagram(X_chord, ax=[ax[1, 0], ax[1, 1]], Fs=Fs_X, clim=[0, 1], xlabel='',\n                         title='Binary templates of the chord recognition result')\nplot_segments_overlay(ann, ax=ax[1, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_segments(ann, ax=ax[2, 0], time_max=x_dur, time_label='Time (seconds)',\n                       colors=color_ann,  alpha=0.3)\nax[2,1].axis('off')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#accuracy-precision-recall-f-measure",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#accuracy-precision-recall-f-measure",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "Accuracy, Precision, Recall, F-Measure",
    "text": "Accuracy, Precision, Recall, F-Measure\n\n크로마 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\)와 가능한 화음 라벨 집합 \\(\\Lambda := \\{\\mathbf{C},\\mathbf{C}^\\sharp,\\ldots,\\mathbf{B}, \\mathbf{Cm}, \\mathbf{Cm^\\sharp}, \\ldots,\\mathbf{Bm}\\}\\)가 주어졌을 때, 템플릿-기반 화음 잇기의 결과물은 각 프레임 \\(n\\in[1:N]\\)에 대한 화음 라벨 \\(\\lambda_{n}\\in\\Lambda\\)이었다. 또한 프레임 방식 \\(\\lambda^\\mathrm{Ref}_{n}\\in\\Lambda\\) for \\(n\\in[1:N]\\)로도 지정되는 참조 주석이 제공된다고 가정한다.\n그러나 모든 프레임에 주석을 달아야 한다고 가정할 필요는 없다. 예를 들어, 녹음이 묵음으로 시작하거나 박수로 끝나는 경우 이러한 구간에 대한 의미 있는 화음 주석이 없으며 해당 프레임은 평가에서 고려되지 않은 상태로 두어야 한다.\n이를 모델링하기 위해 비화음 라벨(non-chord label)을 나타내는 추가 기호 \\(\\mathbf{N}\\)로 라벨 집합을 확장한다. 따라서 새 라벨의 집합에는 \\(25\\)개 원소가 있으며 다음과 같이 정의된다. \\[\\Lambda':= \\Lambda \\cup  \\{\\mathbf{N}\\}\\]\n다음에서 참조 주석은 라벨 \\(\\lambda^\\mathrm{Ref}_{n}\\in\\Lambda'\\) for \\(n\\in[1:N]\\)로 주어진다. 또한, \\(\\lambda_{n}\\in\\Lambda'\\) for \\(n\\in[1:N]\\)를 가정한다. 즉, 비화음 라벨도 출력할 수 있다(예: 모든 유사성이 크로마 프레임과 템플릿 사이의 값이 특정 임계값 미만).\n그런 다음 계산된 라벨 \\(\\lambda_{n}\\)을 참조 라벨 \\(\\lambda^\\mathrm{Ref}_{n}\\)와 비교하여 프레임 단위로 평가할 수 있다. 주어진 프레임 \\(\\lambda_{n}=\\lambda^\\mathrm{Ref}_{n}\\) for \\(n\\in[1:N]\\)인 경우 화음 인식 접근 방식에 의해 예측된 라벨이 “맞는 것”이라고 한다. 그렇지 않으면 “틀린 것”으로 간주된다.\n정확도(accuracy) \\(\\mathrm{A}\\)는 올바르게 예측된 라벨의 비율로 정의된다. \\[\\mathrm{A} = \\frac{\\big|\\{n\\in[1:N]: \\lambda_{n}=\\lambda^\\mathrm{Ref}_{n} \\} \\big|}{N}\\]\n정확도의 대안을 보면,\n\n먼저 아이템 집합을 \\(\\mathcal{I}=[1:N]\\times \\Lambda\\)로 정의한다. 비화음 라벨 \\(\\mathbf{N}\\)은 고려되지 않는다. 그러면 \\(\\mathcal{I}^\\mathrm{Ref}_+:=\\big\\{(n,\\lambda^\\mathrm{Ref}_{n})\\in\\mathcal{I} : n\\in[1:N]\\big\\}\\)은 positive (or relevant items)이고, \\(\\mathcal{I}^\\mathrm{Est}_+:=\\big\\{(n,\\lambda_{n})\\in\\mathcal{I} : n\\in[1:N]\\big\\}\\)은 positive로 추정(estimated) (or retrieved items) 된 아이템이다.\n라벨이 맞는 것(\\(\\lambda_{n}=\\lambda^\\mathrm{Ref}_{n}\\))이면 아이템 \\((n,\\lambda_{n})\\)은 true positive (TP),\n틀린 것이면 \\((n,\\lambda_{n})\\)이 false positive (FP), \\((n,\\lambda^\\mathrm{Ref}_{n})\\)은 false negative(FN)라고 한다.\n나머지는 true negative(TN)이다.\n그리고 precision, recall, F-measure는 다음과 같다 (이는 5.5. 음악 처리의 평가 방법에서 자세히 다루었으니 참고하길 바람) \\[\\mathrm{P} = \\frac{\\#\\mathrm{TP}}{\\#\\mathrm{TP}+\\#\\mathrm{FP}}, \\quad\n\\mathrm{R} = \\frac{\\#\\mathrm{TP}}{\\#\\mathrm{TP}+\\#\\mathrm{FN}}, \\quad\n\\mathrm{F} = \\frac{2\\cdot \\mathrm{P}\\cdot \\mathrm{R}}{\\mathrm{P} + \\mathrm{R}}\\]\n\n위에서 보았던 비틀즈의 “Let It Be”의 템플릿 기반 화음 인식을 계속해서 보자. 다음 그림은 음악 전문가가 제공하는 두 가지 다른 화음 주석과 함께 악보 표현을 보여준다. 첫 번째 주석은 반음 수준(2분 음표마다)에 지정되며 \\(24\\)개 장조 및 단조 3화음만 사용된다. 두 번째 주석은 더 미세한 시간 수준에서 지정될 뿐만 아니라 7도화음을 포함하여 더 많은 화음 유형을 허용한다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F16_score.png\", width=800)\n\n\n\n\n\n다음에서는 우선 첫 번째의 단순한 주석을 참조로 사용한다.\n먼저 음악 녹음을 크로마 벡터의 시퀀스로 변환한다. 그런 다음 템플릿 기반 화음 인식 접근 방식을 적용하여 각 시간 프레임에 대한 화음 라벨을 추정한다. 결과는 이진 시간-화음 표현의 형태로 시각화된다.\n\n\n# Compute chromagram\nN = 4096\nH = 2048\nX, Fs_X, x, Fs, x_dur = compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=0.1, version='STFT')\n#X, Fs_X, x, Fs, x_dur = libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=100, version='IIR')\n#X, Fs_X, x, Fs, x_dur = libfmp.c5.compute_chromagram_from_filename(fn_wav, N=N, H=H, version='CQT')\n\n# Chord recognition\nchord_sim, chord_max = chord_recognition_template(X, norm_sim='max')\nchord_labels = get_chord_labels()\n\n#Plot\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1, 2]}, figsize=(9, 6))\n\ntitle = 'Chromagram (N = %d)' % X.shape[1]\nplot_chromagram(X, Fs=1, ax=[ax[0, 0], ax[0, 1]],\n                         chroma_yticks = [0, 4, 7, 11], clim=[0, 1], cmap='gray_r',\n                         title=title, ylabel='Chroma', colorbar=True)\n\ntitle = 'Time–chord representation of chord recognition result (N = %d)' % X.shape[1]\nplot_matrix(chord_max, ax=[ax[1, 0], ax[1, 1]], Fs=1, \n                     title=title, ylabel='Chord', xlabel='Time (frames)')\nax[1, 0].set_yticks(np.arange(len(chord_labels)))\nax[1, 0].set_yticklabels(chord_labels)\nax[1, 0].grid()\nplt.tight_layout()\n\n\n\n\n\n다음으로 평가에 필요한 참조 주석이 포함된 주석 파일을 사용한다. 시나리오에서 이러한 주석은 라벨이 지정된 세그먼트의 형태로 제공되며, 각각은 시작 및 종료 시간이 \\(s,t\\in\\mathbb{R}\\)(초)및 라벨 \\(\\lambda\\)인 \\((s,t,\\lambda)\\)로 지정된다.\n프레임별 평가 측정을 적용하려면 몇 가지 준비가 필요하다.\n\n먼저 세그먼트 기반 주석을 크로마 시퀀스에 사용되는 피쳐 레이트에 맞게 조정된 프레임 기반 라벨 시퀀스로 변환해야 한다. 샘플링에 의해 도입된 시간 그리드가 세그먼트 경계와 일치하지 않을 수 있으므로 이 단계는 trivial하지 않다.\n두 번째로, 주석 파일에 사용된 라벨을 자동화된 접근 방식에 사용된 라벨 규칙과 일치하도록 변환해야 할 수 있다. 이 단계는 종종 문제가 되는데, 특히 참조 주석이 다른 sementic level에서 제공될 때(예: \\(24\\) 이상의 장,단3화음 사용) 그렇다.\n\n다음 코드 셀에서 원본 주석 파일을 읽고 이를 다양한 형식으로 변환하는 기능을 제공한다.\n\n세그먼트 기반 주석 (초 단위)\n세그먼트 기반 주석 (인덱스 단위)\n라벨 시퀀스 (프레임 레벨)\n세그먼트 기반 주석으로 라벨 시퀀스 인코딩 (인덱스 단위)\n이진 시간-화음 표현으로 라벨 시퀀스 인코딩\n\n\n\ndef convert_chord_label(ann):\n    \"\"\"Replace for segment-based annotation in each chord label the string ':min' by 'm'\n    and convert flat chords into sharp chords using enharmonic equivalence\n\n    Args:\n        ann (list): Segment-based annotation with chord labels\n\n    Returns:\n        ann_conv (list): Converted segment-based annotation with chord labels\n    \"\"\"\n    ann_conv = copy.deepcopy(ann)\n\n    for k in range(len(ann)):\n        ann_conv[k][2] = ann_conv[k][2].replace(':min', 'm')\n        ann_conv[k][2] = ann_conv[k][2].replace('Db', 'C#')\n        ann_conv[k][2] = ann_conv[k][2].replace('Eb', 'D#')\n        ann_conv[k][2] = ann_conv[k][2].replace('Gb', 'F#')\n        ann_conv[k][2] = ann_conv[k][2].replace('Ab', 'G#')\n        ann_conv[k][2] = ann_conv[k][2].replace('Bb', 'A#')\n    return ann_conv\n\n\ndef convert_sequence_ann(seq, Fs=1):\n    \"\"\"Convert label sequence into segment-based annotation\n\n    Args:\n        seq (list): Label sequence\n        Fs (scalar): Feature rate (Default value = 1)\n\n    Returns:\n        ann (list): Segment-based annotation for label sequence\n    \"\"\"\n    ann = []\n    for m in range(len(seq)):\n        ann.append([(m-0.5) / Fs, (m+0.5) / Fs, seq[m]])\n    return ann\n\n\ndef convert_chord_ann_matrix(fn_ann, chord_labels, Fs=1, N=None, last=False):\n    \"\"\"Convert segment-based chord annotation into various formats\n\n    Args:\n        fn_ann (str): Filename of segment-based chord annotation\n        chord_labels (list): List of chord labels\n        Fs (scalar): Feature rate (Default value = 1)\n        N (int): Number of frames to be generated (by cutting or extending).\n            Only enforced for ann_matrix, ann_frame, ann_seg_frame (Default value = None)\n        last (bool): If 'True' uses for extension last chord label, otherwise uses nonchord label 'N'\n            (Default value = False)\n\n    Returns:\n        ann_matrix (np.ndarray): Encoding of label sequence in form of a binary time-chord representation\n        ann_frame (list): Label sequence (specified on the frame level)\n        ann_seg_frame (list): Encoding of label sequence as segment-based annotation (given in indices)\n        ann_seg_ind (list): Segment-based annotation with segments (given in indices)\n        ann_seg_sec (list): Segment-based annotation with segments (given in seconds)\n    \"\"\"\n    ann_seg_sec, _ = read_structure_annotation(fn_ann)\n    ann_seg_sec = convert_chord_label(ann_seg_sec)\n    ann_seg_ind, _ = read_structure_annotation(fn_ann, Fs=Fs, index=True)\n    ann_seg_ind = convert_chord_label(ann_seg_ind)\n\n    ann_frame = convert_ann_to_seq_label(ann_seg_ind)\n    if N is None:\n        N = len(ann_frame)\n    if N < len(ann_frame):\n        ann_frame = ann_frame[:N]\n    if N > len(ann_frame):\n        if last:\n            pad_symbol = ann_frame[-1]\n        else:\n            pad_symbol = 'N'\n        ann_frame = ann_frame + [pad_symbol] * (N-len(ann_frame))\n    ann_seg_frame = convert_sequence_ann(ann_frame, Fs=1)\n\n    num_chords = len(chord_labels)\n    ann_matrix = np.zeros((num_chords, N))\n    for n in range(N):\n        label = ann_frame[n]\n        # Generates a one-entry only for labels that are contained in \"chord_labels\"\n        if label in chord_labels:\n            label_index = chord_labels.index(label)\n            ann_matrix[label_index, n] = 1\n    return ann_matrix, ann_frame, ann_seg_frame, ann_seg_ind, ann_seg_sec\n\n\n# Annotations\nchord_labels = get_chord_labels(ext_minor='m', nonchord=False)\nN_X = X.shape[1]\nann_matrix, ann_frame, ann_seg_frame, ann_seg_ind, ann_seg_sec = convert_chord_ann_matrix(fn_ann, chord_labels, \n                                                                           Fs=Fs_X, N=N_X, last=True)\n\ncolor_ann = {'C': [1, 0.5, 0, 1], 'G': [0, 1, 0, 1], \n             'Am': [1, 0, 0, 1], 'F': [0, 0, 1, 1], 'N': [1, 1, 1, 1]}\n\n\n# Plot\ncmap = compressed_gray_cmap(alpha=1, reverse=False)\nfig, ax = plt.subplots(4, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [4, 0.3, 0.3, 0.3]}, \n                       figsize=(9, 6))\n\nplot_matrix(ann_matrix, ax=[ax[0, 0], ax[0, 1]], Fs=1, \n                     title='Time–chord representation of reference annotations (N=%d)' % ann_matrix.shape[1],\n                     ylabel='Chord', xlabel='')\nax[0, 0].set_yticks(np.arange( len(chord_labels) ))\nax[0, 0].set_yticklabels(chord_labels)\nplot_segments_overlay(ann_seg_frame, ax=ax[0, 0], \n                               print_labels=False, colors=color_ann, alpha=0.1)\nax[0, 0].grid()\nplot_segments(ann_seg_ind, ax=ax[1, 0], time_label='Time (frames)', time_max=N_X,\n                       colors=color_ann,  alpha=0.3)\nax[1, 1].axis('off')\nplot_segments(ann_seg_frame, ax=ax[2, 0], time_label='Time (frames)', \n                       colors=color_ann,  alpha=0.3, print_labels=False)\nax[2, 1].axis('off')\nplot_segments(ann_seg_sec, ax=ax[3, 0], time_max=x_dur, time_label='Time (seconds)',\n                       colors=color_ann,  alpha=0.3)\nax[3, 1].axis('off')\nplt.tight_layout()\n\n\n\n\n\n이제 평가 지표를 보자. 시간-화음 그리드(집합 \\(\\mathcal{I}\\)에 해당)를 기반으로 시각화는 TP-, FP- 및 FN-항목을 색상 화음 형식으로 표시한다.\n\n\ndef compute_eval_measures(I_ref, I_est):\n    \"\"\"Compute evaluation measures including precision, recall, and F-measure\n\n    Args:\n        I_ref (np.ndarray): Reference set of items\n        I_est (np.ndarray): Set of estimated items\n\n    Returns:\n        P (float): Precision\n        R (float): Recall\n        F (float): F-measure\n        num_TP (int): Number of true positives\n        num_FN (int): Number of false negatives\n        num_FP (int): Number of false positives\n    \"\"\"\n    assert I_ref.shape == I_est.shape, \"Dimension of input matrices must agree\"\n    TP = np.sum(np.logical_and(I_ref, I_est))\n    FP = np.sum(I_est > 0, axis=None) - TP\n    FN = np.sum(I_ref > 0, axis=None) - TP\n    P = 0\n    R = 0\n    F = 0\n    if TP > 0:\n        P = TP / (TP + FP)\n        R = TP / (TP + FN)\n        F = 2 * P * R / (P + R)\n    return P, R, F, TP, FP, FN\n\n\ndef plot_matrix_chord_eval(I_ref, I_est, Fs=1, xlabel='Time (seconds)', ylabel='Chord',\n                           title='', chord_labels=None, ax=None, grid=True, figsize=(9, 3.5)):\n    \"\"\"Plots TP-, FP-, and FN-items in a color-coded form in time–chord grid\n\n    Args:\n        I_ref: Reference set of items\n        I_est: Set of estimated items\n        Fs: Feature rate (Default value = 1)\n        xlabel: Label for x-axis (Default value = 'Time (seconds)')\n        ylabel: Label for y-axis (Default value = 'Chord')\n        title: Title of figure (Default value = '')\n        chord_labels: List of chord labels used for vertical axis (Default value = None)\n        ax: Array of axes (Default value = None)\n        grid: If \"True\" the plot grid (Default value = True)\n        figsize: Size of figure (if axes are not specified) (Default value = (9, 3.5))\n\n    Returns:\n        fig: The created matplotlib figure or None if ax was given.\n        ax: The used axes\n        im: The image plot\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n        ax = [ax]\n    I_TP = np.sum(np.logical_and(I_ref, I_est))\n    I_FP = I_est - I_TP\n    I_FN = I_ref - I_TP\n    I_vis = 3 * I_TP + 2 * I_FN + 1 * I_FP\n\n    eval_cmap = colors.ListedColormap([[1, 1, 1], [1, 0.3, 0.3], [1, 0.7, 0.7], [0, 0, 0]])\n    eval_bounds = np.array([0, 1, 2, 3, 4])-0.5\n    eval_norm = colors.BoundaryNorm(eval_bounds, 4)\n    eval_ticks = [0, 1, 2, 3]\n\n    T_coef = np.arange(I_vis.shape[1]) / Fs\n    F_coef = np.arange(I_vis.shape[0])\n    x_ext1 = (T_coef[1] - T_coef[0]) / 2\n    x_ext2 = (T_coef[-1] - T_coef[-2]) / 2\n    y_ext1 = (F_coef[1] - F_coef[0]) / 2\n    y_ext2 = (F_coef[-1] - F_coef[-2]) / 2\n    extent = [T_coef[0] - x_ext1, T_coef[-1] + x_ext2, F_coef[0] - y_ext1, F_coef[-1] + y_ext2]\n\n    im = ax[0].imshow(I_vis,  origin='lower', aspect='auto', cmap=eval_cmap, norm=eval_norm, extent=extent,\n                      interpolation='nearest')\n    if len(ax) == 2:\n        cbar = plt.colorbar(im, cax=ax[1], cmap=eval_cmap, norm=eval_norm, boundaries=eval_bounds, ticks=eval_ticks)\n    elif len(ax) == 1:\n        plt.sca(ax[0])\n        cbar = plt.colorbar(im, cmap=eval_cmap, norm=eval_norm, boundaries=eval_bounds, ticks=eval_ticks)\n    cbar.ax.set_yticklabels(['TN', 'FP', 'FN', 'TP'])\n    ax[0].set_xlabel(xlabel)\n    ax[0].set_ylabel(ylabel)\n    ax[0].set_title(title)\n    if chord_labels is not None:\n        ax[0].set_yticks(np.arange(len(chord_labels)))\n        ax[0].set_yticklabels(chord_labels)\n    if grid is True:\n        ax[0].grid()\n    return fig, ax, im\n\n\nP, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_max)\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 0.2]}, figsize=(9, 4.5))\n\ntitle = 'Evaluation result (N=%d, TP=%d, FP=%d, FN=%d, P=%.3f, R=%.3f, F=%.3f)' % (N_X, TP, FP, FN, P,R,F)\nplot_matrix_chord_eval(ann_matrix, chord_max, ax=[ax[0, 0], ax[0, 1]], Fs=1, \n                       title=title, ylabel='Chord', xlabel='Time (frames)', chord_labels=chord_labels)\n\nplot_segments(ann_seg_ind, ax=ax[1, 0], time_label='Time (frames)', time_max=N_X,\n                       colors=color_ann,  alpha=0.3)\nax[1, 1].axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F16_score.png\", width=700)\n\n\n\n\n\n이 화음 인식 결과를 자세히 살펴보자.\n약 \\(10.8~\\mathrm{Hz}\\)의 피쳐 레이트를 사용하여 \\(N=139\\) 프레임을 얻는다. “half-measure” 수준의 주석은 녹음의 물리적 시간 축으로 전송되고 오디오 프레임과 일치하도록 추가로 양자화된다.\n평가 측정의 경우 \\(\\mathrm{P}=\\mathrm{R}=\\mathrm{F}=0.727\\)를 얻는다. 대부분의 잘못된 라벨은 추가 전달 또는 일시 중단된 음으로 인해 화음 모호성(ambiguities) 이 있는 프레임에서 발생한다. 이는 더 미세한 수준의 주석을 볼 때 명확해진다. 예를 들어, 두 번째 소절의 \\(\\mathbf{Fmaj7}\\) 화음은 4개의 피치 클래스 \\(\\mathrm{F}\\), \\(\\mathrm{A}\\), \\(\\mathrm{C}\\) 및 \\(\\mathrm{E}\\)으로 구성된다. 음악적으로 \\(\\mathbf{F}\\)에 가깝지만 네 가지 피치 클래스 중 세 가지가 \\(\\mathbf{Am}\\) 화음에서도 발생하기 때문에, 결과적으로 자동화된 절차는 이러한 프레임 중 일부를 \\(\\mathbf{Am}\\)(및 일부는 \\(\\mathbf{Dm}\\))로 잘못 표시했다.\n편차의 두 번째 원인는 화음 사이의 전환 영역(transition region) 으로, 한 화음의 종료 사운드가 다음 화음의 시작 사운드와 동일한 분석 윈도우에 나타날 수 있다. 예를 들어 첫 번째 소절에서 \\(\\mathbf{C}\\) 화음에서 다음 \\(\\mathbf{G}\\) 화음으로 전환할 때 일부 잘못 분류된 프레임을 발견할 수 있다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#비화음-라벨-non-chord-labels",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#비화음-라벨-non-chord-labels",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "비화음 라벨 (Non-Chord Labels)",
    "text": "비화음 라벨 (Non-Chord Labels)\n\n이전 예에서는 음악 녹음의 모든 프레임이 화음 라벨에 할당되어야 한다고 가정했다. 하지만 실제로 이것이 항상 의미 있는 것은 아니다. 예를 들어, 녹음이 묵음으로 시작하거나 박수로 끝나는 경우, 이 구간에는 의미 있는 화음 주석이 없다. 이러한 경우 해당 프레임은 평가에서 고려하지 않은 상태로 두어야 한다.\n위에서 이미 설명한 것처럼 비화음(non-chord) 라벨을 나타내는 추가 기호 \\(\\mathbf{N}\\)를 라벨 집합에 추가하여 이 속성을 모델링할 수 있다. 따라서 주어진 프레임 \\(n\\in[1:N]\\)에 대해 의미 있는 참조 화음 라벨이 없는 경우 \\(\\lambda^\\mathrm{Ref}_{n}=\\mathbf{N}\\)로 설정힌다. 유사하게, 화음 인식기는 화음이 아닌 라벨 \\(\\lambda_{n}=\\mathbf{N}\\)도 출력하도록 수정될 수 있다(예: 크로마 특징과 템플릿 간의 모든 유사성 값이 특정 값 미만인 경우).\n정밀도, 재현율 및 F-measure을 기반으로 한 평가 측정에서 화음이 아닌 라벨을 단순히 추가 레이블로 취급하지 않았다. 대신 화음이 아닌 라벨이 있는 모든 항목을 평가할 때 이를 무시한다. 즉, 추정 또는 참조 주석의 이진 시간-화음 표현에는 갭이 있을 수 있으며, 특정 프레임은 할당된 화음 라벨 없이 남아 있다. 이 경우 정밀도, 재현율 및 F-measure이 다른 값이 다를 수 있다. 이는 참조 주석에서 화음 라벨 \\(\\mathbf{F}\\)를 화음이 아닌 라벨 \\(\\mathbf{N}\\)로 대체하는 다음 예에서 확인할 수 있다.\n결과적으로 이러한 모든 프레임의 추정은 이제 잘못된 것으로 간주되어 FP 항목이 증가한다. 동시에 TP- 및 FN-항목이 감소한다.\n\n\nann_matrix[5, :] = 0\nann_seg_ind[3][2] = 'N'\nann_seg_ind[6][2] = 'N'\n\nP, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_max)\n\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 0.2]}, figsize=(9, 4.5))\n\ntitle = 'Evaluation result (N=%d, TP=%d, FP=%d, FN=%d, P=%.3f, R=%.3f, F=%.3f)' % (N_X, TP, FP, FN, P,R,F)\nplot_matrix_chord_eval(ann_matrix, chord_max, ax=[ax[0, 0], ax[0, 1]], Fs=1, \n                       title=title, ylabel='Chord', xlabel='Time (frames)', chord_labels=chord_labels)\n\nplot_segments(ann_seg_ind, ax=ax[1, 0], time_label='Time (frames)', time_max=N_X,\n                       colors=color_ann,  alpha=0.3)\nax[1,1].axis('off')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#화음-인식의-모호함-ambiguities-in-chord-recognition",
    "href": "posts/6. Chord Recognition/6.2.Template-Based_Chord_Recognition.html#화음-인식의-모호함-ambiguities-in-chord-recognition",
    "title": "6.2. 템플릿 기반 화음 인식",
    "section": "화음 인식의 모호함 (Ambiguities in Chord Recognition)",
    "text": "화음 인식의 모호함 (Ambiguities in Chord Recognition)\n\n다음에서는 자동화된 화음 인식에서 직면하는 가장 일반적인 문제 중 일부를 다룬다.\n위의 실험에서 알 수 있듯이 서로 다른 화음은 일부 음을 공유함으로써 밀접하게 관련될 수 있다. 대부분의 오분류는 화음 모델의 과도한 단순화(oversimplification) 로 인한 화음 모호성(chord ambiguities) 에서 비롯된다(예: \\(24\\)장,단3화음만 고려할 때). 이러한 모호성은 다음 그림과 같이 설명된다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F17.png\", width=500)\n\n\n\n\n\n첫 번째 예는 \\(\\mathbf{C}\\) 화음이 각각 \\(\\mathbf{Am}\\), \\(\\mathbf{Cm}\\) 및 \\(\\mathbf{Em}\\) 화음과 두 개의 음을 공유하는 반면, 두 번째 예는 \\(\\mathbf{Cmaj7}\\) 화음이 \\(\\mathbf{C}\\) 및 \\(\\mathbf{Em}\\) 화음을 포함함을 나타낸다.\n이때 화음 라벨의 집합을 확장하여 오분류 문제를 완화할 수 있다. 예를 들어 장,단3화음 외에도 장7도화음에 해당하는 화음 템플릿을 도입할 수도 있다. 그러나 단점으로는 가능한 화음의 수를 늘리면 분류 단계에서 혼동 확률도 높아진다는 것이다.\n또 다른 문제는 녹음된 음악의 음향적(acoustic) 특성에서 비롯된다. 특히, 음향 모호성은 부분음(harmonic partials)이 도입되어 화음 인식기의 결과에 상당한 영향을 미칠 수 있다. 예를 들어, 악기에서 \\(\\mathrm{C3}\\) 단일 음을 연주하면, 크로마그램 표현 결과에서 \\(\\mathrm{G}\\)-band(3차 고조파) 또는 \\(\\mathrm{E}\\)-band(5차 고조파)에 상당한 에너지가 발생할 수 있다. 이는 종종 major-minor 혼동으로 알려진 문제를 야기한다.\n가정된 중심 주파수에서 벗어나면 크로마 기반 특징과 같은 음악적으로 정보가 있는 오디오 특징의 품질이 심각하게 저하될 수 있다. 이러한 조율 문제는 의도적으로 발생할 수 있지만(예: 오케스트라가 다른 참조 조율을 사용한 경우), 재생 속도 수정 또는 다른 후처리 작업의 적용으로 인해 발생할 수도 있다.\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F20a.png\", width=600))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer.wav\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n이 예에서 각 half-measure은 저음(bass note)으로 시작한다. 그런 다음 다른 음이 합류하여 점차 전체 화음의 소리를 만든다. 음을 동시에 연주하지 않더라도 끊어진 화음 전체가 하나의 화성 단위로 인식될 수 있다.\n신호를 짧은 프레임으로 자르고 각 프레임을 개별적으로 분류하는 기본적인 프레임 수준의 화음 인식 절차의 경우 이러한 끊어진 화음은 문제가 된다.\n다음 코드 셀에서는 입력 크로마그램을 계산하는 데 사용되는 다양한 윈도우 크기에 대한 결과를 보여주는 화음 인식을 보여준다.\n\n\ndef experiment_chord_recognition_feature(fn_wav, fn_ann, color_ann, N_chroma, H_chroma, gamma=1,\n                                         version='STFT'):\n    # Compute chromagram\n    X, Fs_X, x, Fs, x_dur = compute_chromagram_from_filename(fn_wav, N=N_chroma, H=H_chroma, \n                                                                       gamma=gamma, version=version)\n    N_X = X.shape[1]\n\n    # Chord recogntion\n    chord_sim, chord_max = chord_recognition_template(X, norm_sim='max')\n    chord_labels = get_chord_labels(nonchord=False)\n\n    # Annotations\n    chord_labels = get_chord_labels(ext_minor='m', nonchord=False)\n    ann_matrix, ann_frame, ann_seg_frame, ann_seg_ind, ann_seg_sec = \\\n        convert_chord_ann_matrix(fn_ann, chord_labels, Fs=Fs_X, N=N_X, last=True)\n\n    P, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_max)\n\n    fig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                              'height_ratios': [1, 2, 0.2]}, figsize=(9, 6))\n\n    title = title='Chromagram with window size = %.3f (seconds)' % (N_chroma / Fs)\n    plot_chromagram(X, ax=[ax[0, 0], ax[0, 1]], Fs=1, clim=[0, 1], xlabel='', title=title)\n    plot_segments_overlay(ann_seg_frame, ax=ax[0, 0], \n                                   print_labels=False, colors=color_ann, alpha=0.1)\n\n    title = 'Evaluation result (N=%d, TP=%d, FP=%d, FN=%d, F=%.3f)' % (N_X, TP, FP, FN, F)\n    plot_matrix_chord_eval(ann_matrix, chord_max, ax=[ax[1, 0], ax[1, 1]], Fs=1, \n                         title=title, ylabel='Chord', xlabel='Time (frames)', chord_labels=chord_labels)\n\n    plot_segments(ann_seg_ind, ax=ax[2, 0], time_label='Time (frames)', time_max=N_X,\n                           colors=color_ann,  alpha=0.3)\n    ax[2, 1].axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\nfn_wav = \"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer.wav\"\nfn_ann = \"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer_ChordAnnotations.csv\"\n\ncolor_ann = {'C': [1, 0.5, 0, 1], 'G': [0, 1, 0, 1], 'Dm': [1, 0, 0, 1], 'N': [1, 1, 1, 1]}\n\nexperiment_chord_recognition_feature(fn_wav, fn_ann, color_ann, N_chroma=2048, H_chroma=1024)\nexperiment_chord_recognition_feature(fn_wav, fn_ann, color_ann, N_chroma=2048*4, H_chroma=1024)\nexperiment_chord_recognition_feature(fn_wav, fn_ann, color_ann, N_chroma=2048*16, H_chroma=1024)\n\n\n\n\n\n\n\n\n\n\n\n바흐 녹음에서 16분음표는 대략 \\(150~\\mathrm{msec}\\)의 길이를 가진다. 따라서 약 \\(90~\\mathrm{msec}\\) 기간의 분석 윈도우를 사용하면 각 크로마 프레임은 최대 하나의 음 시작점을 포함한다.\n각 음표의 소리가 기보된 길이보다 훨씬 더 오래 지속될 수 있지만 각 프레임의 화성은 단 하나 또는 두 개의 음(부분음 포함)에 의해 지배된다. 이것은 첫 번째 설정(작은 윈도우 크기)의 인식 결과에서 오분류 및 많은 화음 라벨 변경을 설명한다.\n바흐 예에서 화음 인식 결과를 개선하기 위한 확실한 전략은 주석의 절반 또는 측정 수준에 더 잘 일치하는 더 큰 윈도우 크기를 사용하는 것이다. 이는 두 번째 설정(중간 윈도우 크기, \\(372~\\mathrm{msec}\\)) 및 세 번째 설정(큰 윈도우 크기, \\(1486~\\mathrm{msec}\\))에서 확인할 수 있다. 그러나 단점으로 더 큰 분석 윈도우는 서로 다른 화음 사이의 원래의 날카로운 전환을 부드럽게 하여, 화음 변경 시 문제를 일으킬 수 있다.\n요약하면 다음 사항을 염두에 두어야 한다.\n\n참조 주석을 항상 신뢰하지 말 것!\n기본 모델 가정을 명시적으로 지정하고 그 영향을 이해할 것\n변환 단계(예: 시간 샘플링)와 도입된 모호성을 이해할 것\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S2_ChordRec_Templates.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S2_ChordRec_Eval.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "",
    "text": "HMM(Hidden Markov Model)과 Viterbi 알고리즘을 설명하고 HMM 기반 화음 인식 방법과 그 예를 다룬다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#markov-chains",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#markov-chains",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "Markov Chains",
    "text": "Markov Chains\n\n한 화음에서 다른 화음으로의 전환은 어느 한 특정 전환이 다른 전환보다 일어날 가능성이 더 높다. 이러한 우도(likelihood)를 포착하기 위해 마코프 체인(Markov chains) 이라는 개념을 사용할 수 있다.\n화음 인식 시나리오에서 고려할 화음 유형이 집합으로 표현된다고 가정하자. \\[\\mathcal{A}:=\\{\\alpha_{1},\\alpha_{2},\\ldots,\\alpha_{I}\\}\\] of size \\(I\\in\\mathbb{N}\\)\n원소 \\(\\alpha_{i}\\) for \\(i\\in[1:I]\\) 는 상태(state)라고 한다. 화음의 진행은 어떤 state \\(s_{n}\\in\\mathcal{A}\\)에 있는 것으로, 어느 시간 \\(n=1,2,3,\\ldots\\)로 설명될 수 있는 시스템에 의해 실현된다. 한 상태에서 다른 상태로의 변경은 각 state와 관련된 확률 집합에 따라 지정된다. 일반적으로 그러한 시스템의 확률적 설명은 매우 복잡할 수 있다.\n모델을 단순화하기 위해 현재 상태 \\(s_{n}\\)에서 다음 상태 \\(s_{n+1}\\)로 변경될 확률은 그보다 앞선 이벤트가 아닌 현재 state에만 의존한다고 가정한다.\n조건부 확률로 이 속성을 표현하면 다음과 같다. \\[P[s_{n+1}=\\alpha_{j}|s_{n}=\\alpha_{i},s_{n-1}=\\alpha_{k},\\ldots]\n   = P[s_{n+1}=\\alpha_{j}|s_{n}=\\alpha_{i}]\\]\n이러한 “기억상실”을 마코프 속성(Markov property) 이라고 한다.\n이 속성 외에도 시스템이 시간 이동 하에서 불변이라고 가정하는 경우가 많으며, 이는 정의에 따라 다음 계수가 인덱스 \\(n\\)과 독립적이 된다는 것을 의미한다: \\[a_{ij} := P[s_{n+1}=\\alpha_{j} | s_{n}=\\alpha_{i}] \\in [0,1]\\] for \\(i,j\\in[1:I]\\)\n이러한 계수는 상태 전이 확률(state transition probabilities) 이라고도 한다. 상태 전이 확률은 표준(standard) 확률적 제약 조건 \\(\\sum_{j=1}^{I} a_{ij} = 1\\)를 따르고 \\((I\\times I)\\) 행렬로 표현할 수 있으며 \\(A\\)로 표시한다. 이러한 특성을 만족하는 시스템을 (이산 시간) 마코프 체인이라고 한다.\n다음 그림은 이러한 정의를 보여준다. 장(major)화음 \\(\\mathbf{C}\\), \\(\\mathbf{G}\\), \\(\\mathbf{F}\\)에 해당하는 \\(I=3\\)개 상태 \\(\\alpha_{1}\\), \\(\\alpha_{2}\\), \\(\\alpha_{3}\\)로 구성된 Markov 체인을 정의한다. 그래프 표현에서 상태는 노드(node), 에지(edge)로의 전이 및 에지에 부착된 라벨로의 전이 확률에 해당한다.\n\n예를 들어 \\(\\alpha_{1}=\\mathbf{C}\\) 상태를 유지할 전이 확률은 \\(a_{11}=0.8\\)인 반면, \\(\\alpha_{1}=\\mathbf{C}\\)에서 \\(\\alpha_{2}=\\mathbf{G}\\)로 바뀔 전이 확률은 \\(a_{12}=0.1\\)이다.\n\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F24.png\",width=700)\n\n\n\n\n\n모델은 가능한 모든 화음 변화의 확률을 표현한다. 주어진 화음 진행의 확률을 계산하려면 모델이 시작되는 방법에 대한 정보도 필요하다. 이 정보는 초기 상태 확률(initial state probabilities) 이라는 추가 모델 매개변수에 의해 지정된다. 일반적인 Markov 체인의 경우 이러한 확률은 숫자로 지정된다. \\[c_{i} := P[s_{1}=\\alpha_{i}] \\in [0,1]\\] for \\(i\\in[1:I]\\)\n합이 1이 되는 이러한 계수는 길이 \\(I\\)의 \\(C\\)로 표시되는 벡터로 표현될 수 있다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hidden-markov-models",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hidden-markov-models",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\n\nMarkov 체인을 기반으로 상태 또는 화음 유형의 시퀀스로 구성된 주어진 관찰에 대해 확률을 계산할 수 있다. 그러나 화음 인식 시나리오에서 이것이 꼭 필요한 것은 아니다.\n화음 유형의 시퀀스를 관찰하는 대신 화음 유형과 어떤 식으로든 관련된 크로마 벡터 시퀀스를 관찰할 수 있다. 즉, 상태 시퀀스가 직접적으로 보이지 않고, 상태 시퀀스를 기반으로 생성된 흐릿한 관찰 시퀀스만 존재한다.\n이로 인해 Markov 체인이 Hidden Markov 모델(HMM)이라고 하는 통계 모델로 확장된다. 확률적 프레임워크를 사용하여 관찰된 특징 벡터와 화음 유형 사이의 관계를 나타내는 것이다. 각 상태는 특정 화음 유형이 특정 특징 벡터를 출력하거나 방출할 가능성을 나타내는 확률 함수를 갖추고 있다.\n결과적으로 hidden layer와 observable layer로 구성된 2계층 프로세스를 얻는다. hidden layer은 관찰할 수 없는(“숨겨진”) 상태 시퀀스를 생성하지만, 상태-종속(state-dependent) 확률 함수를 기반으로 관찰 시퀀스를 생성한다.\nHMM의 첫 번째 레이어는 위에서 소개한 마코프 체인이다.\nHMM의 두 번째 레이어를 정의하려면 가능한 출력 값의 공간과 각 상태에 대한 확률 함수를 지정해야 한다.\n일반적으로 출력 공간은 실수, 벡터 공간 또는 모든 종류의 특징 공간을 포함하는 모든 집합이 될 수 있다. 예를 들어, 화음 인식의 경우 이 공간은 가능한 모든 \\(12\\) 차원의 크로마 벡터로 구성된 특징 공간 \\(\\mathcal{F}=\\mathbb{R}^{12}\\)으로 모델링될 수 있다.\n단순화를 위해 출력 공간이 불연속적이고 유한한 것으로 가정되는 이산(discrete) HMM의 경우만 고려한다. 이 경우 공간을 유한 집합으로 모델링할 수 있다. \\[\\mathcal{B} = \\{\\beta_{1},\\beta_{2},\\ldots,\\beta_{K}\\}\\] 개별 출력 요소 \\(\\beta_{k}\\), \\(k\\in[1:K]\\)로 구성된 \\(K\\in\\mathbb{N}\\) 크기를 가지며, 이를 관찰 기호(obeservation symbols) 라고도 한다.\nHMM은 방출(emission) 확률 또는 출력(output) 확률이라고도 하는 확률 함수를 각 상태와 연결한다. 불연속적인 경우 방출 확률은 다음의 계수로 지정된다. \\[b_{ik}\\in[0,1]\\] for \\(i\\in[1:I]\\) and \\(k\\in[1:K]\\)\n각 계수 \\(b_{ik}\\)는 상태 \\(\\alpha_{i}\\)에 있을 때 관측 기호 \\(\\beta_{k}\\)를 출력할 시스템의 확률을 나타낸다. 상태 전이 확률과 유사하게, 방출 확률은 확률적 제약 조건 \\(\\sum_{k=1}^{K} \\beta_{ik} = 1\\) for \\(i\\in[1:I]\\)를 충족하는 데 필요하다(따라서 각 상태에 대한 확률 분포 형성).\n계수는 \\(B\\)로 표시되는 \\((I\\times K)\\) 행렬로 표현할 수 있다. 요약하면 HMM은 다음의 튜플로 지정된다. \\[\\Theta:=(\\mathcal{A},A,C,\\mathcal{B},B)\\]\n\\(\\mathcal{A}\\) 및 \\(\\mathcal{B}\\) 집합은 일반적으로 모델의 고정 구성 요소로 간주되는 반면, \\(A\\), \\(B\\) 및 \\(C\\)로 지정된 확률 값은 결정할 수 이쓴 매개변수이다. 이는 전문가 자신의 음악 지식 혹은 적절하게 라벨링된 교육 데이터를 기반으로 한 학습 절차를 사용하여 수행할 수 있다.\n위의 예에 이어 다음 그림은 state-dependent 방출 확률이 점선 화살표의 라벨로 표시되는 Hidden Markov 모델을 보여준다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F25.png\", width=500)\n\n\n\n\n\n다음 코드 셀에서 그림에 지정된 대로 상태 전이 확률 행렬 \\(A\\)와 출력 확률 \\(B\\)를 정의한다.\n\n여기서는 \\(\\alpha_{1}=\\mathbf{C}\\), \\(\\alpha_{2}=\\mathbf{G}\\), \\(\\alpha_{3}=\\mathbf{F}\\)로 가정한다.\n또한 출력 공간 \\(\\mathcal{B} = \\{\\beta_{1},\\beta_{2},\\beta_{3}\\}\\)의 요소는 왼쪽에서 오른쪽으로 정렬된 세 개의 크로마 벡터를 나타낸다.\n마지막으로 초기 상태 확률 벡터 \\(C\\)가 \\(c_{1}=0.6\\), \\(c_{2}=0.2\\), \\(c_{3}=0.2\\) 값으로 주어진다고 가정한다.\n\n\n\nA = np.array([[0.8, 0.1, 0.1], \n              [0.2, 0.7, 0.1], \n              [0.1, 0.3, 0.6]])\nC = np.array([0.6, 0.2, 0.2])\nB = np.array([[0.7, 0.0, 0.3], \n              [0.1, 0.9, 0.0], \n              [0.0, 0.2, 0.8]])"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm-기반-시퀀스-생성",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm-기반-시퀀스-생성",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "HMM 기반 시퀀스 생성",
    "text": "HMM 기반 시퀀스 생성\n\nHMM이 \\(\\Theta:=(\\mathcal{A},A,C,\\mathcal{B},B)\\)로 지정되면, 다양한 분석 및 합성 응용에 사용할 수 있다.\n이제 주어진 HMM을 기반으로 관측 시퀀스 \\(O=(o_{1},o_{2},\\ldots,o_{N})\\) (of length \\(N\\in\\mathbb{N}\\) with \\(o_n\\in \\mathcal{B}\\), \\(n\\in[1:N]\\))를 (인위적으로) 생성하는 방법에 대해 논의한다.\n생성 절차는 다음과 같다.\n\n초기 상태 분포 \\(C\\)에 따라 \\(n=1\\)로 설정하고 초기 상태 \\(s_n=\\alpha_i\\) (for some \\(i\\in[1:I]\\))를 선택한다.\n\\(s_n=\\alpha_i\\) state의 방출 확률에 따라 관찰 \\(o_n=\\beta_k\\) (for some \\(k\\in[1:K]\\))를 생성한다(\\(B\\)의 \\(i^{\\mathrm{th}}\\) 행에 의해 지정됨)\n\\(n=N\\)이면 프로세스를 종료한다. 그렇지 않고 \\(n<N\\)인 경우, 상태 \\(s_n=\\alpha_i\\)의 상태 전이 확률에 따라 새로운 상태 \\(s_{n+1}=\\alpha_{j}\\)로 이동한다(\\(A\\)의 \\(i^{\\mathrm{th}}\\) 행에 의해 지정됨). 그런 다음 \\(n\\)을 1씩 늘리고 2단계로 돌아간다.\n\n다음 코드 셀에서는 이 절차를 구현하고 위에서 지정한 예제 HMM에 적용한다.\n\n\ndef generate_sequence_hmm(N, A, C, B, details=False):\n    \"\"\"Generate observation and state sequence from given HMM\n\n    Args:\n        N (int): Number of observations to be generated\n        A (np.ndarray): State transition probability matrix of dimension I x I\n        C (np.ndarray): Initial state distribution of dimension I\n        B (np.ndarray): Output probability matrix of dimension I x K\n        details (bool): If \"True\" then shows details (Default value = False)\n\n    Returns:\n        O (np.ndarray): Observation sequence of length N\n        S (np.ndarray): State sequence of length N\n    \"\"\"\n    assert N > 0, \"N should be at least one\"\n    I = A.shape[1]\n    K = B.shape[1]\n    assert I == A.shape[0], \"A should be an I-square matrix\"\n    assert I == C.shape[0], \"Dimension of C should be I\"\n    assert I == B.shape[0], \"Column-dimension of B should be I\"\n\n    O = np.zeros(N, int)\n    S = np.zeros(N, int)\n    for n in range(N):\n        if n == 0:\n            i = np.random.choice(np.arange(I), p=C)\n        else:\n            i = np.random.choice(np.arange(I), p=A[i, :])\n        k = np.random.choice(np.arange(K), p=B[i, :])\n        S[n] = i\n        O[n] = k\n        if details:\n            print('n = %d, S[%d] = %d, O[%d] = %d' % (n, n, S[n], n, O[n]))\n    return O, S\n\n\nN = 10\nO, S = generate_sequence_hmm(N, A, C, B, details=True)\nprint('State sequence S:      ', S)\nprint('Observation sequence O:', O)\n\nn = 0, S[0] = 0, O[0] = 0\nn = 1, S[1] = 0, O[1] = 2\nn = 2, S[2] = 0, O[2] = 0\nn = 3, S[3] = 0, O[3] = 0\nn = 4, S[4] = 0, O[4] = 0\nn = 5, S[5] = 0, O[5] = 0\nn = 6, S[6] = 0, O[6] = 0\nn = 7, S[7] = 0, O[7] = 0\nn = 8, S[8] = 0, O[8] = 0\nn = 9, S[9] = 0, O[9] = 2\nState sequence S:       [0 0 0 0 0 0 0 0 0 0]\nObservation sequence O: [0 2 0 0 0 0 0 0 0 2]\n\n\n\n시퀀스 생성 접근 방식의 타당성에 대한 건전성 검사로서 이제 생성된 시퀀스가 HMM의 확률을 잘 반영하는지 확인한다. 이를 위해 생성된 관찰 시퀀스 \\(O\\)와 상태 시퀀스 \\(S\\)로부터 원래 전이 확률 행렬 \\(A\\)와 출력 확률 행렬 \\(B\\)를 추정한다.\n\n\\(A\\)의 \\(a_{ij}\\) 항목의 추정치를 얻기 위해 \\(S(n)=\\alpha_i\\) 및 \\(S(n+1)=\\alpha_j\\)을 사용하여 \\(n\\)에서 \\(n+1\\)로의 모든 전환을 계산한다. 그런 다음 이 숫자를 \\(\\alpha_i\\)로 시작해서 총 전환 수로 나눈다.\n마찬가지로, \\(B\\)의 \\(b_{ik}\\) 항목에 대한 추정치를 얻기 위해 \\(S(n)=\\alpha_i\\) 및 \\(O(n)=\\beta_k\\)에서 발생 횟수 \\(n\\)을 계산한다. 이 숫자를 \\(S\\)에서 \\(\\alpha_i\\)의 총 발생 수로 나눈다.\n\n숫자 \\(N\\)을 증가시켜 더 긴 시퀀스를 생성할 때, 결과 추정치는 \\(A\\) 및 \\(B\\)의 원래 값에 근접해야 한다.\n주의: 실제로 훈련 데이터에서 HMM 모델 매개변수를 추정할 때 일반적으로 관측 시퀀스만 사용할 수 있으며 상태 시퀀스(숨겨진 생성 프로세스를 반영)는 알려져 있지 않다. 관찰 시퀀스에서만 파라미터를 학습하면 아래에서 설명하는 것처럼 훨씬 더 어려운 추정 문제가 발생한다.\n\n\ndef estimate_hmm_from_o_s(O, S, I, K):\n    \"\"\"Estimate the state transition and output probability matrices from\n    a given observation and state sequence\n\n    Args:\n        O (np.ndarray): Observation sequence of length N\n        S (np.ndarray): State sequence of length N\n        I (int): Number of states\n        K (int): Number of observation symbols\n\n    Returns:\n        A_est (np.ndarray): State transition probability matrix of dimension I x I\n        B_est (np.ndarray): Output probability matrix of dimension I x K\n    \"\"\"\n    # Estimate A\n    A_est = np.zeros([I, I])\n    N = len(S)\n    for n in range(N-1):\n        i = S[n]\n        j = S[n+1]\n        A_est[i, j] += 1\n    A_est = normalize(A_est, axis=1, norm='l1')\n\n    # Estimate B\n    B_est = np.zeros([I, K])\n    for i in range(I):\n        for k in range(K):\n            B_est[i, k] = np.sum(np.logical_and(S == i, O == k))\n    B_est = normalize(B_est, axis=1, norm='l1')\n    return A_est, B_est\n\n\nN = 100\nprint('======== Estimation results when using N = %d ========' % N)\nO, S = generate_sequence_hmm(N, A, C, B, details=False)\nA_est, B_est = estimate_hmm_from_o_s(O, S, A.shape[1], B.shape[1])\nnp.set_printoptions(formatter={'float': \"{: 7.3f}\".format})\nprint('A =', A, sep='\\n')\nprint('A_est =', A_est, sep='\\n')\nprint('B =', B, sep='\\n')\nprint('B_est =', B_est, sep='\\n')\n\nN = 10000\nprint('======== Estimation results when using N = %d ========' % N)\nO, S = generate_sequence_hmm(N, A, C, B, details=False)\nA_est, B_est = estimate_hmm_from_o_s(O, S, A.shape[1], B.shape[1])\nnp.set_printoptions(formatter={'float': \"{: 7.3f}\".format})\nprint('A =', A, sep='\\n')\nprint('A_est =', A_est, sep='\\n')\nprint('B =', B, sep='\\n')\nprint('B_est =', B_est, sep='\\n')\n\n======== Estimation results when using N = 100 ========\nA =\n[[  0.800   0.100   0.100]\n [  0.200   0.700   0.100]\n [  0.100   0.300   0.600]]\nA_est =\n[[  0.633   0.167   0.200]\n [  0.154   0.744   0.103]\n [  0.133   0.200   0.667]]\nB =\n[[  0.700   0.000   0.300]\n [  0.100   0.900   0.000]\n [  0.000   0.200   0.800]]\nB_est =\n[[  0.567   0.000   0.433]\n [  0.125   0.875   0.000]\n [  0.000   0.167   0.833]]\n======== Estimation results when using N = 10000 ========\nA =\n[[  0.800   0.100   0.100]\n [  0.200   0.700   0.100]\n [  0.100   0.300   0.600]]\nA_est =\n[[  0.808   0.094   0.098]\n [  0.198   0.706   0.096]\n [  0.100   0.321   0.580]]\nB =\n[[  0.700   0.000   0.300]\n [  0.100   0.900   0.000]\n [  0.000   0.200   0.800]]\nB_est =\n[[  0.697   0.000   0.303]\n [  0.095   0.905   0.000]\n [  0.000   0.195   0.805]]"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm의-세-가지-문제",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm의-세-가지-문제",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "HMM의 세 가지 문제",
    "text": "HMM의 세 가지 문제\n\n이제 매개변수의 지정 및 관찰 시퀀스의 평가와 관련된 HMM에 대한 세 가지 유명한 알고리즘 문제를 살펴본다.\n\n\n평가 문제 (Evaluation Problem)\n\n첫 번째 문제는 평가 문제이다. \\(\\Theta=(\\mathcal{A},A,C,\\mathcal{B},B)\\)로 지정된 HMM과 관측 시퀀스 \\(O=(o_{1},o_{2},\\ldots,o_{N})\\)가 주어지면, 주어진 모델의 관찰 시퀀스의 확률 \\(P[O|\\Theta]\\)을 계산해야 한다.\n조금 다른 관점에서 보면, 이 확률은 주어진 모델이 주어진 관측 시퀀스와 얼마나 잘 일치하는지를 나타내는 점수 값으로 볼 수 있다. 이러한 해석은 여러 경쟁 모델 중에서 하나를 선택하려는 경우에 유용하다. 따라서 해결책은 관측 시퀀스와 가장 일치하는 모델을 선택하는 것이다.\n\\(P[O|\\Theta]\\)를 계산하기 위해 먼저 길이가 \\(N\\)이고 적절한 \\(i_n\\in[1:I]\\), \\(n\\in[1:N]\\)에 대한 \\(s_n=\\alpha_{i_n}\\in\\mathcal{A}\\)인 고정 상태 시퀀스 \\(S=(s_1,s_2,\\ldots,s_N)\\)를 고려한다.\n상태 시퀀스 \\(S\\)와 관찰 시퀀스 \\(O\\)를 생성할 확률 \\(P[O,S|\\Theta]\\)는 다음과 같이 주어진다. \\[P[O,S|\\Theta] = c_{i_1}\\cdot b_{i_1k_1} \\cdot a_{i_1i_2}\\cdot b_{i_2k_2} \\cdot ...\\cdot a_{i_{N-1} i_N}\\cdot b_{i_Nk_N}\\]\n다음으로 전체 확률 \\(P[O|\\Theta]\\)를 얻으려면 \\(|S|=N\\) 길이의 모든 가능한 상태 시퀀스 \\(S\\)를 고려하여 이러한 모든 확률을 합산해야 한다. \\[P[O|\\Theta] = \\sum_{S: |S|=N}P[O,S|\\Theta]\n= \\sum_{i_1=1}^I \\sum_{i_2=1}^I \\ldots \\sum_{i_N=1}^I\nc_{i_1}\\cdot b_{i_1k_1} \\cdot a_{i_1i_2}\\cdot b_{i_2k_2} \\cdot ...\\cdot a_{i_{N-1}i_N}\\cdot b_{i_Nk_N}\\]\n이는 관찰 시퀀스의 길이 \\(N\\)에서 기하급수적인 숫자인 \\(I^N\\) summands로 이어진다. 따라서 실제로 이 잔인한 계산 절차는 작은 \\(N\\)에 대해서도 계산적으로 불가능하다. 단, 동적 프로그래밍 패러다임을 기반으로 하는 알고리즘을 사용하여 \\(P[O|\\Theta]\\)를 계산하는 보다 효율적인 방법이 있다. Forward-Backward Algorithm이라고 하는 이 절차에는 \\(I^2N\\) (\\(I^N\\) 대신) 정도의 많은 작업이 필요하다.\n\n\n\nUncovering 문제\n\n두번째 문제는 “uncovering” 문제이다.\n다시 \\(\\Theta=(\\mathcal{A},A,C,\\mathcal{B},B)\\)로 주어진 HMM과 관찰 시퀀스 \\(O=(o_{1},o_{2 },\\ldots,o_{N})\\)가 있을 때, 모든 가능한 상태 시퀀스를 고려해야 하는 전체 확률 \\(P[O|\\Theta]\\) for \\(O\\)를 찾는 대신, uncovering 문제의 목표는 관찰 시퀀스를 “가장 잘 설명”하는 단일 상태 시퀀스 \\(S=(s_{1},s_{2},\\ldots,s_{N})\\)을 찾는 것이다.\n지금까지 언급된 uncovering 문제는 일반적으로 관찰 시퀀스를 생성하는 단일의 “올바른” 상태 시퀀스가 없기 때문에 잘 정의되지 않았다. 실제로 최상의 설명력에 대해 의미하는 바를 구체화하는 일종의 최적화 기준이 필요하다. 그러한 기준에 대한 몇 가지 합리적인 선택이 있으며, 의도된 응용에 따라 달라진다. 그 중 하나인 효율적인 알고리즘인 Viterbi algorithm에 대해 밑에서 논의하기로 한다. 이 알고리즘은 일종의 context-sensitive smoothing 절차로 생각할 수 있다.\n\n\n\n추정 (Estimation) 문제\n\nHMM의 세번째 문제는 추정 문제이다.\n관찰 시퀀스 \\(O\\)가 주어지면 목표는 확률 \\(P[O|\\Theta ]\\)을 최대화하는 \\(\\Theta\\)의 (\\(A\\), \\(C\\), \\(B\\)로 지정된) 모델의 자유 매개변수를 결정하는 것이다. 다시말해, 관찰 시퀀스를 가장 잘 설명하는 매개변수를 추정하는 것이다.\n이것은 일련의 관찰 시퀀스가 HMM 매개변수를 조정하거나 학습하기 위한 훈련 자료(training material) 역할을 하는 최적화 문제의 전형적인 예이다. 추정 문제는 단연 HMM의 가장 어려운 문제이다. 사실 주어진 최적화 문제를 명시적으로 풀 수 있는 알려진 방법은 없다. 다만, 지역적으로 최적해를 찾는 반복적인 절차가 제시되어 왔다. 이러한 절차 중 하나는 Baum-Welch 알고리즘으로 알려져 있다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#uncovering-문제와-viterbi-알고리즘",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#uncovering-문제와-viterbi-알고리즘",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "Uncovering 문제와 Viterbi 알고리즘",
    "text": "Uncovering 문제와 Viterbi 알고리즘\n\n위에서 화음 인식에서 발생하는 uncovering 문제에 대해 다루었다. 목표는 주어진 관측 시퀀스 \\(O=(o_{1},o_{2},\\ldots,o_{N})\\)를 “가장 잘 설명하는” 단일(single) 상태(state) 시퀀스 \\(S=(s_{1},s_{2},\\ldots,s_{N})\\)를 찾는 것이다. 화음 인식 시나리오에서 관측 시퀀스는 오디오 녹음으로부터 추출한 크로마 벡터의 시퀀스이다. 최적 상태 시퀀스는 화음 라벨의 시퀀스이며 화음 인식 절차의 결과이다. 최적화 기준 중 하나로 관측 시퀀스 \\(0\\)에 반하여 평가될 때 가장 높은 확률 \\(\\mathrm{Prob}^\\ast\\)을 가지는 상태 시퀀스 \\(S^\\ast\\)를 정하는 것이 있다. \\[\\mathrm{Prob}^\\ast\n= \\underset{S=(s_{1},s_{2},\\ldots,s_{N})}{\\max} \\,\\,P[O,S|\\Theta],\\\\\nS^\\ast\n= \\underset{S=(s_{1},s_{2},\\ldots,s_{N})}{\\mathrm{argmax}} \\,\\,\\,P[O,S|\\Theta]\\]\nnaive한 접근 방식을 사용하여 시퀀스 \\(S^\\ast\\)를 찾으려면, 길이 \\(N\\)의 \\(I^N\\)개의 가능한 상태 시퀀스 각각에 대한 확률 값 \\(P[O,S|\\Theta]\\)를 계산해야 하고 최대화 인수를 찾아야 한다.\n다행히도 훨씬 더 효율적인 방법으로 최적화 상태 시퀀스를 찾는 Viterbi 알고리즘이라는 기술이 있다. DTW 알고리즘과 유사한 Viterbi 알고리즘은 동적 프로그래밍(dynamic progarmming) 을 기반으로 한다.\n하위 문제에 대한 최적 솔루션에서 최적(즉, 확률 최대화) 상태 시퀀스를 재귀적으로 계산한다. 여기에서 관측 시퀀스의 잘린 버전을 고려한다.\n\\(O=(o_{1},o_{2},\\ldots o_{N})\\)를 관측 시퀀스라고 하자. 그러면 다음과 같이 정의할 수 있다. \\[O(1\\!:\\!n):=(o_{1},\\ldots,o_{n})\\] of length \\(n\\in[1:N]\\). \\[ \\mathbf{D}(i,n):=\\underset{(s_1,\\ldots,s_n)}{\\max} P[O(1:n),(s_1,\\ldots, s_{n-1},s_n=\\alpha_i)|\\Theta]\\] for \\(i\\in[1:I]\\).\n즉, \\(\\mathbf{D}(i,n)\\)는 첫 번째 \\(n\\)개 관측을 설명하고 \\(s_{n}=\\alpha_{i}\\) 상태에서 끝나는 단일 상태 시퀀스 \\((s_{1},\\ldots,s_{n})\\)에서 가장 높은 확률이다.\n다음의 최대값을 산출하는 상태 시퀀스는 uncovering 문제의 해답이다. \\[\\mathbf{Prob}^\\ast = \\underset{i\\in[1:I]}{\\max}\\,\\,\\mathbf{D}(i,N)\\]\n\\((I\\times N)\\) 행렬 \\(\\mathbf{D}\\)는 열 인덱스 \\(n\\in[1:N]\\)를 따라 재귀적으로 계산될 수 있다.\nViterbi 알고리즘은 다음과 같다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_T02.png\", width=600)\n\n\n\n\n\n다음 그림은 Viterbi 알고리즘의 주요 단계를 보여준다.\n\n파란색 셀은 알고리즘의 초기화 역할을 하는 \\(\\mathbf{D}(i,1)\\) 항목을 나타낸다.\n빨간색 셀은 주 반복(main iteration) 의 계산을 나타낸다.\n검은색 셀은 최적의 상태 시퀀스를 얻기 위해 역 추적에 사용되는 최대화 인덱스를 나타낸다.\n행렬 \\(E\\)는 재귀에서 최대화 인덱스를 추적하는 데 사용된다. 이 정보는 두 번째 단계에서 백트래킹(backtracking) 을 사용하여 최적의 상태 시퀀스를 구성할 때 필요하다.\n\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F27.png\", width=600)\n\n\n\n\n\n전체 절차는 DTW 알고리즘과 유사하며, 먼저 누적 비용 매트릭스를 구성한 다음 백트래킹을 사용하여 최적의 워핑 경로를 얻는다.\n앞의 그림과 같이 Viterbi 알고리즘은 각각 \\(I\\)개 노드(상태)로 구성된 \\(N\\)개 레이어로 그래프와 같은 구조를 구축하는 반복적인 방식으로 진행된다.\n또한 두 개의 이웃 레이어는 \\(I^2\\) 에지로 연결되며, 이는 주어진 레이어에서 새 레이어를 구성하는 데 필요한 작업 수의 순서도 결정한다.\n이로부터 Viterbi 알고리즘의 계산 복잡도는 \\(O(N\\cdot I^2)\\)로, 나이브한 접근에 필요한 \\(O(I^N)\\)보다 훨씬 낫다.\n예시\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F28a.png\", width=700))\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F28b.png\", width=700))"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#viterbi-알고리즘-실행",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#viterbi-알고리즘-실행",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "Viterbi 알고리즘 실행",
    "text": "Viterbi 알고리즘 실행\n\ndef viterbi(A, C, B, O):\n    \"\"\"Viterbi algorithm for solving the uncovering problem\n\n    Args:\n        A (np.ndarray): State transition probability matrix of dimension I x I\n        C (np.ndarray): Initial state distribution  of dimension I\n        B (np.ndarray): Output probability matrix of dimension I x K\n        O (np.ndarray): Observation sequence of length N\n\n    Returns:\n        S_opt (np.ndarray): Optimal state sequence of length N\n        D (np.ndarray): Accumulated probability matrix\n        E (np.ndarray): Backtracking matrix\n    \"\"\"\n    I = A.shape[0]    # Number of states\n    N = len(O)  # Length of observation sequence\n\n    # Initialize D and E matrices\n    D = np.zeros((I, N))\n    E = np.zeros((I, N-1)).astype(np.int32)\n    D[:, 0] = np.multiply(C, B[:, O[0]])\n\n    # Compute D and E in a nested loop\n    for n in range(1, N):\n        for i in range(I):\n            temp_product = np.multiply(A[:, i], D[:, n-1])\n            D[i, n] = np.max(temp_product) * B[i, O[n]]\n            E[i, n-1] = np.argmax(temp_product)\n\n    # Backtracking\n    S_opt = np.zeros(N).astype(np.int32)\n    S_opt[-1] = np.argmax(D[:, -1])\n    for n in range(N-2, -1, -1):\n        S_opt[n] = E[int(S_opt[n+1]), n]\n\n    return S_opt, D, E\n\n\n# Define model parameters\nA = np.array([[0.8, 0.1, 0.1], \n              [0.2, 0.7, 0.1], \n              [0.1, 0.3, 0.6]])\n\nC = np.array([0.6, 0.2, 0.2])\n\nB = np.array([[0.7, 0.0, 0.3], \n              [0.1, 0.9, 0.0], \n              [0.0, 0.2, 0.8]])\n\n\nO = np.array([0, 2, 0, 2, 2, 1]).astype(np.int32)\n#O = np.array([1]).astype(np.int32)\n#O = np.array([1, 2, 0, 2, 2, 1]).astype(np.int32)\n\n# Apply Viterbi algorithm\nS_opt, D, E = viterbi(A, C, B, O)\n#\nprint('Observation sequence:   O = ', O)\nprint('Optimal state sequence: S = ', S_opt)\nnp.set_printoptions(formatter={'float': \"{: 7.4f}\".format})\nprint('D =', D, sep='\\n')\nnp.set_printoptions(formatter={'float': \"{: 7.0f}\".format})\nprint('E =', E, sep='\\n')\n\nObservation sequence:   O =  [0 2 0 2 2 1]\nOptimal state sequence: S =  [0 0 0 2 2 1]\nD =\n[[ 0.4200  0.1008  0.0564  0.0135  0.0033  0.0000]\n [ 0.0200  0.0000  0.0010  0.0000  0.0000  0.0006]\n [ 0.0000  0.0336  0.0000  0.0045  0.0022  0.0003]]\nE =\n[[0 0 0 0 0]\n [0 0 0 0 2]\n [0 2 0 2 2]]"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#viterbi-algorithm의-log-domain-실행",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#viterbi-algorithm의-log-domain-실행",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "Viterbi Algorithm의 log-domain 실행",
    "text": "Viterbi Algorithm의 log-domain 실행\n\nViterbi 알고리즘의 각 반복에서 \\(n-1\\) 단계에서 누적된 \\(\\mathbf{D}\\)의 확률값에 \\(A\\)와 \\(B\\)의 두 확률값을 곱한다.\n더 정확하게는 다음과 같다.\n\n\\[\\mathbf{D}(i,n) = \\max_{j\\in[1:I]}\\big(a_{ij} \\cdot\\mathbf{D}(j,n-1) \\big) \\cdot b_ {i_{k_n}}\\]\n\n모든 확률 값은 \\([0,1]\\) 구간에 있기 때문에 이러한 값의 곱은 반복 횟수 \\(n\\)에 따라 기하급수적으로 감소한다. 결과적으로 \\(N\\)이 큰 입력 시퀀스 \\(O=(o_{1},o_{2},\\ldots o_{N})\\)의 경우 \\(\\mathbf{D}\\)의 값은 매우 작아지며, 결국 수치적 언더플로(underflow)로 이어질 수 있다.\n확률 값의 곱을 다룰 때 잘 알려진 트릭은 log-domain에서 작업하는 것이다. 이를 위해 모든 확률 값에 로그를 적용하고 곱셈을 합산으로 대체한다. 로그는 강한 단조 함수이기 때문에 순서 관계는 로그 도메인에서 보존되어 최대화 또는 최소화와 같은 작업을 전달할 수 있다.\n다음 코드 셀에서는 Viterbi 알고리즘의 로그 변형을 제공한다. 이 변형은 누적 로그 확률 행렬 \\(\\log(\\mathbf{D})\\) (D_log)(로그가 각 항목에 적용됨)뿐만 아니라 원래 알고리즘과 정확히 동일한 최적 상태 시퀀스 \\(S^\\ast\\) 및 동일한 백트래킹 행렬 \\(E\\)를 생성한다. 또한 온전성 검사로서 계산된 로그 행렬 D_log에 지수 함수를 적용하여 위의 원래 Viterbi 알고리즘에 의해 계산된 행렬 D를 산출해야 한다.\n\n\ndef viterbi_log(A, C, B, O):\n    \"\"\"Viterbi algorithm (log variant) for solving the uncovering problem\n\n    Args:\n        A (np.ndarray): State transition probability matrix of dimension I x I\n        C (np.ndarray): Initial state distribution  of dimension I\n        B (np.ndarray): Output probability matrix of dimension I x K\n        O (np.ndarray): Observation sequence of length N\n\n    Returns:\n        S_opt (np.ndarray): Optimal state sequence of length N\n        D_log (np.ndarray): Accumulated log probability matrix\n        E (np.ndarray): Backtracking matrix\n    \"\"\"\n    I = A.shape[0]    # Number of states\n    N = len(O)  # Length of observation sequence\n    tiny = np.finfo(0.).tiny\n    A_log = np.log(A + tiny)\n    C_log = np.log(C + tiny)\n    B_log = np.log(B + tiny)\n\n    # Initialize D and E matrices\n    D_log = np.zeros((I, N))\n    E = np.zeros((I, N-1)).astype(np.int32)\n    D_log[:, 0] = C_log + B_log[:, O[0]]\n\n    # Compute D and E in a nested loop\n    for n in range(1, N):\n        for i in range(I):\n            temp_sum = A_log[:, i] + D_log[:, n-1]\n            D_log[i, n] = np.max(temp_sum) + B_log[i, O[n]]\n            E[i, n-1] = np.argmax(temp_sum)\n\n    # Backtracking\n    S_opt = np.zeros(N).astype(np.int32)\n    S_opt[-1] = np.argmax(D_log[:, -1])\n    for n in range(N-2, -1, -1):\n        S_opt[n] = E[int(S_opt[n+1]), n]\n\n    return S_opt, D_log, E\n\n\n# Apply Viterbi algorithm (log variant)\nS_opt, D_log, E = viterbi_log(A, C, B, O)\n\nprint('Observation sequence:   O = ', O)\nprint('Optimal state sequence: S = ', S_opt)\nnp.set_printoptions(formatter={'float': \"{: 7.2f}\".format})\nprint('D_log =', D_log, sep='\\n')\nnp.set_printoptions(formatter={'float': \"{: 7.4f}\".format})\nprint('exp(D_log) =', np.exp(D_log), sep='\\n')\nnp.set_printoptions(formatter={'float': \"{: 7.0f}\".format})\nprint('E =', E, sep='\\n')\n\nObservation sequence:   O =  [0 2 0 2 2 1]\nOptimal state sequence: S =  [0 0 0 2 2 1]\nD_log =\n[[  -0.87   -2.29   -2.87   -4.30   -5.73 -714.35]\n [  -3.91 -711.57   -6.90 -713.57 -715.00   -7.44]\n [-710.01   -3.39 -712.30   -5.40   -6.13   -8.25]]\nexp(D_log) =\n[[ 0.4200  0.1008  0.0564  0.0135  0.0033  0.0000]\n [ 0.0200  0.0000  0.0010  0.0000  0.0000  0.0006]\n [ 0.0000  0.0336  0.0000  0.0045  0.0022  0.0003]]\nE =\n[[0 0 0 0 0]\n [0 0 0 0 2]\n [0 2 0 2 2]]"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#방출-우도의-구체화-specification-of-emission-likelihoods",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#방출-우도의-구체화-specification-of-emission-likelihoods",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "방출 우도의 구체화 (Specification of Emission Likelihoods)",
    "text": "방출 우도의 구체화 (Specification of Emission Likelihoods)\n\n화음 인식 시나리오에서 관측값(observations)은 주어진 오디오 녹음에서 이전에 추출된 크로마 벡터이다. 즉, 관측값은 연속적인 특징 공간 \\(\\mathcal{F}=\\mathbb{R}^{12}\\)의 요소인 \\(12\\)차원의 실수 벡터이다.\n지금까지는 관측값이 유한 출력 공간 \\(\\mathcal{B}\\)에서 나오는 이산 기호인 이산(discrete) HMM의 경우만 고려했다. 이산 HMM을 시나리오에 적용하기 위해 가능한 절차 중 하나는 소위 코드북(codebook) 이라고 하는 프로토타입 벡터의 유한 세트를 도입하는 것이다. 이러한 코드북은 연속적인 특징 공간 \\(\\mathcal{F}=\\mathbb{R}^{12}\\)의 이산화로 간주될 수 있으며, 여기서 각 코드북 벡터는 전체 범위의 특징 벡터를 나타낸다. 그런 다음 이 유한한 코드북 벡터 세트를 기반으로 방출 확률(emission probability)을 결정할 수 있다.\n대안으로 다음 HMM 변형에서 이산 출력 공간 \\(\\mathcal{B}\\)를 연속적인 특징 공간 \\(\\mathcal{F}=\\mathbb{R}^{12}\\) 및 우도 함수(likelihood functions) 에 의한 방출 확률 행렬 \\(B\\)로 대체한다. 특히, 주어진 상태의 방출 확률은 state-dependent한 정규화된 템플릿과 정규화된 관측(크로마) 벡터의 내적으로 정의되는 정규화된 유사성(similarity) 값으로 대체된다.\n\\(s:\\mathcal{F} \\times \\mathcal{F} \\to [0,1]\\)를 정규화된 크로미 벡터의 내적에 의해 정의된 유사도 측정(similarity measure) 이라고 하자(단, 0으로 나누지 못하도록 임계값을 설정한 정규화 사용): \\[s(x, y) = \\frac{\\langle x,y\\rangle}{\\|x\\|_2\\cdot\\|y\\|_2}\\] for \\(x,y\\in\\mathcal{F}\\)\n\\(I=24\\)개의 장,단3화음 (상태 \\(\\mathcal{A}\\)로 인코딩되고 \\([1:I]\\) 집합으로 인덱싱됨)을 기반으로 이진 화음 템플릿(binary chord templates) \\(\\mathbf{t}_i\\in \\mathcal{F}\\) for \\(i\\in [1:I]\\)을 고려한다.\n그런 다음 state-dependent likelihood function \\(b_i:\\mathcal{F}\\to [0,1]\\)를 다음과 같이 정의한다. \\[b_i(x) := \\frac{s(x, \\mathbf{t}_i)}{\\sum_{j\\in[1:I]}s(x, \\mathbf{t}_j)}\\] for \\(x\\in\\mathcal{F}\\) and \\(i\\in [1:I]\\)\n시나리오에서 관측 시퀀스 \\(O=(o_{1},o_{2},\\ldots,o_{N})\\)는 크로마 벡터 \\(o_n\\in\\mathcal{F}\\)의 시퀀스이다.\n관측-종속적인 \\((I\\times N)\\)-행렬 \\(B[O]\\)를 다음과 같이 정의한다. \\[B[O](i,n) = b_i(o_n)\\] for \\(i\\in[1:I]\\) and \\(n\\in[1:N]\\)\n행렬은 템플릿 기반 화음 인식에 대한 설명에서 소개되었듯이, 시간-화음 표현의 형태로 시각화된 column-wise \\(\\ell^1\\)-정규화를 사용하는 화음 유사성 행렬(chord similarity matrix) 이다. Viterbi 알고리즘의 맥락에서 likelihood \\(B[O](i,n)\\)는 확률 값 \\(b_{ik_n}\\)를 대체하는 데 사용된다.\n\n\nipd.display(ipd.Image(\"../img/6.chord_recognition/FMP_C5_F20a.png\", width=500))\nipd.display(ipd.Audio(\"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer.wav\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Specify \nfn_wav = \"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer.wav\"\nfn_ann = \"../data_FMP/FMP_C5_F20_Bach_BWV846-mm1-4_Fischer_ChordAnnotations.csv\"\ncolor_ann = {'C': [1, 0.5, 0, 1], 'G': [0, 1, 0, 1], 'Dm': [1, 0, 0, 1], 'N': [1, 1, 1, 1]}\n\nN = 4096\nH = 1024\nX, Fs_X, x, Fs, x_dur = \\\n    compute_chromagram_from_filename(fn_wav, N=N, H=H, gamma=0.1, version='STFT')\nN_X = X.shape[1]\n\n# Chord recogntion\nchord_sim, chord_max = chord_recognition_template(X, norm_sim='1')\nchord_labels = get_chord_labels(nonchord=False)\n\n# Annotations\nchord_labels = get_chord_labels(ext_minor='m', nonchord=False)\nann_matrix, ann_frame, ann_seg_frame, ann_seg_ind, ann_seg_sec = convert_chord_ann_matrix(\n    fn_ann, chord_labels, Fs=Fs_X, N=N_X, last=True)\n#P, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_max)\n\n\n# Plot\ncmap = compressed_gray_cmap(alpha=1, reverse=False)\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1.5, 3, 0.2]}, figsize=(8, 6))\n\nplot_chromagram(X, ax=[ax[0, 0], ax[0, 1]], Fs=Fs_X, clim=[0, 1], xlabel='',\n                         title='Observation sequence (chromagram with feature rate = %0.1f Hz)' % (Fs_X))\nplot_segments_overlay(ann_seg_sec, ax=ax[0, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_matrix(chord_sim, ax=[ax[1, 0], ax[1, 1]], Fs=Fs_X, clim=[0, np.max(chord_sim)],\n                     title='Likelihood matrix (time–chord representation)',\n                     ylabel='Chord', xlabel='')\nax[1, 0].set_yticks(np.arange(len(chord_labels)))\nax[1, 0].set_yticklabels(chord_labels)\nplot_segments_overlay(ann_seg_sec, ax=ax[1, 0], time_max=x_dur,\n                               print_labels=False, colors=color_ann, alpha=0.1)\n\nplot_segments(ann_seg_sec, ax=ax[2, 0], time_max=x_dur, time_label='Time (seconds)',\n                       colors=color_ann,  alpha=0.3)\nax[2,1].axis('off')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#전이-확률의-구체화-specification-of-transition-probabilities",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#전이-확률의-구체화-specification-of-transition-probabilities",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "전이 확률의 구체화 (Specification of Transition Probabilities)",
    "text": "전이 확률의 구체화 (Specification of Transition Probabilities)\n\n음악에서 특정 화음 전환은 다른 화음 전환보다 더 자주 발생하기 마련이다. 따라서 HMM을 사용할 수 있으며, 다양한 화음 사이의 1차(first-oreder) 시간(temporal) 관계는 전이 확률 행렬(transition probability matrix) \\(A\\)로 캡처할 수 있다.\n다음에서 \\(\\alpha_{i}\\rightarrow\\alpha_{j}\\) 표기법을 사용하여 \\(\\alpha_{i}\\) 상태에서 \\(\\alpha_{j}\\) 상태로 전환하는 것을 나타낸다. (for \\(i,j\\in[1:I]\\))\n예를 들어, 계수 \\(a_{1,2}\\)는 전환 \\(\\alpha_{1}\\rightarrow\\alpha_{2}\\)(\\(\\mathbf{C}\\rightarrow\\mathbf{C} ^\\sharp\\)에 해당)의 확률을 표현하고, 반면 \\(a_{1,8}\\)는 \\(\\alpha_{1}\\rightarrow\\alpha_{8}\\)(\\(\\mathbf{C}\\rightarrow\\mathbf{G}\\)에 해당)의 확률을 나타낸다.\n실제 음악에서는 으뜸음(tonic)에서 딸림음(dominant)으로의 변화가 반음 하나 변화하는 것보다 훨씬 더 많으므로 \\(a_{1,8}\\)는 \\(a_{1,2}\\)보다 훨씬 커야 한다.\n계수 \\(a_{i,i}\\)는 \\(\\alpha_{i}\\)(for \\(i\\in[1:I]\\)) (즉, \\(\\alpha_{i}\\rightarrow\\alpha_{i}\\)) 상태에 머무를 확률을 나타낸다. 이러한 계수는 자기 전이(self-transition) 확률이라고도 한다.\n전이 확률 행렬은 여러 가지 방법으로 구체화될 수 있다.\n\n예를 들어, 행렬은 화성 이론(harmony theory)의 규칙에 따라 음악 전문가가 수동으로 정의할 수 있다. 가장 일반적인 접근 방식은 레이블이 지정된 데이터에서 전환 확률을 추정하여 이러한 행렬을 자동으로 생성하는 것이다.\n\n다음 그림에서는 세 가지 다른 전이 행렬을 보여준다(시각화 목적으로 로그 확률 척도를 사용).\n\n첫 번째는 비틀즈 컬렉션을 기반으로 레이블이 지정된 프레임 시퀀스에서 바이그램(bigram)(인접 요소 쌍)을 사용하여 레이블이 지정된 훈련 데이터에서 학습되었다. 예를 들어 계수 \\(a_{1,8}\\)(전환 \\(\\mathbf{C}\\rightarrow\\mathbf{G}\\)에 해당)가 강조 표시되어있다.\n두 번째 행렬은 이전 행렬에서 얻은 조옮김 불변(transposition-invariant) 전이 확률 행렬이다. 조옮김 불변성을 달성하기 위해 레이블이 지정된 훈련 데이터 세트는 고려되는 바이그램에 대해 가능한 모든 12개의 순환 크로마 이동(cyclic chroma shifts)을 고려하여 보강된다.\n세 번째 행렬은 주 대각선(자기 전이)에서 큰 값을 갖고 나머지 모든 위치에서 훨씬 작은 값을 갖는 균일(uniform) 전이 확률 행렬이다.\n\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F29-30-32.png\")\n\n\n\n\n\n다음 코드 셀에서 비틀즈 컬렉션을 기반으로 추정된(6.4.에서 이를 다루기로 함) 미리 계산된 전환 행렬이 포함된 CSV 파일을 읽는다. 그림에서는 확률 값과 로그 확률 값을 모두 표시한다.\n\n\ndef plot_transition_matrix(A, log=True, ax=None, figsize=(6, 5), title='',\n                           xlabel='State (chord label)', ylabel='State (chord label)',\n                           cmap='gray_r', quadrant=False):\n    \"\"\"Plot a transition matrix for 24 chord models (12 major and 12 minor triads)\n\n    Args:\n        A: Transition matrix\n        log: Show log probabilities (Default value = True)\n        ax: Axis (Default value = None)\n        figsize: Width, height in inches (only used when ax=None) (Default value = (6, 5))\n        title: Title for plot (Default value = '')\n        xlabel: Label for x-axis (Default value = 'State (chord label)')\n        ylabel: Label for y-axis (Default value = 'State (chord label)')\n        cmap: Color map (Default value = 'gray_r')\n        quadrant: Plots additional lines for C-major and C-minor quadrants (Default value = False)\n\n    Returns:\n        fig: The created matplotlib figure or None if ax was given.\n        ax: The used axes.\n        im: The image plot\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n        ax = [ax]\n\n    if log is True:\n        A_plot = np.log(A)\n        cbar_label = 'Log probability'\n        clim = [-6, 0]\n    else:\n        A_plot = A\n        cbar_label = 'Probability'\n        clim = [0, 1]\n    im = ax[0].imshow(A_plot, origin='lower', aspect='equal', cmap=cmap, interpolation='nearest')\n    im.set_clim(clim)\n    plt.sca(ax[0])\n    cbar = plt.colorbar(im)\n    ax[0].set_xlabel(xlabel)\n    ax[0].set_ylabel(ylabel)\n    ax[0].set_title(title)\n    cbar.ax.set_ylabel(cbar_label)\n\n    chord_labels = get_chord_labels()\n    chord_labels_squeezed = chord_labels.copy()\n    for k in [1, 3, 6, 8, 10, 11, 13, 15, 17, 18, 20, 22]:\n        chord_labels_squeezed[k] = ''\n\n    ax[0].set_xticks(np.arange(24))\n    ax[0].set_yticks(np.arange(24))\n    ax[0].set_xticklabels(chord_labels_squeezed)\n    ax[0].set_yticklabels(chord_labels)\n\n    if quadrant is True:\n        ax[0].axvline(x=11.5, ymin=0, ymax=24, linewidth=2, color='r')\n        ax[0].axhline(y=11.5, xmin=0, xmax=24, linewidth=2, color='r')\n\n    return fig, ax, im\n\n\n# Load transition matrix estimated on the basis of the Beatles collection\nfn_csv = '../data_FMP/FMP_C5_transitionMatrix_Beatles.csv'\nA_est_df = pd.read_csv(fn_csv, delimiter=';')\nA_est = A_est_df.to_numpy('float64')\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 1], \n                                          'height_ratios': [1]}, \n                       figsize=(10, 3.8))\n\nplot_transition_matrix(A_est, log=False, ax=[ax[0]], title='Transition matrix')\nplot_transition_matrix(A_est, ax=[ax[1]], title='Transition matrix with log probabilities')\nplt.tight_layout()\n\n\n\n\n\n조옮김-불변 전이 행렬 (transposition-invariant transition matrix)을 얻기 위해 원래 행렬의 4개 사분면(장화음, 단화음 영역으로 정의됨)을 순환 이동하고 평균화하여 행렬 수준에서 순환 크로마 이동을 시뮬레이션한다.\n그림에서 원본의 전이 행렬과 결과의 옮김-불변 전이 행렬을 보여준다.\n\n\ndef matrix_circular_mean(A):\n    \"\"\"Computes circulant matrix with mean diagonal sums\n\n    Args:\n        A (np.ndarray): Square matrix\n\n    Returns:\n        A_mean (np.ndarray): Circulant output matrix\n    \"\"\"\n    N = A.shape[0]\n    A_shear = np.zeros((N, N))\n    for n in range(N):\n        A_shear[:, n] = np.roll(A[:, n], -n)\n    circ_sum = np.sum(A_shear, axis=1)\n    A_mean = circulant(circ_sum) / N\n    return A_mean\n    \n    \ndef matrix_chord24_trans_inv(A):\n    \"\"\"Computes transposition-invariant matrix for transition matrix\n    based 12 major chords and 12 minor chords\n\n    Args:\n        A (np.ndarray): Input transition matrix\n\n    Returns:\n        A_ti (np.ndarray): Output transition matrix\n    \"\"\"\n    A_ti = np.zeros(A.shape)\n    A_ti[0:12, 0:12] = matrix_circular_mean(A[0:12, 0:12])\n    A_ti[0:12, 12:24] = matrix_circular_mean(A[0:12, 12:24])\n    A_ti[12:24, 0:12] = matrix_circular_mean(A[12:24, 0:12])\n    A_ti[12:24, 12:24] = matrix_circular_mean(A[12:24, 12:24])\n    return A_ti\n\n\nA_ti = matrix_chord24_trans_inv(A_est)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 1], \n                                          'height_ratios': [1]}, \n                       figsize=(10, 3.8))\n\nplot_transition_matrix(A_est, ax=[ax[0]], quadrant=True, \n                       title='Transition matrix')\nplot_transition_matrix(A_ti, ax=[ax[1]], quadrant=True, \n                       title='Transposition-invariant transition matrix')\nplt.tight_layout()\n\n\n\n\n\n마지막으로 uniform 전이 확률 행렬을 생성하는 기능을 보자. 이 함수에는 자기 전이 확률(주대각선의 값)을 결정하는 매개변수 \\(p\\in[0,1]\\)가 있다. 나머지 위치에 대한 확률은 결과 행렬이 확률 행렬(즉, 모든 행과 열의 합이 1이 됨)이 되도록 설정한다.\n\n\ndef uniform_transition_matrix(p=0.01, N=24):\n    \"\"\"Computes uniform transition matrix\n\n    Args:\n        p (float): Self transition probability (Default value = 0.01)\n        N (int): Column and row dimension (Default value = 24)\n\n    Returns:\n        A (np.ndarray): Output transition matrix\n    \"\"\"\n    off_diag_entries = (1-p) / (N-1)     # rows should sum up to 1\n    A = off_diag_entries * np.ones([N, N])\n    np.fill_diagonal(A, p)\n    return A\n\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 1], \n                                          'height_ratios': [1]}, \n                       figsize=(10, 3.8))\n\np = 0.5\nA_uni = uniform_transition_matrix(p)\nplot_transition_matrix(A_uni, ax=[ax[0]], title='Uniform transition matrix (p=%0.2f)' % p)\np = 0.9\nA_uni = uniform_transition_matrix(p)\nplot_transition_matrix(A_uni, ax=[ax[1]], title='Uniform transition matrix (p=%0.2f)' % p)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm-기반-화음-인식-hmm-based-chord-recognition",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#hmm-기반-화음-인식-hmm-based-chord-recognition",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "HMM 기반 화음 인식 (HMM-Based Chord Recognition)",
    "text": "HMM 기반 화음 인식 (HMM-Based Chord Recognition)\n\n앞에서 설명한 것처럼 HMM 기반 모델의 자유 매개변수는 훈련 세트에서 자동으로 학습하거나 음악적 지식을 사용하여 수동으로 설정할 수 있다.\nBach 예제를 계속 사용하여 HMM을 화음 인식 시나리오에 적용한 효과를 본다. 다음 설정을 사용한다.\n\n관찰 시퀀스 \\(O\\)으로, 크로마 벡터 시퀀스를 사용한다.\n전이 확률 행렬 \\(A\\)는 균일(uniform) 전이 행렬을 사용한다.\n초기 상태 확률 벡터 \\(C\\)는 균일 분포를 사용한다.\n방출(emission) 확률 행렬 \\(B\\)는 템플릿 기반 화음 인식에도 사용되는 화음 유사성 행렬의 정규화된 버전인 liklihood 행렬 \\(B[O]\\)로 대체한다.\n프레임별 화음 인식 결과는 Viterbi 알고리즘에 의해 계산된 상태 시퀀스에 의해 제공된다.\n\n방출 확률 대신 likelihood 행렬 \\(B[O]\\)를 사용하려면 원래 알고리즘을 약간 수정해야 한다. 다음 코드 셀에서는 수치적으로 안정적인 로그 버전을 사용하여 수정한다. 그런 다음 HMM 기반 결과와 템플릿 기반 접근 방식을 비교하여 평가 결과를 시간-화음 시각화 형태로 각각 보여준다.\n\n\ndef viterbi_log_likelihood(A, C, B_O):\n    \"\"\"Viterbi algorithm (log variant) for solving the uncovering problem\n\n    Args:\n        A (np.ndarray): State transition probability matrix of dimension I x I\n        C (np.ndarray): Initial state distribution  of dimension I\n        B_O (np.ndarray): Likelihood matrix of dimension I x N\n\n    Returns:\n        S_opt (np.ndarray): Optimal state sequence of length N\n        S_mat (np.ndarray): Binary matrix representation of optimal state sequence\n        D_log (np.ndarray): Accumulated log probability matrix\n        E (np.ndarray): Backtracking matrix\n    \"\"\"\n    I = A.shape[0]    # Number of states\n    N = B_O.shape[1]  # Length of observation sequence\n    tiny = np.finfo(0.).tiny\n    A_log = np.log(A + tiny)\n    C_log = np.log(C + tiny)\n    B_O_log = np.log(B_O + tiny)\n\n    # Initialize D and E matrices\n    D_log = np.zeros((I, N))\n    E = np.zeros((I, N-1)).astype(np.int32)\n    D_log[:, 0] = C_log + B_O_log[:, 0]\n\n    # Compute D and E in a nested loop\n    for n in range(1, N):\n        for i in range(I):\n            temp_sum = A_log[:, i] + D_log[:, n-1]\n            D_log[i, n] = np.max(temp_sum) + B_O_log[i, n]\n            E[i, n-1] = np.argmax(temp_sum)\n\n    # Backtracking\n    S_opt = np.zeros(N).astype(np.int32)\n    S_opt[-1] = np.argmax(D_log[:, -1])\n    for n in range(N-2, -1, -1):\n        S_opt[n] = E[int(S_opt[n+1]), n]\n\n    # Matrix representation of result\n    S_mat = np.zeros((I, N)).astype(np.int32)\n    for n in range(N):\n        S_mat[S_opt[n], n] = 1\n\n    return S_mat, S_opt, D_log, E\n\n\nA = uniform_transition_matrix(p=0.5)\nC = 1 / 24 * np.ones((1, 24))\nB_O = chord_sim\nchord_HMM, _, _, _ = viterbi_log_likelihood(A, C, B_O)\n\nP, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_HMM)\ntitle = 'HMM-Based approach (N=%d, TP=%d, FP=%d, FN=%d, P=%.2f, R=%.2f, F=%.2f)' % (N_X, TP, FP, FN, P, R, F)\nfig, ax, im = plot_matrix_chord_eval(ann_matrix, chord_HMM, Fs=1, \n                     title=title, ylabel='Chord', xlabel='Time (frames)', chord_labels=chord_labels)\nplt.tight_layout()\nplt.show()\n\nP, R, F, TP, FP, FN = compute_eval_measures(ann_matrix, chord_max)\ntitle = 'Template-based approach (N=%d, TP=%d, FP=%d, FN=%d, P=%.2f, R=%.2f, F=%.2f)' %\\\n    (N_X, TP, FP, FN, P, R, F)\nfig, ax, im = plot_matrix_chord_eval(ann_matrix, chord_max, Fs=1, \n                     title=title, ylabel='Chord', xlabel='Time (frames)', chord_labels=chord_labels)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n이 예에서는 HMM 기반 화음 인식기가 템플릿 기반 접근 방식보다 우수하다.\nHMM 기반 접근 방식의 개선은 특히 상황에 맞는 평활화(smoothing)를 도입하는 전이 모델(transition model) 에서 비롯된다. 높은 자기-전이 확률의 경우, 화음 인식기는 다른 화음으로 변경하기보다 현재 화음에 머무르는 경향이 있으며, 이는 일종의 평활화라고 볼 수 있다. 이 효과는 깨진(broken) 화음이 짧은 시간의 많은 화음 모호성을 유발하는 Bach 예제에서도 입증된다. 이러한 효과는 간단한 템플릿 기반 화음 인식기를 사용할 때 많은 무작위와 같은 화음 변경으로 이어지게 한다.\nHMM 기반 접근 방식을 사용하면 상대적으로 낮은 전이 확률이 방출 확률의 충분한 증가로 보상되는 경우에만 화음 변경이 수행된다. 결과적으로 지배적인 화음 변경만 남는다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#사전-필터링-vs.-사후-필터링-prefiltering-vs.-postfiltering",
    "href": "posts/6. Chord Recognition/6.3.HMM-Based_Chord_Recognition.html#사전-필터링-vs.-사후-필터링-prefiltering-vs.-postfiltering",
    "title": "6.3. HMM 기반 화음 인식",
    "section": "사전 필터링 vs. 사후 필터링 (Prefiltering vs. Postfiltering)",
    "text": "사전 필터링 vs. 사후 필터링 (Prefiltering vs. Postfiltering)\n\n화음 인식 평가에서 다룬 입력 크로마그램을 계산할 때 더 긴 윈도우 크기를 적용하여 개선을 달성할 수 있음을 Bach 예제로 보여주었다.\n더 긴 윈도우 크기를 적용하면 관측 시퀀스의 시간적 평활화 정도가 더 많거나 적다. 이 평활화는 패턴 매칭 단계 이전에 수행되므로, 이 전략을 사전 필터링(prefiltering) 이라고도 한다. 이러한 사전 필터링 단계는 노이즈와 같은 프레임을 부드럽게 처리할 뿐만 아니라 특징적인 크로마 정보를 제거하고 전환을 흐리게 한다.\n사전 필터링과 달리 HMM 기반 접근 방식은 특징(feature) 표현을 그대로 둔다. 또한, 평활화는 패턴 매칭 단계와 조합하여 수행된다. 이러한 이유로 이 접근 방식을 사후 필터링(postfiltering) 이라고도 한다. 결과적으로 원래 크로마 정보가 보존되고 특징 표현의 전환이 선명하게 유지된다.\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F13.png\", width=700)\n\n\n\n\n\n여기서는 기본적인 HMM 변형만 다루었지만, 여러가지 변형과 확장적인 HMM(e.g. 연속 HMMs, HMMs with specific state transition topologies)을 쓸 수도 있다. HMM은 모형 매개변수를 고정하는 대신 자동적으로 자유 매개변수를 학습시키는(e.g. Baum-Welch 알고리즘) 장점이 있다. 모델 매개변수를 수동으로 고정하는 대신 일반 HMM은 학습 예제를 기반으로 자유 매개변수를 자동으로 학습하는 것이다(예: [Baum-Welch 알고리즘] 사용).\n참고: Hidden Markov Models for Speech Recognition by Huang et al. (1990).\n\n\nipd.Image(\"../img/6.chord_recognition/FMP_C5_F33.png\" , width=600)\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S3_HiddenMarkovModel.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S3_Viterbi.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S3_ChordRec_HMM.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html",
    "href": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html",
    "title": "6.4. 화음 인식 예시: 비틀즈",
    "section": "",
    "text": "간단한 예시로 비틀즈의 노래를 이용해 화음 인식 절차를 확인해보고, 결과를 해석해본다.\n실험: 비틀즈 컬렉션"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#사전-필터링prefiltering-실험",
    "href": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#사전-필터링prefiltering-실험",
    "title": "6.4. 화음 인식 예시: 비틀즈",
    "section": "사전 필터링(Prefiltering) 실험",
    "text": "사전 필터링(Prefiltering) 실험\n\n위에서 특징(feature)유형이 화음 인식 결과에 큰 영향을 미치는 것을 볼 수 있었다.\n마찬가지로 STFT 창 크기를 확대하면 화음 인식의 동작에 상당한 영향을 미칠 수 있다. 각 크로마 차원에 대해 시간 경과에 따른 일종의 local 평균을 계산하는 temporal smoothing를 적용할 때 유사한 효과를 얻을 수 있다.\n이제 \\(21.5~\\mathrm{Hz}\\)의 feature rate를 갖는 세 가지 크로마그램 표현(STFT, CQT, IIR)으로 시작하여 화음 인식 정확도에 대한 temporal smoothing의 효과를 확인해본다.\n\\(L\\in\\mathbb{N}\\)를 스무딩 필터의 길이(프레임 단위로 측정)라고 하자. 스무딩을 위해 각각 평균 또는 중앙값을 사용하는 두 함수 중 하나를 적용하여 중심(centered) 방식으로 평균을 계산한다.\n다운샘플링을 적용하지 않으므로 원래 feature rate를 \\(21.5~\\mathrm{Hz}\\)로 유지한다.\n다음에서는 스무딩 매개변수 \\(L\\in\\{1,3,5\\ldots,63,65\\}\\)의 sweep을 고려한다. 예를 들어 \\(L=21\\) 매개변수는 오디오 녹음의 약 1초에 해당한다. 스무딩은 실제 패턴 매칭 전에 적용되므로 이 단계를 사전 필터링(prefiltering)이라고 한다.\n\n\ndef compute_mean_result(result_dict, song_selected):\n    S = len(song_selected)\n    result_mean =  np.copy(result_dict[song_selected[0]])\n    for s in range(1, S):\n        result_mean = result_mean + result_dict[song_selected[s]]\n    result_mean = result_mean / S\n    return result_mean\n\n\ndef plot_statistics(para_list, song_dict, song_selected, result_dict, ax, \n                    ylim=None, title='', xlabel='', ylabel='F-measure', legend=True):\n    for s in song_selected:\n        color = song_dict[s][1]\n        song_id = song_dict[s][0]\n        ax.plot(para_list, result_dict[s], color=color, \n                linestyle=':', linewidth='2', label=song_id)\n    ax.plot(para_list, compute_mean_result(result_dict, song_selected), color='k', \n            linestyle='-',linewidth='2', label='Mean')\n    if legend==True:\n        ax.legend(loc='upper right', fontsize=8).get_frame().set_alpha(.6)\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.grid()\n    if ylim is not None:\n        ax.set_ylim(ylim)\n    ax.set_xlim([para_list[0], para_list[-1]])\n\n    \ndef experiment_chord_recognition(song_selected, song_dict, X_dict, ann_dict, \n                                 para_list, para_type=None, p=0.15, \n                                 filt_len=None, filt_type='mean', detail=True):\n    M = len(para_list)\n    result_F_Tem = np.zeros(M)\n    result_F_HMM = np.zeros(M)\n    result_F_Tem_dict = {}\n    result_F_HMM_dict = {}\n    for s in song_selected:\n        if detail is True:            \n            print('Processing:', song_dict[s][0])\n        for m in range(M): \n            if para_type == 'smooth':\n                filt_len = para_list[m]\n            if para_type == 'p':\n                p = para_list[m]\n            output = chord_recognition_all(X_dict[s], ann_dict[s][0], \n                                           filt_len=filt_len, filt_type=filt_type, p=p)\n            result_Tem, result_HMM, chord_Tem, chord_HMM, chord_sim = output\n            result_F_Tem[m] = result_Tem[2]\n            result_F_HMM[m] = result_HMM[2]\n        result_F_Tem_dict[s] = np.copy(result_F_Tem)\n        result_F_HMM_dict[s] = np.copy(result_F_HMM)\n    print()\n    return result_F_Tem_dict, result_F_HMM_dict\n\n\n4곡 각각에 대해 평활(smoothing) 매개변수 \\(L\\)에 따른 \\(\\mathrm{F}\\)스코어를 보여주는 평가 곡선을 계산한다. 또한, 4곡의 평균 곡선이 표시된다. 세 가지 크로마그램(STFT, CQT, IIR)과 두 개의 화음 인식기(템플릿, HMM)에 대한 곡선을 본다.\n\n\nsong_selected = [0, 1, 2, 3]\npara_list = np.arange(32) * 2 + 1    \nprint('===== Prefiltering experiment using STFT-based chromagrams =====')\nresult_STFT = experiment_chord_recognition(song_selected, song_dict, X_dict_STFT, \n                                           ann_dict_STFT, para_list, para_type='smooth', \n                                           p=0.15, filt_len=None, filt_type='mean')\nprint('===== Prefiltering experiment using CQT-based chromagrams =====')\nresult_CQT  = experiment_chord_recognition(song_selected, song_dict, X_dict_CQT, \n                                           ann_dict_CQT, para_list, para_type='smooth', \n                                           p=0.15, filt_len=None, filt_type='mean')\nprint('===== Prefiltering experiment using IIR-based chromagrams =====')\nresult_IIR  = experiment_chord_recognition(song_selected, song_dict, X_dict_IIR, \n                                           ann_dict_IIR, para_list, para_type='smooth', \n                                           p=0.15, filt_len=None, filt_type='mean')\n\n===== Prefiltering experiment using STFT-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane\n\n===== Prefiltering experiment using CQT-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane\n\n===== Prefiltering experiment using IIR-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane\n\n\n\n\n# Plot result\nfig, ax = plt.subplots(2, 3, figsize=(12, 8))\nxlabel='Smoothing length (frames)'\nylim = [0.3, 1]\ntitle='STFT, Template'\nplot_statistics(para_list, song_dict, song_selected, result_STFT[0], ax[0, 0], \n                ylim=ylim, title=title, xlabel=xlabel)\ntitle='STFT, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_STFT[1], ax[1, 0], \n                ylim=ylim, title=title, xlabel=xlabel)\ntitle='CQT, Template'                           \nplot_statistics(para_list, song_dict, song_selected, result_CQT[0], ax[0, 1], \n                ylim=ylim, title=title, xlabel=xlabel)\ntitle='CQT, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_CQT[1], ax[1, 1], \n                ylim=ylim, title=title, xlabel=xlabel)\ntitle='IIR, Template'                           \nplot_statistics(para_list, song_dict, song_selected, result_IIR[0], ax[0, 2], \n                ylim=ylim, title=title, xlabel=xlabel)\ntitle='IIR, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_IIR[1], ax[1, 2], \n                ylim=ylim, title=title, xlabel=xlabel)\nplt.tight_layout()  \n\n\n\n\n\nSTFT 경우와 템플릿 기반 화음 인식기에서 F-measure는 \\(L=1\\)에서 \\(\\mathrm{F}=0.39\\)이다. \\(L\\)가 증가함에 따라 향상되고 \\(20\\)에서 \\(30\\) 사이 \\(L\\)에 대해 대략 \\(\\mathrm{F}=0.55\\)의 최대값에 도달한다. 그런 다음 \\(L\\)를 더 올리면 다시 감소한다. CQT 기반 및 IIR 기반 크로마그램을 사용할 때도 유사한 경향을 관찰할 수 있다.\n모든 경우(STFT, CQT, IIR)에서 스무딩은 특징의 시간 변동(temporal fluctuations) 및 로컬 이상값(local outliers)을 제거하여 프레임별 분류 결과를 개선한다. 또한 스무딩은 음악적으로 동일한 화음에 속하는 비동기 음(nonsynchronous notes)을 통합할 수 있다. 반면에 스무딩은 시간 해상도 (temporal resolution)를 감소시키고 인식기가 짧은 듀레이션의 화음을 감지하지 못하게 할 수 있다.\n비틀즈 노래의 경우 오디오의 약 1~2초에 해당하는 스무딩 윈도우는 이상값에 대한 견고성 증가와 시간 해상도 감소 사이에서 적절한 절충안으로 보인다. 그러나 이 trade-off는 실험에 사용된 기본 오디오 자료의 화음 변화 속도에 크게 의존한다.\nHMM 기반 화음 인식기의 경우 특징의 시간적 스무딩은 화음 인식 정확도에 덜 중요하거나 심지어 부정적인 영향을 미친다. 이미 분류 단계에서 일부 사후 필터링(postfiltering) 을 통합한다. 따라서 분류에 앞서 추가적인 평활화는 전체 인식 결과를 더 이상 개선하지 않는다."
  },
  {
    "objectID": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#자기-전이-확률-self-transition-probability",
    "href": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#자기-전이-확률-self-transition-probability",
    "title": "6.4. 화음 인식 예시: 비틀즈",
    "section": "자기 전이 확률 (Self-Transition Probability)",
    "text": "자기 전이 확률 (Self-Transition Probability)\n\nHMM 기반 사후필터링의 맥락에서 Cho and Bello는 연구에서 자기 전이 확률(self-transition probability) 의 중요성을 강조했다.\n\\(\\mathbf{C}\\)에서 \\(\\mathbf{G}\\)로의 전환과 같이 음악적으로 두드러진 특정 전환의 강화(예: 으뜸음-딸림음 관계 표현)가 음악적으로 의미 있는 특정 화음의 증폭으로 이어질 수 있지만, 주요 개선은 기본적으로 화음 변경 횟수를 줄이는 높은 자기 전이 확률로부터 얻는다.\n균일 전리 확률 행렬을 사용할 수 있다. 이 행렬에서는 자기 전환을 제외하고 모든 전환에 동일한(상대적으로 작은) 확률 값이 할당된다. Cho and Bello의 연구에 따르면, 이 균일한 전이 확률 행렬을 사용하더라도 더 복잡한 전이 확률 행렬을 사용할 때와 유사한 개선을 얻을 수 있다.\n균일 전리 확률 행렬을 정의하려면 주대각선으로 인코딩된 자기 전이에 사용되는 매개변수 \\(p\\in[0,1]\\)를 지정해야 한다. 대각선이 아닌 확률은 다음과 같이 지정된다. \\[q = \\frac{1-p}{I}\\] 여기서 \\(I\\)는 HMM 상태(state)의 수이다(이 시나리오에서는 \\(I=24\\)임).\n다음 코드 셀에서 자기-전이 매개변수 \\(p\\in[0, 0.02, 0.04, \\ldots, 0.98, 1]\\)의 스윕(sweep)을 고려하는 작은 실험을 제시한다.\n관찰할 수 있듯이 \\(p\\approx0.22\\)(STFT 기반 크로마그램 사용 시)에 대해 대략 \\(\\mathrm{F}=0.60\\), \\(p\\approx0.18\\)(CQT 기반 및 IIR 기반 크로마그램 사용 시)에 대해 \\(\\mathrm{F}=0.80\\)를 얻는다.\n\\(p\\) 매개변수는 예상되는 화음 변화율과 관련된 속성을 반영하며, 이는 화음 주석들의 통계 및 사용된 feature rate에 크게 의존한다. 데이터 기반 기계 학습에서 자기 전환 매개변수는 독립적인 validation set를 사용하여 최적화할 수 있는 hyperparameter로 간주할 수 있다.\n\n\nsong_selected = [0, 1, 2, 3]\npara_list = (np.arange(51)) * 0.02\nprint('===== Self-transition probability experiment using STFT-based chromagrams =====')\nresult_STFT = experiment_chord_recognition(song_selected, song_dict, X_dict_STFT, ann_dict_STFT, \n                                           para_list, para_type='p', p=0.15, filt_len=None)\nprint('===== Self-transition probability experiment using CQT-based chromagrams =====')\nresult_CQT  = experiment_chord_recognition(song_selected, song_dict, X_dict_CQT, ann_dict_CQT, \n                                           para_list, para_type='p', p=0.15, filt_len=None)\nprint('===== Self-transition probability experiment using IIR-based chromagrams =====')\nresult_IIR  = experiment_chord_recognition(song_selected, song_dict, X_dict_IIR, ann_dict_IIR, \n                                           para_list, para_type='p', p=0.15, filt_len=None)\n\n# Plot result\nfig, ax = plt.subplots(1,3, figsize=(11, 3.5))\nxlabel='Transition probability'\nylim = [0.3, 0.95]\ntitle='STFT, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_STFT[1], ax[0], ylim=ylim, title=title, xlabel=xlabel)\ntitle='CQT, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_CQT[1], ax[1], ylim=ylim, title=title, xlabel=xlabel)\ntitle='IIR, HMM'\nplot_statistics(para_list, song_dict, song_selected, result_IIR[1], ax[2], ylim=ylim, title=title, xlabel=xlabel)\nplt.tight_layout()  \n\n===== Self-transition probability experiment using STFT-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane\n\n===== Self-transition probability experiment using CQT-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane\n\n===== Self-transition probability experiment using IIR-based chromagrams =====\nProcessing: Let It Be\nProcessing: Here Comes The Sun\nProcessing: Ob-La-Di Ob-La-Da\nProcessing: Penny Lane"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#클래스-불균형class-imbalance",
    "href": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#클래스-불균형class-imbalance",
    "title": "6.4. 화음 인식 예시: 비틀즈",
    "section": "클래스 불균형(Class Imbalance)",
    "text": "클래스 불균형(Class Imbalance)\n\n이전에 이미 언급했듯이 자동화된 화성 분석 및 참조 주석을 기반으로 한 평가는 일반적으로 상당히 단순화된다. 따라서 평가 측정(예: F-measure)은 알고리즘의 성능과 인식 결과의 음악적 관련성에 대해 매우 제한된 관점만 제공한다.\n구체적인 예로서 노래의 일부 전면적 통계를 기반으로 단일 장, 단3화음을 결정하고 모든 시간 프레임에 해당하는 화음 라벨을 출력하는 다소 둔한 화음 인식기를 고려해 보자.\n\n노래 “Ob-La-Di, Ob-La-Da”의 경우, 이 접근법은 노래의 키 \\(\\mathrm{B}^\\flat\\) major를 인식하고 tonic 화음 \\(\\mathrm{B}^\\flat\\)에 대해 노래에 대한 동일한 화음 라벨을 전체 길이 동안 출력할 수 있다.\n\n다음 코드 셀에서 볼 수 있듯이 이 절차는 \\(\\mathrm{F}=0.551\\)의 F-점수를 달성한다. 특히 CQT 기반 크로마그램을 사용하는 템플릿 기반 접근 방식이 \\(\\mathrm{F}=0.503\\)의 정확도만 달성한다는 점을 고려할 때 이는 나쁘지 않은 것 같다.\n\n그렇다면 \\(\\mathrm{F}=0.551\\) 값은 무엇을 의미할까?\n이 단조로운 절차가 템플릿 기반 접근 방식보다 “더 나은” 것인가?\n\n화음 라벨링 작업을 해결하는 대신, 이 단순한 인식기는 가장 흔한 화음 발생에 대한 해당 데이터의 통계에 대해 더 많은 정보를 알려준다.\n일반적으로 평가 결과를 보다 잘 평가하기 위해서는 데이터 분포에 대한 정보를 포함하는 것이 중요하다.\n특히 클래스 분포의 불균형(imbalance) 은 분류(classification) 접근 방식의 실제 성능에 대해서는 어떤 것도 이야기 할 수 없지만, 매우 높은 평가 측정이 나올 수 있다.\n이 논의를 마무리하기 위해, HMM 기반 접근법이 \\(\\mathrm{F}=0.895\\)의 F-측정값을 생성한다는 점에 주목하자. 이는 위의 단순한 접근법의 \\(\\mathrm{F}=0.551\\)보다 훨씬 높다.\n시각화에서 볼 수 있듯이(예: \\(1700\\)와 \\(2000\\) 프레임 사이의 섹션 참조) HMM 기반 접근 방식은 템플릿 기반 접근 방식의 일부 오류를 수정한다. 여기서 메이저 화음 \\(\\mathrm{E}^\\flat\\) (참조)는 마이너 화음 \\(\\mathrm{E}^\\flat\\mathrm{m}\\) (추정)과 혼동된다.\n이러한 “장-단 화음 혼동”은 자동화된 절차에서 발생하는 가장 일반적인 인식 오류 중 하나이다.\n\n이러한 혼동은 두 개의 3화음이 3개 음 중 2개인 근음(root note)과 5도(fifth)를 공유한다는 사실에 기인한다. 일반적으로 조화 부분음의 존재로 인해 더 많은 혼란이 발생할 수 있다(예: 근음의 5번째 배음은 장3도와 같음).\n\n\n\nsong_selected = [2]    \nfor s in song_selected:\n    output = chord_recognition_all(X_dict_CQT[s], ann_dict_CQT[s][0], p=0.15)\n    result_Tem, result_HMM, chord_Tem, chord_HMM, chord_sim = output\n    chord_dull = np.zeros(chord_Tem.shape)\n    chord_dull[10, :] = 1    \n    result_dull = compute_eval_measures(ann_dict_CQT[s][0], chord_dull)    \n\n    title='Song: %s [CQT; Dull]' % song_dict[s][0]\n    plot_chord_recognition_result(ann_dict_CQT[s][0], result_dull,\n                                  chord_dull, chord_labels, title=title)\n    title='Song: %s [CQT; Template]' % song_dict[s][0]\n    plot_chord_recognition_result(ann_dict_CQT[s][0], result_Tem, \n                                  chord_Tem, chord_labels, title=title)\n    title='Song: %s [CQT; HMM]' % song_dict[s][0]\n    plot_chord_recognition_result(ann_dict_CQT[s][0], result_HMM, \n                                  chord_HMM, chord_labels, title=title)"
  },
  {
    "objectID": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#화음-라벨-감소",
    "href": "posts/6. Chord Recognition/6.4.Chord_Recognition_with_the_Beatles_Example.html#화음-라벨-감소",
    "title": "6.4. 화음 인식 예시: 비틀즈",
    "section": "화음 라벨 감소",
    "text": "화음 라벨 감소\n\n화음 인식 방식의 또 다른 주요 문제는 \\(12\\) 장조 및 \\(12\\) 단조 3화음에 해당하는 \\(24\\) 화음 클래스로만 제한된다는 것이다.\n보다 일반적인 설정을 설명하기 위해 참조 주석에 사용되는 다른 코드 유형(예: 7도, 증가(augmented) 및 감소(diminished) 화음)을 장조 및 단조 3화음으로 줄였다. 이하에서는 이 변환을 트라이어드(=3화음) 축소 (triad reduction) 라고 하자.\n하나의 명백한 물음은 화음 인식 오류가 그러한 화음 유형 축소가 사용된 악절에서 더 자주 발생하는지 여부이다.\n이 질문을 조사하기 위해 \\(24\\) 장,단3화음에 포함되지 않은 화음 유형을 비화음(non-chord) 라벨 \\(\\mathbf{N}\\)로 대체하는 두 번째 축소 표현을 고려해본다. 이하에서는 이 변환을 비화음 축소(non-chord reduction) 라고 하자.\n예를 들어, 원래 참조 주석에서 이러한 추가 화음 유형이 많이 발생하는 노래 “Penny Lane”을 보자. 다음 그림에서 결과적인 3화음 축소와 비화음 축소(비화음 라벨 \\(\\mathbf{N}\\)가 흰색으로 표시됨)를 보여준다.\n\n\nfor s in [3]:\n    song_id = song_dict[s][0]\n    fn_ann = song_dict[s][3]\n    fn_ann_orig = fn_ann.replace('_simplified', '')\n    \n    # Read original annotation (Harte) and triad-reduced annotation\n    ann_orig, _ = read_structure_annotation(fn_ann_orig)\n    ann, _ = read_structure_annotation(fn_ann)\n    # Replace in original annotation all labels that were reducued by non-chord label \n    for k in range(len(ann)):\n        if ann[k][2] != ann_orig[k][2]:\n            ann_orig[k][2] = 'N'\n\n    plot_segments(ann, figsize=(12, 1.2), time_label='Time (seconds)', \n                           print_labels=False, colors=color_ann, alpha=1)\n    plt.title('Song: %s: Triad reduction' % song_id)    \n    plt.show()\n    plot_segments(ann_orig, figsize=(12, 1), \n                           print_labels=False, colors=color_ann, alpha=1)\n    plt.title('Song: %s: Non-chord reduction' % song_id)\n    plt.show()\n\n\n\n\n\n\n\n\n다음으로, 3화음 축소에 기반한 참조 주석을 한 번 사용하고, 비화음 축소에 기반한 주석을 한 번 사용하여 HMM 기반 화음 인식 접근 방식을 평가해보자.\n\n\n아이템 \\((n, \\lambda)\\)는 세트 \\(\\mathcal{I}=[1:N]\\times \\Lambda\\)의 요소이다. 이 규칙에서 화음이 아닌 라벨 \\(\\mathbf{N}\\)는 고려되지 않은 상태로 남는다.\n결과적으로 관련(relevant)(또는 양수) 아이템은 \\((n, \\lambda^\\mathrm{Ref}_{n})\\)와 \\(\\lambda^\\mathrm{Ref}_{n}\\not=\\mathbf{N}\\)의 쌍이다.\nHMM 기반 화음 인식 접근 방식은 각 프레임 \\(n\\in[1:N]\\)에 대한 추정 \\(\\lambda_{n}\\in\\Lambda\\)를 생성하므로, 쌍 \\((n,\\lambda_{n})\\)는 참조 주석의 프레임이 \\(\\lambda^\\mathrm{Ref}_{n}=\\mathbf{N}\\)인 경우 false positive으로 계산된다.\n\n\n이를 염두에 두고 다음 그림에 표시된 화음 인식 결과를 살펴보자.\n\n3화음 축소에 기반한 참조 주석을 사용하여 precision \\(\\mathrm{P}=0.704\\), recall \\(\\mathrm{R}=0.726\\) 및 F-측정값 \\(\\mathrm{F}=0.715\\)를 얻는다.\n비화음 축소에 기반한 참조 주석을 사용하여 \\(\\mathrm{P}=0.485\\), \\(\\mathrm{R}=0.925\\) 및 \\(\\mathrm{F}=0.636\\)를 얻는다.\n\nrecall의 증가는 축소된 화음 라벨에 대해 화음 인식 오류가 종종 발생한다는 가설을 정확히 뒷받침한다.\nprecision 감소는 화음 인식기가 모든 프레임 \\(n\\in[1:N]\\)에 대해 화음 라벨 \\(\\lambda_n\\in\\Lambda\\)를 출력하기 때문이다.\n이 결과는 예를 들어 모든 \\(24\\) 화음 라벨의 출력 확률이 특정 임계값 아래로 떨어지는 경우, 화음 인식기가 비화음의 라벨도 출력하도록 허용함으로써 개선될 수 있다.\n\n\nsong_selected = [3] \nfor s in song_selected:\n    output = chord_recognition_all(X_dict_CQT[s], ann_dict_CQT[s][0], p=0.15)\n    result_Tem, result_HMM, chord_Tem, chord_HMM, chord_sim = output\n    song_id = song_dict[s][0]\n    fn_ann = song_dict[s][3]\n    fn_ann_orig = fn_ann.replace('_simplified', '')    \n    ann_orig, _ = read_structure_annotation(fn_ann_orig)\n    output =  convert_chord_ann_matrix(fn_ann_orig, chord_labels, Fs=Fs_X_dict_CQT[s],\n                                                 N=X_dict_CQT[s].shape[1], last=False)    \n    ann_matrix_N = output[0]    \n    output = chord_recognition_all(X_dict_CQT[s], ann_matrix_N, p=0.15)\n    result_Tem_N, result_HMM_N, chord_Tem_N, chord_HMM_N, chord_sim_N = output    \n    title = 'Triad reduction: %s [CQT; HMM] ' % song_dict[s][0]\n    plot_chord_recognition_result(ann_dict_CQT[s][0], result_HMM, chord_HMM, chord_labels, title=title)    \n    title = 'Non-chord label: %s [CQT; HMM] ' % song_dict[s][0]\n    plot_chord_recognition_result(ann_matrix_N, result_HMM_N, chord_HMM_N, chord_labels, title=title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html",
    "title": "7.1. 온셋 감지",
    "section": "",
    "text": "음악의 온셋(onset)을 설명하고, 온셋을 감지하기 위해 변화를 포착하는 노벨티(novelty) 함수의 네 가지 종류를 설명하고 비교한다. 또한 피크-선택(peak-picking) 방법을 알아본다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#일반-파이프라인-general-pipeline",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#일반-파이프라인-general-pipeline",
    "title": "7.1. 온셋 감지",
    "section": "일반 파이프라인 (General Pipeline)",
    "text": "일반 파이프라인 (General Pipeline)\n\n온셋 감지를 위한 많은 방식은 유사한 알고리즘 파이프라인을 따르지만 온셋 후보를 도출하기 위해 활용되는 신호 속성이 다르다. 이 파이프라인에서 주요 단계는 다음과 같다.\n\n먼저 신호는 관심 속성을 더 잘 반영하는 적절한 특징 표현으로 변환된다.\n그런 다음 특징 시퀀스에 미분 연산자 유형을 적용하고 노벨티 함수(novelty function) 를 도출한다.\n마지막으로 피크(peak) 선택 알고리즘을 사용하여 온셋 후보를 찾는다.\n\n특히 주어진 음악 신호를 신호 에너지 또는 스펙트럼의 특정 변화를 캡처하는 노벨티 표현으로 변환하는 방법을 본다. 이러한 표현의 피크는 음의 온셋 후보에 대한 좋은 지표를 제공한다.\n\n에너지-기반 노벨티\n스펙트럼-기반 노벨티\n단계-기반 노벨티\n복소-영역 노벨티"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#예시-another-one-bites-the-dust",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#예시-another-one-bites-the-dust",
    "title": "7.1. 온셋 감지",
    "section": "예시: Another One Bites the Dust",
    "text": "예시: Another One Bites the Dust\n\n예를 들어 Queen의 “Another one bites the dust”를 살펴보자. 베이스로만 연주되는 두 개의 16분 음표로 구성된 색다른 비트로 시작하여 네 개의 타악기 비트(킥 드럼, 스네어 드럼, 하이햇 및 베이스로 연주)가 이어진다. 또한 두 개의 후속 비트 사이에 추가 하이햇 스트로크가 있다. 다음 그림은 온셋 위치에 주석이 달린 파형을 보여준다.\n\n\nipd.display(Image(path_img+\"FMP_C6_F01a.png\", width=400))\nipd.display(Audio(path_data+\"FMP_C6_F01_Queen.wav\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n다음 스펙트로그램 표현은 4분음표 드럼 비트와 하이햇 사운드가 트랜지언트(수직선)와 함께 진행되는 반면 처음 두 개의 베이스 소리는 확산 온셋 속성이 더 크다는 것을 보여준다.\n\n\nfn_wav = 'FMP_C6_F01_Queen.wav'\nplot_wav_spectrogram(path_data+fn_wav, audio=False)\n\n\n\n\n\ndef read_annotation_pos(fn_ann, label='', header=True, print_table=False):\n    \"\"\"Read and convert file containing either list of pairs (number,label) or list of (number)\n\n    Args:\n        fn_ann (str): Name of file\n        label (str): Name of label (Default value = '')\n        header (bool): Assumes header (True) or not (False) (Default value = True)\n        print_table (bool): Prints table if True (Default value = False)\n\n    Returns:\n        ann (list): List of annotations\n        label_keys (dict): Dictionaries specifying color and line style used for labels\n    \"\"\"\n    df = pd.read_csv(fn_ann, sep=';', keep_default_na=False, header=0 if header else None)\n    if print_table:\n        print(df)\n    num_col = df.values[0].shape[0]\n    if num_col == 1:\n        df = df.assign(label=[label] * len(df.index))\n    ann = df.values.tolist()\n\n    label_keys = {'beat': {'linewidth': 2, 'linestyle': ':', 'color': 'r'},\n                  'onset': {'linewidth': 1, 'linestyle': ':', 'color': 'r'}}\n    return ann, label_keys\n\n\ndef compute_local_average(x, M):\n    \"\"\"Compute local average of signal\n    Args:\n        x (np.ndarray): Signal\n        M (int): Determines size (2M+1) in samples of centric window  used for local average\n    Returns:\n        local_average (np.ndarray): Local average signal\n    \"\"\"\n    L = len(x)\n    local_average = np.zeros(L)\n    for m in range(L):\n        a = max(m - M, 0)\n        b = min(m + M + 1, L)\n        local_average[m] = (1 / (2 * M + 1)) * np.sum(x[a:b])\n    return local_average\n\n\ndef compute_novelty_spectrum(x, Fs=1, N=1024, H=256, gamma=100.0, M=10, norm=True):\n    \"\"\"Compute spectral-based novelty function\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sampling rate (Default value = 1)\n        N (int): Window size (Default value = 1024)\n        H (int): Hop size (Default value = 256)\n        gamma (float): Parameter for logarithmic compression (Default value = 100.0)\n        M (int): Size (frames) of local average (Default value = 10)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n    Returns:\n        novelty_spectrum (np.ndarray): Energy-based novelty function\n        Fs_feature (scalar): Feature rate\n    \"\"\"\n    X = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Fs_feature = Fs / H\n    Y = np.log(1 + gamma * np.abs(X))\n    Y_diff = np.diff(Y)\n    Y_diff[Y_diff < 0] = 0\n    novelty_spectrum = np.sum(Y_diff, axis=0)\n    novelty_spectrum = np.concatenate((novelty_spectrum, np.array([0.0])))\n    if M > 0:\n        local_average = compute_local_average(novelty_spectrum, M)\n        novelty_spectrum = novelty_spectrum - local_average\n        novelty_spectrum[novelty_spectrum < 0] = 0.0\n    if norm:\n        max_value = max(novelty_spectrum)\n        if max_value > 0:\n            novelty_spectrum = novelty_spectrum / max_value\n    return novelty_spectrum, Fs_feature\n\n\nfn_ann = 'FMP_C6_F01_Queen.csv'\nann, label_keys = read_annotation_pos(path_data+fn_ann, print_table=True)\n\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs)\n\nfig, ax = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(6, 3))\nplot_signal(nov, Fs_nov, ax=ax[0], color='k', title='Novelty function');\nplot_annotation_line(ann, ax=ax[1], label_keys=label_keys,\n                    time_min=0, time_max=x_duration)\nax[1].set_title('Annotated onset and beat positions')\nax[1].set_xlabel('Time (seconds)')\n\nplt.tight_layout()\n\n   position  label\n0  0.117460  onset\n1  0.247619  onset\n2  0.372698   beat\n3  0.646349  onset\n4  0.911111   beat\n5  1.184762  onset\n6  1.445442   beat\n7  1.735238  onset\n8  2.000000   beat\n9  2.287619  onset\n\n\n\n\n\n\n마지막으로 피크 선택(peak picking) 전략을 적용하여 노벨티 함수의 로컬 최대값 또는 피크를 찾는다.\n피크의 위치는 온셋 위치에 대한 후보이다. 다음 코드 셀에서는 scipy.signal.find_peaks 함수에서 제공하는 피크 선택 전략을 적용한다. 또한 원본 오디오 녹음에 추가된 클릭(click) 트랙을 통해 음성화(sonification) 와 함께 피크의 시각화를 생성한다.\n\n\npeaks, properties = signal.find_peaks(nov, prominence=0.02)\nT_coef = np.arange(nov.shape[0]) / Fs_nov\npeaks_sec = T_coef[peaks]\nfig, ax, line = plot_signal(nov, Fs_nov, color='k', \n                    title='Novelty function with detected peaks')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys, \n                    nontime_axis=True, time_min=0, time_max=x_duration)\nplt.plot(peaks_sec, nov[peaks], 'ro')\nplt.show()\n\nx_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x)) # click sound\nipd.display(Audio(x + x_peaks, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#librosa-함수-예",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#librosa-함수-예",
    "title": "7.1. 온셋 감지",
    "section": "Librosa 함수 예",
    "text": "Librosa 함수 예\n\nx, sr = librosa.load('../audio/classic_rock_beat.wav')\nipd.display(ipd.Audio(x, rate=sr))\nplt.figure(figsize=(10, 3))\nlibrosa.display.waveshow(y=x, sr=sr)\nplt.show()\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n[librosa.onset.onset_detect]\n\nonset_frames = librosa.onset.onset_detect(y=x, sr=sr, wait=1, pre_avg=1, post_avg=1, pre_max=1, post_max=1)\nonset_times = librosa.frames_to_time(onset_frames)\n\n\nS = librosa.stft(x)\nlogS = librosa.amplitude_to_db(abs(S))\n\nplt.figure(figsize=(10, 3))\nlibrosa.display.specshow(logS, sr=sr, x_axis='time', y_axis='log', cmap='Reds')\nplt.vlines(onset_times, 0, 10000, color='#3333FF')\n\nplt.figure(figsize=(10, 3))\nlibrosa.display.waveshow(x, sr=sr)\nplt.vlines(onset_times, -0.8, 0.79, color='r', alpha=0.8) \n\n<matplotlib.collections.LineCollection at 0x291cb3fe460>\n\n\n\n\n\n\n\n\n\nclicks = librosa.clicks(frames=onset_frames, sr=sr, length=len(x))\nAudio(x + clicks, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#에너지-기반-노벨티-energy-based-novelty",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#에너지-기반-노벨티-energy-based-novelty",
    "title": "7.1. 온셋 감지",
    "section": "에너지 기반 노벨티 (Energy-Based Novelty)",
    "text": "에너지 기반 노벨티 (Energy-Based Novelty)\n\n로컬(local) 에너지 함수\n\n종종 음의 온셋은 신호 에너지의 갑작스러운 증가와 함께 진행된다.\n이 가정에 기초하여 음 온셋을 감지하는 간단한 방법은 신호를 각 시간 인스턴스에 대한 신호의 로컬(local) 에너지를 나타내는 함수로 변환한 다음 이 함수에서 급격한 변화를 찾는 것이다.\n\\(x:\\mathbb{Z}\\to\\mathbb{R}\\)를 DT(discrete time)-signal이라고 하자. 또한, \\(w:[-M:M]\\to\\mathbb{R}\\) (for some \\(M\\in\\mathbb{N}\\))를 시간 0을 중심으로 하는 종 모양의 윈도우 함수(예: Hann window)라고 하자.\n\\(w\\)에 대한 \\(x\\)의 로컬 에너지는 다음과 같이 주어진 함수 \\(E_w^x:\\mathbb{Z}\\to\\mathbb{R}\\)로 정의된다. \\[E_w^x(n) := \\sum_{m=-M}^{M} |x(n+m)w(m)|^2 = \\sum_{m\\in\\mathbb{Z}}| x(m)w(m-n)|^2\\]\n\n\nann, label_keys = read_annotation_pos(path_data+fn_ann)\n\nx, Fs = librosa.load(path_data+fn_wav) # Queen example\nx_duration = len(x)/Fs\n\nN = 2048\nw = signal.hann(N)\n\n#Calculate local energy\nx_square = x**2\nenergy_local = np.convolve(x_square, w**2, 'same')\n\nplot_signal(x, Fs, title='Waveform')\nplot_signal(x_square, Fs, title='Waveform (squared)')\nfig, ax, line = plot_signal(energy_local, Fs, color='k', \n                    title='Local energy function (Hann window)')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n이산 미분과 반파 정류기 (Discrete Derivation and Half-Wave Rectification)\n\n에너지 변화를 측정하기 위해 로컬 에너지 함수의 미분을 취한다. 불연속적인 경우, 그러한 도함수를 실현하는 가장 쉬운 방법은 두 개의 후속 에너지 값 사이의 차이를 취하는 것이다. 또한 우리는 에너지 증가(감소가 아닌)에 관심이 있기 때문에 양의 차이만 유지하고 음의 차이는 0으로 설정할 수 있다. 후자의 단계를 반파 정류기(half-wave rectification)라고 하며 다음과 같이 표기한다. \\[|r|_{\\geq 0} := \\frac{r+|r|}{2} =\n\\left\\{\\begin{array}{ll}\n  r, &\\,\\, \\mbox{if $r\\geq 0$,}\\\\\n  0, &\\,\\, \\mbox{if $r< 0$,}\n\\end{array}\\right.\\] for \\(r\\in\\mathbb{R}\\).\n전체적으로 다음과 같이 주어진 에너지 기반 노벨티 함수 \\(\\Delta_\\mathrm{Energy}:\\mathrm{Z}\\to\\mathbb{R}\\)를 얻는다.\n\n\\[\\Delta_\\mathrm{energy}(n):= |E_w^x(n+1)-E_w^x(n)|_{\\geq 0}\\]\n\n#Differentiation and half-wave rectification\nenergy_local_diff = np.diff(energy_local)\nenergy_local_diff = np.concatenate((energy_local_diff, np.array([0])))\nnovelty_energy = np.copy(energy_local_diff)\nnovelty_energy[energy_local_diff < 0] = 0\n\nplot_signal(energy_local_diff, Fs, color='k', \n                    title='Discrete derivative (Hann window)')\nfig, ax, line = plot_signal(novelty_energy, Fs, color='k',\n                    title='Energy-based novelty function (Hann window)')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\nplt.tight_layout()\n\n\n\n\n\n\n\n\n이 노벨티 함수는 타악기 비트에 많은 에너지가 포함되어 있음을 잘 나타낸다. 그러나 그 사이의 저에너지 하이햇 스트로크는 보이지 않는다. 또한 간단한 프레임별 차이 함수를 적용할 때 이 절차에서 종 모양의 Hann 윈도우에 의해 도입된 스무딩(smoothing) 효과가 필수적이라는 점에 유의해야 한다. 예를 들어 직사각형 윈도우를 대신 사용하면 차이 함수가 노이즈가 많은 에너지 함수로 이어지는 작은 로컬 변동에 반응한다.\n\n\n# Use rectangular window\nw = signal.boxcar(N)\nx_square = x**2\nenergy_local = np.convolve(x_square, w**2, 'same')\nenergy_local_diff = np.diff(energy_local)\nenergy_local_diff = np.concatenate((energy_local_diff, np.array([0])))\nnovelty_energy = np.copy(energy_local_diff)\nnovelty_energy[energy_local_diff < 0] = 0\n\nfig, ax, line = plot_signal(energy_local, Fs, color='k',\n                    title='Local energy function (rectangular window)')\nplot_signal(energy_local_diff, Fs, color='k', \n                    title='Discrete derivative (rectangular window)')\nfig, ax, line = plot_signal(novelty_energy, Fs, color='k', title='Energy-based novelty function (rectangular window)')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n로그 압축 (Logarithmic Compression)\n\n소리 강도(intensity)에 대한 인간의 인식이 본질적으로 대수적(logarithmic)이라는 사실을 설명하기 위해 예를 들어 로그 데시벨로 전환하거나 로그 압축을 적용하여 에너지 값에 로그를 적용하는 경우가 많다.\n후자의 경우 \\(\\Gamma_\\gamma:\\mathbb{R}_{>0} \\to \\mathbb{R}_{>0}\\) 함수를 사용한다. \\[\\Gamma_\\gamma(v):=\\log(1+ \\gamma \\cdot v)\\] (양의 상수 \\(\\gamma\\in\\mathbb{R}_{>0}\\)는 압축 정도를 조절)\n그 결과 노벨티 함수는 다음과 같이 주어진다. \\[\\Delta_\\mathrm{Energy}^\\mathrm{Log}(n):= |\\Gamma_\\gamma(E_w^x(n+1))-\\Gamma_\\gamma(E_w^x(n))|_{\\geq 0}\\]\n윈도우 길이 \\(N\\) 외에도 다음 구현에서 홉 크기 매개변수 \\(H\\)를 도입하여 계산된 노벨티의 특징 샘플링 비율을 줄일 수 있다. 또한 최대값으로 나누어 노벨티 함수를 정규화한다. 다음 그림에서 로그 노벨티 함수를 사용했을 때 약한 하이햇 온셋 부분이 표시되는 것을 관찰할 수 있다 (\\(t=1.3~\\mathrm{sec}\\) 및 \\(t=2.3~\\mathrm{sec}\\) 참조).\n로그 압축의 단점으로 일부 소음과 같은 소리 구성 요소가 증폭되어 거짓의 피크가 발생할 수 있다. 이 예에서 볼 수 있듯이 로그 압축은 또한 일부 피크 위치를 앞으로 이동하여 이제 참조 주석(빨간색으로 표시됨) 앞에 있다. 청취자가 수동으로 생성한 참조 주석이 약간 지연된 “인식된” 온셋 위치에 해당함을 알 수 있다.\n\n\ndef compute_novelty_energy(x, Fs=1, N=2048, H=128, gamma=10.0, norm=True):\n    \"\"\"Compute energy-based novelty function\n\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sampling rate (Default value = 1)\n        N (int): Window size (Default value = 2048)\n        H (int): Hop size (Default value = 128)\n        gamma (float): Parameter for logarithmic compression (Default value = 10.0)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n\n    Returns:\n        novelty_energy (np.ndarray): Energy-based novelty function\n        Fs_feature (scalar): Feature rate\n    \"\"\"\n    # x_power = x**2\n    w = signal.hann(N)\n    Fs_feature = Fs / H\n    energy_local = np.convolve(x**2, w**2, 'same')\n    energy_local = energy_local[::H]\n    if gamma is not None:\n        energy_local = np.log(1 + gamma * energy_local)\n    energy_local_diff = np.diff(energy_local)\n    energy_local_diff = np.concatenate((energy_local_diff, np.array([0])))\n    novelty_energy = np.copy(energy_local_diff)\n    novelty_energy[energy_local_diff < 0] = 0\n    if norm:\n        max_value = max(novelty_energy)\n        if max_value > 0:\n            novelty_energy = novelty_energy / max_value\n    return novelty_energy, Fs_feature\n\n\nN = 2048\nH = 128\nnov_1, Fs_nov = compute_novelty_energy(x, Fs=Fs, N=N, H=H, gamma=None)\nnov_2, Fs_nov = compute_novelty_energy(x, Fs=Fs, N=N, H=H, gamma=1000)\n\nfig, ax, line = plot_signal(nov_1, Fs=Fs_nov, color='k', \n                    title='Novelty function (original)')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\nfig, ax, line = plot_signal(nov_2, Fs=Fs_nov, color='k',\n                    title='Novelty function with logarithmic compression')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\n\n\n\n\n\n\n\n\n예시: 악기에 따라\n\n온셋 감지의 또 다른 일반적인 문제는 비브라토 또는 트레몰로의 결과로 인한 비정상(nonsteady) 소리의 에너지 변동이다. 특히 순전히 에너지 기반의 절차의 경우 진폭 변조는 결과로 나타나는 노벨티 함수에서 가짜 피크로 이어지는 경우가 많다.\n다른 악기로 연주되는 음 C4에 대한 에너지 기반 노벨티 함수를 보여주는 다음 예를 보자. 노벨티 함수는 피아노 소리의 경우 하나의 선명한 피크를 나타내지만 바이올린이나 플루트 소리의 경우 추가 피크가 많이 있다. 게다가, 바이올린 소리의 시작 부분에서 상대적으로 느린 에너지 증가는 얼룩지고 일시적으로 부정확한 온셋 피크로 이어진다.\n\n\nfn_ann = 'FMP_C6_F04_NoteC4_PTVF.csv'\nann, label_keys = read_annotation_pos(path_data+fn_ann, label='onset', header=0)\n\nfn_wav = 'FMP_C6_F04_NoteC4_PTVF.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\nN = 2048\nH = 256\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, N=N, H=H, gamma=None)\n\nplt.figure(figsize=(9,4))\nax = plt.subplot(2,1,1)\nfig, ax, line = plot_signal(x, Fs, ax = ax, title='Waveform')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration)\n\nax = plt.subplot(2,1,2)\nfig, ax, line = plot_signal(nov, Fs=Fs_nov, ax = ax, color='k', \n                     title='Novelty function')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_duration)\nplt.ylim([0, 0.5]);\nplt.tight_layout()\n\nipd.display(Audio(data=x, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\nLibrosa 함수 예제\n\nx, sr = librosa.load('../audio/simple_loop.wav')\nprint(x.shape, sr)\n\nipd.display(Audio(x, rate=sr))\nplt.figure(figsize=(10, 3))\nlibrosa.display.waveshow(x, sr=sr)\nplt.show()\n\n(49613,) 22050\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n# rms energy\n\nhop_length = 512\nframe_length = 1024\nrmse = librosa.feature.rms(y=x, frame_length=frame_length, hop_length=hop_length).flatten()\nrmse_diff = np.zeros_like(rmse)\nrmse_diff[1:] = np.diff(rmse)\n\n\nprint(rmse.shape)\nprint(rmse_diff.shape)\n\n(97,)\n(97,)\n\n\n\nenergy_novelty = np.max([np.zeros_like(rmse_diff), rmse_diff], axis=0)\n\nframes = np.arange(len(rmse))\nt = librosa.frames_to_time(frames, sr=sr)\n\n\nplt.figure(figsize=(10, 3))\nplt.plot(t, rmse, 'b--', t, rmse_diff, 'g--^', t, energy_novelty, 'r-')\nplt.xlim(0, t.max())\nplt.xlabel('Time (sec)')\nplt.legend(('RMSE', 'delta RMSE', 'energy novelty')) \nplt.show()\n\n\n\n\n\n#log energy\n\nlog_rmse = np.log1p(10*rmse)\nlog_rmse_diff = np.zeros_like(log_rmse)\nlog_rmse_diff[1:] = np.diff(log_rmse)\n\n\nlog_energy_novelty = np.max([np.zeros_like(log_rmse_diff), log_rmse_diff], axis=0)\n\n\nplt.figure(figsize=(10, 3))\nplt.plot(t, log_rmse, 'b--', t, log_rmse_diff, 'g--^', t, log_energy_novelty, 'r-')\nplt.xlim(0, t.max())\nplt.xlabel('Time (sec)')\nplt.legend(('log RMSE', 'delta log RMSE', 'log energy novelty')) \n\n<matplotlib.legend.Legend at 0x291ce700220>"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#스펙트럼-기반-노벨티-spectral-based-novelty",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#스펙트럼-기반-노벨티-spectral-based-novelty",
    "title": "7.1. 온셋 감지",
    "section": "스펙트럼 기반 노벨티 (Spectral-Based Novelty)",
    "text": "스펙트럼 기반 노벨티 (Spectral-Based Novelty)\n\n스펙트럼 기반 노벨티\n\n동시에 발생하는 소리 이벤트가 있는 다성 음악의 경우 온셋 감지는 종종 어렵고 정의가 어려운 문제가 된다. 예를 들어, 낮은 강도의 음악 이벤트는 높은 강도의 이벤트에 의해 가려질 수 있다. 그러나 잡음과 같은 광대역 트랜지언트는 다성 혼합에서도 특정 주파수 대역에서 여전히 관찰될 수 있다.\n이러한 관찰에 동기를 부여받은 스펙트럼 기반 노벨티 탐지의 아이디어는 먼저 신호를 시간-주파수 표현으로 변환하고 그런 다음 주파수 내용의 변경 사항을 캡처하는 것이다.\n다음에서 \\(\\mathcal{X}\\)를 샘플링 속도 \\(F_\\mathrm{s}\\), 윈도우 길이 \\(N\\), 홉 사이즈 \\(H\\) 을 사용한 DT-신호 \\(x\\)의 이산 STFT라고 하자.\n\\(\\mathcal{X}(n,k)\\in\\mathbb{C}\\)는 주파수 인덱스 \\(k\\in[0:K]\\) 및 시간 프레임 \\(n\\in\\mathbb{Z}\\)에 대한 \\(k^\\mathrm{th}\\) 푸리에 계수를 나타낸다. 여기서 \\(K=N/2\\)는 Nyquist 주파수에 해당하는 주파수 인덱스이다.\n신호의 스펙트럼 변화를 감지하기 위해 기본적으로 적절한 거리 측정을 사용하여 후속 스펙트럼 벡터 간의 차이를 계산한다. 그 결과 스펙트럼 플럭스(spectral flux) 라고도 하는 스펙트럼 기반 노벨티 함수가 생성된다.\nSTFT와 거리 측정의 매개변수뿐만 아니라 종종 적용되는 전처리 및 후처리 단계에 따라 달라지는 노벨티 함수를 계산하는 방법에는 여러 가지가 있다.\n\n\n\n로그 압축\n\n먼저 약한 스펙트럼 성분을 강화하기 위해 로그 압축을 스펙트럼 계수에 적용한다. 이를 위해 크기 스펙트로그램 \\(|\\mathcal{X}|\\)에 \\(\\Gamma_\\gamma\\) 함수를 적용하면 다음을 얻는다. \\[\\mathcal{Y}:=\\Gamma_\\gamma(|\\mathcal{X}|)=\\log(1+ \\gamma \\cdot |\\mathcal{X}|)\\] (적절한 상수 \\(\\gamma \\geq 1\\)에 대해)\n\\(\\gamma=1\\)로 압축된 스펙트로그램을 사용하면 트랜지언트의 수직 구조가 더욱 두드러진다. 연속된 비트 사이의 하이햇의 약한 트랜지언트도 표시된다. \\(\\gamma\\)를 높이면 특히 스펙트로그램의 고주파수 범위에서 낮은 강도 값이 더욱 향상된다. 단점으로는 압축 계수 \\(\\gamma\\)가 크면 관련 없는 노이즈와 같은 구성 요소가 증폭될 수도 있다.\n\n\nfn_ann = \"FMP_C6_F01_Queen.csv\"\nann, label_keys = read_annotation_pos(path_data+fn_ann)\n\nfn_wav = 'FMP_C6_F01_Queen.wav'\n\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\nN, H = 1024, 256\nX = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann')\n\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.05], 'height_ratios': [1, 1, 1]}, figsize=(6.5, 6))        \n\nY = np.abs(X)\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0], ax[0,1]], title='No compression')\n\ngamma = 1\nY = np.log(1 + gamma * np.abs(X))\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[1,0], ax[1,1]], title='Compression using $\\gamma=%0.0f$'%gamma)\n\ngamma = 100\nY = np.log(1 + gamma * np.abs(X))\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[2,0], ax[2,1]], title='Compression using $\\gamma=%0.0f$'%gamma)\nplt.tight_layout()\n\n\n\n\n\n\n이상 미분, 반파 정류기, 누적\n\n다음 단계에서는 압축된 스펙트럼 \\(\\mathcal{Y}\\)의 이산 시간 도함수를 계산한다. 에너지 기반 노벨티 함수와 유사하게 반파 정류기(half-wave rectification)를 적용하여 양의 차이(강도 증가)만 고려하고 음의 차이는 버린다. 마지막으로 주파수 축(누적 단계)에서 양의 차이를 합산하여 스펙트럼 기반 노벨티 함수 \\(\\Delta_\\mathrm{Spectral}:\\mathbb{Z}\\to \\mathbb{R}\\)를 얻는다. \\[\\Delta_\\mathrm{Spectral}(n):= \\sum_{k=0}^K  \\big|\\mathcal{Y}(n+1,k)-\\mathcal{Y}(n,k)\\big|_{\\geq 0}\\] for \\(n\\in\\mathbb{Z}\\).\nQueen 예에서, 결과의 노벨티 곡선은 사이사이의 낮은 에너지 하이햇 스트로크뿐만 아니라 높은 에너지를 가진 타악기 비트를 잘 나타낸다.\n\n\nY = np.log(1 + 100 * np.abs(X))\nY_diff = np.diff(Y, n=1)\nY_diff[Y_diff < 0] = 0\nnov = np.sum(Y_diff, axis=0)\nnov = np.concatenate((nov, np.array([0])))\nFs_nov = Fs/H\n\nfig, ax, line = plot_signal(nov, Fs_nov, color='k', title='Spectral-based novelty function')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,  time_min=0, time_max=x_duration);\n\n\n\n\n\n\n로컬 평균 빼기\n\n적절한 후처리 단계를 적용하여 노벨티 함수의 속성을 더욱 향상시킬 수 있다. 예를 들어, 연속의 피크 선택 단계의 관점에서 한가지 목적은 작은 변동을 억제하면서 노벨티 함수의 피크 구조를 향상시키는 것일 수 있다.\n이를 위해 로컬 평균 함수 \\(\\mu:\\mathbb{Z}\\to\\mathbb{R}\\)를 다음과 같이 설정한다. \\[\\mu(n):= \\frac{1}{2M+1}\\sum_{m=-M}^M \\Delta_\\mathrm{Spectral}(n+m)\\] for \\(n\\in\\mathbb{Z}\\), 여기서 \\(M\\in\\mathbb{N}\\) 매개변수는 평균 창의 크기를 결정\n향상된 노벨티 함수 \\(\\bar{\\Delta}_\\mathrm{Spectral}\\)은 \\(\\Delta_\\mathrm{Spectral}\\)에서 로컬 평균을 빼고 양수 부분만 유지하여 얻는다(half-wave rectification). \\[\\bar{\\Delta}_\\mathrm{Spectral}(n):= \\big|\\Delta_\\mathrm{Spectral}(n)-\\mu(n)\\big|_{\\geq 0}\\] for \\(n\\in\\mathbb{Z}\\).\n다음 구현에서는 평균 윈도우 길이를 초 단위로 지정한다. 또한 결과 노벨티 함수를 최대값으로 나누어 정규화한다.\n\n\nM_sec = 0.1\nM = int(np.ceil(M_sec * Fs_nov))\n\nlocal_average = compute_local_average(nov, M)\nnov_norm =  nov - local_average\nnov_norm[nov_norm<0]=0\nnov_norm = nov_norm / max(nov_norm)\n\nplot_signal(nov, Fs_nov, color='k', \n    title='Novelty function with local average curve');\n\nt_novelty = np.arange(nov.shape[0]) / Fs_nov\nplt.plot(t_novelty, local_average, 'r')\nplt.tight_layout()\n\nfig, ax, line = plot_signal(nov_norm, Fs_nov, color='k', \n                    title='Novelty function after local normalization')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\n\n\n\n\n\n\n\n전체 절차를 compute_novelty_spectrum 함수로 보자.\n스펙트럼 기반과 에너지 기반 노벨티 곡선을 비교해보자.\n\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=1024, H=256, gamma=10, M=10, norm=True)\nfig, ax, line = plot_signal(nov, Fs_nov, color='k', \n                    title='Spectral-based novelty function with normalization')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, N=1024, H=256, gamma=10)\nfig, ax, line = plot_signal(nov, Fs_nov, color='k',\n                    title='Energy-based novelty function with normalization')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\n\n\n\n\n\n\n\n\nLibrosa 함수 예제\n\nsr = 22050\n\ndef generate_tone(midi):\n    T = 0.5\n    t = np.linspace(0, T, int(T*sr), endpoint=False)\n    f = librosa.midi_to_hz(midi)\n    return np.sin(2*np.pi*f*t)\n\nx = np.concatenate([generate_tone(midi) for midi in [48, 52, 55, 60, 64, 67, 72, 76, 79, 84]])\nAudio(x, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# energy 기반의 경우\nhop_length = 512\nframe_length = 1024\nrmse = librosa.feature.rms(y=x, frame_length=frame_length, hop_length=hop_length).flatten()\nrmse_diff = np.zeros_like(rmse)\nrmse_diff[1:] = np.diff(rmse)\nenergy_novelty = np.max([np.zeros_like(rmse_diff), rmse_diff], axis=0)\nframes = np.arange(len(rmse))\nt = librosa.frames_to_time(frames, sr=sr)\nplt.figure(figsize=(10, 3))\nplt.plot(t, rmse, 'b--', t, rmse_diff, 'g--^', t, energy_novelty, 'r-')\nplt.xlim(0, t.max())\nplt.xlabel('Time (sec)')\nplt.legend(('RMSE', 'delta RMSE', 'energy novelty')) \nplt.show()\n\n\n\n\nlibrosa.onset.onset_strength\n\nspectral_novelty = librosa.onset.onset_strength(y=x, sr=sr)\n\nframes = np.arange(len(spectral_novelty))\nt = librosa.frames_to_time(frames, sr=sr)\n\nplt.figure(figsize=(10,3))\nplt.plot(t, spectral_novelty, 'r-')\nplt.xlim(0, t.max())\nplt.xlabel('Time (sec)')\nplt.legend(('Spectral Novelty',))\nplt.show()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#위상-기반-노벨티-phase-based-novelty",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#위상-기반-노벨티-phase-based-novelty",
    "title": "7.1. 온셋 감지",
    "section": "위상-기반 노벨티 (Phase-Based Novelty)",
    "text": "위상-기반 노벨티 (Phase-Based Novelty)\n\n위상 표현 (Phase Representation)\n\n스펙트럼 기반 노벨티 함수의 정의에서는 스펙트럼 계수(Spectral Coefficient)의 크기만을 사용하였다.\n이제 위상(phase) 정보가 온셋 감지에 어떻게 사용될 수 있는지 본다. 특히 고정음(stationary tones)은 위상(phase)이 안정적이고 과도음(transient)은 위상이 불안정하다는 사실을 이용한다.\n\\(\\mathcal{X}\\)를 샘플링 속도 \\(F_\\mathrm{s}\\), 윈도우 길이 \\(N\\) 및 홉 크기 \\(H\\)를 사용한 DT 신호 \\(x\\)의 이산 STFT라고 하자. 극좌표(polar coordinate representation)를 사용하면 복소 계수 \\(\\mathcal{X}(n,k)\\in\\mathbb{C}\\)를 다음과 같이 쓸 수 있다. \\[\\mathcal{X}(n,k)= |\\mathcal{X}(n,k)| \\,\\,\\mathrm{exp}(2\\pi i\\varphi(n,k))\\] with the phase \\(\\varphi(n,k)\\in[0,1)\\)\n직관적으로 위상 \\(\\varphi(n,k)\\)는 주파수 \\(F_\\mathrm{coef}(k)= F_\\mathrm{s}\\cdot k/N\\)의 정현파(sinusoid)가 \\(n^{\\mathrm{th}}\\) 프레임에 해당하는 윈도우 신호로 최상의 상관관계를 갖기 위해 어떻게 이동해야 하는지를 결정한다.\n신호 \\(x\\)가 이 정현파와 높은 상관관계를 가지며(즉, \\(|\\mathcal{X}(n,k)|\\)가 큼) 여러 후속 프레임 \\(\\ldots,n-2,n-1,n,n+1,\\ldots\\)에서 꾸준한 동작을 보인다고 가정하자. 그런 다음 위상 \\(\\ldots,\\varphi(n-2,k)\\), \\(\\varphi(n-1,k)\\), \\(\\varphi(n,k)\\), \\(\\varphi(n+1,k),\\ldots\\)는 STFT의 홉 크기 \\(H\\)에서 선형으로 프레임마다 증가한다. 따라서 이 영역의 프레임별 위상차는 대략 일정하게 유지된다. \\[\\varphi(n,k)- \\varphi(n-1,k) \\approx \\varphi(n-1,k)- \\varphi(n-2,k)\\]\n다음 그림에 위상의 각도 표현이 원으로 표시되어 있다.\n\n\nImage(path_img+\"FMP_C6_F08.png\")\n\n\n\n\n\n\n주요 편각 함수 (Principal Argument Function)\n\n위상차를 고려할 때, 위상이 주기적이라는 사실로 인한 위상 래핑(wrapping) 불연속성에 주의해야 한다.\n위상 언래핑(unwrapping) 전략 외에 또 다른 대안은 위상 차이를 \\([-0.5,0.5]\\) 범위로 매핑하는 다음의 주요 편각 함수(principal argument function) 를 사용하는 것이다. \\[\\Psi:\\mathbb{R}\\to\\left[-0.5,0.5\\right]\\]\n이를 위해 원래의 위상차에 적당한 정수값을 더하거나 빼서 \\([-0.5,0.5]\\)의 값을 얻는다.\n\n\ndef principal_argument(v):\n    \"\"\"Principal argument function\n    \n    Args:\n        v (float or np.ndarray): Value (or vector of values)\n\n    Returns:\n        w (float or np.ndarray): Principle value of v\n    \"\"\"\n    w = np.mod(v + 0.5, 1) - 0.5\n    return w\n\n\nv = np.arange(-1,2,0.01)\n\nplt.figure(figsize=(6,2))\nplt.plot(v, principal_argument(v), 'r')\nplt.title(r'Principle argument function $\\Psi$')\nplt.xlabel(r'$v$')\nplt.ylabel(r'$\\Psi(v)$')\nplt.xlim([v[0], v[-1]])\nplt.tight_layout()\n\n\n\n\n\n\n위상 기반 노벨티 함수\n\n주요 편각 함수(principle argument function)를 사용하여 1차 위상차(first-order phase difference)와 2차 위상차(second-order phase difference)를 다음과 같이 정의한다.\n\n\\[\\varphi'(n,k) := \\Psi\\big(\\varphi(n,k)- \\varphi(n-1,k)\\big), \\\\\n\\varphi''(n,k) := \\Psi\\big(\\varphi'(n,k)- \\varphi'(n-1,k)\\big).\\]\n\n위에서 언급한 바와 같이, \\(x\\)의 정상(steady) 영역에서 프레임별 위상차는 거의 일정하게 유지되며, 이는 \\(\\varphi''(n,k)\\approx 0\\)를 의미한다. 그러나 트랜지언트 영역에서 위상은 전체 주파수 범위에서 상당히 예측할 수 없게 동작한다. 결과적으로 \\(\\varphi''(n,k)\\) (for \\(k\\in[0:K]\\)) 값의 동시적 교란은 음 온셋에 대한 좋은 지표이다.\n이 관찰로부터 위상 기반(phase-based) 노벨티 함수 \\(\\Delta_\\mathrm{Phase}\\)를 다음과 같이 정의한다. \\[\\Delta_\\mathrm{Phase}(n) = \\sum_{k=0}^{K} |\\varphi''(n,k)|\\] for \\(n\\in\\mathbb{Z}\\).\n주요 편각 함수를 사용하여 위상 래핑으로 인한 불연속성을 처리할 수 있다. 단점으로는 주요 편각 함수가 음 온셋으로 인한 위상 차이의 큰 불연속성을 상쇄할 수도 있다. 그러나 모든 주파수 지수에 대한 위상차의 합을 고려할 것이기 때문에 이 효과는 약화된다.\n\n\n\n구현\n\n다음 코드 셀에서 위상 기반 노벨티 함수를 계산한다. 스펙트럼 기반 노벨티 함수의 경우, 로컬 평균을 빼고 곡선을 최대값으로 나누어 노벨티 곡선을 추가로 후처리할 수 있다.\n\n\ndef compute_novelty_phase(x, Fs=1, N=1024, H=64, M=40, norm=True):\n    \"\"\"Compute phase-based novelty function\n\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sampling rate (Default value = 1)\n        N (int): Window size (Default value = 1024)\n        H (int): Hop size (Default value = 64)\n        M (int): Determines size (2M+1) in samples of centric window  used for local average (Default value = 40)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n\n    Returns:\n        novelty_phase (np.ndarray): Energy-based novelty function\n        Fs_feature (scalar): Feature rate\n    \"\"\"\n    X = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Fs_feature = Fs / H\n    phase = np.angle(X) / (2*np.pi)\n    phase_diff = principal_argument(np.diff(phase, axis=1))\n    phase_diff2 = principal_argument(np.diff(phase_diff, axis=1))\n    novelty_phase = np.sum(np.abs(phase_diff2), axis=0)\n    novelty_phase = np.concatenate((novelty_phase, np.array([0, 0])))\n    if M > 0:\n        local_average = compute_local_average(novelty_phase, M)\n        novelty_phase = novelty_phase - local_average\n        novelty_phase[novelty_phase < 0] = 0\n    if norm:\n        max_value = np.max(novelty_phase)\n        if max_value > 0:\n            novelty_phase = novelty_phase / max_value\n    return novelty_phase, Fs_feature\n\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs, M=0, norm=0) \nplot_signal(nov, Fs_nov, color='k', \n    title='Phase-based novelty function');\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs, M=10, norm=1) \nfig, ax, line = plot_signal(nov, Fs_nov, color='k', \n                    title='Phase-based novelty function with post-processing')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\n\n\n\n\n\n\n\n\n홉 크기 매개변수의 역할\n\n일반적으로 위상 기반 접근 방식은 홉 크기에 매우 민감하다. 특히 큰 홉 크기를 사용하는 경우 후속 프레임에 대한 위상 예측이 종종 부정확해진다. 따라서 위상 기반 접근 방식에서는 작은 홉 크기를 선택하는 것이 유리하다.\n\n\nH_set = [256, 128, 64]\n\nfor H in H_set:\n    nov, Fs_nov = compute_novelty_phase(x, Fs=Fs, N=1024, H=H, M=10, norm=1)\n    fig, ax, line = plot_signal(nov, Fs_nov, color='k',\n                        title='Phase-based novelty function (H=%d) with post-processing'%H)\n    plot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                        nontime_axis=True, time_min=0, time_max=x_duration);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#복소-영역-노벨티-complex-domain-novelty",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#복소-영역-노벨티-complex-domain-novelty",
    "title": "7.1. 온셋 감지",
    "section": "복소 영역 노벨티 (Complex-Domain Novelty)",
    "text": "복소 영역 노벨티 (Complex-Domain Novelty)\n\n위상과 크기 (Phase and Magnitude)\n\n정현파가 신호와 잘 연관되어 있는 경우, 신호 내의 정상 영역이 위상-기반 기준으로 특징지어질 수 있음을 확인했다.\n그러나 Fourier 계수 \\(\\mathcal{X}(n,k)\\)의 크기가 매우 작은 경우 위상 \\(\\varphi(n,k)\\)는 잡음과 같은 작은 변동으로 인해 신호의 정상 영역 내에서도 발생할 수 있는 다소 혼란스러운 동작을 보일 수 있다. 보다 강력한 감지기를 얻기 위한 한 가지 아이디어는 스펙트럼 계수의 크기(magnitude)로 위상 정보에 가중치를 두는 것이다. 이는 위상과 크기를 공동으로 고려하는 노벨티 함수의 복소 영역 변형으로 이어진다.\n이 변형의 가정은 위상 차이와 크기가 정상 영역에서 다소 일정하게 유지된다는 것이다. 따라서 푸리에 계수 \\(\\mathcal{X}(n,k)\\)가 주어졌을 때, 다음 프레임에 대한 정상-상태(steady-state) 추정 \\(\\hat{\\mathcal{X}}(n+1,k)\\)를 다음과 같이 얻는다. \\[\\varphi'(n,k) := \\varphi(n,k)- \\varphi(n-1,k), \\\\\n\\hat{\\mathcal{X}}(n+1,k) := |\\mathcal{X}(n,k)|\\,\\, \\mathrm{exp}(2\\pi i(\\varphi(n,k)+\\varphi'(n,k)))\\]\n그런 다음 추정치 \\(\\hat{\\mathcal{X}}(n+1,k)\\)와 실제 계수 \\(\\mathcal{X}(n+1,k)\\) 사이의 크기를 사용하여 다음의 노벨티 측정값을 얻을 수 있다: \\[\\mathcal{X}'(n+1,k) = |\\hat{\\mathcal{X}}(n+1,k)-\\mathcal{X}(n+1,k)|\\]\n복소영역 차이 \\(\\mathcal{X}'(n,k)\\)는 프레임 \\(n\\) 및 계수 \\(k\\)에 대한 비정상성의 정도를 정량화한다.\n\n\nImage(path_img+\"FMP_C6_F10.png\", width=500)\n\n\n\n\n\n\n복소 영역 노벨티 함수\n\n이 수는 음 온셋(에너지 증가)과 노트 오프셋(에너지 감소)을 구별하지 않는다.\n따라서 \\(\\mathcal{X}'(n,k)\\)를 크기가 증가하는 \\(\\mathcal{X}^+(n,k)\\) 성분과 크기가 감소하는 \\(\\mathcal{X}^-( n,k)\\) 성분으로 분해한다:\n\n\\[\\mathcal{X}^+(n,k) =  \\left\\{ \\begin{array}{cl}\n                \\mathcal{X}'(n,k), \\quad \\mbox{for $|\\mathcal{X}(n,k)|>|\\mathcal{X}(n-1,k)|$}\\\\\n                 0  ,\\quad \\mbox{otherwise,}\n                 \\end{array} \\right.\\]\n\\[\\mathcal{X}^-(n,k) =  \\left\\{ \\begin{array}{cl}\n                \\mathcal{X}'(n,k),  \\quad \\mbox{for $|\\mathcal{X}(n,k)|\\leq |\\mathcal{X}(n-1,k)|$}\\\\\n                 0  ,\\quad \\mbox{otherwise.}\n                 \\end{array} \\right.\\]\n\n음 온셋 감지를 위한 복소 영역(complex-domain) 노벨티 함수 \\(\\Delta_\\mathrm{Complex}\\)는 모든 주파수 계수에 대해 \\(\\mathcal{X}^+(n,k)\\) 값을 합산하여 다음과 같이 정의할 수 있다. \\[\\Delta_\\mathrm{Complex}(n,k) = \\sum_{k=0}^{K}\\mathcal{X}^+(n,k)\\]\n유사하게, 일반적인 트랜지언트 또는 음 오프셋(offset)을 감지하기 위해 각각 \\(\\mathcal{X}'(n,k)\\) 또는 \\(\\mathcal{X}^-(n,k)\\)를 사용하여 노벨티 함수를 계산할 수 있다.\n\n\n\n구현\n\n스펙트럼 기반 노벨티 함수의 경우처럼 크기의 로그 압축, 로컬 평균 빼기, 곡선을 최대값으로 나누는 정규화와 같은 추가 처리 단계를 추가할 수 있다.\n\n\ndef compute_novelty_complex(x, Fs=1, N=1024, H=64, gamma=10.0, M=40, norm=True):\n    \"\"\"Compute complex-domain novelty function\n\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sampling rate (Default value = 1)\n        N (int): Window size (Default value = 1024)\n        H (int): Hop size (Default value = 64)\n        gamma (float): Parameter for logarithmic compression (Default value = 10.0)\n        M (int): Determines size (2M+1) in samples of centric window used for local average (Default value = 40)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n\n    Returns:\n        novelty_complex (np.ndarray): Energy-based novelty function\n        Fs_feature (scalar): Feature rate\n    \"\"\"\n    X = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Fs_feature = Fs / H\n    mag = np.abs(X)\n    if gamma > 0:\n        mag = np.log(1 + gamma * mag)\n    phase = np.angle(X) / (2*np.pi)\n    phase_diff = np.diff(phase, axis=1)\n    phase_diff = np.concatenate((phase_diff, np.zeros((phase.shape[0], 1))), axis=1)\n    X_hat = mag * np.exp(2*np.pi*1j*(phase+phase_diff))\n    X_prime = np.abs(X_hat - X)\n    X_plus = np.copy(X_prime)\n    for n in range(1, X.shape[0]):\n        idx = np.where(mag[n, :] < mag[n-1, :])\n        X_plus[n, idx] = 0\n    novelty_complex = np.sum(X_plus, axis=0)\n    if M > 0:\n        local_average = compute_local_average(novelty_complex, M)\n        novelty_complex = novelty_complex - local_average\n        novelty_complex[novelty_complex < 0] = 0\n    if norm:\n        max_value = np.max(novelty_complex)\n        if max_value > 0:\n            novelty_complex = novelty_complex / max_value\n    return novelty_complex, Fs_feature\n\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs, gamma=10, M=0, norm=1) \nfig, ax, line = plot_signal(nov, Fs_nov, color='k', \n                    title='Complex-domain novelty function')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys, \n                    nontime_axis=True, time_min=0, time_max=x_duration);\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs, gamma=10, M=40, norm=1) \nfig, ax, line = plot_signal(nov, Fs_nov, color='k', \n                    title='Complex-domain novelty function with post-processing')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,  \n                    nontime_axis=True, time_min=0, time_max=x_duration);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#노벨티-비교",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#노벨티-비교",
    "title": "7.1. 온셋 감지",
    "section": "노벨티 비교",
    "text": "노벨티 비교\n\n위에서 소개한 네 가지 방법의 결과를 비교한다.\n\nEnergy-based novelty approach\nSpectral-based novelty approach\nPhase-based novelty approach\nComplex-domain novelty approach\n\n\n\nnov_dic = {}\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=None)\nnov_dic.update( {0 : [nov, Fs_nov, r'Energy-based novelty function (Fs = %d)'%Fs_nov]} )\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=1000)\nnov_dic.update( {1 : [nov, Fs_nov, 'Energy-based novelty function with compression (Fs = %d)'%Fs_nov]} )\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs)\nnov_dic.update( {2 : [nov, Fs_nov, 'Spectral-based novelty function (Fs = %d)'%Fs_nov]} )\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs)\nnov_dic.update( {3 : [nov, Fs_nov, 'Phase-based novelty function (Fs = %d)'%Fs_nov]} )\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs)\nnov_dic.update( {4 : [nov, Fs_nov, 'Complex-domain novelty function (Fs = %d)'%Fs_nov]} )\n\nfor k in nov_dic:\n    fig, ax, line = plot_signal(nov_dic[k][0], Fs=nov_dic[k][1], \n                        color='k', title=nov_dic[k][2])\n    plot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_duration)    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n시간 축 리샘플링\n\n서로 다른 접근법은 서로 다른 피쳐 레이트를 가진 노벨티 함수로 이어질 수 있다. 이러한 노벨티 함수를 직접 비교하거나 결합하기 위해 이제 피쳐 레이트를 조정하기 위한 리샘플링(resampling) 방식을 도입한다.\n보다 일반적으로 이 접근법은 선형 보간법 (linear interpolation) 을 사용하여 주어진 피쳐 레이트(Fs_in)를 가진 입력 신호를 주어진 목표 프레임 레이트(Fs_out)를 가진 다른 신호로 변환한다. 예를 들어, 이 함수는 복소 영역 novelty function(feature rate \\(344.531~\\mathrm{Hz}\\))를 목표 피처 \\(100~\\mathrm{Hz}\\)인 노벨티 함수로 변환하는 데 적용된다.\n\n\ndef resample_signal(x_in, Fs_in, Fs_out=100, norm=True, time_max_sec=None, sigma=None):\n    \"\"\"Resample and smooth signal\n\n    Args:\n        x_in (np.ndarray): Input signal\n        Fs_in (scalar): Sampling rate of input signal\n        Fs_out (scalar): Sampling rate of output signal (Default value = 100)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n        time_max_sec (float): Duration of output signal (given in seconds) (Default value = None)\n        sigma (float): Standard deviation for smoothing Gaussian kernel (Default value = None)\n\n    Returns:\n        x_out (np.ndarray): Output signal\n        Fs_out (scalar): Feature rate of output signal\n    \"\"\"\n    if sigma is not None:\n        x_in = ndimage.gaussian_filter(x_in, sigma=sigma)\n    T_coef_in = np.arange(x_in.shape[0]) / Fs_in\n    time_in_max_sec = T_coef_in[-1]\n    if time_max_sec is None:\n        time_max_sec = time_in_max_sec\n    N_out = int(np.ceil(time_max_sec*Fs_out))\n    T_coef_out = np.arange(N_out) / Fs_out\n    if T_coef_out[-1] > time_in_max_sec:\n        x_in = np.append(x_in, [0])\n        T_coef_in = np.append(T_coef_in, [T_coef_out[-1]])\n    x_out = interp1d(T_coef_in, x_in, kind='linear')(T_coef_out)\n    if norm:\n        x_max = max(x_out)\n        if x_max > 0:\n            x_out = x_out / max(x_out)\n    return x_out, Fs_out\n\n\nnov, Fs_nov = compute_novelty_complex(x, Fs)\nplot_signal(nov, Fs=1, xlabel='Time (samples)', color='k', \n        title='Fs = %0.1f'%Fs_nov);\n\nnov_out, Fs_out = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\nplot_signal(nov_out, Fs=1, xlabel='Time (samples)', color='k',\n        title='Fs = %0.1f'%Fs_out);\n\n\n\n\n\n\n\n\n선형 보간을 통해 피쳐 레이트를 줄이는 것은 노벨티 함수가 시간에 따라 강하게 국한된(localized) 피크와 같은 구조를 갖는 경우 문제가 될 수 있다. 이 경우 리샘플링 방식에 의해 피크가 감쇠될 수 있다. 이는 위상 기반 노벨티 함수를 보여주는 다음 예를 보면 알 수 있다.\n피크 구조를 더 잘 보존할 수 있는 한 가지 가능한 전략은 리샘플링 전에 스무딩 필터를 적용하는 것이다(시간적 국소화가 번지는 비용으로).\n\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1,1], 'height_ratios': [1,1]}, \n                       figsize=(10,4))\n\nnov, Fs_nov = compute_novelty_phase(x, Fs)\nplot_signal(nov, Fs=1, ax=ax[0,0], xlabel='Time (samples)', color='k', \n        title='Original novelty function (Fs = %0.1f)'%Fs_nov)\n\nnov_out, Fs_out = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\nplot_signal(nov_out, ax=ax[1,0], Fs=1, xlabel='Time (samples)', color='k', \n        title='Novelty function after resampling (Fs = %0.1f)'%Fs_out);\n\nFs_out = Fs_nov\nnov_smooth, Fs_out = resample_signal(nov, Fs_in=Fs_nov, Fs_out=Fs_out, sigma=4)\nplot_signal(nov_smooth, Fs=1, ax=ax[0,1], xlabel='Time (samples)', color='k', \n        title='Smoothed novelty function (Fs = %0.1f)'%Fs_nov)\n\nFs_out = 100\nnov_smooth_out, Fs_out = resample_signal(nov, Fs_in=Fs_nov, Fs_out=Fs_out, sigma=2)\nplot_signal(nov_smooth_out, ax=ax[1,1], Fs=1, xlabel='Time (samples)', color='k', \n        title='Smoothed novelty function after resampling (Fs = %0.1f)'%Fs_out);\n\nplt.tight_layout()\n\n\n\n\n\n\n행렬 기반 시각화와 평균화\n\n서로 다른 노벨티 함수를 공통 이산 시간 축으로 변환한 후 이 함수들을 색상 형식으로 쉽게 시각화할 수 있다.\n\n\ndef average_nov_dic(nov_dic, time_max_sec, Fs_out=100, norm=True, sigma=None):\n    \"\"\"Average respamples set of novelty functions\n\n    Args:\n        nov_dic (dict): Dictionary of novelty functions\n        time_max_sec (float): Duration of output signals (given in seconds)\n        Fs_out (scalar): Sampling rate of output signal (Default value = 100)\n        norm (bool): Apply max norm (if norm==True) (Default value = True)\n        sigma (float): Standard deviation for smoothing Gaussian kernel (Default value = None)\n\n    Returns:\n        nov_matrix (np.ndarray): Matrix containing resampled output signal (last one is average)\n        Fs_out (scalar): Sampling rate of output signals\n    \"\"\"\n    nov_num = len(nov_dic)\n    N_out = int(np.ceil(time_max_sec*Fs_out))\n    nov_matrix = np.zeros([nov_num + 1, N_out])\n    for k in range(nov_num):\n        nov = nov_dic[k][0]\n        Fs_nov = nov_dic[k][1]\n        nov_out, Fs_out = resample_signal(nov, Fs_in=Fs_nov, Fs_out=Fs_out,\n                                          time_max_sec=time_max_sec, sigma=sigma)\n        nov_matrix[k, :] = nov_out\n    nov_average = np.sum(nov_matrix, axis=0)/nov_num\n    if norm:\n        max_value = np.max(nov_average)\n        if max_value > 0:\n            nov_average = nov_average / max_value\n    nov_matrix[nov_num, :] = nov_average\n    return nov_matrix, Fs_out\n\n\ncmap = compressed_gray_cmap(alpha=1)\nFs_out = 100\nnov_matrix, Fs_out = average_nov_dic(nov_dic, time_max_sec=x_duration, Fs_out=Fs_out)\n\nplt.figure(figsize=[8,3])\nax = plt.subplot(1,1,1)\nim = ax.imshow(nov_matrix, cmap=cmap, aspect='auto', clim=[0,1],\n          extent=[0, x_duration, nov_matrix.shape[0]+0.5, 0.5])\nax.set_xlabel('Time (seconds)')\nax.set_yticks([1,2,3,4,5,6])\nax.set_yticklabels([r'Energy', r'EnergyLog', r'Spectral', r'Phase', r'Complex', r'Average'])\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\nplt.colorbar(im)\nplt.tight_layout()\n\n\n\n\n\n앞에서 언급했듯이 노벨티 곡선을 리샘플링하기 전에 스무딩 필터(smoothing filter) 를 적용하는 것이 도움이 될 수 있다. 또한 스무딩은 평균화를 다양한 노벨티 함수에 걸친 작은 시간적 편차에 덜 취약하게 만든다. 미분 이전과 고조파 합산 적용 이전에 유사한 스무딩 전략이 사용된다.\n\n\nFs_out = 100\nsigma = 2\n\nnov_matrix_smooth, Fs_out = average_nov_dic(nov_dic, time_max_sec=x_duration, \n                                            Fs_out=Fs_out, sigma=sigma)\n\nplt.figure(figsize=[8,3])\nax = plt.subplot(1,1,1)\nim = ax.imshow(nov_matrix_smooth, cmap=cmap, aspect='auto', clim=[0,1],\n          extent=[0, x_duration, nov_matrix.shape[0]+0.5, 0.5])\nax.set_xlabel('Time (seconds)')\nax.set_yticks([1,2,3,4,5,6])\nax.set_yticklabels([r'Energy', r'EnergyLog', r'Spectral', r'Phase', r'Complex', r'Average'])\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_duration);\nplt.colorbar(im)\nplt.tight_layout()\n\nfig, ax, line = plot_signal(nov_matrix[-1,:], Fs=Fs_out, \n                        color='k', title='Average novelty function without smoothing')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_duration)    \n\nfig, ax, line = plot_signal(nov_matrix_smooth[-1,:], Fs=Fs_out, \n                        color='k', title='Average novelty function with smoothing')\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_duration);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#다른-예",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#다른-예",
    "title": "7.1. 온셋 감지",
    "section": "다른 예",
    "text": "다른 예\n\nNote C4\n\npiano trumpet violin flute\n\n\nfn_ann = 'FMP_C6_F04_NoteC4_PTVF.csv'\nann, label_keys = read_annotation_pos(path_data+fn_ann, label='onset', header=0)\n\nfn_wav = 'FMP_C6_F04_NoteC4_PTVF.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nx_dur = len(x)/Fs\n\n\nN, H = 2048, 512\ngamma = 10\nfigsize=(8,2)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=N, H=H, gamma=gamma)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Spectral-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=None)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Energy-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Phase-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Complex-domain novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\nAudio(data=x,rate=Fs)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnov_dic = {}\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=None)\nnov_dic.update( {0 : [nov, Fs_nov, 'Energy-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=1000)\nnov_dic.update( {1 : [nov, Fs_nov, 'Energy-based novelty function with compression']} )\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs)\nnov_dic.update( {2 : [nov, Fs_nov, 'Spectral-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs)\nnov_dic.update( {3 : [nov, Fs_nov, 'Phase-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs)\nnov_dic.update( {4 : [nov, Fs_nov, 'Complex-domain novelty function']} )\n\nFs_out = 100\nsigma = 8\nnov_matrix_smooth, Fs_out = average_nov_dic(nov_dic, time_max_sec=x_dur, \n                                            Fs_out=Fs_out, sigma=sigma)\n\nplt.figure(figsize=[8,3])\nax = plt.subplot(1,1,1)\nim = ax.imshow(nov_matrix_smooth, cmap=cmap, aspect='auto', clim=[0,1],\n          extent=[0, x_dur, nov_matrix.shape[0]+0.5, 0.5])\nax.set_xlabel('Time (seconds)')\nax.set_yticks([1,2,3,4,5,6])\nax.set_yticklabels([r'Energy', r'EnergyLog', r'Spectral', r'Phase', r'Complex', r'Average'])\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_dur);\nplt.colorbar(im)\nplt.tight_layout()\n\n\n\n\n\n\nShostakovich Waltz\n\nfn_ann = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.csv'\nann, label_keys = read_annotation_pos(path_data+fn_ann, label='onset', header=0)\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nx_dur = len(x)/Fs\n\n\nN, H = 2048, 512\ngamma = 10\nfigsize=(8,2)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=N, H=H, gamma=gamma)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Spectral-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=None)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Energy-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Phase-based novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs)\nfig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', \n    title='Complex-domain novelty function');\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                nontime_axis=True, time_min=0, time_max=x_dur);\nAudio(data=x,rate=Fs)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnov_dic = {}\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=None)\nnov_dic.update( {0 : [nov, Fs_nov, 'Energy-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs, gamma=1000)\nnov_dic.update( {1 : [nov, Fs_nov, 'Energy-based novelty function with compression']} )\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs)\nnov_dic.update( {2 : [nov, Fs_nov, 'Spectral-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_phase(x, Fs=Fs)\nnov_dic.update( {3 : [nov, Fs_nov, 'Phase-based novelty function']} )\n\nnov, Fs_nov = compute_novelty_complex(x, Fs=Fs)\nnov_dic.update( {4 : [nov, Fs_nov, 'Complex-domain novelty function']} )\n\nFs_out = 100\nsigma = 8\nnov_matrix_smooth, Fs_out = average_nov_dic(nov_dic, time_max_sec=x_dur, \n                                            Fs_out=Fs_out, sigma=sigma)\n\nplt.figure(figsize=[8,3])\nax = plt.subplot(1,1,1)\nim = ax.imshow(nov_matrix_smooth, cmap=cmap, aspect='auto', clim=[0,1],\n          extent=[0, x_dur, nov_matrix.shape[0]+0.5, 0.5])\nax.set_xlabel('Time (seconds)')\nax.set_yticks([1,2,3,4,5,6])\nax.set_yticklabels([r'Energy', r'EnergyLog', r'Spectral', r'Phase', r'Complex', r'Average'])\nplot_annotation_line(ann, ax=ax, label_keys=label_keys,\n                    nontime_axis=True, time_min=0, time_max=x_dur);\nplt.colorbar(im)\nplt.tight_layout()\n\n\n\n\n\n스펙트럼 기반이 가장 온셋을 가장 잘나타내는 모습이다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-간단한-방식",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-간단한-방식",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: 간단한 방식",
    "text": "피크 선택: 간단한 방식\n\n직관적으로 피크 또는 로컬 최대값은 노벨티 곡선이 증가하는 상태(양의 미분)에서 감소하는 상태(음의 미분)로 변하는 특성을 특징으로 한다.\n다음 코드 셀에서는 이러한 모든 전환점을 찾는 첫 번째 간단한 피크 선택 방식을 정의한다. 또한 지정된 임계값 아래에 있는 피크를 버린다.\n\n\ndef peak_picking_simple(x, threshold=None):\n    \"\"\"Peak picking strategy looking for positions with increase followed by descrease\n\n    Args:\n        x (np.ndarray): Input function\n        threshold (float): Lower threshold for peak to survive\n\n    Returns:\n        peaks (np.ndarray): Array containing peak positions\n    \"\"\"\n    peaks = []\n    if threshold is None:\n        threshold = np.min(x) - 1\n    for i in range(1, x.shape[0] - 1):\n        if x[i - 1] < x[i] and x[i] > x[i + 1]:\n            if x[i] >= threshold:\n                peaks.append(i)\n    peaks = np.array(peaks)\n    return peaks\n\n\ndef plot_function_peak_positions(nov, Fs_nov, peaks, title='', figsize=(8,2)):\n    peaks_sec = peaks/Fs_nov\n    fig, ax, line = plot_signal(nov, Fs_nov, figsize=figsize, color='k', title=title);\n    plt.vlines(peaks_sec, 0, 1.1, color='r', linestyle=':', linewidth=1);\n\n\npeaks = peak_picking_simple(nov, threshold=None)\ntitle='Simple peak picking (without threshold condition)'\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\n\nthreshold = 0.2\npeaks = peak_picking_simple(nov, threshold=threshold)\ntitle='Simple peak picking (threshold=%.1f)'%threshold\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nplt.hlines(threshold, 0, x_duration, color='cyan', linewidth=2);\nplt.legend(['Novelty function',  'Peak positions',  'Global threshold'], \n           loc='upper right', framealpha=1);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-msaf",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-msaf",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: MSAF",
    "text": "피크 선택: MSAF\n\n노이즈와 같은 변동의 영향을 줄이려면 노벨티 함수에 스무딩 필터를 적용하는 것이 종종 유리하다. 또한, 노이즈와 같은 작은 피크를 폐기하기 위한 전역 임계값을 고려하는 대신, 적응(adaptive) 임계값 설정의 아이디어는 해당 값이 노벨티 함수의 로컬 평균을 초과하는 경우에만 피크를 선택하는 것이다.\n이 두 전략의 조합은 Python 패키지 MSAF(Music Structure Analysis Framework)에서 사용되는 다음과 같은 피크 선택 방식에 적용된다.\n\n\ndef peak_picking_MSAF(x, median_len=16, offset_rel=0.05, sigma=4.0):\n    \"\"\"Peak picking strategy following MSFA using an adaptive threshold (https://github.com/urinieto/msaf)\n\n    Args:\n        x (np.ndarray): Input function\n        median_len (int): Length of media filter used for adaptive thresholding (Default value = 16)\n        offset_rel (float): Additional offset used for adaptive thresholding (Default value = 0.05)\n        sigma (float): Variance for Gaussian kernel used for smoothing the novelty function (Default value = 4.0)\n\n    Returns:\n        peaks (np.ndarray): Peak positions\n        x (np.ndarray): Local threshold\n        threshold_local (np.ndarray): Filtered novelty curve\n    \"\"\"\n    offset = x.mean() * offset_rel\n    x = gaussian_filter1d(x, sigma=sigma)\n    threshold_local = median_filter(x, size=median_len) + offset\n    peaks = []\n    for i in range(1, x.shape[0] - 1):\n        if x[i - 1] < x[i] and x[i] > x[i + 1]:\n            if x[i] > threshold_local[i]:\n                peaks.append(i)\n    peaks = np.array(peaks)\n    return peaks, x, threshold_local\n\n\nmedian_len = 24\nsigma = 2\npeaks, x_smooth, threshold_local = peak_picking_MSAF(nov, median_len=median_len, \n                                                     offset_rel=0.05, sigma=sigma)\ntitle='MSAF peak picking (sigma=%1.0f, median_len=%2.0f)'%(sigma, median_len)\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nt = np.arange(nov.shape[0]) / Fs_nov \nplt.plot(t, x_smooth, color='magenta', linewidth=2);\nplt.plot(t, threshold_local, color='cyan', linewidth=2);\nplt.legend(['Novelty function', 'Smoothed Novelty', 'Local threshold'], \n           loc='upper right', framealpha=1);\n\nmedian_len = 16\nsigma = 4\npeaks, x_smooth, threshold_local = peak_picking_MSAF(nov, median_len=median_len, \n                                                     offset_rel=0.05, sigma=sigma)\ntitle='MSAF peak picking (sigma=%1.0f, median_len=%2.0f)'%(sigma, median_len)\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nt = np.arange(nov.shape[0]) / Fs_nov \nplt.plot(t, x_smooth, color='magenta', linewidth=2);\nplt.plot(t, threshold_local, color='cyan', linewidth=2);\nplt.legend(['Novelty function',  'Smoothed Novelty', 'Local threshold'], \n           loc='upper right', framealpha=1);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-scipy",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-scipy",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: Scipy",
    "text": "피크 선택: Scipy\n\nscipy의 신호 처리 패키지 scipy.signal에 find_peaks라는 피크 선택 함수가 있다. 이 함수는 1차원 배열을 사용하고 이웃 값의 단순 비교를 통해 모든 로컬 최대값을 찾는다. 선택적으로 피크 속성에 대한 조건을 지정하여 이러한 피크의 하위 집합을 선택할 수 있다.\nprominence 매개변수는 피크가 신호의 주변 기준선에서 얼마나 두드러지는지를 나타내는 피크 속성에 해당하며, 피크와 가장 낮은 등고선 사이의 수직 거리로 정의된다.\n\n\nprominence = 0.05\npeaks = signal.find_peaks(nov, prominence=prominence)[0]\ntitle='Scipy peak picking (prominence=%.2f)'%prominence\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\n\nprominence = 0.1\npeaks = signal.find_peaks(nov, prominence=prominence)[0]\ntitle='Scipy peak picking (prominence=%.2f)'%prominence\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\n\n\n\n\n\n\n\n\nheight 매개변수는 필요한 피크 높이를 지정한다. 그것은 숫자일 수도, None일수도, 시퀀스와 일치하는 배열, 또는 전자의 두-요소 시퀀스일수도 있다. 첫 번째 요소는 항상 최소값으로 해석되고 두 번째 요소(제공된 경우)는 필요한 최대 높이로 해석된다.\n다음 코드 셀에서 전역적으로 정의된 임계값(최소 및 최대 높이)과 로컬로 정의된 임계값 곡선(MSAF 피크 선택 접근 방식과 유사)의 두 가지 예를 보자.\n\n\nheight = (0.1, 0.5)\npeaks = signal.find_peaks(nov, height=height)[0]\ntitle='Scipy peak picking (height = (%.1f, %.1f))'%(height[0],height[1])\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nplt.hlines(height[0], 0, x_duration, color='cyan', linewidth=2);\nplt.hlines(height[1], 0, x_duration, color='cyan', linewidth=2);\n\nheight = median_filter(nov, size=8) + 0.1\npeaks = signal.find_peaks(nov, height=height)[0]\ntitle='Scipy peak picking (height = local median average)'\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nt = np.arange(nov.shape[0]) / Fs_nov \nplt.plot(t, height, color='cyan', linewidth=2);\nplt.legend(['Novelty function', 'Local threshold'], \n           loc='upper right', framealpha=1);\n\n\n\n\n\n\n\n\ndistance 매개변수는 인접한 피크 사이에 필요한 최소 수평 거리(샘플로 주어짐)를 지정한다. 이 접근 방식에서는 나머지 모든 피크에 대한 조건이 충족될 때까지 더 작은 피크가 먼저 제거된다. 이러한 거리 조건은 거리에 대한 최소 경계를 의미하는 특정 주기성 속성을 가진 신호에 특히 유용하다. 예를 들어 비트 트래킹(beat tracking)의 경우 예상 최대 템포는 인접한 두 비트 위치 사이의 최소 거리를 산출한다.\n\n\ndistance_list = [5,10,15]\n\nfor d in distance_list:\n    peaks = signal.find_peaks(nov, distance=d)[0]\n    title='Scipy peak picking (distance=%2.0f samples, Fs=%3.0f)'%(d, Fs_nov)\n    plot_function_peak_positions(nov, Fs_nov, peaks, title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-librosa",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-librosa",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: LibROSA",
    "text": "피크 선택: LibROSA\n\n파이썬 라이브러리 LibROSA는 피크 선택 함수 [librosa.util.peak_pick]를 제공한다.\n신호 x가 주어졌을 때, 다음의 3 조건이 만족됐을 때 샘플 n을 피크 포지션으로 정한다.\n\n로컬 이웃에서 최대:  x[n] == max(x[n - pre_max:n + post_max])\n이웃의 평균보다 충분히 큼:  x[n] >= mean(x[n - pre_avg:n + post_avg]) + delta\n두 피크 포지션 사이에 최소 거리를 둠:  n - previous_n > wait (where previous_n is the peak position preceding n)\n\n거리 조건은 greedy 방식으로 구현된다. 다양한 조건의 상호 작용(greedy 구현과 함께)은 눈에 띄는 피크가 거부될 수 있는 결과를 초래할 수 있다(밑의 예를 보자).\n\n\nwait_list = [5,10,15]\nfor wait in wait_list:\n    peaks = librosa.util.peak_pick(nov, pre_max=5, post_max=5, pre_avg=5, post_avg=5, \n                                   delta=0.01, wait=wait)\n    title='LibROSA peak picking (wait=%2.0f)'%wait\n    plot_function_peak_positions(nov, Fs_nov, peaks, title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-boeck",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-boeck",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: Boeck",
    "text": "피크 선택: Boeck\n\n다음의 피크 선택 방법은 Böck, Krebs, and Schedl의 논문 Online Capabilities of Onset Detection Methods에 소개된 방법이다.\n\n\ndef peak_picking_boeck(activations, threshold=0.5, fps=100, include_scores=False, combine=False,\n                       pre_avg=12, post_avg=6, pre_max=6, post_max=6):\n    \"\"\"Detects peaks.\n    | Implements the peak-picking method described in:\n    | \"Evaluating the Online Capabilities of Onset Detection Methods\"\n    | Sebastian Boeck, Florian Krebs and Markus Schedl\n    | Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR), 2012\n    Modified by Jan Schlueter, 2014-04-24\n    Args:\n        activations (np.nadarray): Vector of activations to process\n        threshold (float): Threshold for peak-picking (Default value = 0.5)\n        fps (scalar): Frame rate of onset activation function in Hz (Default value = 100)\n        include_scores (bool): Include activation for each returned peak (Default value = False)\n        combine (bool): Only report 1 onset for N seconds (Default value = False)\n        pre_avg (float): Use N past seconds for moving average (Default value = 12)\n        post_avg (float): Use N future seconds for moving average (Default value = 6)\n        pre_max (float): Use N past seconds for moving maximum (Default value = 6)\n        post_max (float): Use N future seconds for moving maximum (Default value = 6)\n    Returns:\n        peaks (np.ndarray): Peak positions\n    \"\"\"\n\n    activations = activations.ravel()\n\n    # detections are activations equal to the moving maximum\n    max_length = int((pre_max + post_max) * fps) + 1\n    if max_length > 1:\n        max_origin = int((pre_max - post_max) * fps / 2)\n        mov_max = maximum_filter1d(activations, max_length, mode='constant', origin=max_origin)\n        detections = activations * (activations == mov_max)\n    else:\n        detections = activations\n\n    # detections must be greater than or equal to the moving average + threshold\n    avg_length = int((pre_avg + post_avg) * fps) + 1\n    if avg_length > 1:\n        avg_origin = int((pre_avg - post_avg) * fps / 2)\n        mov_avg = uniform_filter1d(activations, avg_length, mode='constant', origin=avg_origin)\n        detections = detections * (detections >= mov_avg + threshold)\n    else:\n        # if there is no moving average, treat the threshold as a global one\n        detections = detections * (detections >= threshold)\n\n    # convert detected onsets to a list of timestamps\n    if combine:\n        stamps = []\n        last_onset = 0\n        for i in np.nonzero(detections)[0]:\n            # only report an onset if the last N frames none was reported\n            if i > last_onset + combine:\n                stamps.append(i)\n                # save last reported onset\n                last_onset = i\n        stamps = np.array(stamps)\n    else:\n        stamps = np.where(detections)[0]\n\n    # include corresponding activations per peak if needed\n    if include_scores:\n        scores = activations[stamps]\n        if avg_length > 1:\n            scores -= mov_avg[stamps]\n        return stamps / float(fps), scores\n    else:\n        return stamps / float(fps)\n\n\npeaks = peak_picking_boeck(nov, threshold=0.01, fps=1, \n            include_scores=False, combine=0,\n            pre_avg=0.1*Fs_nov, post_avg=0.1*Fs_nov, pre_max=0.1*Fs_nov, post_max=0.1*Fs_nov)\ntitle='Boeck peak picking'\nplot_function_peak_positions(nov, Fs_nov, peaks, title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-roeder",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#피크-선택-roeder",
    "title": "7.1. 온셋 감지",
    "section": "피크 선택: Roeder",
    "text": "피크 선택: Roeder\n\n또다른 피크 선택 방법은 Tido Röder에 의해 원래 구현된 방법이다.\n\n\ndef peak_picking_roeder(x, direction=None, abs_thresh=None, rel_thresh=None, descent_thresh=None, tmin=None, tmax=None):\n    \"\"\"| Computes the positive peaks of the input vector x\n    | Python adaption from the Matlab Roeder_Peak_Picking script peaks.m from the internal Sync Toolbox\n    | reckjn 2017\n    Args:\n        x (np.nadarray): Signal to be searched for (positive) peaks\n        direction (int): +1 for forward peak searching, -1 for backward peak searching.\n            default is dir == -1. (Default value = None)\n        abs_thresh (float): Absolute threshold signal, i.e. only peaks\n            satisfying x(i)>=abs_thresh(i) will be reported.\n            abs_thresh must have the same number of samples as x.\n            a sensible choice for this parameter would be a global or local\n            average or median of the signal x.\n            If omitted, half the median of x will be used. (Default value = None)\n        rel_thresh (float): Relative threshold signal. Only peak positions i with an\n            uninterrupted positive ascent before position i of at least\n            rel_thresh(i) and a possibly interrupted (see parameter descent_thresh)\n            descent of at least rel_thresh(i) will be reported.\n            rel_thresh must have the same number of samples as x.\n            A sensible choice would be some measure related to the\n            global or local variance of the signal x.\n            if omitted, half the standard deviation of W will be used.\n        descent_thresh (float): Descent threshold. during peak candidate verfication, if a slope change\n            from negative to positive slope occurs at sample i BEFORE the descent has\n            exceeded rel_thresh(i), and if descent_thresh(i) has not been exceeded yet,\n            the current peak candidate will be dropped.\n            this situation corresponds to a secondary peak\n            occuring shortly after the current candidate peak (which might lead\n            to a higher peak value)!\n            |\n            | The value descent_thresh(i) must not be larger than rel_thresh(i).\n            |\n            | descent_thresh must have the same number of samples as x.\n            a sensible choice would be some measure related to the\n            global or local variance of the signal x.\n            if omitted, 0.5*rel_thresh will be used. (Default value = None)\n        tmin (int): Index of start sample. peak search will begin at x(tmin). (Default value = None)\n        tmax (int): Index of end sample. peak search will end at x(tmax). (Default value = None)\n    Returns:\n        peaks (np.nadarray): Array of peak positions\n    \"\"\"\n\n    # set default values\n    if direction is None:\n        direction = -1\n    if abs_thresh is None:\n        abs_thresh = np.tile(0.5*np.median(x), len(x))\n    if rel_thresh is None:\n        rel_thresh = 0.5*np.tile(np.sqrt(np.var(x)), len(x))\n    if descent_thresh is None:\n        descent_thresh = 0.5*rel_thresh\n    if tmin is None:\n        tmin = 1\n    if tmax is None:\n        tmax = len(x)\n\n    dyold = 0\n    dy = 0\n    rise = 0  # current amount of ascent during a rising portion of the signal x\n    riseold = 0  # accumulated amount of ascent from the last rising portion of x\n    descent = 0  # current amount of descent (<0) during a falling portion of the signal x\n    searching_peak = True\n    candidate = 1\n    P = []\n\n    if direction == 1:\n        my_range = np.arange(tmin, tmax)\n    elif direction == -1:\n        my_range = np.arange(tmin, tmax)\n        my_range = my_range[::-1]\n\n    # run through x\n    for cur_idx in my_range:\n        # get local gradient\n        dy = x[cur_idx+direction] - x[cur_idx]\n\n        if dy >= 0:\n            rise = rise + dy\n        else:\n            descent = descent + dy\n\n        if dyold >= 0:\n            if dy < 0:  # slope change positive->negative\n                if rise >= rel_thresh[cur_idx] and searching_peak is True:\n                    candidate = cur_idx\n                    searching_peak = False\n                riseold = rise\n                rise = 0\n        else:  # dyold < 0\n            if dy < 0:  # in descent\n                if descent <= -rel_thresh[candidate] and searching_peak is False:\n                    if x[candidate] >= abs_thresh[candidate]:\n                        P.append(candidate)  # verified candidate as True peak\n                    searching_peak = True\n            else:  # dy >= 0 slope change negative->positive\n                if searching_peak is False:  # currently verifying a peak\n                    if x[candidate] - x[cur_idx] <= descent_thresh[cur_idx]:\n                        rise = riseold + descent  # skip intermediary peak\n                    if descent <= -rel_thresh[candidate]:\n                        if x[candidate] >= abs_thresh[candidate]:\n                            P.append(candidate)    # verified candidate as True peak\n                    searching_peak = True\n                descent = 0\n        dyold = dy\n    peaks = np.array(P)\n    return peaks\n\n\npeaks = peak_picking_roeder(nov, direction=None, abs_thresh=None, \n                                      rel_thresh=None, descent_thresh=None, \n                                      tmin=None, tmax=None)\ntitle='Roeder peak picking'\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\n\nnov_smooth = gaussian_filter1d(nov, sigma=2)\npeaks = peak_picking_roeder(nov_smooth, direction=None, abs_thresh=None, \n                                      rel_thresh=None, descent_thresh=None, \n                                      tmin=None, tmax=None)\ntitle='Roeder peak picking (applied to a smoothed input signal)'\nplot_function_peak_positions(nov, Fs_nov, peaks, title)\nt = np.arange(nov.shape[0]) / Fs_nov \nplt.plot(t, nov_smooth, color='magenta', linewidth=2);\nplt.legend(['Novelty function', 'Smoothed novelty'], \n           loc='upper right', framealpha=1);"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#librosa-함수-예제-2",
    "href": "posts/7. Tempo and Beat Tracking/7.1.Onset_Detection.html#librosa-함수-예제-2",
    "title": "7.1. 온셋 감지",
    "section": "Librosa 함수 예제",
    "text": "Librosa 함수 예제\n\nx, sr = librosa.load('../audio/58bpm.wav')\nAudio(x, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nhop_length = 256\nonset_envelope = librosa.onset.onset_strength(y=x, sr=sr, hop_length=hop_length)\n\nN = len(x)\nT = N/float(sr)\nt = np.linspace(0, T, len(onset_envelope))\n\n\n# onset envelop\nplt.figure(figsize=(10, 3))\nplt.plot(t, onset_envelope)\nplt.xlabel('Time (sec)')\nplt.xlim(xmin=0)\nplt.ylim(0)\nplt.show()\n\n\n\n\n\nonset_frames = librosa.util.peak_pick(onset_envelope, pre_max=7, post_max=7, pre_avg=7, post_avg=7, delta=0.5, wait=5)\nonset_frames\n\narray([  5,  78,  91, 136, 168, 180, 225, 255, 268, 314, 347, 358, 403,\n       433, 447, 492, 522, 537, 581, 611, 625, 659, 670, 703], dtype=int64)\n\n\n\nplt.figure(figsize=(10,3))\nplt.plot(t, onset_envelope)\nplt.grid(False)\nplt.vlines(t[onset_frames], 0, onset_envelope.max(), color='r', alpha=0.7)\nplt.xlabel('Time (sec)')\nplt.xlim(0, T)\nplt.ylim(0)\nplt.show()\n\n\n\n\n\nclicks = librosa.clicks(frames=onset_frames, sr=22050, hop_length=hop_length, length=N)\nAudio(x+clicks, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_OnsetDetection.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_NoveltyEnergy.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_NoveltySpectral.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_NoveltyPhase.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_NoveltyComplex.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_NoveltyComparison.html\nhttps://colab.research.google.com/github/stevetjoa/musicinformationretrieval.com/blob/gh-pages/onset_detection.ipynb\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html",
    "title": "7.2. 템포 분석",
    "section": "",
    "text": "템포(tempo)와 비트(beat)에 대해 설명하고, 푸리에 템포그램(fourier tempogram), 자기상관 템포그램(autocorrelation tempogram) 및 순환 템포그램(cyclic tempogram)을 소개한다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포그램tempogram-표현",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포그램tempogram-표현",
    "title": "7.2. 템포 분석",
    "section": "템포그램(Tempogram) 표현",
    "text": "템포그램(Tempogram) 표현\n\n푸리에 분석에서 크기(magnitude) 스펙트로그램(spectrogram)는 주어진 신호에 대한 시간-주파수 표현이다. 스펙트로그램의 큰 값 \\(\\mathcal{Y}(t,\\omega)\\)는 신호가 시간 인스턴스 \\(t\\)에 주파수 \\(\\omega\\)에 해당하는 주기적(periodic) 성분을 포함하고 있음을 나타낸다.\n이제 주어진 음악 녹음에 대한 특정 템포의 로컬 관련성을 각 시간 인스턴스에 대해 나타내는 템포그램(tempogram) 이라는 유사한 개념을 소개한다. 수학적으로, 템포그램을 다음과 같은 함수로 모델링한다. \\[\\mathcal{T}:\\mathbb{R}\\times \\mathbb{R}_{>0}\\to \\mathbb{R}_{\\geq 0}\\] 초 단위의 시간 매개변수 \\(t\\in\\mathbb{R}\\)와 분 당 비트(beats per minute = BPM)단위의 템포 매개변수로 측정\n직관적으로 \\(\\mathcal{T}(t,\\tau)\\) 값은 신호가 시간 인스턴스 \\(t\\) 근처에서 주어진 템포 \\(\\tau\\)의 로컬 주기 펄스(pulse)를 포함하는 정도를 나타낸다.\n스펙트로그램과 마찬가지로 실제로는 이산 시간-템포 그리드에서만 템포그램을 계산한다. 이전과 같이 샘플링된 시간 축은 \\([1:N]\\)로 주어진다고 가정한다. 경계(boundary) 케이스를 피하고, 이후 표기를 단순화하기 위해 이 축을 \\(\\mathbb{Z}\\)로 확장한다. 또한 \\(\\Theta\\subset\\mathbb{R}_{>0}\\)를 \\(\\mathrm{BPM}\\)으로 지정된 유한한 템포 집합이라고 한다.\n그러면 이산 템포그램(discrete tempogram) 은 다음의 함수이다. \\[\\mathcal{T}:\\mathbb{Z}\\times \\Theta\\to \\mathbb{R}_{\\geq 0}.\\]\n주어진 오디오 녹음에서 템포그램 표현을 유도하는 대부분의 방식은 두 단계로 진행된다.\n\n일반적으로 펄스 위치는 음 온셋과 함께 진행된다는 가정하에 음악 신호를 먼저 노벨티 함수로 변환한다. 이 함수는 일반적으로 임펄스 같은(impulse-like) 스파이크(spike)로 구성되며 각 스파이크는 음 온셋 위치를 나타낸다.\n두번째 단계에서는 노벨티 함수의 로컬 주기성(locally periodic behavior)을 분석한다.\n\n템포그램을 얻기 위해 주어진 시간 인스턴스 근처에서 다양한 기간 \\(T>0\\)(초 단위)에 대한 주기적인 행동을 정량화한다. 속도 \\(\\omega=1/T\\)(\\(\\mathrm{Hz}\\)로 측정)와 템포 \\(\\tau\\)(\\(\\mathrm{BPM}\\)로 측정)는 다음과 같이 연관된다: \\(\\tau = 60 \\cdot \\omega\\)\n예를 들어, \\(T=0.5~\\mathrm{sec}\\) 주기로 규칙적으로 간격을 impulse-like spike 시퀀스는 \\(\\omega=1/T=2~\\mathrm{Hz}\\) 또는 \\(\\tau=120~\\mathrm{BPM}\\)의 템포에 해당한다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#펄스-수준-pulse-levels",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#펄스-수준-pulse-levels",
    "title": "7.2. 템포 분석",
    "section": "펄스 수준 (Pulse Levels)",
    "text": "펄스 수준 (Pulse Levels)\n\n음악 녹음의 템포를 결정하는 주요 문제 중 하나는 음악의 펄스가 종종 리듬을 나타내는 복잡한 계층으로 구성된다는 사실이다. 특히 템포와 비트에 대한 인간의 인식에 기여하는 다양한 수준이 있다.\n“tactus” 수준은 일반적으로 4분 음표 수준에 해당하며 종종 발 두드리기 속도와 일치한다.\n더 큰 음계에서 생각하면 특히 빠른 음악이나 강한 루바토가 있는 표현력이 풍부한 음악을 들을 때 “measure” 수준에서 템포를 인식할 수도 있다.\n마지막으로 신호에서 발생하는 음악적으로 의미 있는 악센트의 가장 빠른 반복 속도를 나타내는 “tatum”(시간 원자) 수준을 고려할 수도 있다. 다음 예는 “Happy Birthday to you”라는 노래를 사용하여 이러한 개념을 설명한다.\n\n\nipd.display(Image(path_img+\"FMP_C6_F12.png\", width=500))\nipd.display(Audio(path_data+\"FMP_C6_Audio_HappyBirthday.wav\"))\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n다음 코드 셀에서는 스펙트럼 기반 노벨티 함수 위에 표시되는 다양한 펄스 수준에 대해 수동으로 주석이 달린 펄스 위치를 시각화한다. 또한 원본 음악 녹음과 겹쳐진 펄스 위치의 sonification를 들을 수 있다.\n\n\ntitle = 'Pulse on measure level'\nfn_ann = 'FMP_C6_Audio_HappyBirthday_measure.csv'\nfn_wav = 'FMP_C6_Audio_HappyBirthday.wav'\nplot_sonify_novelty_beats(path_data+fn_wav, path_data+fn_ann, title)\n\ntitle = 'Pulse on tactus level'\nfn_ann = 'FMP_C6_Audio_HappyBirthday_tactus.csv'\nfn_wav = 'FMP_C6_Audio_HappyBirthday.wav'\nplot_sonify_novelty_beats(path_data+fn_wav, path_data+fn_ann, title)\n\ntitle = 'Pulse on tatum level'\nfn_ann = 'FMP_C6_Audio_HappyBirthday_tatum.csv'\nfn_wav = 'FMP_C6_Audio_HappyBirthday.wav'\nplot_sonify_novelty_beats(path_data+fn_wav, path_data+fn_ann, title)\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-옥타브-하모닉-서브하모닉",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-옥타브-하모닉-서브하모닉",
    "title": "7.2. 템포 분석",
    "section": "템포 옥타브, 하모닉, 서브하모닉",
    "text": "템포 옥타브, 하모닉, 서브하모닉\n\n종종 서로 다른 펄스 수준의 존재로 인해 발생하는 템포 모호성은 템포그램 \\(\\mathcal{T}\\)에도 반영된다. 더 높은 펄스 레벨은 종종 주어진 템포 \\(\\tau\\)의 정수 배수 \\(\\tau,2\\tau,3\\tau,\\ldots\\)에 해당한다.\n피치와 마찬가지로 이러한 정수 배수를 \\(\\tau\\)의 (템포) 하모닉(=배음,고조파)(harmonics) 이라고 한다.\n또한, 정수 분수 \\(\\tau,\\tau/2,\\tau/3,\\ldots\\)는 \\(\\tau\\)의 (템포) 서브하모닉(subharmonics) 이라고 한다.\n음악 피치의 옥타브 개념과 유사하게, 값이 절반 또는 두 배인 두 템포 사이의 차이를 템포 옥타브(octave) 라고 한다.\n다음 예는 증가하는 템포(\\(20~\\mathrm{sec}\\) 과정에서 \\(170\\)에서 \\(200~\\mathrm{BPM}\\)로 상승)의 클릭 트랙에 대한 두 가지 유형의 템포그램을 보여준다.\n\n첫 번째 템포그램은 템포 하모닉을 강조하고 두 번째 템포그램은 템포 서브하모닉을 강조한다.\n빨간색 사각형으로 표시된 항목은 음악 신호가 \\(t=5~\\mathrm{sec}\\) 시간 위치를 중심으로 \\(\\tau=180~\\mathrm{BPM}\\)의 우세한(dominant) 템포를 가지고 있음을 나타낸다.\n\n\n\nipd.display(Image(path_img+\"FMP_C6_F11.png\", width=500))\nipd.display(Audio(path_data+\"FMP_C6_F11_ClickTrack-BPM170-200.wav\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#전역적-템포-global-tempo",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#전역적-템포-global-tempo",
    "title": "7.2. 템포 분석",
    "section": "전역적 템포 (Global Tempo)",
    "text": "전역적 템포 (Global Tempo)\n\n어느 정도 일정한 템포를 가정하면 전체 녹음에 대해 하나의 전역적(global) 템포 값을 결정하는 것으로 충분하다. 이러한 값은 프레임별 주기성 분석을 통해 얻은 템포 값을 평균하여 얻을 수 있다. 예를 들어, 템포그램 표현을 기반으로 모든 시간 프레임에 대한 템포 값을 평균화하여 \\(\\tau\\in\\Theta\\)에만 의존하는\\(\\mathcal{T}_\\mathrm{Average}:\\Theta\\to\\mathbb{R}_{\\geq0}\\) 함수를 얻을 수 있다.\n관련된 시간 위치가 \\([1:N]\\) 간격에 있다고 가정하면 다음과 같이 \\(\\mathcal{T}_\\mathrm{Average}\\)를 정의할 수 있다. \\[\\mathcal{T}_\\mathrm{Average}(\\tau) := \\frac{1}{N}\\sum_{n\\in[1:N]} \\mathcal{T}(n,\\tau)\\]\n이 함수의 최대값 \\[\\hat{\\tau} := \\max\\{\\mathcal{T}_\\mathrm{Average}(\\tau)\\,\\mid\\, \\tau\\in\\Theta\\}\\] 는 녹음의 전체 템포에 대한 추정치를 산출한다.\n물론 단일 템포 값을 추정하는 보다 정교한 방법이 적용될 수 있다. 예를 들어 단순 평균을 사용하는 대신 이상값과 노이즈에 더 강력한 중앙값 필터링을 적용할 수 있다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#정의",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#정의",
    "title": "7.2. 템포 분석",
    "section": "정의",
    "text": "정의\n\n피크가 음 온셋 후보를 나타내는 이산 시간 노벨티 함수 \\(\\Delta:\\mathbb{Z}\\to\\mathbb{R}\\)가 주어진다고 가정하자.\n푸리에 분석의 아이디어는 윈도우(windowed) 정현파와 비교하여 노벨티 곡선의 로컬 주기성을 감지하는 것이다. \\(\\Delta\\)의 로컬 섹션과 윈도우 정현파의 높은 상관관계는 정현파 주파수의 주기성을 나타낸다(적절한 위상이 주어짐).\n이 (위상과 함께) 상관 관계는 단시간 푸리에 변환을 사용하여 계산할 수 있다. 이를 위해 \\(n=0\\)을 중심으로 하는 유한 길이의 윈도우 함수 \\(w:\\mathbb{Z}\\to\\mathbb{R}\\)를 수정한다(예: 샘플링된 Hann 윈도우). 그러면 주파수 매개변수 \\(\\omega\\in\\mathbb{R}_{\\geq 0}\\) 및 시간 매개변수 \\(n\\in\\mathbb{Z}\\)에 대해 복소 푸리에 계수 \\(\\mathcal{F}(n ,\\omega)\\)는 다음과 같이 정의된다. \\[\\mathcal{F}(n,\\omega) := \\sum_{m\\in\\mathbb{Z}}\n\\Delta(m)\\overline{w}(m-n)\\mathrm{exp}(-2\\pi i\\omega m).\\]\n이 정의는 홉 크기 \\(H=1\\)를 사용할 때 이산 단시간 푸리에 변환과 유사하다.\n주파수를 템포 값으로 변환하면 (이산) 푸리에 템포그램 \\(\\mathcal{T}^\\mathrm{F}: \\mathbb{Z} \\times \\Theta \\to \\mathbb{R}_{\\geq 0}\\)를 다음과 같이 정의한다. \\[\\mathcal{T}^\\mathrm{F}(n,\\tau) := |\\mathcal{F}(n,\\tau/60)|\\]\n실제 적용을 위해 \\(\\mathcal{T}^\\mathrm{F}\\)는 소수의 템포 매개변수에 대해서만 계산된다.\n\n예를 들어, \\(30\\)와 \\(600~\\mathrm{BPM}\\) 사이의 (정수) 음악 템포를 포함하는 \\(\\Theta=[30:600]\\) 세트를 선택할 수 있다.\n이 경계는 약 \\(100~\\mathrm{msec}\\) (\\(600~\\mathrm{BPM}\\))와 \\(2~\\mathrm{sec}\\) (\\(30~\\mathrm{BPM}\\))사이의 시간적 분리를 보여주는 음악 이벤트만 템포 인식에 기여한다는 가정하에 주어진 것이다.\n\n각각의 응용과 음악 녹음의 특성에 따라 오디오의 \\(4\\) ~ \\(12~\\mathrm{sec}\\)에 해당하는 창 크기를 고르는 것이 합리적인 범위이다. 또한 홉 크기 매개변수 \\(H\\)를 도입하여 결과로 나타나는 템포그램의 피쳐 레이트를 조정할 수 있다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-해상도-tempo-resolution",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-해상도-tempo-resolution",
    "title": "7.2. 템포 분석",
    "section": "템포 해상도 (Tempo Resolution)",
    "text": "템포 해상도 (Tempo Resolution)\n\n\\(\\Theta=[30:600]\\)와 같은 템포 집합을 사용하려면 고해상도의 스펙트럼 분석이 필요하다. (특히 낮은 주파수 범위에서)\n예를 들어, 입력 노벨티 함수 \\(\\Delta\\)가 특징 샘플링 레이트 \\(F_\\mathrm{s}^\\Delta = 100~\\mathrm{Hz}\\)이고 윈도우 \\(w\\)의 길이가 \\(N=1000\\) (노벨티 함수 \\(\\Delta\\)의 \\(10\\)초를 커버)라고 가정하자.\n그런 다음 \\(N\\) 크기의 DFT를 적용하면 다음의 물리적 주파수에 해당하는 푸리에 계수가 생성된다. \\[\\omega = F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}^\\Delta}{N} = k\\cdot 0.1 ~\\mathrm{Hz}\\] for \\(k\\in[0:500]\\)\n\n템포 \\(\\tau= k\\cdot 6~\\mathrm{BPM}\\)에 해당\n\n즉, DFT는 \\(\\tau=0\\)에서 \\(\\tau=3000~\\mathrm{BPM}\\) 범위의 템포 추정치로 \\(6~\\mathrm{BPM}\\)의 템포 해상도를 생성한다. \\(\\Theta=[30:600]\\)와 같은 특정 템포 집합에 관심이 있는 경우, 푸리에 계수(\\(k\\in[5:100]\\))의 작은 부분만 필요하다. 또한 \\(6~\\mathrm{BPM}\\)의 템포 해상도는 충분하지 않다. 따라서 DFT를 기반으로 하는 간단한 STFT는 적합하지 않을 수 있다. 한 가지 대안은 필요한 푸리에 계수를 개별적으로 계산하는 것이다(DFT를 사용하지 않음). 이는 FFT 알고리즘을 통해 수행할 수 없지만 상대적으로 적은 수의 푸리에 계수(템포 집합 \\(\\Theta\\)에 해당)만 계산하면 되므로 계산 복잡성은 여전히 합리적일 수 있다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#구현",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#구현",
    "title": "7.2. 템포 분석",
    "section": "구현",
    "text": "구현\n\n다음의 식과 비슷한 구현을 보자. \\[\\mathcal{F}(n,\\omega) = \\sum_{m\\in\\mathbb{Z}} \\Delta(m)\\overline{w}(m-n)\\mathrm{exp}(-2\\pi i \\omega m).\\]\n노벨티 함수 \\(\\Delta\\)이 길이 \\(L\\in\\mathbb{N}\\)를 가진다고 가정하자. 윈도우 함수 \\(w\\)는 \\(N\\in\\mathbb{N}\\) 길이의 Hann 윈도우로 하며, 홉 크기 \\(H\\in\\mathbb{N}\\)만큼 이동한다. 또한 “centered view”를 사용하여 노벨티 함수을 윈도우 길이의 반만큼 제로-패딩시킨다.\n결과적으로 \\(n=0\\)에 의해 인덱싱된 \\(\\mathcal{F}\\)의 첫 번째 프레임은 물리적 시간 위치 \\(t=0~\\mathrm{sec}\\)에 해당한다. 홉 크기 매개변수 \\(H\\)는 템포그램의 프레임 속도를 \\(F_\\mathrm{s}^\\Delta/H\\)로 줄인다. 인풋으로 템포 값을 포함하는 유한 집합 \\(\\Theta\\)를 지정할 수 있다.\n밑의 알고리즘에는 \\(\\tau\\in\\Theta\\)를 반복하는 외부 루프가 있다. 각 주파수 \\(\\omega=\\tau/60\\)에 대해 \\(\\mathcal{F}(n,\\omega)\\) 값은 프레임 인덱스 \\(n\\)에서 반복되는 내부 루프에서 계산된다. 10초 듀레이션의 합성 노벨티 함수를 사용하며, 곡선은 전반부의 템포가 \\(150~\\mathrm{BPM}\\)이고 후반부의 템포가 \\(120~\\mathrm{BPM}\\)인 클릭 트랙으로 구성된다.\n\n\ndef compute_tempogram_fourier(x, Fs, N, H, Theta=np.arange(30, 601, 1)):\n    \"\"\"Compute Fourier-based tempogram [FMP, Section 6.2.2]\n\n    Args:\n        x (np.ndarray): Input signal\n        Fs (scalar): Sampling rate\n        N (int): Window length\n        H (int): Hop size\n        Theta (np.ndarray): Set of tempi (given in BPM) (Default value = np.arange(30, 601, 1))\n\n    Returns:\n        X (np.ndarray): Tempogram\n        T_coef (np.ndarray): Time axis (seconds)\n        F_coef_BPM (np.ndarray): Tempo axis (BPM)\n    \"\"\"\n    win = np.hanning(N)\n    N_left = N // 2\n    L = x.shape[0]\n    L_left = N_left\n    L_right = N_left\n    L_pad = L + L_left + L_right\n    # x_pad = np.pad(x, (L_left, L_right), 'constant')  # doesn't work with jit\n    x_pad = np.concatenate((np.zeros(L_left), x, np.zeros(L_right)))\n    t_pad = np.arange(L_pad)\n    M = int(np.floor(L_pad - N) / H) + 1\n    K = len(Theta)\n    X = np.zeros((K, M), dtype=np.complex_)\n\n    for k in range(K):\n        omega = (Theta[k] / 60) / Fs\n        exponential = np.exp(-2 * np.pi * 1j * omega * t_pad)\n        x_exp = x_pad * exponential\n        for n in range(M):\n            t_0 = n * H\n            t_1 = t_0 + N\n            X[k, n] = np.sum(win * x_exp[t_0:t_1])\n        T_coef = np.arange(M) * H / Fs\n        F_coef_BPM = Theta\n    return X, T_coef, F_coef_BPM\n\n\nFs = 100\nL = 10*Fs\nx = np.zeros(L)\npeaks = np.concatenate((np.arange(40,L//2,40),np.arange(500,L,50)))\nx[peaks]=1\n\nN = 300 #corresponding to 3 seconds (Fs = 100)\nH = 10\nTheta = np.arange(50,410,10)\n\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(x, Fs, N=N, H=H, Theta=Theta)\ntempogram = np.abs(X)\n#tempogram = libfmp.c3.normalize_feature_sequence(tempogram, norm='max')\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2]}, figsize=(8, 5))        \nplot_signal(x, Fs, ax=ax[0,0], color='k', title='Novelty function')\nax[0,1].set_axis_off()\nplot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, ax=[ax[1,0], ax[1,1]], \n                     title='Fourier tempogram', ylabel='Tempo (BPM)', colorbar=True);\nplt.tight_layout()\n\n\n\n\n\n푸리에 템포그램 \\(\\mathcal{T}^\\mathrm{F}\\)의 시각화는 시간이 지남에 따라 우세한 템포를 나타낸다. 실제로 처음 5초 동안 \\(\\tau=150~\\mathrm{BPM}\\)에 대한 높은 값과 마지막 5초 동안 \\(\\tau=120~\\mathrm{BPM}\\) 템포에 대한 높은 값이 있다.\n푸리에 분석을 기반으로 하므로 항목 \\(\\mathcal{T}^\\mathrm{F}(n,\\tau)\\)는 템포 \\(\\tau\\)(또는 주파수 \\(\\omega=\\tau/60\\))를 나타내는 윈도우(windowed) 정현파로 된 \\(n\\) 근처의 노벨티 함수 \\(\\Delta\\)를 비교하여 얻어진다.\n다음 코드 셀에서 이러한 분석은 템포그램 시각화에서 빨간색 점으로 표시된 다양한 시간-템포 쌍으로 설명된다.\n\n\ndef compute_sinusoid_optimal(c, tempo, n, Fs, N, H):\n    \"\"\"Compute windowed sinusoid with optimal phase\n\n    Args:\n        c (complex): Coefficient of tempogram (c=X(k,n))\n        tempo (float): Tempo parameter corresponding to c (tempo=F_coef_BPM[k])\n        n (int): Frame parameter of c\n        Fs (scalar): Sampling rate\n        N (int): Window length\n        H (int): Hop size\n\n    Returns:\n        kernel (np.ndarray): Windowed sinusoid\n        t_kernel (np.ndarray): Time axis (samples) of kernel\n        t_kernel_sec (np.ndarray): Time axis (seconds) of kernel\n    \"\"\"\n    win = np.hanning(N)\n    N_left = N // 2\n    omega = (tempo / 60) / Fs\n    t_0 = n * H\n    t_1 = t_0 + N\n    phase = - np.angle(c) / (2 * np.pi)\n    t_kernel = np.arange(t_0, t_1)\n    kernel = win * np.cos(2 * np.pi * (t_kernel*omega - phase))\n    t_kernel_sec = (t_kernel - N_left) / Fs\n    return kernel, t_kernel, t_kernel_sec\n\n\ndef plot_signal_kernel(x, t_x, kernel, t_kernel, xlim=None, figsize=(8, 2), title=None):\n    \"\"\"Visualize signal and local kernel\n\n    Args:\n        x: Signal\n        t_x: Time axis of x (given in seconds)\n        kernel: Local kernel\n        t_kernel: Time axis of kernel (given in seconds)\n        xlim: Limits for x-axis (Default value = None)\n        figsize: Figure size (Default value = (8, 2))\n        title: Title of figure (Default value = None)\n\n    Returns:\n        fig: Matplotlib figure handle\n    \"\"\"\n    if xlim is None:\n        xlim = [t_x[0], t_x[-1]]\n    fig = plt.figure(figsize=figsize)\n    plt.plot(t_x, x, 'k')\n    plt.plot(t_kernel, kernel, 'r')\n    plt.title(title)\n    plt.xlim(xlim)\n    plt.tight_layout()\n    return fig\n\n\nt_x = np.arange(x.shape[0])/Fs\ncoef_n = [20, 30, 70]\ncoef_k = [11, 5, 19]\n\nfig, ax, im = plot_matrix(tempogram, \n                T_coef=T_coef, F_coef=F_coef_BPM, figsize=(9,3),\n                title='Fourier tempogram', ylabel='Tempo (BPM)', colorbar=True);\nax[0].plot(T_coef[coef_n],F_coef_BPM[coef_k],'ro')\n\nfor i in range(len(coef_k)):\n    k = coef_k[i]\n    n = coef_n[i]\n    tempo = F_coef_BPM[k]\n    time = T_coef[n]\n    corr = np.abs(X[k,n])\n    kernel, t_kernel, t_kernel_sec = compute_sinusoid_optimal(X[k,n], \n                        F_coef_BPM[k], n, Fs, N, H)\n    title=r'Windowed sinusoid (t = %0.1f sec, $\\tau$ = %0.0f BPM, corr = %0.2f)'%(time, tempo, corr)\n    fig = plot_signal_kernel(x, t_x, kernel, t_kernel_sec, title=title)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n첫 번째 시간-템포 쌍(\\(t=2~\\mathrm{sec}\\), \\(\\tau=160~\\mathrm{BPM}\\))의 경우, 윈도우 정현파의 양의 부분이 노벨티 함수 \\(\\Delta\\)의 impulse-like 피크와 잘 정렬된다. 반면 정현파의 음수 부분은 \\(\\Delta\\)의 0 영역에 속한다. 결과적으로 윈도우 정현파와 \\(\\Delta\\) 사이에는 높은 상관관계가 있으며, 이는 큰 계수 \\(\\mathcal{T}^\\mathrm{F}(n,\\tau)\\)로 이어진다.\n두 번째 쌍(\\(t=3\\), \\(\\tau=100\\))의 경우 정현파는 작은 계수로 이어진다. 이 경우 \\(\\Delta\\)의 일부 피크는 정현파의 양수 부분에 속하고 다른 피크는 정현파의 음수 부분에 속한다. 취소의 결과로 인해 \\(\\Delta\\)와 정현파 사이의 전체 상관 관계는 작다.\n마지막으로 세 번째 쌍(\\(t=7\\), \\(\\tau=240\\))은 주 템포의 두 배를 나타내는 정현파를 사용할 때 높은 상관관계를 얻는 것을 보여준다. 이 경우 \\(\\Delta\\)의 피크는 정현파의 모든 두 번째 양의 부분과 정렬되는 반면 정현파의 다른 모든 부분은 \\(\\Delta\\)의 0 영역에 속한다. 이 논의는 푸리에 템포그램이 일반적으로 템포 하모닉을 나타내지만 템포 서브하모닉을 억제한다는 것을 보여준다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#예시",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#예시",
    "title": "7.2. 템포 분석",
    "section": "예시",
    "text": "예시\n\n\\(F_\\mathrm{s}^\\Delta = 100~\\mathrm{Hz}\\)로 리샘플링된 스펙트럼 기반 노벨티 함수로 시작한다. 또한 \\(5\\)초(\\(N=500\\))에 해당하는 윈도우 크기와 해상도 \\(1~\\mathrm{BPM}\\)의 템포 집합 \\(\\Theta=[30:600]\\)를 사용한다. 홉 크기 매개변수 \\(H=10\\)를 사용하면 결과 템포그램의 피쳐 레이트는 \\(10~\\mathrm{Hz}\\)가 된다.\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nFs = 22050\nx, Fs = librosa.load(path_data+fn_wav, sr=Fs) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nN = 500 #corresponding to 5 seconds (Fs_nov = 100)\nH = 10\nTheta = np.arange(30, 601)\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, N=N, H=H, Theta=Theta)\ntempogram = np.abs(X)\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2]}, figsize=(8,5))        \nplot_signal(nov, Fs_nov, ax=ax[0,0], color='k', title='Novelty function')\nax[0,1].set_axis_off()\nplot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, ax=[ax[1,0], ax[1,1]], \n                     title='Fourier tempogram', ylabel='Tempo (BPM)', colorbar=True);\nplt.tight_layout()\n\n\n\n\n\n푸리에 템포그램 \\(\\mathcal{T}^\\mathrm{F}\\)에서 알 수 있듯이 이 발췌의 지배적 템포는 \\(200\\)와 \\(300~\\mathrm{BPM}\\) 사이이다. 대략 \\(\\tau=225~\\mathrm{BPM}\\)부터 시작하여 시간이 지남에 따라 템포가 약간 증가한다. 흥미롭게도 3/4박자 내에서 세 번째 박자마다 약한 강박으로 인해 템포그램 \\(\\mathcal{T}^\\mathrm{F}\\)도 메인 템포의 \\(1/3\\) 및 \\(2/3\\)에 해당하는 더 큰 계수를 보여준다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#자기상관-autocorrelation",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#자기상관-autocorrelation",
    "title": "7.2. 템포 분석",
    "section": "자기상관 (Autocorrelation)",
    "text": "자기상관 (Autocorrelation)\n\n두 번째 주기성 추정 방법으로 푸리에 기반 접근 외에 자기상관 기반 접근에 대해 논의한다. 일반적으로 자기상관은 신호와 신호의 시간 이동 버전의 유사성을 측정하기 위한 수학적 도구이다.\n\\(x:\\mathbb{Z}\\to\\mathbb{R}\\)가 에너지가 유한한 이산시간 신호라고 하자. 실수 값 신호 \\(x\\)의 자기상관 \\(R_{xx}:\\mathbb{Z}\\to\\mathbb{R}\\)는 다음과 같이 정의된다. \\[R_{xx}(\\ell) = \\sum_{m\\in\\mathbb{Z}} x(m)x(m-\\ell)\\] 이는 time-shift 또는 lag 매개변수 \\(\\ell\\in\\mathbb{Z}\\)에 의존하는 함수를 생성한다.\n자기 상관 \\(R_{xx}(\\ell)\\)은 \\(\\ell=0\\)에서 최대이고 \\(\\ell\\)에서 대칭이다. 직관적으로, 주어진 lag에 대한 자기상관이 큰 경우, 신호에는 lag 매개변수로 구체화되는, 시간 간격으로 구분되는 반복적 패턴이 포함된다.\n다음 예에서는 period 9와 period 14의 두 가지 클릭 트랙 패턴으로 구성된 신호를 고려한다.\n자기상관은 lag \\(\\ell=0\\)(신호가 자체와 유사함)에서 사소하게 피크를 가지며, \\(\\ell=9\\)(첫 번째 period), \\(\\ell=14\\)(두 번째 period), \\(\\ ell=18\\)(첫 번째 period의 두 배), \\(\\ell=27\\)(첫 번째 period의 세 배, \\(\\ell=28\\)에서 두 번째 period의 두 배) 등이다. 특히 자기상관이 lag \\(\\ell\\)에서 피크를 갖는다면 \\(\\ell\\)의 정수배에서도 피크를 갖는다.\n\n\nN = 100\nx = np.zeros(N)\nx[np.arange(1,N,14)] = 1\nx[np.arange(3,N,9)] = 1\n\nfig, ax = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1, 1.5, 1.5]}, figsize=(6, 5))       \n\nax[0].plot(x, 'k')\nax[0].set_title('Signal $x$')\nax[0].set_xlabel('Time (samples)')\n\nr_xx = np.correlate(x, x, mode='full')\nlag_axis = np.arange(-(N-1),N)\nax[1].plot(lag_axis,r_xx, 'r')\nax[1].set_title('Autocorrelation (full mode)')\nax[1].set_xlabel(r'Lag parameter (samples)')\n\nr_xx = np.correlate(x, x, mode='full')\nlag_axis = np.arange(-(N-1),N)\nax[2].plot(lag_axis,r_xx, 'r')\nax[2].set_title('Autocorrelation (full mode)')\nax[2].set_xlabel(r'Lag parameter (samples)')\nax[2].set_xlim([0,50])\nax[2].set_ylim([0,11])\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#단기-자기상관-short-time-autocorrelation",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#단기-자기상관-short-time-autocorrelation",
    "title": "7.2. 템포 분석",
    "section": "단기 자기상관 (Short-Time Autocorrelation)",
    "text": "단기 자기상관 (Short-Time Autocorrelation)\n\n주어진 시간 매개변수 \\(n\\) 근처에서 주어진 노벨티 함수 \\(\\Delta:\\mathbb{Z}\\to\\mathbb{R}\\)를 분석하기 위해 로컬 방식으로 자기상관을 적용한다. 이를 위해 윈도우 함수 \\(w:\\mathbb{Z}\\to\\mathbb{R}\\)를 고정한다.\n\\(n\\in\\mathbb{Z}\\) 지점에 localized된 windowed 버전 \\(\\Delta_{w,n}:\\mathbb{Z}\\to\\mathbb{R}\\)는 다음과 같이 정의된다. \\[\\Delta_{w,n}(m):=\\Delta(m)w(m-n)\\] for \\(m\\in\\mathbb{Z}\\)\n단시간(short-time) 자기상관 \\(\\mathcal{A}:\\mathbb{Z}\\times\\mathbb{Z}\\to\\mathbb{R}\\)를 얻기 위해, \\(\\Delta_{w,n}\\)의 자기상관을 계산한다. \\[\\mathcal{A}(n,\\ell) := \\sum_{m\\in\\mathbb{Z}}\\Delta(m)w(m-n)\\Delta(m-\\ell) w(m-n-\\ell).\\]\n다음에서는 윈도우 함수 \\(w:[0:N-1]\\to\\mathbb{R}\\)가 유한한 길이 \\(N\\in\\mathbb{N}\\)를 가진다고 가정한다. (일반적으로 수학적 편의를 위해 \\([0:N-1]\\) 구간 밖에서 \\(w\\)를 0으로 확장할 수 있다.)\n그러면 유한한 수의 시차(time lag) 매개변수를 제외하고는 localized 노벨티 함수의 자기상관이 모두 0이 된다. 보다 정확하게는 \\(\\mathcal{A}(n,\\ell)=0\\) for \\(|\\ell|> N\\)임을 보여줄 수 있다.\n이 속성과 자기 상관의 대칭성 때문에 시차 매개변수 \\(\\ell\\in[0:N-1]\\)만 고려하면 된다. 게다가, 윈도윙(windowing) 때문에 최대 \\(N-\\ell\\)의 summands는 nonzero이다.\n윈도우 효과의 균형을 맞추기 위해 \\(\\mathcal{A}(n,\\ell)\\) 값을 윈도우 속성 및 윈도우의 \\(N-\\ell\\) 오버랩에 따라 달라지는 요인과 그것의 시간 이동 버전으로 나눌 수 있다.\n단시간 자기상관 \\(\\mathcal{A}\\)를 시각화하면 프레임 매개변수 \\(n\\in\\mathbb{Z}\\) 및 시차 매개변수 \\(\\ell\\in[0:N-1]\\)로 시간-시차(time-lag) 표현이 된다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#구현-1",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#구현-1",
    "title": "7.2. 템포 분석",
    "section": "구현",
    "text": "구현\n\n자기상관(autocorrelation) 템포그램를 계산하기 위한 구현을 살펴보자.\n노벨티 곡선 \\(\\Delta\\)의 길이는 \\(L\\in\\mathbb{N}\\)이고 샘플링 레이트는 \\(F_\\mathrm{s}^\\Delta\\)라고 가정한다. 윈도우 함수 \\(w\\)로 길이가 \\(N\\in\\mathbb{N}\\)인 직사각형 윈도우을 선택한다. 또한 hopsize 매개변수 \\(H\\in\\mathbb{N}\\)를 도입한다. 노벨티 함수가 윈도우 길이의 절반만큼 0으로 채워지는 centered view를 사용한다.\n결과적으로 \\(n=0\\)에 의해 인덱스된 \\(\\mathcal{A}\\)의 첫 번째 프레임은 물리적 시간 위치 \\(t=0~\\mathrm{sec}\\)에 해당한다. 또한 홉크기 매개변수 \\(H\\)는 템포그램의 프레임 속도를 \\(F_\\mathrm{s}^\\Delta/H\\)로 줄인다.\n이 알고리즘은 프레임 인덱스 \\(n\\)를 반복하고 단일 자기상관을 사용하여 모든 \\(\\ell\\in[0:N-1]\\)에 대해 \\(\\mathcal{A}(n,\\ell)\\)를 계산한다. 예시로 쇼스타코비치의 왈츠 2번 녹음에서 발췌한 부분을 본다. 또한 \\(F_\\mathrm{s}^\\Delta = 100~\\mathrm{Hz}\\) 및 \\(N=300\\) 매개변수를 사용한다.\n\n\ndef compute_autocorrelation_local(x, Fs, N, H, norm_sum=True):\n    \"\"\"Compute local autocorrelation [FMP, Section 6.2.3]\n\n    Args:\n        x (np.ndarray): Input signal\n        Fs (scalar): Sampling rate\n        N (int): Window length\n        H (int): Hop size\n        norm_sum (bool): Normalizes by the number of summands in local autocorrelation (Default value = True)\n\n    Returns:\n        A (np.ndarray): Time-lag representation\n        T_coef (np.ndarray): Time axis (seconds)\n        F_coef_lag (np.ndarray): Lag axis\n    \"\"\"\n    # L = len(x)\n    L_left = round(N / 2)\n    L_right = L_left\n    x_pad = np.concatenate((np.zeros(L_left), x, np.zeros(L_right)))\n    L_pad = len(x_pad)\n    M = int(np.floor(L_pad - N) / H) + 1\n    A = np.zeros((N, M))\n    win = np.ones(N)\n    if norm_sum is True:\n        lag_summand_num = np.arange(N, 0, -1)\n    for n in range(M):\n        t_0 = n * H\n        t_1 = t_0 + N\n        x_local = win * x_pad[t_0:t_1]\n        r_xx = np.correlate(x_local, x_local, mode='full')\n        r_xx = r_xx[N-1:]\n        if norm_sum is True:\n            r_xx = r_xx / lag_summand_num\n        A[:, n] = r_xx\n    Fs_A = Fs / H\n    T_coef = np.arange(A.shape[1]) / Fs_A\n    F_coef_lag = np.arange(N) / Fs\n    return A, T_coef, F_coef_lag\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\n\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=1)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\n#Autocorrelation parameters\nN = 300 #corresponding to 3 seconds (Fs_nov = 100 Hz)\nH = 10\n\nA, T_coef, F_coef_lag = compute_autocorrelation_local(nov, Fs_nov, N, H, norm_sum=False)\nA_norm, T_coef, F_coef_lag = compute_autocorrelation_local(nov, Fs_nov, N, H)\n\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2, 2]}, figsize=(8,7))        \n\nplot_signal(nov, Fs_nov, ax=ax[0,0], color='k', title='Novelty function')\nax[0,1].set_axis_off()\n\nplot_matrix(A, ax=[ax[1,0], ax[1,1]], T_coef=T_coef, F_coef=F_coef_lag,\n                     title=r'Time-lag representation $\\mathcal{A}$', ylabel='Lag (seconds)', colorbar=True);\n\nplot_matrix(A_norm, ax=[ax[2,0], ax[2,1]], T_coef=T_coef, F_coef=F_coef_lag,\n                     title=r'Time-lag representation $\\mathcal{A}$ (with sum normalization)', \n                     ylabel='Lag (seconds)', colorbar=True);\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#해석",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#해석",
    "title": "7.2. 템포 분석",
    "section": "해석",
    "text": "해석\n\n이 예에서 사용된 윈도우 \\(w\\)는 원본 오디오 녹음의 \\(3~\\mathrm{sec}\\)에 해당하는 길이를 갖는 직사각형 윈도우이다.\n먼저 시간 인스턴스 \\(t=2~\\mathrm{sec}\\)에 해당하는 시간 인덱스 \\(n\\)을 보자.\n\n\\(\\mathcal{A}(n,\\ell)\\)를 계산하려면 \\(0.5~\\mathrm{sec}\\)와 \\(3.5~\\mathrm{sec}\\) 사이의 novelty function \\(\\Delta\\)의 섹션만 고려한다.\nShostakovich 녹음의 템포는 이 섹션에서 대략 \\(230~\\mathrm{BPM}\\)이다. 즉, 두 개의 후속 비트 사이의 간격의 지속시간은 대략 \\(s=0.26~\\mathrm{sec}\\)이다.\n\n다음으로 \\(s=0.26~\\mathrm{sec}\\)의 시간 이동에 해당하는 시차 매개변수 \\(\\ell\\)을 살펴보자.\n\n이 섹션의 노벨티 함수는 자신의 시간 이동된 버전과 잘 연관된다. 섹션의 피크는 한 비트 기간만큼 이동된 섹션의 피크에 떨어진다. 섹션을 2, 3 또는 그 이상의 비트 주기로 이동할 때도 마찬가지이다. 예를 들어 \\(s=0.78~\\mathrm{sec}\\)(3박자 기간)의 경우이다. 이 주기는 measure level의 템포인 \\(77~\\mathrm{BPM}\\)의 템포에 해당한다.\n반대로 비트 주기 \\(s=0.13~\\mathrm{sec}\\)의 절반에 해당하는 lag \\(\\ell\\)을 사용하면(double tempo \\(461~\\mathrm{BPM}\\)), 섹션의 피크와 이동된 부분의 피크는 서로를 맞지 않으므로 계수 \\(\\mathcal{A}(n,\\ell)\\)는 0에 가깝다.\n\n\n\ndef plot_signal_local_lag(x, t_x, local_lag, t_local_lag, lag, xlim=None, figsize=(8, 1.5), title=''):\n    \"\"\"Visualize signal and local lag [FMP, Figure 6.14]\n\n    Args:\n        x: Signal\n        t_x: Time axis of x (given in seconds)\n        local_lag: Local lag\n        t_local_lag: Time axis of kernel (given in seconds)\n        lag: Lag (given in seconds)\n        xlim: Limits for x-axis (Default value = None)\n        figsize: Figure size (Default value = (8, 1.5))\n        title: Title of figure (Default value = '')\n\n    Returns:\n        fig: Matplotlib figure handle\n    \"\"\"\n    if xlim is None:\n        xlim = [t_x[0], t_x[-1]]\n    fig = plt.figure(figsize=figsize)\n    plt.plot(t_x, x, 'k:', linewidth=0.5)\n    plt.plot(t_local_lag, local_lag, 'k', linewidth=3.0)\n    plt.plot(t_local_lag+lag, local_lag, 'r', linewidth=2)\n    plt.title(title)\n    plt.ylim([0, 1.1 * np.max(x)])\n    plt.xlim(xlim)\n    plt.tight_layout()\n    return fig\n\n\ntime_sec = np.array([2, 2, 2])\nlag_sec = np.array([0.13, 0.26, 0.78])\ncoef_n = (time_sec * Fs_nov/H).astype(int)\ncoef_k = (lag_sec * Fs_nov).astype(int)\n\nfig, ax, im = plot_matrix(A, T_coef=T_coef, F_coef=F_coef_lag, figsize=(9,3),\n                     title=r'Time-lag representation $\\mathcal{A}$', ylabel='Lag (seconds)', colorbar=True);\nax[0].plot(T_coef[coef_n], F_coef_lag[coef_k],'ro')\n\nL = len(nov)\nL_left = round(N/2)\nL_right = L_left\nnov_pad = np.concatenate( ( np.zeros(L_left), nov, np.zeros(L_right) ) )\nL_pad = len(nov_pad)\nwin = np.ones(N)\n\ntime_sec = np.array([2, 2, 2, 2])\nlag_sec = np.array([0, 0.13, 0.26, 0.78])\nt_nov = np.arange(nov.shape[0])/Fs_nov\n\nfor i in range(len(time_sec)):\n    t_0 = time_sec[i] * Fs_nov\n    t_1 = t_0 + N\n    nov_local = win*nov_pad[t_0:t_1]\n    t_nov_local = (np.arange(t_0,t_1) - L_left)/Fs_nov\n    lag = lag_sec[i]\n    title=r'Shifted novelty function (t = %0.2f sec, $\\ell$ = %0.2f sec)'%(time_sec[i], lag)\n    fig = plot_signal_local_lag(nov, t_nov, nov_local, t_nov_local, lag, title=title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#자기상관-템포그램",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#자기상관-템포그램",
    "title": "7.2. 템포 분석",
    "section": "자기상관 템포그램",
    "text": "자기상관 템포그램\n\n시간-시차(time-lag) 표현에서 시간-템포(time-tempo) 표현을 얻으려면 lag 매개변수를 템포 매개변수로 변환해야 한다. 이를 위해 노벨티 함수의 프레임 속도 또는 시간 분해능(time resolution)이 필요하다.\n각 시간 프레임이 \\(r\\)초에 해당하고 \\(\\ell\\)의 time lag(프레임으로 제공됨)이 \\(\\ell\\cdot r\\)초에 해당한다고 가정한다. \\(\\ell\\cdot r\\)초의 이동은 \\(1/(\\ell\\cdot r)~\\mathrm{Hz}\\)의 속도에 해당하므로 아래의 템포를 얻는다. \\[\\tau = \\frac{60}{r\\cdot \\ell}~\\mathrm{BPM}윈도우\\]\n예를 들어 노벨티 함수 \\(\\Delta\\)의 feature rate이 \\(F_\\mathrm{s}^\\Delta = 100~\\mathrm{Hz}\\)이고 \\(r=0.01~\\mathrm{sec}\\)라고 가정하자. 이 경우 lag 매개변수 \\(\\ell=10\\)는 \\(600~\\mathrm{BPM}\\)에 해당하고 \\(\\ell=200\\)는 \\(30~\\mathrm{BPM}\\)에 해당한다. 의미 있는 범위의 템포 값을 얻으려면 실제로 최소 lag 매개변수 \\(\\ell_\\mathrm{max}\\)를 사용하여 최소 템포를 지정하고 최소 lag 매개변수 \\(\\ell_\\mathrm{min}\\)를 사용하여 최대 템포를 지정한다.\n윈도우 길이 \\(N\\)을 사용하는 경우 다음이 필요하다. \\[1\\leq \\ell_\\mathrm{min} \\leq \\ell_\\mathrm{max} \\leq N-1\\]\nlag에서 BPM으로의 변환을 기반으로 lag 축은 tempo 축으로 해석될 수 있다.\n다음과 같은 설정을 통해 자기상관 템포그램(autocorrelation tempogram) \\(\\mathcal{T}^\\mathrm{A}\\)를 정의할 수 있다. \\[\\mathcal{T}^\\mathrm{A}(n,\\tau) := \\mathcal{A}(n,\\ell)\\] for each tempo \\(\\tau=60/(r\\cdot\\ell)\\) with \\(\\ell\\in[ \\ell_\\mathrm{min}: \\ell_\\mathrm{max}]\\)\n이 경우 tempo 값은 선형 샘플링된 lag 값과 역수이므로 tempo 축은 비선형 방식으로 샘플링된다.\n푸리에 템포그램 \\(\\mathcal{T}^\\mathrm{F}\\)와 동일한 템포 집합 \\(\\Theta\\)에서 정의된 템포그램 \\(\\mathcal{T}^\\mathrm{A}: \\mathbb{Z} \\times \\Theta \\to \\mathbb{R}_{\\geq 0}\\)을 얻으려면 tempo 영역에 적용된 표준 리샘플링(standard resampling) 및 보간(interpolation) 기술을 사용할 수 있다.\n다음 코드셀에 lag에서 tempo로의 변환과 선형 tempo 축을 얻기 위한 보간이 설명되어있다.\n\n\ndef compute_tempogram_autocorr(x, Fs, N, H, norm_sum=False, Theta=np.arange(30, 601)):\n    \"\"\"Compute autocorrelation-based tempogram\n\n    Args:\n        x (np.ndarray): Input signal\n        Fs (scalar): Sampling rate\n        N (int): Window length\n        H (int): Hop size\n        norm_sum (bool): Normalizes by the number of summands in local autocorrelation (Default value = False)\n        Theta (np.ndarray): Set of tempi (given in BPM) (Default value = np.arange(30, 601))\n\n    Returns:\n        tempogram (np.ndarray): Tempogram tempogram\n        T_coef (np.ndarray): Time axis T_coef (seconds)\n        F_coef_BPM (np.ndarray): Tempo axis F_coef_BPM (BPM)\n        A_cut (np.ndarray): Time-lag representation A_cut (cut according to Theta)\n        F_coef_lag_cut (np.ndarray): Lag axis F_coef_lag_cut\n    \"\"\"\n    tempo_min = Theta[0]\n    tempo_max = Theta[-1]\n    lag_min = int(np.ceil(Fs * 60 / tempo_max))\n    lag_max = int(np.ceil(Fs * 60 / tempo_min))\n    A, T_coef, F_coef_lag = compute_autocorrelation_local(x, Fs, N, H, norm_sum=norm_sum)\n    A_cut = A[lag_min:lag_max+1, :]\n    F_coef_lag_cut = F_coef_lag[lag_min:lag_max+1]\n    F_coef_BPM_cut = 60 / F_coef_lag_cut\n    F_coef_BPM = Theta\n    tempogram = interp1d(F_coef_BPM_cut, A_cut, kind='linear',\n                         axis=0, fill_value='extrapolate')(F_coef_BPM)\n    return tempogram, T_coef, F_coef_BPM, A_cut, F_coef_lag_cut\n\n\nTheta = np.arange(30, 601)\ntempogram, T_coef, F_coef, A, F_coef_lag = compute_tempogram_autocorr(nov, \n                            Fs_nov, N, H, norm_sum=False, Theta=Theta)\n\nfig, ax = plt.subplots(1, 3, gridspec_kw={'width_ratios': [1, 1, 1]}, figsize=(10,3)) \n\nplot_matrix(A, T_coef=T_coef, F_coef=F_coef_lag, ax=[ax[0]],\n                     title='Lag axis', ylabel='Lag (seconds)', colorbar=True);\nlag_yticks = np.array([F_coef_lag[0], 0.5, 1.0, 1.5, F_coef_lag[-1]])\nax[0].set_yticks(lag_yticks)\n\nplot_matrix(A, T_coef=T_coef, F_coef=F_coef_lag, ax=[ax[1]],\n                     title='Lag converted to BPM', ylabel='Beat (BPM)', colorbar=True);\nax[1].set_yticks(lag_yticks)\nax[1].set_yticklabels(60/lag_yticks)\n\nplot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef, ax=[ax[2]],\n                     title='Linear BPM axis', ylabel='Beat (BPM)', colorbar=True);\nax[2].set_yticks([F_coef[0], 120, 240, 360, 480, F_coef[-1]]);\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-하모닉과-서브하모닉",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-하모닉과-서브하모닉",
    "title": "7.2. 템포 분석",
    "section": "템포 하모닉과 서브하모닉",
    "text": "템포 하모닉과 서브하모닉\n\n다음에서는 템포 증가(\\(20~\\mathrm{sec}\\)에 걸쳐 \\(170\\)에서 \\(200~\\mathrm{BPM}\\)로 상승)하는 클릭(click) 트랙을 고려하여 하모닉(=고조파) 및 서브하모닉을 설명한다. 이 예는 자기상관 템포그램이 템포 하모닉을 억제하면서 템포 서브하모닉을 나타내는 것을 보여준다.\n이 이유는 노벨티 함수의 로컬 섹션과 \\(\\ell\\) 샘플 이동된 섹션과의 높은 상관관계가 정수 \\(k\\in\\mathbb{N}\\)에 대해 \\(k\\cdot\\ell\\) lag만큼 이동된 섹션과의 높은 상관관계 또한 의미하기 때문이다.\n\\(\\ell\\)이 tempo \\(\\tau\\)에 해당한다고 가정하면, lag \\(k\\cdot\\ell\\)은 서브하모닉 \\(\\tau/k\\)에 해당한다. 이는 템포 하모닉스를 강조하지만 템포 서브하모닉을 억제하는 푸리에 템포그램과 대조된다.\n\n\nfn_wav = \"FMP_C6_F11_ClickTrack-BPM170-200.wav\"\nAudio(path_data+ fn_wav)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=1)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\n#Autocorrelation parameters\nN = 500 #corresponding to 5 seconds (Fs_nov = 100 Hz)\nH = 10\nTheta = np.arange(30, 601)\ntempogram_A, T_coef, F_coef, A, F_coef_lag = compute_tempogram_autocorr(nov, Fs_nov, N, H, \n                                                 norm_sum=False, Theta=Theta)\n\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2, 2]}, figsize=(8,7))        \nplot_signal(nov, Fs_nov, ax=ax[0,0], color='k', title='Novelty function')\nax[0,1].set_axis_off()\nplot_matrix(tempogram_A, T_coef=T_coef, F_coef=F_coef, ax=[ax[1,0], ax[1,1]], \n                     title='Autocorrelation tempogram', ylabel='Tempo (BPM)', colorbar=True);\n\n\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, N=N, H=H, Theta=Theta)\ntempogram_F = np.abs(X)\n\nplot_matrix(tempogram_F, T_coef=T_coef, F_coef=F_coef_BPM, ax=[ax[2,0], ax[2,1]], \n                     title='Fourier tempogram', ylabel='Tempo (BPM)', colorbar=True)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#정의-1",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#정의-1",
    "title": "7.2. 템포 분석",
    "section": "정의",
    "text": "정의\n\n연속의 경우 (continuous)\n\n피치의 맥락에서 고조파의 존재가 있는 것처럼 다양한 펄스 레벨이 있을 수 있다. 고조파의 영향을 줄이기 위해 크로마 기반 오디오 특징의 개념을 소개한 적 있다. 한 옥타브 또는 여러 옥타브 차이가 나는 피치를 식별하여 음색의 변화에 강건한 화성 정보를 캡처하는 순환(cyclic) 중간-레벨 표현을 얻었다. 크로마 특징의 개념에서 영감을 받아 이제 순환 템포그램(cyclic tempogram)의 개념을 소개한다.\n기본 개념은 2의 제곱만큼 다른 템포를 식별하여 템포 등가 클래스를 형성하는 것이다. 더 정확하게는 \\(\\tau_1\\) 및 \\(\\tau_2\\) 두 템포가 어떤 \\(k\\in \\mathbb{Z}\\)에 대해 \\(\\tau_1 = 2^{k} \\tau_2\\)로 관련되어 있다면 이를 옥타브 동등(equivalent) 이라고 한다.\n템포 매개변수 \\(\\tau\\)의 경우, 결과 템포 등가 클래스를 \\([\\tau]\\)로 표시한다. 예를 들어 \\(\\tau=120\\)의 경우 \\([\\tau]=\\{\\ldots,30,60,120,240,480\\ldots\\}\\)를 얻는다. 템포그램 표현 \\(\\mathcal{T}:\\mathbb{Z}\\times \\mathbb{R}_{>0} \\to \\mathbb{R}_{\\geq 0}\\)가 주어지면 다음에 의해 순환 템포그램을 정의한다. \\[\\mathcal{C}(n,[\\tau]) := \\sum_{\\lambda\\in[\\tau]} \\mathcal{T}(n,\\lambda)\\]\n템포 등가 클래스는 위상적으로 원에 해당한다. 참조 템포 \\(\\tau_0\\)을 고정하면, 순환 템포그램은 \\(\\mathcal{C}_{\\tau_0}:\\mathbb{Z}\\times \\mathbb{R}_{>0} \\to \\mathbb{R}_{\\geq 0}\\) 매핑으로 나타나며 다음과 같이 정의된다. \\[\\mathcal{C}_{\\tau_0}(n,s):= \\mathcal{C}(n,[s\\cdot\\tau_0])\\] for \\(n\\in \\mathbb{Z}\\) and scaling parameter(스케일링 매개변수) \\(s\\in\\mathbb{R}_{>0}\\)\n또한 다음을 참고할 수 있다. \\[\\mathcal{C}_{\\tau_0}(n,s)=\\mathcal{C}_{\\tau_0}(n,2^ks)\\] for \\(k\\in\\mathbb{Z}\\)\n특히 \\(\\mathcal{C}_{\\tau_0}\\)는 \\(s\\in[1,2)\\) 값에 의해 완전히 결정된다.\n\n\n\n이산의 경우 (discrete)\n\n위는 템포 매개변수의 공간이 연속적이라고 가정했다. 하지만 실제로는 한정된 수의 매개변수 \\(s\\in[1,2)\\)에 대해서만 순환 템포그램 \\(\\mathcal{C}_{\\tau_0}\\)를 계산할 수 있다.\n\\(\\mathcal{C}_{\\tau_0}(n,s)\\) 값을 계산하려면 템포 매개변수 \\(\\tau\\in \\{s\\cdot\\tau_0\\cdot2^k\\,\\mid\\,k\\in\\mathbb{Z}\\}\\)에 대해 \\(\\mathcal{T}(n,\\tau)\\) 값을 더해야 한다. 즉, 필요한 템포 값은 템포 축에서 기하급수적으로 배치된다. 따라서 로그 주파수 축을 사용하는 크로마 특징과 마찬가지로 순환 템포그램을 계산하기 위해서는 로그-템포 축이 필요하다.\n이를 위해 각 템포 옥타브에 주어진 숫자 \\(M\\in\\mathbb{N}\\)에 대한 \\(M\\) 템포 빈(bins) 이 포함되도록 템포 범위를 로그 방식으로 샘플링한다. 그런 다음 이전과 같이 서로 다른 옥타브의 해당 값을 더함으로써 이산 순환 템포그램 \\(\\mathcal{C}_{\\tau_0}\\)를 얻는다. 이는 모든 시간 프레임 \\(n\\in\\mathbb{Z}\\)에 대해 \\(M\\) 차원의 특징 벡터를 산출하며, 순환 템포 축은 \\(M\\) 위치에서 샘플링된다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#순환-푸리에-템포그램",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#순환-푸리에-템포그램",
    "title": "7.2. 템포 분석",
    "section": "순환 푸리에 템포그램",
    "text": "순환 푸리에 템포그램\n\n푸리에 템포그램으로 시작하자. 순환 버전은 \\(\\mathcal{C}^\\mathrm{F}_{\\tau_0}\\)로 표시되는 순환 푸리에 템포그램이라고 한다. 다음에서는 템포가 증가하는 클릭 트랙(\\(110\\)에서 \\(130~\\mathrm{BPM}\\))을 예로 사용한다.\n\n\nfn_wav = \"FMP_C6_Audio_ClickTrack-BPM110-130.wav\"\nAudio(path_data+fn_wav)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n세 가지 단계로 나누어 볼 수 있다.\n\n먼저 \\(\\Theta=[30:600]\\)에 해당하는 선형 템포 축으로 푸리에 템포그램을 계산한다.\n그런 다음 선형 템포 축을 로그 템포 축으로 변환한다. 이 단계에서는 참조 템포 \\(\\tau_0=30~\\mathrm{BPM}\\)를 사용하고 각각 \\(M=40\\) 템포 빈을 포함하는 4개의 템포 옥타브를 커버한다.\n마지막으로 템포 옥타브를 식별하여 템포 축을 주기적으로 접는다(fold). 이는 \\(M\\) 차원의 순환 템포그램을 생성한다.\n\n\n\ndef compute_cyclic_tempogram(tempogram, F_coef_BPM, tempo_ref=30,\n                             octave_bin=40, octave_num=4):\n    \"\"\"Compute cyclic tempogram\n\n    Args:\n        tempogram (np.ndarray): Input tempogram\n        F_coef_BPM (np.ndarray): Tempo axis (BPM)\n        tempo_ref (float): Reference tempo (BPM) (Default value = 30)\n        octave_bin (int): Number of bins per tempo octave (Default value = 40)\n        octave_num (int): Number of tempo octaves to be considered (Default value = 4)\n\n    Returns:\n        tempogram_cyclic (np.ndarray): Cyclic tempogram tempogram_cyclic\n        F_coef_scale (np.ndarray): Tempo axis with regard to scaling parameter\n        tempogram_log (np.ndarray): Tempogram with logarithmic tempo axis\n        F_coef_BPM_log (np.ndarray): Logarithmic tempo axis (BPM)\n    \"\"\"\n    F_coef_BPM_log = tempo_ref * np.power(2, np.arange(0, octave_num*octave_bin)/octave_bin)\n    F_coef_scale = np.power(2, np.arange(0, octave_bin)/octave_bin)\n    tempogram_log = interp1d(F_coef_BPM, tempogram, kind='linear', axis=0, fill_value='extrapolate')(F_coef_BPM_log)\n    K = len(F_coef_BPM_log)\n    tempogram_cyclic = np.zeros((octave_bin, tempogram.shape[1]))\n    for m in np.arange(octave_bin):\n        tempogram_cyclic[m, :] = np.mean(tempogram_log[m:K:octave_bin, :], axis=0)\n    return tempogram_cyclic, F_coef_scale, tempogram_log, F_coef_BPM_log\n\n\ndef set_yticks_tempogram_cyclic(ax, octave_bin, F_coef_scale, num_tick=5):\n    \"\"\"Set yticks with regard to scaling parmater\n\n    Args:\n        ax (mpl.axes.Axes): Figure axis\n        octave_bin (int): Number of bins per tempo octave\n        F_coef_scale (np.ndarra): Tempo axis with regard to scaling parameter\n        num_tick (int): Number of yticks (Default value = 5)\n    \"\"\"\n    yticks = np.arange(0, octave_bin, octave_bin // num_tick)\n    ax.set_yticks(yticks)\n    ax.set_yticklabels(F_coef_scale[yticks].astype((np.unicode_, 4)))\n\n\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\n\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, \n                                                            N=500, H=10, \n                                                            Theta=np.arange(30, 601))\ntempogram = np.abs(X)\ntempo_ref = 30\noctave_bin = 40\noctave_num = 4\noutput = compute_cyclic_tempogram(tempogram, F_coef_BPM, \n              tempo_ref=tempo_ref, octave_bin=octave_bin, octave_num=octave_num)\ntempogram_cyclic = output[0]\nF_coef_scale = output[1]\ntempogram_log = output[2]\nF_coef_BPM_log = output[3]\n\n\nfig, ax = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1.5, 1.5, 1]}, figsize=(7, 8))       \n\n# Fourier tempogram\nim_fig, im_ax, im = plot_matrix(tempogram, ax=[ax[0]],T_coef=T_coef, F_coef=F_coef_BPM,\n                                         title='Fourier tempogram', \n                                         ylabel='Tempo (BPM)', colorbar=True);\nax[0].set_yticks([F_coef_BPM[0],100, 200, 300, 400, 500, F_coef_BPM[-1]]);\n\n# Fourier tempogram with log tempo axis\nim_fig, im_ax, im = plot_matrix(tempogram_log, ax=[ax[1]], T_coef=T_coef,\n                                         title='Fourier tempogram with log-tempo axis', \n                                         ylabel='Tempo (BPM)', colorbar=True);\nyticks = np.arange(octave_num) * octave_bin\nax[1].set_yticks(yticks)\nax[1].set_yticklabels(F_coef_BPM_log[yticks].astype(int));\n\n# Cyclic Fourier tempogram\nim_fig, im_ax, im = plot_matrix(tempogram_cyclic, ax=[ax[2]], T_coef=T_coef,\n                                         title='Cyclic Fourier tempogram', \n                                         ylabel='Scaling', colorbar=True);\nset_yticks_tempogram_cyclic(ax[2], octave_bin, F_coef_scale, num_tick=5)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#순환-자기상관-템포그램",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#순환-자기상관-템포그램",
    "title": "7.2. 템포 분석",
    "section": "순환 자기상관 템포그램",
    "text": "순환 자기상관 템포그램\n\n비슷하게 자기상관 템포그램으로 시작하여, 순환 버전으로 \\(\\mathcal{C}^\\mathrm{A}_{\\tau_0}\\)로 표기되는 순환 자기상관 템포그램(cyclic autocorrelation tempogram) 을 얻는다.\n참조 템포 \\(\\tau_0=30~\\mathrm{BPM}\\) 및 클릭 트랙을 예로 사용하여 원래 자기 상관 템포그램, 로그 템포 축이 있는 템포그램 및 옥타브당 \\(M=40\\) 템포 빈을 사용하는 순환 템포그램의 그림을 보자.\n\n\nN = 500 \nH = 10\nTheta = np.arange(30, 601)\noutput = compute_tempogram_autocorr(nov, Fs_nov, N=N, H=H, \n                                              norm_sum=False, Theta=np.arange(30, 601))\ntempogram = output[0]\nT_coef = output[1]\nF_coef_BPM = output[2]\n\ntempo_ref = 30\noctave_bin = 40\noctave_num = 4\noutput = compute_cyclic_tempogram(tempogram, F_coef_BPM, tempo_ref=tempo_ref, \n                                  octave_bin=octave_bin, octave_num=octave_num)\ntempogram_cyclic = output[0]\nF_coef_scale = output[1]\ntempogram_log = output[2]\nF_coef_BPM_log = output[3]\n\n\nfig, ax = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1.5, 1.5, 1]}, figsize=(7, 8))       \n\n# Autocorrelation tempogram\nim_fig, im_ax, im = plot_matrix(tempogram, ax=[ax[0]], T_coef=T_coef, \n                                         F_coef=F_coef_BPM, \n                                         figsize=(6,3), ylabel='Tempo (BPM)', colorbar=True,\n                                         title='Autocorrelation tempogram');\nax[0].set_yticks([Theta[0],100, 200, 300, 400, 500, Theta[-1]]);\n\n# Autocorrelation tempogram with log tempo axis\nim_fig, im_ax, im = plot_matrix(tempogram_log, ax=[ax[1]], T_coef=T_coef, \n                                         figsize=(6,3), ylabel='Tempo (BPM)', colorbar=True,\n                                         title='Autocorrelation tempogram with log-tempo axis');\nyticks = np.arange(octave_num) * octave_bin\nax[1].set_yticks(yticks)\nax[1].set_yticklabels(F_coef_BPM_log[yticks].astype(int));\n\n# Cyclic autocorrelation tempogram\nim_fig, im_ax, im = plot_matrix(tempogram_cyclic, ax=[ax[2]], T_coef=T_coef, \n                                         figsize=(6,2), ylabel='Scaling', colorbar=True,\n                                         title='Cyclic autocorrelation tempogram', );\nset_yticks_tempogram_cyclic(ax[2], octave_bin, F_coef_scale, num_tick=5)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-하모닉과-서브하모닉-1",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-하모닉과-서브하모닉-1",
    "title": "7.2. 템포 분석",
    "section": "템포 하모닉과 서브하모닉",
    "text": "템포 하모닉과 서브하모닉\n\n이전에 논의한 것처럼 푸리에 템포그램은 템포 하모닉(=고조파)를 강조하고 자기상관 템포그램은 템포 서브하모닉을 강조한다. 다음 그림에 설명된 대로 이러한 속성은 순환 버전의 템포그램에도 반영된다.\n클릭 트랙의 순환 푸리에 템포그램에서 템포 우세(dominant) 는 시간 \\(t=0\\)에서 \\(s=1.33\\)로 시작하는 약하게 증가하는 선으로 표시된다. 순환 자기상관 템포그램에서 템포 우세는 시간 \\(t=0\\)에서 \\(s=1.2\\)로 시작하는 약한 증가 선으로 나타난다.\n또한 다음 그림은 작은 템포 해상도(resolution)(\\(M=15\\) 템포 빈)를 사용하는 버전뿐만 아니라 열 방향으로 정규화된 버전도 보여준다.\n\n\ndef plot_tempogram_Fourier_autocor(tempogram_F, tempogram_A, T_coef, F_coef_BPM, \n                                   octave_bin, title_F, title_A, norm=None):\n    \"\"\"Visualize Fourier-based and autocorrelation-based tempogram\n    \"\"\"\n    fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1,1]}, figsize=(12, 1.5))       \n\n    output = compute_cyclic_tempogram(tempogram_F, F_coef_BPM, octave_bin=octave_bin)\n    tempogram_cyclic_F = output[0]\n    F_coef_scale = output[1]\n    if norm is not None:\n        tempogram_cyclic_F = normalize_feature_sequence(tempogram_cyclic_F, \n                                                                  norm=norm)\n    plot_matrix(tempogram_cyclic_F, T_coef=T_coef, ax=[ax[0]], \n                         title=title_F, ylabel='Scaling', colorbar=True);\n    set_yticks_tempogram_cyclic(ax[0], octave_bin, F_coef_scale, num_tick=5)\n\n    output = compute_cyclic_tempogram(tempogram_A, F_coef_BPM, octave_bin=octave_bin)\n    tempogram_cyclic_A  = output[0]\n    F_coef_scale = output[1]\n    if norm is not None:\n        tempogram_cyclic_A = normalize_feature_sequence(tempogram_cyclic_A, \n                                                                  norm=norm)    \n    plot_matrix(tempogram_cyclic_A, T_coef=T_coef, ax=[ax[1]], \n                         title=title_A, ylabel='Scaling', colorbar=True);\n    set_yticks_tempogram_cyclic(ax[1], octave_bin, F_coef_scale, num_tick=5)\n\n\nN = 500 \nH = 10\nTheta = np.arange(30, 601)\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, N=N, H=H, \n                                                            Theta=Theta)\ntempogram_F = np.abs(X)\noutput = compute_tempogram_autocorr(nov, Fs_nov, N=N, H=H, \n                                    Theta=Theta, norm_sum=False)\ntempogram_A = output[0]\n\noctave_bin=40\ntitle_F = r'Fourier ($M=%d$)'%octave_bin\ntitle_A = r'Autocorrelation ($M=%d$)'%octave_bin\nplot_tempogram_Fourier_autocor(tempogram_F, tempogram_A, T_coef, F_coef_BPM, \n                               octave_bin, title_F, title_A)\n\noctave_bin=40\ntitle_F = r'Fourier ($M=%d$, max-normalized)'%octave_bin\ntitle_A = r'Autocorrelation ($M=%d$, max-normalized)'%octave_bin\nplot_tempogram_Fourier_autocor(tempogram_F, tempogram_A, T_coef, F_coef_BPM, \n                               octave_bin, title_F, title_A, norm='max')\n\noctave_bin=15\ntitle_F = r'Fourier ($M=%d$, max-normalized)'%octave_bin\ntitle_A = r'Autocorrelation ($M=%d$, max-normalized)'%octave_bin\nplot_tempogram_Fourier_autocor(tempogram_F, tempogram_A, T_coef, F_coef_BPM, \n                               octave_bin, title_F, title_A, norm='max')"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-피쳐-tempo-features",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#템포-피쳐-tempo-features",
    "title": "7.2. 템포 분석",
    "section": "템포 피쳐 (Tempo features)",
    "text": "템포 피쳐 (Tempo features)\n\n순환 템포그램 표현은 하모니 기반(harmony-based) 크로마그램 표현의 템포 기반(tempo-based) 대응이다.\n표준 템포그램과 비교하여 순환 버전은 다양한 펄스 레벨로 인해 발생하는 모호성에 더 강하다. 또한 순환 템포그램을 주기적으로 이동하여 템포의 변화를 시뮬레이션할 수 있다. 이는 피치 변조를 시뮬레이션하기 위해 주기적으로 이동할 수 있는 크로마그램의 속성과 유사하다.\n한 가지 추가 이점으로, 이산 순환 템포그램의 저차원 버전도 여전히 기본 음악 신호의 중요한 로컬 템포 정보를 담고 있다는 것이다.\n템포 기반 오디오 특징의 잠재력을 설명하기 위해 음악 구조 분석 작업을 고려해 보자.\n노벨티(novelty) 기반, 반복(repetition) 기반 및 동질성(homogeneity) 기반 접근 방식을 포함하여 음악 신호를 세분화하기 위한 다양한 전략을 고려했다. 동질성 기반은 음악 신호를 특정 음악 속성과 관련하여 동질적인 세그먼트로 분할하는 것이다. 이러한 맥락에서 음색, 하모니, 템포와 같은 다양한 음악적 속성을 포착하는 다양한 특징 표현을 고려했다.\n이제 순환 템포그램이 템포 기반 분할에 어떻게 유용한지 알아보자.\n\n\n예시: Brahms\n\nBrahms의 헝가리 무곡을 예로 보자. 음악적 구조는 \\(A_1A_2B_1B_2CA_3B_3B_4D\\)아며, 서로 다른 파트는 서로 다른 템포로 연주된다. 또한 일부 파트 내에서도 급격한 템포 변화가 많이 있다.\n다음 그림에서는 순환 자기상관과 푸리에 템포그램 표현을 보여준다. 이러한 표현은 정확한 템포를 나타내지는 않지만 동질성 기반 구조 분석에 유용할 수 있는 템포 관련 정보를 캡처한다.\nBrahms 예에서 순환 템포그램은 낮은 차원의 템포 표현을 기반으로 음악적으로 의미 있는 세그먼트를 생성한다. 이러한 세그먼트는 MFCC 또는 크로마 특징을 사용하여 복구할 수 없다. 동질성 가정은 음색이나 하모니와 관련해서는 유지되지 않기 때문이다.\n\n\n# Annotation\nfn_ann = 'FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.csv'\nann, color_ann = read_structure_annotation(path_data+fn_ann,\n                                                     Fs=1, remove_digits=False)\n\n# Audio file \nfn_wav = 'FMP_C4_Audio_Brahms_HungarianDances-05_Ormandy.wav'\n\nx, Fs = librosa.load(path_data+fn_wav)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\noctave_bin = 15\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, N=500, H=50, \n                                                            Theta=np.arange(30, 601))\ntempogram_F = np.abs(X)\n\ntempogram_A, T_coef, F_coef_BPM, _, _ = compute_tempogram_autocorr(nov, Fs_nov,\n                                                                             N=500, H=50,\n                                                                             norm_sum=False,\n                                                                             Theta=np.arange(30, 601))\n\n\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 2, 1]}, figsize=(8, 5))       \n\noutput = compute_cyclic_tempogram(tempogram_F, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_F = output[0]\nF_coef_scale = output[1]\n\ntempogram_cyclic_F = normalize_feature_sequence(tempogram_cyclic_F, norm='max')\nplot_matrix(tempogram_cyclic_F, T_coef=T_coef, ax=[ax[0,0], ax[0,1]], clim=[0,1],\n                     title='Fourier ($M=15$, max-normalized)', \n                     ylabel='Scaling', colorbar=True);\nset_yticks_tempogram_cyclic(ax[0,0], octave_bin, F_coef_scale, num_tick=5)\n\noutput = compute_cyclic_tempogram(tempogram_A, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_A = output[0]\nF_coef_scale = output[1]\n\ntempogram_cyclic_A = normalize_feature_sequence(tempogram_cyclic_A, norm='max')\nplot_matrix(tempogram_cyclic_A, T_coef=T_coef, ax=[ax[1,0], ax[1,1]], clim=[0,1],\n                     title='Autocorrelation ($M=15$, max-normalized)', \n                     ylabel='Scaling', colorbar=True);\nset_yticks_tempogram_cyclic(ax[1,0], octave_bin, F_coef_scale, num_tick=5)\n\nplot_segments(ann, ax=ax[2,0], time_max=(x.shape[0])/Fs, \n                       colors=color_ann, time_label='Time (seconds)')\nax[2,1].axis('off')\n\nplt.tight_layout()\n\n\n\n\n\n\n예시: Zager and Evans\n\n다음의 예는 “In the Year 2525” - Zager and Evans이다. 이 노래는 \\(IV_1V_2V_3V_4V_5V_6V_7BV_8O\\) 구조이며, 다소 모호한 템포와 리듬의 느린 인트로로 시작한다 (\\(I\\)-part). 음악은 주로 기타의 끊임없는 스트러밍과 함께 목소리가 지배적이다. 곡 후반부 브릿지(\\(B\\)-파트)도 같은 스타일로 연주된다. 인트로와 브릿지와 달리 8개의 반복되는 벌스 섹션(\\(V\\)-파트)은 타악기의 명확한 템포와 리듬으로 훨씬 빠르게 연주된다.\n다음 그림에서 볼 수 있듯이 느린 부분은 두 순환 템포그램 \\(\\mathcal{C}^\\mathrm{F}_{60}\\) 및 \\(\\mathcal{C}^\\mathrm{ A}_{60}\\)에서 빠른 부분과 명확히 구분된다.\n\n느린 부분에서 템포그램은 명확한 템포가 보이지 않는 잡음과 같은 특성을 나타낸다.\n대조적으로 빠른 부분에서 템포그램은 벌스 부분의 실제 상수 템포 \\(\\tau=s\\cdot 60\\cdot 2=126~\\mathrm{BPM}\\)을 반영하는 스케일링 매개변수 값 \\(s=1.05\\)에 해당하는 지배적인 템포를 가진다.\n\n\n\n# Annotation\nfn_ann = 'FMP_C4_F13_ZagerEvans_InTheYear2525.csv'\nann, color_ann = read_structure_annotation(path_data+fn_ann, fn_ann_color='InTheYear2525', \n                                                     Fs=1, remove_digits=False)\n\n# Audio file \nfn_wav = 'FMP_C4_F13_ZagerEvans_InTheYear2525.wav'\nx, Fs = librosa.load(path_data+fn_wav)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\noctave_bin = 15\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs_nov, N=500, H=50, \n                                                            Theta=np.arange(30, 601))\ntempogram_F = np.abs(X)\n\ntempogram_A, T_coef, F_coef_BPM, _, _ = compute_tempogram_autocorr(nov, Fs_nov,\n                                                                             N=500, H=50,\n                                                                             norm_sum=False,\n                                                                             Theta=np.arange(30, 601))\n\n\nfig, ax = plt.subplots(3, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [2, 2, 1]}, figsize=(8, 5))\n\noutput = compute_cyclic_tempogram(tempogram_F, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_F = output[0]\nF_coef_scale = output[1]\n\ntempogram_cyclic_F = normalize_feature_sequence(tempogram_cyclic_F, norm='max')\nplot_matrix(tempogram_cyclic_F, T_coef=T_coef, ax=[ax[0,0], ax[0,1]], clim=[0,1],\n                     title='Fourier ($M=%d$, max-normalized)'%octave_bin, \n                     ylabel='Scaling', colorbar=True);\nset_yticks_tempogram_cyclic(ax[0,0], octave_bin, F_coef_scale, num_tick=5)\n\noutput = compute_cyclic_tempogram(tempogram_A, F_coef_BPM, octave_bin=octave_bin)\ntempogram_cyclic_A = output[0]\nF_coef_scale = output[1]\n\ntempogram_cyclic_A = normalize_feature_sequence(tempogram_cyclic_A, norm='max')\nplot_matrix(tempogram_cyclic_A, T_coef=T_coef, ax=[ax[1,0], ax[1,1]], clim=[0,1],\n                     title='Autocorrelation ($M=%d$, max-normalized)'%octave_bin, \n                     ylabel='Scaling', colorbar=True);\nset_yticks_tempogram_cyclic(ax[1,0], octave_bin, F_coef_scale, num_tick=5)\n\nplot_segments(ann, ax=ax[2,0], time_max=(x.shape[0])/Fs, \n                       colors=color_ann, time_label='Time (seconds)')\nax[2,1].axis('off')\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#fourier-tempogram",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#fourier-tempogram",
    "title": "7.2. 템포 분석",
    "section": "Fourier Tempogram",
    "text": "Fourier Tempogram\n\nhop_length = 200 # samples per frame\nonset_env = librosa.onset.onset_strength(y=x, sr=sr, hop_length=hop_length, n_fft=2048)\n\n\nframes = range(len(onset_env))\nt = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)\n\n\nplt.figure(figsize=(10,3))\nplt.plot(t, onset_env)\nplt.xlim(0, t.max())\nplt.ylim(0)\nplt.xlabel('Time (sec)')\nplt.title('Novelty Function')\n\nText(0.5, 1.0, 'Novelty Function')\n\n\n\n\n\n\n# Fourier Tempogram\nS = librosa.stft(onset_env, hop_length=1, n_fft=512)\nfourier_tempogram = np.absolute(S)\n\nplt.figure(figsize=(10,3))\nplt.title('Fourier tempogram')\nlibrosa.display.specshow(fourier_tempogram, sr=sr, hop_length=hop_length, x_axis='time')\nplt.show()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#autocorrelation-tempogram",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#autocorrelation-tempogram",
    "title": "7.2. 템포 분석",
    "section": "Autocorrelation Tempogram",
    "text": "Autocorrelation Tempogram\n\ntmp = np.log1p(onset_env)\nr = librosa.autocorrelate(tmp)\n\n\nplt.figure(figsize=(10,3))\nplt.title(\"autocorrelation plot\")\nplt.plot(t, r)\nplt.xlim(0, t.max())\nplt.xlabel('Lag (sec)')\nplt.ylim(0)\n\n(0.0, 324.6555850873676)\n\n\n\n\n\n\ntempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr, hop_length=hop_length, win_length=400)\n\nplt.figure(figsize=(10,3))\nplt.title('Fourier tempogram')\nlibrosa.display.specshow(tempogram, sr=sr, hop_length=hop_length, x_axis='time', y_axis='tempo')\nplt.show()"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#estimating-global-tempo",
    "href": "posts/7. Tempo and Beat Tracking/7.2.Tempo_Analysis.html#estimating-global-tempo",
    "title": "7.2. 템포 분석",
    "section": "Estimating Global Tempo",
    "text": "Estimating Global Tempo\n\ntempo = librosa.beat.tempo(y=x, sr=sr)\nT = len(x)/float(sr)\nseconds_per_beat = 60.0/tempo[0]\nbeat_times = np.arange(0, T, seconds_per_beat)\n\nplt.figure(figsize=(10,3))\nlibrosa.display.waveshow(y=x)\nplt.vlines(beat_times, -1, 1, color='r')\n\n<matplotlib.collections.LineCollection at 0x238d5a76670>\n\n\n\n\n\n\nclicks = librosa.clicks(times=beat_times, sr=sr, length=len(x))\nipd.Audio(x + clicks, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S1_PeakPicking.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S2_TempoBeat.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S2_TempogramFourier.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S2_TempogramAutocorrelation.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S2_TempogramCyclic.html\nhttps://colab.research.google.com/github/stevetjoa/musicinformationretrieval.com/blob/gh-pages/tempo_estimation.ipynb\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html",
    "title": "7.3. 비트와 펄스 추적",
    "section": "",
    "text": "비트(beat)와 펄스(pulse) 추적(tracking)에 대해 다룬다. 지배적 로컬 펄스(PLP)에 대해 알아보고, 동적 프로그래밍을 통한 비트 추적, 그리고 적응형 윈도우에 대해 예시와 함께 본다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#최적-윈도우-정현파-optimal-windowed-sinusoids",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#최적-윈도우-정현파-optimal-windowed-sinusoids",
    "title": "7.3. 비트와 펄스 추적",
    "section": "최적 윈도우-정현파 (Optimal Windowed Sinusoids)",
    "text": "최적 윈도우-정현파 (Optimal Windowed Sinusoids)\n\n노벨티 함수 \\(\\Delta:\\mathbb{Z}\\to\\mathbb{R}\\)가 주어졌을 때, 다음과 같이 정의된 복소 푸리에 계수 \\(\\mathcal{F}(n,\\omega)\\)에서 푸리에 템포그램을 도출한 바 있다. \\[\\tau_n := \\underset{\\tau\\in\\Theta}{\\mathrm{argmax}} \\mathcal{T}^\\mathrm{F}(n,\\tau).\\]\n이로부터 다음과 같이 설정하여 푸리에 템포그램을 얻었다. \\[\\mathcal{T}^\\mathrm{F}(n,\\tau) = |\\mathcal{F}(n,\\tau/60)|\\]\n각 시간 위치 \\(n\\in\\mathbb{Z}\\)에 대해 이제 \\(\\mathcal{T}^\\mathrm{F}(n,\\tau)\\)를 최대화하는 템포 매개변수 \\(\\tau_n\\in \\Theta\\)를 고려한다: \\[\\tau_n := \\underset{\\tau\\in\\Theta}{\\mathrm{argmax}} \\mathcal{T}^\\mathrm{F}(n,\\tau)\\]\n푸리에 계수 \\(\\mathcal{F}(n,\\omega)\\)로 인코딩된 위상 정보(phase information)는 \\(n\\) 노벨티 함수 \\(\\Delta\\)의 로켈 섹션과 가장 잘 상관되는 템포 \\(\\tau_n\\)의 윈도우 정현파의 위상 \\(\\varphi_n\\)을 유도하는 데 사용할 수 있다.\n위상은 다음과 같이 주어지며, 복소수의 각은 래디언(radians)으로 주어진다(\\([0,2\\pi)\\) 안의 숫자). \\[\\varphi_n = - \\frac{1}{2\\pi} \\mathrm{angle}\\big( \\mathcal{F}(n,\\tau_n/60) \\big)\\]\n\\(\\tau_n\\) 및 \\(\\varphi_n\\)를 기반으로, 아래의 설정을 통해 최적 윈도우 정현파(optimal windowed sinusoid) \\(\\kappa_n:\\mathbb{Z}\\to \\mathbb{R}\\)를 정의한다. \\[\\kappa_n(m) := w(m-n) \\cos\\Big(2\\pi \\big((\\tau_n/60)\\cdot m - \\varphi_n\\big)\\Big)\\] for each time point \\(n\\in\\mathbb{Z}\\)\n\n푸리에 템포그램과 마찬가지로 윈도우 함수 \\(w\\)를 사용\n\n직관적으로 정현파 \\(\\kappa_n\\)은 템포 집합 \\(\\Theta\\)와 관련하여 시간 위치 \\(n\\)에서 노벨티 함수의 로컬 주기적 특성을 가장 잘 설명한다.\n주기 \\(60/\\tau_n\\)은 노벨티 함수의 지배적 주기성(predominant periodicity)에 해당하며 위상 정보 \\(\\varphi_n\\)은 \\(\\kappa_n\\)의 최대값과 노벨티 함수의 피크를 정확하게 정렬한다. 정현파 \\(\\kappa_n\\)의 관련성은 노벨티 함수의 퀄리티뿐 만이 아니라, \\(w\\)의 윈도우 크기와 템포 집합 \\(\\Theta\\)에 따라서도 달라진다.\n\n예: Shostakovich\n\nImage(path_img+\"FMP_C6_F07_Shostakovich_Waltz-02-Section_Score.png\", width=600)\n\n\n\n\n\n다음 그림에서는 Shostakovich 음악에 대한 푸리에 기반 템포그램을 보여준다. 또한 템포그램 시각화에서 빨간색 점으로 표시된 다양한 시간-템포 쌍에 대한 기저의 노벨티 함수 위에 그려진 최적 정현파 커널을 보여준다.\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nx, Fs = librosa.load(path_data+fn_wav) \nipd.display(data=x,rate=Fs)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nN = 500 #corresponding to 5 seconds (Fs_nov = 100 Hz)\nH = 10  #Hopsize leading to a tempogram resolution of 10 Hz\nTheta = np.arange(30, 601) #Tempo range parameter\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs=Fs_nov, N=N, H=H, \n                                                            Theta=Theta)\ntempogram = np.abs(X)\n\nt_nov = np.arange(nov.shape[0]) / Fs_nov\ncoef_n = np.array([0, 10, 20, 30, 40, 50, 60])\ncoef_k = np.zeros(len(coef_n), dtype=int)\n\nfor i in range(len(coef_n)):\n    coef_k[i] = np.argmax(tempogram[:,coef_n[i]])\n\n\nfig, ax, im = plot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, \n                                   figsize=(6.5, 3), ylabel='Tempo (BPM)',\n                                   title='Fourier tempogram')\nax[0].plot(T_coef[coef_n], F_coef_BPM[coef_k], 'ro')\n\nfor i in range(len(coef_n)):\n    n = coef_n[i]\n    k = coef_k[i]\n    tempo = F_coef_BPM[k]\n    time = T_coef[n]\n    corr = np.abs(X[k,n])\n    kernel, t_kernel, t_kernel_sec = compute_sinusoid_optimal(X[k,n], \n                                            F_coef_BPM[k], n, Fs_nov, N, H)\n    title=r'Windowed sinusoid (t = %0.1f sec, $\\tau$ = %0.0f BPM, corr = %0.2f)'% (time, tempo, corr)\n    plot_signal_kernel(nov, t_nov, 0.5*kernel, t_kernel_sec, \n                                 figsize=(5, 1.5), title=title)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#plp-함수의-정의",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#plp-함수의-정의",
    "title": "7.3. 비트와 펄스 추적",
    "section": "PLP 함수의 정의",
    "text": "PLP 함수의 정의\n\n심하게 손상된 피크 구조를 가진 영역에서 최적 윈도우 정현파을 추정하는 것은 문제가 있다. 이는 특히 윈도우 크기가 작은 경우에 그렇다.\n시간적 유연성을 유지하면서 주기성(periodicity) 추정을 보다 견고하게 하기 위해, 정현파를 하나씩 보는 대신 단일 함수를 형성한다. 이를 위해 overlap-add 기술을 적용한다. 여기서 최적의 윈도우 정현파 \\(\\kappa_n\\)는 모든 시간 위치 \\(n\\in\\mathbb{Z}\\)에 걸쳐 누적된다.\n또한 함수 결과의 양의 부분만 고려한다. 더 정확하게는 함수 \\(\\Gamma:\\mathbb{Z}\\to\\mathbb{R}_{\\geq 0}\\)를 다음과 같이 정의한다(half-wave rectification 사용). \\[\\Gamma(m) =\\big|\\textstyle \\sum_{n\\in \\mathbb{Z}}  \\kappa_n(m)\\big|_{\\geq 0}\\] for \\(n\\in\\mathbb{Z}\\)\n결과는 PLP 함수라고 부르는 중간 수준(mid-level) 표현이다.\n아래의 예에서 PLP의 계산 단계를 다음과 같은 그림으로 본다.\n\n노벨티 곡선\n프레임별 최대값이 있는 푸리에 템포그램 (시각화 목적으로 7개의 시간 위치 \\(n\\)에 대해서만 표시됨)\n\\(5\\)초의 윈도우 크기를 사용하는 최적의 윈도우 정현파 \\(\\kappa_n\\) (7개 위치에 대해서만 표시됨)\n모든 시간 프레임에 걸친 최적 정현파의 축적 (overlap-add)\n반파 정류기(Half-wave rectification) 이후 얻은 PLP 함수 \\(\\Gamma\\)\n\n\n\nL = nov.shape[0]\nN_left = N // 2\nL_left = N_left\nL_right = N_left\nL_pad = L + L_left + L_right\nt_pad = np.arange(L_pad)\n\nt_nov = np.arange(nov.shape[0]) / Fs_nov\ncoef_n = np.array([0, 10, 20, 30, 40, 50, 60])\ncoef_k = np.zeros(len(coef_n), dtype=int)\n\nfor i in range(len(coef_n)):\n    coef_k[i] = np.argmax(tempogram[:, coef_n[i]])\n\n\nfig, ax = plt.subplots(5, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2, 2, 1, 1]}, figsize=(8, 9))        \n\nplot_signal(nov, Fs_nov, ax=ax[0, 0], color='k', title='Novelty function')\nax[0, 1].set_axis_off()\n\nplot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, ax=[ax[1,0], ax[1,1]], \n                     title='Fourier tempogram', ylabel='Tempo (BPM)', colorbar=True)\nax[1, 0].plot(T_coef[coef_n], F_coef_BPM[coef_k], 'ro')\n\nplot_signal(nov, Fs_nov, ax=ax[2, 0], color='k', title='Novelty function with windowed sinusoids')\nax[2, 1].set_axis_off()\n\nnov_PLP = np.zeros(L_pad)\nfor i in range(len(coef_n)):\n    n = coef_n[i]\n    k = coef_k[i]\n    tempo = F_coef_BPM[k]\n    time = T_coef[n]\n    kernel, t_kernel, t_kernel_sec = compute_sinusoid_optimal(X[k, n], F_coef_BPM[k], n, Fs_nov, N, H)    \n    nov_PLP[t_kernel] = nov_PLP[t_kernel] + kernel\n    ax[2, 0].plot(t_kernel_sec, 0.5 * kernel, 'r')\n    ax[2, 0].set_ylim([-0.6, 1])\n    \nnov_PLP = nov_PLP[L_left:L_pad-L_right]\nplot_signal(nov_PLP, Fs_nov, ax=ax[3, 0], color='r', title='Accumulated windowed sinusoids')\nax[3,1].set_axis_off()\n\nnov_PLP[nov_PLP < 0] = 0\nplot_signal(nov_PLP, Fs_nov, ax=ax[4, 0], color='r', title='PLP function $\\Gamma$')\nax[4, 1].set_axis_off()\n\nplt.tight_layout()\n\n\n\n\n\n서로 다른 윈도우 정현파의 최대값이 노벨티 함수의 피크뿐만 아니라 겹치는 영역에서 이웃 정현파의 최대값과도 잘 정렬되는 것을 볼 수 있다. 이는 보강 간섭(constructive interferences)으로 이어진다.\n푸리에 템포그램에서 알 수 있듯이 지배적 템포는 약 \\(\\tau=225~\\mathrm{BPM}\\)에서 시작한 약간의 템포 증가와 함께 전체에서는 \\(200\\)와 \\(250~\\mathrm{BPM}\\) 사이를 가진다.\n7개의 서로 다른 시간 위치에 대해 최대화 템포 값과 해당 최적의 윈도우 정현파가 표시된다. 이러한 각각의 윈도우 정현파는 약한 피크와 “이상적인” 주기성(periodicity)의 작은 편차가 균형을 이루는 노벨티 함수의 피크 구조의 로컬 주기적 특성을 설명하려고 한다. 또한, 이러한 펄스 위치 중 일부는 원래의 노벨티 함수에서 다소 약하지만 지배적인 펄스 위치는 \\(\\Gamma\\)의 피크로 명확하게 표시된다. 이런 의미에서 PLP 함수는 지배적 펄스 레벨이 고려된 원래의 노벨티 함수의 로컬 주기성(local periodicity) 향상으로 볼 수 있다.\n다음 코드 셀에서 복소수 푸리에 템포그램을 입력으로 하고 PLP 함수를 출력하는 함수를 정의한다. 여기서 최적의 윈도우 정현파 \\(\\kappa_n\\)는 모든 템포그램 프레임 \\(n\\)에 걸쳐 누적된다.\n시각화는 일부 피크 선택(peak-picking) 전략으로 얻은 피크와 함께 결과 PLP 함수뿐만 아니라 원래의 노벨티 함수를 보여준다. 또한, 원본 오디오 녹음에 클릭 트랙을 통해 피크 소니피케이션이 추가된다.\n\n\ndef compute_plp(X, Fs, L, N, H, Theta):\n    \"\"\"Compute windowed sinusoid with optimal phase\n\n    Args:\n        X (np.ndarray): Fourier-based (complex-valued) tempogram\n        Fs (scalar): Sampling rate\n        L (int): Length of novelty curve\n        N (int): Window length\n        H (int): Hop size\n        Theta (np.ndarray): Set of tempi (given in BPM)\n\n    Returns:\n        nov_PLP (np.ndarray): PLP function\n    \"\"\"\n    win = np.hanning(N)\n    N_left = N // 2\n    L_left = N_left\n    L_right = N_left\n    L_pad = L + L_left + L_right\n    nov_PLP = np.zeros(L_pad)\n    M = X.shape[1]\n    tempogram = np.abs(X)\n    for n in range(M):\n        k = np.argmax(tempogram[:, n])\n        tempo = Theta[k]\n        omega = (tempo / 60) / Fs\n        c = X[k, n]\n        phase = - np.angle(c) / (2 * np.pi)\n        t_0 = n * H\n        t_1 = t_0 + N\n        t_kernel = np.arange(t_0, t_1)\n        kernel = win * np.cos(2 * np.pi * (t_kernel * omega - phase))\n        nov_PLP[t_kernel] = nov_PLP[t_kernel] + kernel\n    nov_PLP = nov_PLP[L_left:L_pad-L_right]\n    nov_PLP[nov_PLP < 0] = 0\n    return nov_PLP\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nL = len(nov)\nN = 500\nH = 10\nTheta = np.arange(30, 601)\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs=Fs_nov, N=N, H=H, \n                                                            Theta=Theta)\nnov_PLP = compute_plp(X, Fs_nov, L, N, H, Theta)\n\nt_nov = np.arange(nov.shape[0]) / Fs_nov\npeaks, properties = signal.find_peaks(nov, prominence=0.02) #scipy\npeaks_sec = t_nov[peaks]\nplot_signal(nov, Fs_nov, color='k', title='Novelty function with detected peaks');\nplt.plot(peaks_sec, nov[peaks], 'ro')\nplt.show()\nx_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x))\nipd.display(Audio(data = x + x_peaks, rate=Fs))\n\npeaks, properties = signal.find_peaks(nov_PLP, prominence=0.02)\npeaks_sec = t_nov[peaks]\nplot_signal(nov_PLP, Fs_nov, color='k', title='PLP function with detected peaks');\nplt.plot(peaks_sec, nov_PLP[peaks], 'ro')\nplt.show()\nx_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x))\nipd.display(Audio(data=x + x_peaks, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n예시: Brahms\n\n두 번째 예로 Brahms의 헝가리 무곡 5번 오케스트라를 보자. 작은 발췌 부분(녹음의 \\(t_1=35~\\mathrm{sec}\\) 및 \\(t_2=53~\\mathrm{sec}\\) 사이의 섹션)만 고려하며, measure \\(26\\) ~ \\(38\\)이 재생된다. 녹음의 전체 음악적 구조 \\(A_1A_2B_1B_2CA_3B_3B_4D\\)와 관련하여, 발췌 부분은 \\(A_2\\) 섹션에서 \\(B_1\\) 섹션으로의 전환으로 구성된다.\n\n\nImage(path_img+\"FMP_C6_F19a.png\", width=700)\n\n\n\n\n\n중요한 로컬 템포 변경으로 인해 템포 추정 및 펄스 추적이 쉽지 않다. 4분 음표 펄스 레벨을 고려하여 수동적으로 본 결과,\n\n\\(90~\\mathrm{BPM}\\)의 템포로 시작(measures \\(26\\)–\\(28\\), seconds \\(35\\)–\\(39\\)),\n그런 다음 갑자기 \\(140~\\mathrm{BPM}\\)로 변경(measures \\(29\\)–\\(32\\), seconds \\(39\\)–\\(41\\)),\n그리고 \\(75~\\mathrm{BPM}\\)(measures \\(33\\)–\\(38\\), seconds \\(41\\)–\\(53\\))로 계속된다.\n\n많은 음의 온셋 부분이 노벨티 함수로 제대로 캡처되지 않는다. 더욱이 다이나믹(dynamics)에서 큰 차이 때문에, 어떤 음 온셋과도 관련되지 않은 가짜 피크와 거의 구별할 수 없는 일부 약한 온셋이 있으며, 노벨티 함수을 지배하는 일부 강한 온셋도 있다. 결과적으로 피크의 높이가 반드시 타당성을 나타내는 유일한 지표는 아니다.\n이러한 문제에도 불구하고 템포는 8분 음표 펄스 레벨(4분 음표 템포의 두 번째 고조파)에서 푸리에 템포그램에 의해 잘 반영된다. 손상되었지만 노벨티 함수의 피크 구조는 여전히 로컬 주기적 규칙성을 가지며, 이는 지배적인 로컬 템포에 해당하는 윈도우 정현파에 의해 캡처된다.\n결과 PLP 함수 \\(\\Gamma\\)는 8분음표 레벨에서 펄스 위치를 표시할 수 있다.\n\n\nfn_wav = 'FMP_C6_F19_Brahms_Ormandy_sec35-53.wav'\nx, Fs = librosa.load(path_data+fn_wav)\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nN = 500\nH = 10\nTheta = np.arange(30, 601)\nL = len(nov)\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs=Fs_nov, N=N, H=H, \n                                                            Theta=Theta)\nnov_PLP = compute_plp(X, Fs_nov, L, N, H, Theta)\n\ntempogram = np.abs(X)\ntitle = 'Fourier tempogram using a window length of %0.1f seconds'%(N/Fs_nov)\nplot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, figsize=(8,2.5),\n                     title=title, ylabel='Tempo (BPM)', colorbar=True);\nfig, ax, im = plot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, \n                                   figsize=(8,2.5), ylabel='Tempo (BPM)', colorbar=True,\n                                   title='Fourier tempogram with dominant local tempo') \ncoef_k = np.argmax(tempogram, axis=0)\nax[0].plot(T_coef, F_coef_BPM[coef_k], 'r.')\n\nt_nov = np.arange(nov.shape[0]) / Fs_nov\npeaks, properties = signal.find_peaks(nov, prominence=0.05)\npeaks_sec = t_nov[peaks]\nplot_signal(nov, Fs_nov, color='k', figsize=(7,2),\n                     title='Novelty function with detected peaks');\nplt.plot(peaks_sec, nov[peaks], 'r.')\nplt.show()\nx_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x))\nipd.display(Audio(data=x + x_peaks, rate=Fs))\n\npeaks, properties = signal.find_peaks(nov_PLP, prominence=0.05)\npeaks_sec = t_nov[peaks]\nplot_signal(nov_PLP, Fs_nov, color='k', figsize=(7,2),\n                     title='PLP function with detected peaks');\nplt.plot(peaks_sec, nov_PLP[peaks], 'ro')\nplt.show()\nx_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x))\nipd.display(Audio(data=x + x_peaks, rate=Fs))\n\n\n\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#정현파-윈도우-길이의-종속성-dependency-of-sinusoid-window-length",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#정현파-윈도우-길이의-종속성-dependency-of-sinusoid-window-length",
    "title": "7.3. 비트와 펄스 추적",
    "section": "정현파 윈도우 길이의 종속성 (Dependency of Sinusoid Window Length)",
    "text": "정현파 윈도우 길이의 종속성 (Dependency of Sinusoid Window Length)\n\nPLP 함수를 계산할 때 많은 매개변수가 있다. 우선, 최종 결과는 다양한 방법으로 계산될 수 있는 인풋 노벨티 곡선에 따라 달라진다. 여기서는 푸리에 템포그램을 계산하는 데 사용되는 윈도우 정현파의 길이를 자세히 살펴본다.\n분명히 모순되는 원칙들 사이 trade-off가 있다. 더 긴 윈도우를 사용하면 더 정확하고 강력한 템포 추정치를 얻을 수 있지만, 시간적 유연성이 떨어진다. 대조적으로, 더 짧은 창을 사용하면 시간 해상도가 증가하지만, 템포 추정이 더 취약해진다.\n브람스 예를 통해 이 장단점을 볼 수 있다. 첫 번째 경우 12초에 해당하는 윈도우 길이를 사용하고 두 번째 경우에는 2초에 불과한 윈도우 길이를 사용한다.\n\n\ndef compute_plot_tempogram_plp(fn_wav, Fs=22050, N=500, H=10, Theta=np.arange(30, 601),\n                               title='', figsize=(8, 4), plot_maxtempo=False):\n    \"\"\"Compute and plot Fourier-based tempogram and PLP function\n\n    Args:\n        fn_wav: Filename of audio file\n        Fs: Sample rate (Default value = 22050)\n        N: Window size (Default value = 500)\n        H: Hop size (Default value = 10)\n        Theta: Set of tempi (given in BPM) (Default value = np.arange(30, 601))\n        title: Title of figure (Default value = '')\n        figsize: Figure size (Default value = (8, 4))\n        plot_maxtempo: Visualize tempo with greatest coefficients in tempogram (Default value = False)\n    \"\"\"\n    x, Fs = librosa.load(fn_wav)\n\n    nov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\n    nov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\n    L = len(nov)\n    H = 10\n    X, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs=Fs_nov, N=N, H=H, Theta=Theta)\n    nov_PLP = compute_plp(X, Fs_nov, L, N, H, Theta)\n    tempogram = np.abs(X)\n\n    fig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05],\n                                              'height_ratios': [2, 1]},\n                           figsize=figsize)\n    plot_matrix(tempogram, T_coef=T_coef, F_coef=F_coef_BPM, title=title,\n                         ax=[ax[0, 0], ax[0, 1]], ylabel='Tempo (BPM)', colorbar=True)\n    if plot_maxtempo:\n        coef_k = np.argmax(tempogram, axis=0)\n        ax[0, 0].plot(T_coef, F_coef_BPM[coef_k], 'r.')\n\n    t_nov = np.arange(nov.shape[0]) / Fs_nov\n    peaks, properties = signal.find_peaks(nov_PLP, prominence=0.05)\n    peaks_sec = t_nov[peaks]\n    plot_signal(nov_PLP, Fs_nov, color='k', ax=ax[1, 0])\n    ax[1, 1].set_axis_off()\n    ax[1, 0].plot(peaks_sec, nov_PLP[peaks], 'ro')\n    plt.show()\n    x_peaks = librosa.clicks(times=peaks_sec, sr=Fs, click_freq=1000, length=len(x))\n    ipd.display(ipd.Audio(data=x + x_peaks, rate=Fs))\n\n\nN=1200 # 12초\ntitle='Fourier tempogram using a window length of %0.1f seconds'%(N/Fs_nov)\ncompute_plot_tempogram_plp(path_data+fn_wav, N=N, title=title, plot_maxtempo=True)\n\nN=200 # 2초\ntitle='Fourier tempogram using a window length of %0.1f seconds'%(N/Fs_nov)\ncompute_plot_tempogram_plp(path_data+fn_wav, N=N, title=title, plot_maxtempo=True)\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#템포-집합의-종속성-dependency-of-tempo-set",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#템포-집합의-종속성-dependency-of-tempo-set",
    "title": "7.3. 비트와 펄스 추적",
    "section": "템포 집합의 종속성 (Dependency of Tempo Set)",
    "text": "템포 집합의 종속성 (Dependency of Tempo Set)\n\n지배적인 템포를 결정하기 위해 프레임별 최대값을 취하는 것에는 장단점이 있다. 급격한 템포 변화에도 PLP 함수를 빠르게 조정할 수 있다는 장점이 있지만, 반면에 템포 옥타브 사이의 임의 전환과 같은 원치 않는 점프로 이어질 수 있다는 단점이 있다.\nBurgmüller의 Piano Etude Op. 100 No. 2의 시작 부분을 통해 이를 확인해 보자.\n\n\nImage(path_img+\"FMP_C6_F20a.png\", width=700)\n\n\n\n\n\n템포 집합 \\(\\Theta=[30:600]\\)를 사용하면 4분음표와 8분음표 레벨 사이에 몇 가지 변화가 발생하는 PLP 함수가 생성된다.\n최대화에서 템포 집합 \\(\\Theta\\)를 제한할 때, 이러한 펄스 레벨의 전환을 피할 수 있다.\n\n예를 들어 제한된 집합 \\(\\Theta=[60:200]\\)를 사용하면, 결과 PLP function은 대략 \\(130~\\mathrm{BPM}\\)의 템포로 4분음표(tactus) 펄스 위치가 정확히 표시된다.\n집합 \\(\\Theta=[200:340]\\)를 사용하면 8분음표 펄스 레벨의 위치가 표시된다.\n집합 \\(\\Theta=[450:600]\\)를 사용하면 16분음표(tatum) 펄스 레벨의 위치가 표시된다.\n\n즉, 예상되는 템포 범위에 대한 사전 지식을 PLP 프레임워크에 쉽게 통합하여 특정 수준의 펄스를 나타낼 수 있다.\n\n\nfn_wav = 'FMP_C6_F20_Burgmueller_Op100-02-FirstPart.wav'\n\ntitle=r'Fourier tempogram and PLP function with $\\Theta=[30:600]$'\ncompute_plot_tempogram_plp(path_data+fn_wav, N=500, Theta=np.arange(30, 601), title=title)\n\ntitle=r'Fourier tempogram and PLP function with $\\Theta=[60:200]$'\ncompute_plot_tempogram_plp(path_data+fn_wav, N=500, Theta=np.arange(60,201), title=title)\n\ntitle=r'Fourier tempogram and PLP function with $\\Theta=[200:340]$'\ncompute_plot_tempogram_plp(path_data+fn_wav, N=500, Theta=np.arange(200,341), title=title)\n\ntitle=r'Fourier tempogram and PLP function with $\\Theta=[450:600]$'\ncompute_plot_tempogram_plp(path_data+fn_wav, N=500, Theta=np.arange(450,601), title=title)\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#모델링",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#모델링",
    "title": "7.3. 비트와 펄스 추적",
    "section": "모델링",
    "text": "모델링\n\n템포가 거의 일정한, 강하고 꾸준한 박자를 가진 많은 종류의 음악이 있다. 이제 비트 위치가 가장 강한 음 온셋과 함께 진행되고, 템포가 거의 일정하다는 가정에 기반한 비트 추적 절차를 설명한다.\n임의의 비트 시퀀스가 이 두 가지 가정을 얼마나 잘 반영하는지 측정하는 점수 함수를 구성한다. 점수를 최대화하는 비트 시퀀스는 최종 비트 추적 결과를 구성한다. 이러한 최적의 비트 시퀀스는 동적 프로그래밍(dynamic programming)을 사용하여 효율적으로 계산할 수 있다.\n비트 추적 절차의 인풋은 노벨티 함수 \\(\\Delta:[1:N]\\to\\mathbb{R}\\)와 전체적인 템포의 대략적인 추정치 \\(\\hat{\\tau}\\in\\mathbb{R}_{>0}\\)로 구성된다. 평소와 같이 \\([1:N]\\) 구간은 노벨티 계산에 사용되는 샘플링된 시간 축을 나타낸다고 가정한다. 템포 추정치 \\(\\hat{\\tau}\\)는 수동으로 지정하거나 자동화된 절차로 얻을 수 있다. \\(\\hat{\\tau}\\) 및 피쳐 레이트에서 비트 주기에 대한 추정치를 도출할 수 있다.\n단순화를 위해 비트 주기가 샘플 또는 피쳐 인덱스(초 단위가 아닌)로 지정된다고 가정한다. \\(\\hat{\\delta}\\in\\mathbb{N}\\)를 이 숫자로 두자. 템포가 거의 일정하다고 가정하면 연속되는 두 비트의 차이 \\(\\delta\\)는 \\(\\hat{\\delta}\\)에 가까워야 한다.\n이상적인 비트 기간 \\(\\hat{\\delta}\\)으로부터 \\(\\delta\\)의 편차를 설명하기 위해 페널티 함수(penalty function) \\(P_{\\hat{\\delta}}:\\mathbb{N}\\to\\mathbb{R}\\)를 다음과 같이 설정하여 도입한다. \\[P_{\\hat{\\delta}}(\\delta) := - \\big( \\log_2 (\\delta/\\hat{\\delta}) \\big)^2\\] for \\(\\delta\\in\\mathbb{N}\\)\n이 함수는 \\(\\delta=\\hat{\\delta}\\)에서 최대값이 0이고, 편차가 클수록 점점 음수 값을 나타낸다. 또한, 템포 편차는 본질적으로 상대적이기 때문에(템포를 두 배로 하는 것은 템포를 반으로 줄이는 것과 같은 정도로 페널티를 받아야 함), 페널티 함수는 로그 축에서 대칭으로 정의된다.\n\n\ndef compute_penalty(N, beat_ref):\n    \"\"\"| Compute penalty funtion used for beat tracking [FMP, Section 6.3.2]\n    | Note: Concatenation of '0' because of Python indexing conventions\n\n    Args:\n        N (int): Length of vector representing penalty function\n        beat_ref (int): Reference beat period (given in samples)\n\n    Returns:\n        penalty (np.ndarray): Penalty function\n    \"\"\"\n    t = np.arange(1, N) / beat_ref\n    penalty = -np.square(np.log2(t))\n    t = np.concatenate((np.array([0]), t))\n    penalty = np.concatenate((np.array([0]), penalty))\n    return penalty\n\n\nbeat_ref = 20\nN = 4 * beat_ref\npenalty = compute_penalty(N, beat_ref)\n\nplt.figure(figsize=(5, 2))\nt = np.arange(0, N) / beat_ref\nplt.plot(t, penalty, 'r')\nplt.ylim([-4, 0.2])\nplt.xlim([t[1], t[-1]])\nplt.xlabel('Beat interval (given in multiples of $\\hat{\\delta}$)')\nplt.title('Penalty')\nplt.show()\n\n\n\n\n\n주어진 \\(N\\in\\mathbb{N}\\)에 대해 \\(B = (b_1,b_2,\\ldots,b_L)\\)는 비트 위치 \\(b_\\ell\\in[1:N]\\) for \\(\\ell\\in[1:L]\\)을 강한 단조 증가하는(strictly monotonically increasing) 길이 \\(L\\in\\mathbb{N}_0\\)의 시퀀스라고 하자.\n이하에서는 이러한 시퀀스를 비트 시퀀스(beat sequence)라고 한다. 정의에 따르면 \\(L=0\\) 길이의 비트 시퀀스는 빈(empty) 시퀀스이다. 또한 \\(\\mathcal{B}^N\\)은 주어진 매개변수 \\(N\\in\\mathbb{N}\\)에 대해 가능한 모든 비트 시퀀스로 구성된 집합을 나타낸다. 비트 시퀀스 \\(B\\in\\mathcal{B}^N\\)의 퀄리티를 측정하기 위해 노벨티 함수 \\(\\Delta\\)의 양수 값과 페널티 함수 \\(P_{\\hat{\\delta}}\\)의 음수 값을 포함하는 점수(score) 값 \\(\\mathbf{S}(B)\\)를 도입한다.\n이를 위해 다음과 같이 설정한다. \\[\\mathbf{S}(B) := \\sum_{\\ell=1}^L \\Delta(b_\\ell) + \\lambda \\sum_{\\ell=2}^L P_{\\hat{\\delta}}(b_\\ell-b_{\\ell-1})\\]\n\\(L=0\\)인 빈 비트 시퀀스에 대해서는 \\(\\mathbf{S}(B)=0\\)를 얻는다. 또한 \\(L=1\\)인 경우 \\(\\mathbf{S}(B)=\\Delta(b_1)\\)를 얻는다. 높은 점수를 얻기 위해서는 노벨티 값 \\(\\Delta(b_\\ell)\\)가 커야 하고 비(非)양수 페널티 값 \\(P_{\\hat{\\delta}}(b_\\ell-b_{\\ell- 1})\\)는 0에 가까워야 한다. 가중치 매개변수 \\(\\lambda\\in\\mathbb{R}_{>0}\\)는 이 두 가지 상충하는 조건의 균형을 맞출 수 있다.\n가능한 모든 비트 시퀀스 중에서 최대 점수를 갖는 비트 시퀀스 \\(B^\\ast := \\mathrm{argmax}\\big\\{\\mathbf{S}(B) \\,\\big|\\, B\\in\\mathcal{B}^N\\big\\}\\)는 비트 추적 문제의 솔루션을 제공한다. 다음 그림은 그 정의를 보여준다.\n\n\nImage(path_img+\"FMP_C6_F22.png\", width=600)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#동적-프로그래밍-알고리즘",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#동적-프로그래밍-알고리즘",
    "title": "7.3. 비트와 펄스 추적",
    "section": "동적 프로그래밍 알고리즘",
    "text": "동적 프로그래밍 알고리즘\n\n가능한 비트 시퀀스의 수는 \\(N\\)에서 기하급수적이다. 그러나 동적 프로그래밍을 사용하여 최적의 비트 시퀀스를 효율적으로 계산할 수 있다. 비용 최소화 워핑 경로(DTW 알고리즘) 또는 확률 최대화 상태 시퀀스(Viterbi 알고리즘)를 계산할 때 이미 이 알고리즘 패러다임을 본 적이 있다. prefix를 고려하여 최적화 문제를 더 간단한 하위 문제로 나누는 것이다.\n비트 추적의 맥락에서는 기저의 노벨티 함수의 prefix를 고려한다.\n좀 더 정확히 말하자면, $N_nN $는 \\(n\\in[0:N]\\)로 끝나는 모든 비트 시퀀스의 집합을 나타낸다고 하자. 즉, 비트 시퀀스 \\(B = (b_1,b_2,\\ldots,b_L)\\in\\mathcal{B}^N_n\\)의 경우 \\(b_L=n\\)이다. \\(n=0\\)의 경우 가능한 유일한 비트 시퀀스는 빈 비트이다.\n\\(\\mathbf{D}(n):= \\max\\left\\{\\mathbf{S}(B) \\,\\mid\\, B\\in\\mathcal{B}^N_n \\right\\}\\)가 \\(n\\in[0:N]\\)로 끝나는 모든 비트 시퀀스에 대한 최대 점수를 나타낸다고 하자. \\(\\mathbf{D}(n)\\) 값은 누적 점수(accumulated score)라고도 한다. 그러면 \\(\\mathbf{S}(B^\\ast) = \\max_{n\\in[0:N]} \\mathbf{D}(n)\\)이다. \\(\\mathbf{D}(n)\\) 값은 \\(n=0,1,\\ldots,N\\)에 대해 반복적인 방식으로 계산될 수 있다. 다음 그림은 절차를 설명한다.\n동적 프로그래밍 덕분에 가능한 모든 비트 시퀀스의 점수를 계산하는 데 필요한 연산의 기하급수적인 수가 \\(N\\)의 2차(quadratic) 연산의 수로 줄어든다.\n\n\nImage(path_img+\"FMP_C6_T01.png\", width=600)"
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#구현",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#구현",
    "title": "7.3. 비트와 펄스 추적",
    "section": "구현",
    "text": "구현\n\ndef compute_beat_sequence(novelty, beat_ref, penalty=None, factor=1.0, return_all=False):\n    \"\"\"| Compute beat sequence using dynamic programming [FMP, Section 6.3.2]\n    | Note: Concatenation of '0' because of Python indexing conventions\n\n    Args:\n        novelty (np.ndarray): Novelty function\n        beat_ref (int): Reference beat period\n        penalty (np.ndarray): Penalty function (Default value = None)\n        factor (float): Weight parameter for adjusting the penalty (Default value = 1.0)\n        return_all (bool): Return details (Default value = False)\n\n    Returns:\n        B (np.ndarray): Optimal beat sequence\n        D (np.ndarray): Accumulated score\n        P (np.ndarray): Maximization information\n    \"\"\"\n    N = len(novelty)\n    if penalty is None:\n        penalty = compute_penalty(N, beat_ref)\n    penalty = penalty * factor\n    novelty = np.concatenate((np.array([0]), novelty))\n    D = np.zeros(N+1)\n    P = np.zeros(N+1, dtype=int)\n    D[1] = novelty[1]\n    P[1] = 0\n    # forward calculation\n    for n in range(2, N+1):\n        m_indices = np.arange(1, n)\n        scores = D[m_indices] + penalty[n-m_indices]\n        maxium = np.max(scores)\n        if maxium <= 0:\n            D[n] = novelty[n]\n            P[n] = 0\n        else:\n            D[n] = novelty[n] + maxium\n            P[n] = np.argmax(scores) + 1\n    # backtracking\n    B = np.zeros(N, dtype=int)\n    k = 0\n    B[k] = np.argmax(D)\n    while P[B[k]] != 0:\n        k = k+1\n        B[k] = P[B[k-1]]\n    B = B[0:k+1]\n    B = B[::-1]\n    B = B - 1\n    if return_all:\n        return B, D, P\n    else:\n        return B\n\n\n# Example from Exercise 6.12 of [Müller, FMP, Springer 2015]\nnov = np.array([0.1, 0.0, 1.0, 0.0, 1.0, 0.8, 0.0, 0.2, 0.4, 1.0, 0.0])\n# Manually specified penality values (in practice, we use the function \"compute_penalty\")\npenalty = np.array([0, -2, -0.2, 1.0, 0.5, -0.1, -1, -1.5, -3, -5, -8])\nfactor  = 1\nbeat = 3 \n\nB,D,P = compute_beat_sequence(nov, beat, penalty=penalty, factor=1, return_all=True)    \ndf = pd.DataFrame([np.arange(1,len(nov)+1), nov, D[1:], P[1:]])\ndf.rename(index={0:'$n$', 1:'$\\Delta(n)$',\n                 2:'$\\mathbf{D}(n)$', 3:'$\\mathbf{P}(n)$'}, inplace=True)\ndf.rename_axis('$n$', axis='rows')\n\nclass Formatter(): \n    \"\"\"Cass for converting column to row format \n    Notebook: C6/C6S3_BeatTracking.ipynb\"\"\"\n    def __init__(self):\n        self.i = 0\n    def formatter(self, s):\n        if self.i == 0 or self.i == 3:\n            return_s = str(int(s))\n        else:\n            return_s = str(s)\n        self.i += 1\n        return return_s\n\nipd.display(ipd.HTML(df.to_html(formatters={i: Formatter().formatter for i in df.columns}, \n                                escape=False, header=False)))\n\nprint('Optimal beat sequency B:', B+1)\n\n\n\n  \n    \n      $n$\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      11\n    \n    \n      $\\Delta(n)$\n      0.1\n      0.0\n      1.0\n      0.0\n      1.0\n      0.8\n      0.0\n      0.2\n      0.4\n      1.0\n      0.0\n    \n    \n      $\\mathbf{D}(n)$\n      0.1\n      0.0\n      1.0\n      1.1\n      2.0\n      2.8\n      2.1\n      3.2\n      4.2\n      4.3\n      4.2\n    \n    \n      $\\mathbf{P}(n)$\n      0\n      0\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      6\n      8\n    \n  \n\n\n\nOptimal beat sequency B: [ 3  6 10]\n\n\n예: Shostakovich\n\n비트 트래킹 절차의 인풋으로 샘플링 레이트 \\(F_\\mathrm{s}^\\Delta = 100~\\mathrm{Hz}\\)의 스펙트럼 기반 노벨티 함수를 사용한다. 지배적 템포 200에서 300BPM사이를 기대하여 \\(\\hat{\\tau}=240~\\mathrm{BPM}\\)에 해당하는 \\(\\hat{\\delta}=25\\)로 설정한다.\n\n\ndef beat_period_to_tempo(beat, Fs):\n    \"\"\"Convert beat period (samples) to tempo (BPM) [FMP, Section 6.3.2]\n\n    Args:\n        beat (int): Beat period (samples)\n        Fs (scalar): Sample rate\n\n    Returns:\n        tempo (float): Tempo (BPM)\n    \"\"\"\n    tempo = 60 / (beat / Fs)\n    return tempo\n    \n    \ndef compute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref, factor, title=None, figsize=(6, 2)):\n    \"\"\"Compute, plot, and sonfy beat sequence from novelty function [FMP, Section 6.3.2]\n\n    Args:\n        x: Novelty function\n        Fs: Sample rate\n        nov: Novelty function\n        Fs_nov: Rate of novelty function\n        beat_ref: Reference beat period\n        factor: Weight parameter for adjusting the penalty\n        title: Title of figure (Default value = None)\n        figsize: Size of figure (Default value = (6, 2))\n    \"\"\"\n    B = compute_beat_sequence(nov, beat_ref=beat_ref, factor=factor)\n\n    beats = np.zeros(len(nov))\n    beats[np.array(B, dtype=np.int32)] = 1\n    if title is None:\n        tempo = beat_period_to_tempo(beat_ref, Fs_nov)\n        title = (r'Optimal beat sequence ($\\hat{\\delta}=%d$, $F_\\mathrm{s}=%d$, '\n                 r'$\\hat{\\tau}=%0.0f$ BPM, $\\lambda=%0.2f$)' % (beat_ref, Fs_nov, tempo, factor))\n\n    fig, ax, line = plot_signal(nov, Fs_nov, color='k', title=title, figsize=figsize)\n    T_coef = np.arange(nov.shape[0]) / Fs_nov\n    ax.plot(T_coef, beats, ':r', linewidth=1)\n    plt.show()\n\n    beats_sec = T_coef[B]\n    x_peaks = librosa.clicks(times=beats_sec, sr=Fs, click_freq=1000, length=len(x))\n    ipd.display(ipd.Audio(x + x_peaks, rate=Fs))\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\ncompute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref=25, factor=1, figsize=(8,1.5))\ncompute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref=75, factor=0.5, figsize=(8,1.5))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n첫번째 결과를 보면 비트가 잘 감지된다.\n두번째로 \\(\\hat{\\tau}=80~\\mathrm{BPM}\\) 템포를 가정하여 “measure” 수준의 최적 비트 시퀀스를 찾는다.\\(\\hat{\\delta}=75\\)로 설정하면 결과가 기대한 것처럼 나오지 않는다. 이유는 3/4미터 안의 다운비트(노벨티 함수로 측정)는 다른 비트보다 훨씬 약하기 때문이다."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#한계점",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#한계점",
    "title": "7.3. 비트와 펄스 추적",
    "section": "한계점",
    "text": "한계점\n\n비트 추적 절차의 주요 한계점은 미리 정의된 단일 템포 \\(\\hat{\\tau}\\)에 대한 종속성(dependency)이다.\n작은 가중치 매개변수 \\(\\lambda\\)를 사용하면 이상적인 비트 주기 \\(\\hat{\\delta}\\)에서 로컬적으로 편차가 있는 경우에도 좋은 비트 추적 결과를 얻을 수 있다.\n그러나 제시된 절차는 천천히 변화하는 템포(예: 리타르단도 또는 악셀레란도) 또는 급격한 템포 변화로 음악을 처리하도록 설계되지 않았다. 이러한 한계점에도 불구하고 비트 추적에 대한 동적 프로그래밍 접근 방식의 단순성과 효율성으로 인해 다양한 유형의 음악에 사용될 수 있다.\n비트 추적기의 동작을 설명하기 위해 오케스트라의 작은 발췌부분(\\(t_1=35~\\mathrm{sec}\\)에서 \\(t_2=53~\\mathrm{sec}\\)까지의 섹션)을 고려하여 까다로운 예를 사용해보자. 브람스의 헝가리 무곡 5번 녹음의 발췌 부분은 몇 가지 갑작스러운 템포 변경으로 인해 비트 추적이 어렵다. 또한, 인풋의 노벨티 함수는 부드럽고 흐릿한 온셋으로 인해 잡음이 매우 많다.\n\n\nImage(path_img+\"FMP_C6_F19a.png\", width=700)\n\n\n\n\n\nfn_wav = 'FMP_C6_F19_Brahms_Ormandy_sec35-53.wav'\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nbeat_ref, factor = 40, 1\ntempo = beat_period_to_tempo(beat_ref, Fs_nov)\ncompute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref=beat_ref, factor=factor, figsize=(8, 1.5))\n\nbeat_ref, factor = 40, 0.1\ntempo = beat_period_to_tempo(beat_ref, Fs_nov)\ncompute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref=beat_ref, factor=factor, figsize=(8, 1.5))\n   \nbeat_ref, factor = 80, 1\ntempo = beat_period_to_tempo(beat_ref, Fs_nov)\ncompute_plot_sonify_beat(x, Fs, nov, Fs_nov, beat_ref=beat_ref, factor=factor, figsize=(8, 1.5))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#비트-동기beat-synchronous-특징-표현",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#비트-동기beat-synchronous-특징-표현",
    "title": "7.3. 비트와 펄스 추적",
    "section": "비트-동기(Beat-Synchronous) 특징 표현",
    "text": "비트-동기(Beat-Synchronous) 특징 표현\n\n비트 정보를 기반으로 하는 적응 윈도우 기술은 많은 음악 분석 및 검색 응용에서 특히 중요하다. 이 경우 윈도우 영역은 2개의 연속된 비트 위치에 의해 결정되므로 비트당 하나의 특징 벡터가 생성된다.\n이러한 비트-동기(beat-synchronous) 특징 표현은 물리적 시간 축(초 단위)이 아닌 음악적 시간 축(비트 단위)을 소유한다는 이점이 있다. 이렇게 하면 템포(BPM 단위)의 차이에 대한 특징 표현이 견고해진다.\n음악 동기화(music synchronization)의 맥락에서 비트 동기화 기능을 사용하면 DTW와 같은 정렬 기술이 쓸모 없게 될 수 있다(적어도 완벽한 비트 위치를 갖는 이상적인 경우). 예를 들어, 비트 위치를 아는 것은 이미 동일한 악보를 따르는 두 가지 다른 연주의 비트별 동기화를 생성한다.\n실제로 이러한 전략은 특히 비트 위치가 자동으로 결정될 때 주의해서 다루어야 한다. 일반적으로 자동화된 비트 추적 절차는 타악기의 온셋과 일정한 템포의 음악에 적합하다. 그러나 로컬 템포 변화가 있는 약한 음 온셋 및 표현력이 풍부한 음악을 처리할 때, 자동화된 비트 위치 생성은 템포 옥타브 혼란과 관련된 문제는 물론이고 오류가 발생하기 쉬운 작업이 된다.\n특징 추출 단계에서 손상된 비트 정보를 사용하면 이후에 해결해야 할 음악 처리 작업에 막대한 영향을 미칠 수 있다. 예를 들어, 음악 동기화 맥락에서 비트 추적 오류를 보상하기 위해 DTW와 같은 오류 허용 기술을 다시 도입해야 할 수 있다.\n\n예시: Shostakovich\n\n비트-동기 피처를 사용하는 예로 Shostakovich 발췌부분을 사용한다.\n\n\nfn_wav = 'FMP_C6_F07_Shostakovich_Waltz-02-Section_IncreasingTempo.wav'\nx, Fs = librosa.load(path_data+fn_wav) \n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=2048, H=512, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\nL_nov = len(nov)\nN_nov = 500\nH_nov = 10\nTheta = np.arange(30, 601)\nX, T_coef, F_coef_BPM = compute_tempogram_fourier(nov, Fs=Fs_nov, \n                                                            N=500, H=10, Theta=Theta)\nnov_PLP = compute_plp(X, Fs_nov, L_nov, N_nov, H_nov, Theta)\npeaks, properties = signal.find_peaks(nov_PLP, prominence=0.02)\nB_adapt_sec = peaks / Fs_nov\n\nN, H = 1024, 512\nX = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, center=True, window='hann')\nY = np.abs(X)\nC = librosa.feature.chroma_stft(S=Y, sr=Fs, norm=1, hop_length=H, n_fft=N)\nB_fix_frame = (B_adapt_sec*Fs/H).astype(int)\nC_adapt = librosa.util.sync(C, B_fix_frame, aggregate=np.mean)\n\n\nipd.display(Audio(data=x,rate=Fs))\n# Visualization\nfig, ax = plt.subplots(5, 2, gridspec_kw={'width_ratios': [1, 0.03], \n                                          'height_ratios': [1, 2, 1, 1, 2]}, figsize=(10,8)) \n\nplot_signal(x, Fs, ax=ax[0,0], title='Signal')\nax[0,1].set_axis_off()\n    \nplot_matrix(C, ax=[ax[1,0], ax[1,1]], xlabel='Time (frames)', \n                     title='Chromgram with fixed-size windowing (frame rate = %0.1f Hz)'%(Fs/H))\n\nplot_signal(nov_PLP, Fs_nov, color='k', ax=ax[2,0],\n                    title='PLP-based novelty function with estimated beat positions')\nax[2,1].set_axis_off()\nplot_beat_grid(B_adapt_sec, ax[2,0])\n\nplot_signal(x, Fs, ax=ax[3,0],\n                    title='Signal with beat positions')\nax[3,1].set_axis_off()\nplot_beat_grid(B_adapt_sec, ax[3,0]) \n\nplot_matrix(C_adapt, ax=[ax[4,0], ax[4,1]], xlabel='Time (frames)',\n                    title='Beat-synchronous chromagram')\n\nplt.tight_layout()\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#트랜지언트-제거-transient-removal",
    "href": "posts/7. Tempo and Beat Tracking/7.3.Beat_and_Pulse_Tracking.html#트랜지언트-제거-transient-removal",
    "title": "7.3. 비트와 펄스 추적",
    "section": "트랜지언트 제거 (Transient Removal)",
    "text": "트랜지언트 제거 (Transient Removal)\n\n다음으로 오디오 특징의 품질을 개선하기 위해 온셋 및 비트 위치에 대한 지식을 다른 방식으로 사용할 수 있는 방법을 보자.\n특히 피아노, 기타 또는 타악기와 같은 악기의 경우 음 온셋이 전체 스펙트럼에 걸쳐 퍼지는 트랜지언트(transients)(노이즈와 같은 에너지 폭발)을 동반하는 경우가 많다. 트랜지언트가 음 온셋 위치를 감지하는 데 유용하지만 화성 또는 멜로디 정보를 캡처하는 피처에서 원하지 않는 부산물을 유발할 수 있다.\n화성 정보를 유지하면서 이러한 잡음과 같은 부산물을 제거하기 위한 한 가지 방법은 각 음 온셋 위치 주변의 이웃을 제외하는 것이다. 이를 위해 이웃의 크기를 결정하는 \\(\\lambda\\in\\mathbb{R}\\), \\(0<\\lambda<1\\) 매개변수를 도입한다. \\(s,t\\in[1:N]\\)가 주어진 적응형 윈도우의 시작 및 끝 위치를 나타낸다고 하자. 그러면 다음과 같이 정의할 수 있다. \\[s_\\lambda := s + \\left\\lfloor \\frac{1-\\lambda}{2}(t-s)\\right\\rfloor \\,\\,\\,\\mbox{and}\\,\\,\\,\n    t_\\lambda := t - \\left\\lfloor \\frac{1-\\lambda}{2}(t-s)\\right\\rfloor,\\]\n\n이는 피처 계산에 사용되는 단축된 윈도우의 시작 및 끝 위치를 결정한다.\n\n이 정의를 사용하면 적응형 윈도우의 중심이 유지되는 반면 크기는 원래 크기 \\((t-s)\\)에 비해 \\(\\lambda\\)만큼 줄어든다.\n다음 코드 셀의 예에서는 특징 표현으로 \\(\\lambda=1\\)를 한 번 사용하고 \\(\\lambda=0.5\\)를 한 번 사용하여 적응형 윈도우를 사용한 크기 스펙트럼을 보여준다. 그러나 이 예에서는 신호 트랜지언트 현상을 제거했기 때문에 노이즈 제거 효과를 거의 볼 수 없다.\n\n\ndef adaptive_windowing(X, B, neigborhood=1, add_start=False, add_end=False):\n    \"\"\"Apply adaptive windowing [FMP, Section 6.3.3]\n\n    Args:\n        X (np.ndarray): Feature sequence\n        B (np.ndarray): Beat sequence (spefied in frames)\n        neigborhood (float): Parameter specifying relative range considered for windowing (Default value = 1)\n        add_start (bool): Add first index of X to beat sequence (if not existent) (Default value = False)\n        add_end (bool): Add last index of X to beat sequence (if not existent) (Default value = False)\n\n    Returns:\n        X_adapt (np.ndarray): Feature sequence adapted to beat sequence\n        B_s (np.ndarray): Sequence specifying start (in frames) of window sections\n        B_t (np.ndarray): Sequence specifying end (in frames) of window sections\n    \"\"\"\n    len_X = X.shape[1]\n    max_B = np.max(B)\n    if max_B > len_X:\n        print('Beat exceeds length of features sequence (b=%d, |X|=%d)' % (max_B, len_X))\n        B = B[B < len_X]\n    if add_start:\n        if B[0] > 0:\n            B = np.insert(B, 0, 0)\n    if add_end:\n        if B[-1] < len_X:\n            B = np.append(B, len_X)\n    X_adapt = np.zeros((X.shape[0], len(B)-1))\n    B_s = np.zeros(len(B)-1).astype(int)\n    B_t = np.zeros(len(B)-1).astype(int)\n    for b in range(len(B)-1):\n        s = B[b]\n        t = B[b+1]\n        reduce = np.floor((1 - neigborhood)*(t-s+1)/2).astype(int)\n        s = s + reduce\n        t = t - reduce\n        if s == t:\n            t = t + 1\n        X_slice = X[:, range(s, t)]\n        X_adapt[:, b] = np.mean(X_slice, axis=1)\n        B_s[b] = s\n        B_t[b] = t\n    return X_adapt, B_s, B_t\n\n\ndef compute_plot_adaptive_windowing(x, Fs, H, X, B, neigborhood=1, add_start=False, add_end=False):\n    \"\"\"Compute and plot process for adaptive windowing [FMP, Section 6.3.3]\n\n    Args:\n        x (np.ndarray): Signal\n        Fs (scalar): Sample Rate\n        H (int): Hop size\n        X (int): Feature sequence\n        B (np.ndarray): Beat sequence (spefied in frames)\n        neigborhood (float): Parameter specifying relative range considered for windowing (Default value = 1)\n        add_start (bool): Add first index of X to beat sequence (if not existent) (Default value = False)\n        add_end (bool): Add last index of X to beat sequence (if not existent) (Default value = False)\n\n    Returns:\n        X_adapt (np.ndarray): Feature sequence adapted to beat sequence\n    \"\"\"\n    X_adapt, B_s, B_t = adaptive_windowing(X, B, neigborhood=neigborhood,\n                                           add_start=add_start, add_end=add_end)\n\n    fig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.03],\n                                              'height_ratios': [1, 3]}, figsize=(10, 4))\n\n    plot_signal(x, Fs, ax=ax[0, 0], title=r'Adaptive windowing using $\\lambda = %0.2f$' % neigborhood)\n    ax[0, 1].set_axis_off()\n    plot_beat_grid(B_s * H / Fs, ax[0, 0], color='b')\n    plot_beat_grid(B_t * H / Fs, ax[0, 0], color='g')\n    plot_beat_grid(B * H / Fs, ax[0, 0], color='r')\n    for k in range(len(B_s)):\n        ax[0, 0].fill_between([B_s[k] * H / Fs, B_t[k] * H / Fs], -1, 1, facecolor='red', alpha=0.1)\n\n    plot_matrix(X_adapt, ax=[ax[1, 0], ax[1, 1]], xlabel='Time (frames)', ylabel='Frequency (bins)')\n    plt.tight_layout()\n    return X_adapt\n\n\nfn_wav = 'FMP_C6_F24_ScaleMajorC-middle.wav'\nx, Fs = librosa.load(path_data+fn_wav) \nipd.display(Audio(data=x,rate=Fs))\n\nN, H = 1024, 256\nX = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, center=True, window='hann')\nX = X[0:100,:]\nY = np.log(1+ 10*np.abs(X))\n\nnov, Fs_nov = compute_novelty_spectrum(x, Fs=Fs, N=512, H=256, \n                                                 gamma=100, M=10, norm=True)\nnov, Fs_nov = resample_signal(nov, Fs_in=Fs_nov, Fs_out=100)\n\npeaks, properties = signal.find_peaks(nov, prominence=0.2)\nB_adapt_sec = peaks/Fs_nov\n\nadd_start = False\nadd_end = False\nneigborhood = 1\nB_adapt_frame = np.round(B_adapt_sec*Fs/H).astype(int)\nY_adapt = compute_plot_adaptive_windowing(x, Fs, H, Y, B_adapt_frame, \n                        neigborhood=neigborhood, add_start=add_start, add_end=add_end)\nneigborhood = 0.5\nY_adapt = compute_plot_adaptive_windowing(x, Fs, H, Y, B_adapt_frame, \n                        neigborhood=neigborhood, add_start=add_start, add_end=add_end)\n\nadd_start = True\nadd_end = True\nneigborhood = 0.5\nY_adapt = compute_plot_adaptive_windowing(x, Fs, H, Y, B_adapt_frame, \n                        neigborhood=neigborhood, add_start=add_start, add_end=add_end)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html",
    "href": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html",
    "title": "8.1. 내용 기반 오디오 검색: 개요",
    "section": "",
    "text": "내용 기반 오디오 검색(Content-Based Audio Retrieval)에 대해 자세히 다루기 전에 간단히 그 개요를 살펴본다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#쿼리별-예제-query-by-example",
    "href": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#쿼리별-예제-query-by-example",
    "title": "8.1. 내용 기반 오디오 검색: 개요",
    "section": "쿼리별 예제 (Query-By-Example)",
    "text": "쿼리별 예제 (Query-By-Example)\n\n많은 콘텐츠-기반 검색 전략은 쿼리별 예제(query-by-example) 패러다임을 따른다. 음악 표현 또는 그 일부(쿼리 또는 예제로 사용됨)가 주어지면, 음악 컬렉션에서 쿼리와 유사한 부분 또는 측면을 포함하는 문서를 자동으로 검색하는 것이다. 이 패러다임을 기반으로 세 가지 콘텐츠 기반 검색 작업의 예를 보자.\n\n쿼리로 작은 오디오가 주어지면 오디오 식별(audio identification)(오디오 핑거프린팅(fingerprinting)이라고도 함) 작업은 쿼리의 원천인 특정 오디오 녹음을 식별하는 것으로 구성된다. 예를 들어, 베토벤 교향곡 5번의 Bernstein의 연주 녹음의 작은 발췌가 주어지면, 목표는 이 녹음을 정확히 식별하는 것이다.\n핑거프린팅 시스템은 Karajan의 연주 녹음과 같은 동일한 작품의 다른 녹음을 식별할 수 없다. 이것이 바로 오디오 매칭(audio matching)이라는 검색 작업의 목표이다. 쿼리 조각이 주어지면 오디오 매칭의 목표는 해당되는 모든 오디오 발췌 부분을 음악적으로 검색하는 것이다. 이 시나리오에서는 일반적으로 다른 연주 및 음악 편곡에서 발생하는 변형을 명시적으로 허용한다.\n유사성의 개념을 더욱 부드럽게 하는 버전 식별(version identification)(때때로 커버 곡 검색(cover song identification)이라고도 함) 작업은 악기 편성 및 템포뿐만 아니라 일반적으로 리믹스 및 커버 곡에서 발생하는 음악 구조, 키 또는 멜로디와 관련하여 더 극단적인 변형을 고려한다.\n마지막으로 카테고리 기반(category-based) 검색 시나리오(장르 분류(genre classification) 포함)의 유사성 관계는 다소 모호하며 문화적 또는 음악학적 범주를 표현한다.\n\n\n\nImage(path_img + \"FMP_C7_F01_BeetFifth.png\", width=500)\n\n\n\n\n\n\n\n\nMetadata\n\n\nAudio\n\n\n\n\nBeethoven: Symphony No. 5 (Bernstein)\n\n\n\n\n\n\n\n\nBeethoven: Symphony No. 5 (Karajan)\n\n\n\n\n\n\n\n\nBeethoven: Symphony No. 5 (Gould)\n\n\n\n\n\n\n\n\nBeethoven: Symphony No. 9 (Bernstein)\n\n\n\n\n\n\n\n\nBeethoven: Symphony No. 3 (Blomstedt)\n\n\n\n\n\n\n\n\nHaydn: Symphony No. 94"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#특이성과-세분성-specificity-and-granularity",
    "href": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#특이성과-세분성-specificity-and-granularity",
    "title": "8.1. 내용 기반 오디오 검색: 개요",
    "section": "특이성과 세분성 (specificity and granularity)",
    "text": "특이성과 세분성 (specificity and granularity)\n\n내용 기반 검색 전략은 특이성(specificity) 및 세분성(granularity)에 따라 느슨하게 분류할 수 있다.\n\n검색 시스템의 특이성은 쿼리와 검색할 데이터베이스 문서 간의 유사성 정도를 의미한다. 특이성이 높은 검색 시스템은 쿼리의 정확하거나 가까운 복사본을 반환하는 반면, 낮은 특이성의 검색 시스템은 수치적 관점에서 원래 쿼리와 상당히 다를 수 있는 항목을 반환한다.\n세분성은 검색 시나리오에서 고려되는 시간적 수준을 나타낸다. 조각 수준(fragment-level) 검색 시나리오에서 쿼리는 오디오 녹음의 짧은 조각으로 구성되며 목표는 데이터베이스 문서에 포함된 모든 관련 조각(fragement)을 검색하는 것이다. 예를 들어, 이러한 조각은 오디오 내용의 몇 초를 커버하거나 모티프, 테마 또는 음악 파트에 해당할 수 있다. 반면 문서 수준(document-level) 검색에서는 쿼리가 전체 문서의 특성을 반영하여 데이터베이스의 전체 문서와 비교한다. 이 경우 전체 유사성 점수가 여전히 로컬 조각 수준 비교를 기반으로 할 수 있지만 유사성 개념이 더 거친 경향이 있다.\n\n다양한 내용 기반 검색 시나리오는 특이성과 세분성에 따라 특이성-세분성 평면에 느슨하게 배열될 수 있다. 다음 그림에서 세 가지 시나리오(오디오 식별, 오디오 매칭 및 버전 식별)는 여러 관련 검색 시나리오를 포함하는 구름으로 표시된다. 카테고리 기반 검색 시나리오를 나타내는 네 번째 구름도 추가된다. 이 분류법은 너무 단순할 수 있지만, 미묘하지만 중요한 차이점을 설명하면서 다양한 검색 패러다임에 대한 직관적인 개요를 제공한다.\n\n\nImage(path_img+\"FMP_C7_F22_small.png\", width=600)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#음악의-버전versions",
    "href": "posts/8. Content-Based Audio Retrieval/8.1.Introduction.html#음악의-버전versions",
    "title": "8.1. 내용 기반 오디오 검색: 개요",
    "section": "음악의 버전(versions)",
    "text": "음악의 버전(versions)\n\n서양 문화에서 음악 작품(piece of music)이라고 하면 일반적으로 악보로 주어지거나 녹음된 트랙의 형태로 주어진 특정 구성을 생각한다. 종종 음악 작품의 기원을 거슬러 올라가 작곡가나 음악 그룹의 이름과 연관시킬 수 있다. 예를 들어 베토벤이 작곡한 교항곡 5번 악보는 교향곡 5번의 원본으로 간주된다. 또는 노래 “Yellow Submarine”의 원 버전은 의심할 여지 없이 영국 록 밴드 “The Beatles”의 녹음이다.\n그러나 일반적으로 음악의 경우 이러한 견해는 다소 단순하며 “원본 버전”이라는 용어는 말할 것도 없고 “piece of music”이라는 용어를 사용하는 것은 문제가 될 수 있다. 다음에서는 단순한 관점을 채택하고 다소 느슨한 방식으로 “piece of music” 및 “버전(version)”이라는 용어를 사용한다. 이러한 용어에 대한 형식적인 정의를 내리기보다는 서양 음악의 맥락에서 몇 가지 전형적인 예를 살펴보도록 한다.\n편곡(arrangement)이란 원래 악보에 표기된 악기와 다른 악기로 연주할 수 있도록 악보를 다시 작업하는 것을 말한다.\n피아노 편곡(piano transcription)은 교향곡과 실내악을 편곡하여 한 대 또는 두 대의 피아노로 연주할 수 있도록 한 것이다.\n쿼들리벳(quodlibet)(15세기에 작곡)은 다양한 멜로디, 일반적으로 대중적인 민속 음악이 유머러스한 방식으로 결합된 음악 작품을 말한다.\n포푸리(potpourri)(19세기에 작곡)는 인기 있는 오페라, 오페레타 또는 노래의 개별 섹션이 강력한 연관성이나 관계 없이 단순히 병치된 작품이다.\n메들리(medley)는 기존 곡의 일부, 일반적으로 노래에서 가장 기억에 남는 부분을 차례로 연주하며, 중첩하여 구성한 작품이다.\nDJ 믹스는 DJ가 기존 오디오 자료를 조작, 블렌딩 및 믹싱(예: 턴테이블 스크래칭을 적용하여 원본 녹음에 혼합된 타악기 사운드를 생성)함으로써 공연 뮤지션 역할을 하는 버전이다.\n샘플링(sampling)은 하나의 녹음에서 일부 또는 샘플을 가져와 다른 곡에서 “새로운” 악기로 재사용하는 기술을 말한다.\n리믹스(remix)는 원래 버전과 다르게 들리도록 편집되었거나 완전히 다시 만들어진 녹음이다. 이러한 수정은 강약, 피치, 템포 및 재생 시간의 변경에서 음성 및 악기 트랙의 완전한 재배열에 이르기까지 다양할 수 있다.\n리마스터링(remastering)의 목표는 기존 녹음의 음질을 향상시키는 것이다.\n매시업(mash-up)은 한 곡의 보컬 트랙을 다른 곡의 악기 트랙 위에 매끄럽게 오버레이하여 생성되는 새로운 구성이다.\n사운드 콜라주(sound collage)는 기존 녹음 또는 음악의 일부를 결합하여 얻은 구성으로, 시각 예술의 콜라주와 유사하다.\n인용문(quotation)은 음악에서 멜로디나 주제와 같은 기존 음악 자료를 새로운 구성에 사용하는 것을 의미한다.\n변주(variation)는 음악 자료가 하모니, 멜로디, 리듬, 음색, 오케스트레이션 또는 이들의 조합을 포함할 수 있는 변화와 함께 변경된 형태로 반복되는 경우를 말한다.\n패러디(parody)는 원작을 조롱하고 하찮게 모방하는 것입니다.\n커버(cover) 버전 또는 커버 곡은 원곡자가 아닌 다른 사람이 이전에 발표한 곡을 새롭게 퍼포먼스 하는 것을 의미한다.\n이들은 음악 작품의 “버전”의 일부 예일 뿐이다. 버전은 음색, 악기 편성, 템포, 키, 하모니, 멜로디, 가사 및 음악 구조의 상당한 변화를 포함하여 여러 면에서 원본 녹음과 다를 수 있다. 예를 들어, 베토벤 교향곡 5번의 버전을 찾을 때 많은 음이 수정되고, 원래 구조의 대부분이 손실된, 예를 들어 록 밴드의 라이브 공연을 검색하는 데 관심이 있을 수 있다. 톤과 템포의 급격한 변화에도 불구하고 수정된 버전에서도 빛을 발하는 원래 구성의 특징적인 선율, 화성 또는 리듬 요소를 통해 원래 구성을 여전히 인식할 수 있다.\n커버 송의 예를 보자. \n\n\n <td style=\"border:none;text-align:left;\">Apocalyptica: Enter Sandman</td><td style=\"border:none;text-align:left;\">Nirvana: Poly (Unplugged)</td><td style=\"border:none;text-align:left;\">Cindy & Bert: Der Hund von Baskerville</td><td style=\"border:none;text-align:left;\">AC/DC: High Voltage (live)</td>\n\n\nSong Name\n\n\nOriginal Version\n\n\nModification\n\n\nCover Version\n\n\nSong Name\n\n\n\n\nBob Dylan: Knockin’ On Heaven’s Door\n\n\n\n\n\n\nKey\n\n\n\n\n\n\nAvril Lavigne: Knockin’ On Heaven’s\n\n\n\n\nMetallica: Enter Sandman\n\n\n\n\n\n\nTimbre\n\n\n\n\n\n\n\n\n\nNirvana: Poly (Incesticide Album)\n\n\n\n\n\n\nTempo\n\n\n\n\n\n\n\n\nBlack Sabbath: Paranoid\n\n\n\n\n\n\nLyrics\n\n\n\n\n\n\n\n\nAC/DC: High Voltage\n\n\n\n\n\n\nRecording conditions\n\n\n\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7_ContentBasedAudioRetrieval.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html",
    "title": "8.2. 오디오 식별",
    "section": "",
    "text": "내용 기반 오디오 검색 중 쿼리와 일치하는 오디오 녹음을 찾는 오디오 식별(audio identification)에 대해 설명한다. 스펙트럼 피크와 매칭 함수, 컨스텔레이션 맵(constellation map) 등에 대해 다룬다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#일반적-요구사항-general-requirements",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#일반적-요구사항-general-requirements",
    "title": "8.2. 오디오 식별",
    "section": "일반적 요구사항 (General Requirements)",
    "text": "일반적 요구사항 (General Requirements)\n\n실제 음악 인식 시스템은 강력하고 계산적으로 효율적이어야 하므로 해결해야 할 기술적 과제가 많다. 특히 이러한 시스템에 사용되는 오디오 지문(fingerprints)은 높은 특이성, 견고성, 소형화 및 확장성을 비롯한 특정 요구 사항을 충족해야 한다.\n\nSpecificity(특이성): 오디오 지문은 몇 초의 오디오 조각으로도 해당 녹음을 안정적으로 식별하고 수백만 개의 다른 녹음과 구별할 수 있도록 높은 특이성을 가져야 한다.\nRobustness(견고성): 신뢰할 만한 식별을 위해 지문은 손실 압축, 피치 이동, 시간 스케일링, 이퀄라이제이션 또는 동적 압축과 같은 배경 잡음 및 신호 왜곡에 대해 견고해야 한다.\nCompactness(소형화): 오디오 지문은 대역폭(bandwidth)이 제한된 채널을 통해서도 전송될 수 있고 데이터베이스 측에 쉽게 저장 및 인덱싱될 수 있도록 하는 작은 크기여야 한다.\nScalability(확장성): 수백만 개의 녹음으로 확장하려면 오디오 지문 계산이 간단하고 효율적이어야 한다. 이는 처리 능력이 제한된 모바일 장치에서 지문을 계산할 때도 필요한 요구 사항이다.\n\n특정 요구 사항을 개선하는 것은 종종 다른 요구 사항의 성능 저하를 의미하며 모순되는 원칙 간의 트레이드-오프에 직면해야 한다.\n예를 들어 견고성을 높이면 일반적으로 잘못된 식별(오탐)이 증가하여 식별 시스템의 정확도가 떨어진다. 유사하게, 연산 및 소형화에서 유리하지만, 지문 크기의 과도한 감소는 식별 능력에 부정적인 영향을 미친다. 반대로 특이성과 견고성이 높은 지문은 계산에 광범위한 처리 능력이 필요한 경우 실제로 사용하지 못할 수 있다.\n\n\n\n\n\nShort name\n\n\nType of distortion\n\n\nAudio\n\n\n\n\nOriginal\n\n\nOriginal song\n\n\n\n\n\n\n\n\nTalking\n\n\nSuperposition with other sources (e.g., people talking in background)\n\n\n\n\n\n\n\n\nNoise\n\n\nSuperposition with Gaussian noise\n\n\n\n\n\n\n\n\nCoding\n\n\nStrong coding artifacts\n\n\n\n\n\n\n\n\nFaster\n\n\nTime scale modification (faster)\n\n\n\n\n\n\n\n\nHigher\n\n\nPitch shifting (higher)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#스펙트럼-피크spectral-peaks-기반의-오디오-지문",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#스펙트럼-피크spectral-peaks-기반의-오디오-지문",
    "title": "8.2. 오디오 식별",
    "section": "스펙트럼 피크(Spectral Peaks) 기반의 오디오 지문",
    "text": "스펙트럼 피크(Spectral Peaks) 기반의 오디오 지문\n\n이제 스펙트럼 피크의 개념을 기반으로 오디오 지문을 설명한다. 시간-주파수 평면의 특징적인 포인트이기 때문에 이러한 피크는 노이즈 및 추가 음원이 있는 경우에도 변경되지 않는 경우가 많다. Avery Wang이 소개한 피크 기반 지문은 이제 상업용 오디오 식별 시스템에서 성공적으로 사용되고 있다.\n샘플링된 파형 형태의 오디오 신호가 주어지면 지문을 도출하는 첫 번째 단계는 STFT \\(\\mathcal{X}\\)를 계산하는 것이다. \\(\\mathcal{X}(n,k)\\)는 \\(n^{\\mathrm{th}}\\) 시간 프레임에 대한 \\(k^{\\mathrm{th}}\\) 푸리에 계수를 나타낸다. (\\(k\\in [0:K]\\) 및 \\(n\\in\\mathbb{Z}\\))\n이하에서 \\(k\\in[0:K]\\) 요소는 주파수 스탬프(frequency stamp)라고도 하고, \\(n\\in\\mathbb{Z}\\) 요소는 타임 스탬프(time stamp)라고도 한다. 따라서 푸리에 계수 \\(\\mathcal{X}(n,k)\\)의 좌표는 타임 스탬프 \\(n\\) 및 주파수 스탬프 \\(k\\)로 구성된 시간-주파수 점 \\((n,k)\\in\\mathbb{Z}\\times[0:K]\\)로 지정된다.\n지문 계산의 두 번째 단계에서 신호의 STFT 표현은 희소한(sparse) 시간-주파수 포인트 집합으로 축소된다.\n이를 위해 각 지점 주변의 영역 내 모든 이웃보다 더 큰 크기를 갖는 시간-주파수 지점을 식별하는 피크 선택 전략을 사용한다. 보다 정확하게는 \\(\\tau\\in\\mathbb{N}\\) 및 \\(\\kappa\\in\\mathbb{N}\\)를 각각 시간 및 주파수 방향에서 이웃의 크기를 결정하는 매개변수라고 한다. 그런 다음 다음과 같은 경우 점 \\((n_0,k_0)\\)이 피크로 선택된다. \\[|X(n_0,k_0)| \\geq  |X(n,k)|\\] for all \\((n,k) \\in \\big(\\left[n_0-\\tau:n_0+\\tau\\right] \\times \\left[k_0-\\kappa:k_0+\\kappa\\right]\\big)\\).\n이러한 정의는 스펙트로그램과 추출된 하나의 피크와 그것의 로컬 이웃을 보여주는 다음 그림에 설명되어 있다. 이웃의 크기를 늘리면 점을 선택하기가 더 어려워진다. 따라서 \\(\\tau\\) 및 \\(\\kappa\\) 매개변수를 사용하여 선택한 피크의 밀도를 조정하고 시간-주파수 평면의 상당히 균일한 커버리지를 얻을 수 있다.\n\n\nImage(path_img+\"FMP_C7_F03a.png\", width=300)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#constellation-map",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#constellation-map",
    "title": "8.2. 오디오 식별",
    "section": "Constellation Map",
    "text": "Constellation Map\n\n피크 선택 단계는 신호의 복잡한 스펙트로그램 표현을 희소한 좌표 세트로 줄인다. 피크의 크기는 더 이상 사용되지 않으며 피크의 시간 및 주파수 스탬프만 고려되어 높은 수준의 견고성을 제공한다. 모든 피크 좌표로 구성된 표현을 constellation map이라고 한다.\n다음 코드 셀에서 주어진 이웃 매개변수 \\(\\tau\\) 및 \\(\\kappa\\)에 대한 constellation map을 추출하기 위한 다소 나이브한 구현을 본다. 이 구현에서는 모든 시간-주파수 지점을 반복하고 이웃 조건(\\(\\kappa\\) 및 \\(\\tau\\)로 지정)이 충족되는지 확인한다. 그림은 위의 Beatles 예제에 대한 constellation map(빨간색으로 표시된 추출된 피크 지문)과 함께 스펙트로그램 표현을 보여준다.\n\n\ndef compute_constellation_map_naive(Y, dist_freq=7, dist_time=7, thresh=0.01):\n    \"\"\"Compute constellation map (naive implementation)\n    Args:\n        Y (np.ndarray): Spectrogram (magnitude)\n        dist_freq (int): Neighborhood parameter for frequency direction (kappa) (Default value = 7)\n        dist_time (int): Neighborhood parameter for time direction (tau) (Default value = 7)\n        thresh (float): Threshold parameter for minimal peak magnitude (Default value = 0.01)\n\n    Returns:\n        Cmap (np.ndarray): Boolean mask for peak structure (same size as Y)\n    \"\"\"\n    # spectrogram dimensions\n    if Y.ndim > 1:\n        (K, N) = Y.shape\n    else:\n        K = Y.shape[0]\n        N = 1\n    Cmap = np.zeros((K, N), dtype=np.bool8)\n\n    # loop over spectrogram\n    for k in range(K):\n        f1 = max(k - dist_freq, 0)\n        f2 = min(k + dist_freq + 1, K)\n        for n in range(N):\n            t1 = max(n - dist_time, 0)\n            t2 = min(n + dist_time + 1, N)\n            curr_mag = Y[k, n]\n            curr_rect = Y[f1:f2, t1:t2]\n            c_max = np.max(curr_rect)\n            if ((curr_mag == c_max) and (curr_mag > thresh)):\n                Cmap[k, n] = True\n    return Cmap\n\n\ndef plot_constellation_map(Cmap, Y=None, xlim=None, ylim=None, title='',\n                           xlabel='Time (sample)', ylabel='Frequency (bins)',\n                           s=5, color='r', marker='o', figsize=(7, 3), dpi=72):\n    \"\"\"Plot constellation map\n    Args:\n        Cmap: Constellation map given as boolean mask for peak structure\n        Y: Spectrogram representation (Default value = None)\n        xlim: Limits for x-axis (Default value = None)\n        ylim: Limits for y-axis (Default value = None)\n        title: Title for plot (Default value = '')\n        xlabel: Label for x-axis (Default value = 'Time (sample)')\n        ylabel: Label for y-axis (Default value = 'Frequency (bins)')\n        s: Size of dots in scatter plot (Default value = 5)\n        color: Color used for scatter plot (Default value = 'r')\n        marker: Marker for peaks (Default value = 'o')\n        figsize: Width, height in inches (Default value = (7, 3))\n        dpi: Dots per inch (Default value = 72)\n\n    Returns:\n        fig: The created matplotlib figure\n        ax: The used axes.\n        im: The image plot\n    \"\"\"\n    if Cmap.ndim > 1:\n        (K, N) = Cmap.shape\n    else:\n        K = Cmap.shape[0]\n        N = 1\n    if Y is None:\n        Y = np.zeros((K, N))\n    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n    im = ax.imshow(Y, origin='lower', aspect='auto', cmap='gray_r', interpolation='nearest')\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n    Fs = 1\n    if xlim is None:\n        xlim = [-0.5/Fs, (N-0.5)/Fs]\n    if ylim is None:\n        ylim = [-0.5/Fs, (K-0.5)/Fs]\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    n, k = np.argwhere(Cmap == 1).T\n    ax.scatter(k, n, color=color, s=s, marker=marker)\n    plt.tight_layout()\n    return fig, ax, im\n    \n    \ndef compute_spectrogram(fn_wav, Fs=22050, N=2048, H=1024, bin_max=128, frame_max=None):\n    x, Fs = librosa.load(fn_wav)\n    x_duration = len(x) / Fs\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    if bin_max is None:\n        bin_max = X.shape[0]\n    if frame_max is None:\n        frame_max = X.shape[0]\n    Y = np.abs(X[:bin_max, :frame_max])\n    return Y\n\n\nwav_dict = {}\nwav_dict['Original'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4.wav'\nwav_dict['Noise'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4_NoiseSNR-9.wav'\nwav_dict['Talking'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4_Talking.wav'\nwav_dict['Coding'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4_16kbits.wav'\nwav_dict['Faster'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4_Faster.wav'\nwav_dict['Higher'] = 'FMP_C7_Audio_Beatles_ActNaturally_63-3_73-4_Higher.wav'\n\n\nfn_wav = wav_dict['Original']\nY = compute_spectrogram(path_data+fn_wav)\n\ndist_freq = 11  # kappa: neighborhood in frequency direction\ndist_time = 3   # tau: neighborhood in time direction\ntitle=r'Constellation map using $\\kappa=%d$, $\\tau=%d$' % (dist_freq, dist_time)\nCmap_naive = compute_constellation_map_naive(Y, dist_freq, dist_time)\nfig, ax, im = plot_constellation_map(Cmap_naive, np.log(1 + 1 * Y), \n                                     color='r', s=30, title=title)\n\n\n\n\n나이브하고 반복적인 구현에 대한 대안으로 다음 코드 셀에서 다차원 이미지 처리를 위한 Python 패키지 scipy.ndimage를 사용하여 훨씬 더 빠른 구현을 제공한다. 다음 코드 셀에서 두 구현의 결과(동일해야 함)와 런타임을 비교한다.\n\ndef compute_constellation_map(Y, dist_freq=7, dist_time=7, thresh=0.01):\n    \"\"\"Compute constellation map (implementation using image processing)\n    Args:\n        Y (np.ndarray): Spectrogram (magnitude)\n        dist_freq (int): Neighborhood parameter for frequency direction (kappa) (Default value = 7)\n        dist_time (int): Neighborhood parameter for time direction (tau) (Default value = 7)\n        thresh (float): Threshold parameter for minimal peak magnitude (Default value = 0.01)\n\n    Returns:\n        Cmap (np.ndarray): Boolean mask for peak structure (same size as Y)\n    \"\"\"\n    result = ndimage.maximum_filter(Y, size=[2*dist_freq+1, 2*dist_time+1], mode='constant')\n    Cmap = np.logical_and(Y == result, result > thresh)\n    return Cmap\n\n\ndist_freq = 7  # kappa: neighborhood in frequency direction\ndist_time = 3  # tau: neighborhood in time direction\n\nstart = time.time()\nCmap_naive = compute_constellation_map_naive(Y, dist_freq, dist_time)\nend = time.time()\nprint('Runtime of naive (iterative) implementation: %.8f seconds' % (end - start))\n\nstart = time.time()\nfor i in range(100):\n    Cmap = compute_constellation_map(Y, dist_freq, dist_time)\nend = time.time()\nprint('Runtime of fast (scipy.ndimage) implementation: %.8f seconds' % ((end - start)/100))\n\nfig, ax, im = plot_constellation_map(Cmap_naive, np.log(1 + 1 * Y), color='r', s=30)\nn, k = np.argwhere(Cmap == 1).T\nax.scatter(k, n, color='cyan', s=5, marker='o')\nplt.legend(['Naive implementation', 'Implementation using scipy.ndimage'], \n           loc='upper right', framealpha=1);\n\nRuntime of naive (iterative) implementation: 0.13366747 seconds\nRuntime of fast (scipy.ndimage) implementation: 0.00055850 seconds\n\n\n\n\n\n\nConstellation map의 견고성(robustness)\n\n앞서 설명한 바와 같이 지문 시스템은 배경 잡음과 신호 왜곡에 견고해야(robust) 한다. 다음 코드 셀에서는 원래 Beatles 예제(Original), 가우시안 노이즈가 중첩된 버전(Noise), 배경 노이즈가 있는 버전(Talking), 코딩 아티팩트(coding)에 의해 왜곡된 버전에 대한 constellation map을 보여준다.\n또한 이웃 매개변수 \\(\\kappa\\) 및 \\(\\tau\\)로 constellation map의 밀도를 제어할 수 있는 방법을 설명한다. 두 매개변수 설정에서 다음을 관찰할 수 있다.\n\n왜곡된 버전의 constellation map는 원본 버전과 일부 피크를 공유한다.\n왜곡된 버전의 피크는 때때로 원래 버전의 피크와 비교하여 시간 및/또는 주파수가 약간 이동한다.\n\n\n\ndef compare_constellation_maps_Beatles(dist_freq = 11, dist_time = 3, wav_dict=wav_dict):\n    fn_wav = wav_dict['Original']\n    Y = compute_spectrogram(path_data+fn_wav)\n    Cmap = compute_constellation_map(Y, dist_freq, dist_time)\n\n    fn_wav = wav_dict['Noise']\n    Y_noise = compute_spectrogram(path_data+fn_wav)\n    Cmap_noise = compute_constellation_map(Y_noise, dist_freq, dist_time)\n\n    fn_wav = wav_dict['Talking']\n    Y_noise = compute_spectrogram(path_data+fn_wav)\n    Cmap_talking = compute_constellation_map(Y_noise, dist_freq, dist_time)\n    \n    fn_wav = wav_dict['Coding']\n    Y_coding = compute_spectrogram(path_data+fn_wav)\n    Cmap_coding = compute_constellation_map(Y_coding, dist_freq, dist_time)\n\n    title=r'Constellation maps using $\\kappa=%d$, $\\tau=%d$' % (dist_freq, dist_time)\n    fig, ax, im = plot_constellation_map(Cmap, color='r', s=50,title=title)\n    \n    n, k = np.argwhere(Cmap_noise == 1).T\n    ax.scatter(k, n, color='cyan', s=20, marker='o')\n    \n    n, k = np.argwhere(Cmap_talking == 1).T\n    ax.scatter(k, n, color='blue', s=40, marker='+')\n\n    n, k = np.argwhere(Cmap_coding == 1).T\n    ax.scatter(k, n, color='k', s=30, marker='x')\n    plt.legend(['Original', 'Noise', 'Talking', 'Coding'], loc='upper right', framealpha=1)\n    return Cmap, Cmap_noise, Cmap_talking, Cmap_coding\n\ncompare_constellation_maps_Beatles(dist_freq = 11, dist_time = 3)\ncompare_constellation_maps_Beatles(dist_freq = 17, dist_time = 5);"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#피크-매칭의-평가-지표",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#피크-매칭의-평가-지표",
    "title": "8.2. 오디오 식별",
    "section": "피크 매칭의 평가 지표",
    "text": "피크 매칭의 평가 지표\n\n실제로는 왜곡이 있는 경우에도 (시간 및 주파수 방향에 약간의 허용 오차를 허용하면서)일치하는 작은 비율의 피크가 녹음을 성공적으로 식별하는 데 충분하다는 것이 밝혀졌다.\n이 관찰은 아래에서 설명할 매칭 절차의 기초이다. 이제 두 constellation map 간의 일치를 나타내는 몇 가지 평가 측정을 소개한다.\n첫 번째 constellation map은 레퍼런스(맞는 것으로 간주되는 피크)로 해석하고 두 번째 constellation map는 추정(추정되는 것으로 간주되는 피크)으로 해석한다. 이를 통해 true positives(TP), false positives(FP) 및 false negatives(FN)과 같은 용어를 사용할 수 있다.\n다음 코드 셀에서는 허용 오차(tolerance)를 어느 정도 허용하면서 TP, FP, FN 값을 측정하는 기능을 구현한다.\n\n\ndef match_binary_matrices_tol(C_ref, C_est, tol_freq=0, tol_time=0):\n    \"\"\"| Compare binary matrices with tolerance\n    | Note: The tolerance parameters should be smaller than the minimum distance of\n      peaks (1-entries in C_ref ad C_est) to obtain meaningful TP, FN, FP values\n\n    Args:\n        C_ref (np.ndarray): Binary matrix used as reference\n        C_est (np.ndarray): Binary matrix used as estimation\n        tol_freq (int): Tolerance in frequency direction (vertical) (Default value = 0)\n        tol_time (int): Tolerance in time direction (horizontal) (Default value = 0)\n\n    Returns:\n        TP (int): True positives\n        FN (int): False negatives\n        FP (int): False positives\n        C_AND (np.ndarray): Boolean mask of AND of C_ref and C_est (with tolerance)\n    \"\"\"\n    assert C_ref.shape == C_est.shape, \"Dimensions need to agree\"\n    N = np.sum(C_ref)\n    M = np.sum(C_est)\n    # Expand C_est with 2D-max-filter using the tolerance parameters\n    C_est_max = ndimage.maximum_filter(C_est, size=(2*tol_freq+1, 2*tol_time+1),\n                                       mode='constant')\n    C_AND = np.logical_and(C_est_max, C_ref)\n    TP = np.sum(C_AND)\n    FN = N - TP\n    FP = M - TP\n    return TP, FN, FP, C_AND\n\n\ndef compare_constellation_maps(fn_wav_D, fn_wav_Q, dist_freq = 11, dist_time = 5, \n                               tol_freq = 1, tol_time = 1):\n    Y_D = compute_spectrogram(fn_wav_D)\n    Cmap_D = compute_constellation_map(Y_D, dist_freq, dist_time)\n    Y_Q = compute_spectrogram(fn_wav_Q)\n    Cmap_Q = compute_constellation_map(Y_Q, dist_freq, dist_time)\n\n    TP, FN, FP, Cmap_AND = match_binary_matrices_tol(Cmap_D, Cmap_Q, \n                                                     tol_freq=tol_freq, tol_time=tol_time)\n    title=r'Matching result (tol_freq=%d and tol_time=%d): TP=%d, FN=%d, FP=%d' % \\\n        (tol_freq,tol_time, TP, FN, FP)\n    fig, ax, im = plot_constellation_map(Cmap_AND, color='green', s=200, marker='+', title=title)\n    n, k = np.argwhere(Cmap_D == 1).T\n    ax.scatter(k, n, color='r', s=50, marker='o')\n    n, k = np.argwhere(Cmap_Q == 1).T\n    ax.scatter(k, n, color='cyan', s=20, marker='o')\n    plt.legend(['Matches (TP)', 'Reference', 'Estimation'], loc='upper right', framealpha=1)\n    plt.tight_layout()\n    plt.show()\n\n\nfn_wav_D = path_data + wav_dict['Original']\n\nfn_wav_Q = path_data + wav_dict['Noise']\ntol_freq = 0\ntol_time = 0\nprint('====== Reference: Original; Estimation: Noise ======')\ncompare_constellation_maps(fn_wav_D, fn_wav_Q, tol_freq=tol_freq, tol_time=tol_time)\n\ntol_freq = 1\ntol_time = 1\nprint('====== Reference: Original; Estimation: Noise ======')\ncompare_constellation_maps(fn_wav_D, fn_wav_Q, tol_freq=tol_freq, tol_time=tol_time)\n\nfn_wav_Q = path_data+wav_dict['Coding']\ntol_freq = 1\ntol_time = 1\nprint('====== Reference: Original; Estimation: Coding ======')\ncompare_constellation_maps(fn_wav_D, fn_wav_Q, tol_freq=tol_freq, tol_time=tol_time)\n\nfn_wav_Q = path_data+wav_dict['Talking']\ntol_freq = 1\ntol_time = 1\nprint('====== Reference: Original; Estimation: Talking ======')\ncompare_constellation_maps(fn_wav_D, fn_wav_Q, tol_freq=tol_freq, tol_time=tol_time)\n\n====== Reference: Original; Estimation: Noise ======\n\n\n\n\n\n====== Reference: Original; Estimation: Noise ======\n\n\n\n\n\n====== Reference: Original; Estimation: Coding ======\n\n\n\n\n\n====== Reference: Original; Estimation: Talking ======"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#매칭-함수-matching-function",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#매칭-함수-matching-function",
    "title": "8.2. 오디오 식별",
    "section": "매칭 함수 (Matching Function)",
    "text": "매칭 함수 (Matching Function)\n\n\\(\\mathcal{Q}\\)라는 짧은 쿼리 오디오 조각과 \\(\\mathcal{D}\\)라는 데이터베이스 녹음을 고려해 생각해보자. 또한, \\(\\mathcal{C}(\\mathcal{Q})\\) 및 \\(\\mathcal{C}(\\mathcal{D})\\)는 \\(\\mathcal{Q}\\) 및 \\(\\mathcal{D}\\) 각각의 constellation map를 나타낸다.\n쿼리 \\(\\mathcal{Q}\\)가 녹음 \\(\\mathcal{D}\\)에 포함되어 있는 경우 constellation map \\(\\mathcal{C}(\\mathcal{Q})\\)는 constellation map \\(\\mathcal{C}(\\mathcal{D})\\) 내의 해당 섹션과 어느 정도 일치해야 한다. 직관적으로 오디오 식별의 기본 아이디어는 스트립 차트에 \\(\\mathcal{D}\\)의 map을 놓고 투명한 플라스틱 조각 위에 \\(\\mathcal{Q}\\)의 map를 배치하는 것이다.\n일치하는 포인트의 수가 많은 경우 쿼리가 문서에 포함된 것으로 간주된다. \\(\\mathcal{C}(\\mathcal{D})\\) 내에서 일치하는 위치의 적절한 시간 오프셋은 이동(shift)으로 주어진다.\n이제 대략적인 매칭 절차를 공식화하자. 피크 위치는 타임스탬프 \\(n\\in\\mathbb{Z}\\)와 주파수스탬프 \\(k\\in[0:K]\\)로 구성된 좌표 \\((n,k)\\)로 지정된다. constellation map은 그러한 좌표의 유한 집합이다. 이전과 마찬가지로 \\(\\mathcal{C}(\\mathcal{Q})\\subset \\mathbb{Z}\\times[0:K]\\) 및 \\(\\mathcal{C}(\\mathcal{D})\\subset \\mathbb {Z}\\times[0:K]\\)는 각각 쿼리 \\(\\mathcal{Q}\\) 및 문서 \\(\\mathcal{D}\\)의 constellation map이다.\n쿼리를 \\(m\\in\\mathbb{Z}\\) 위치만큼 이동하면 다음에 의해 정의된 constellation map \\(m+\\mathcal{C}(\\mathcal{Q})\\)가 생성된다. \\[m+\\mathcal{C}(\\mathcal{Q}) := \\left\\{(m+n,k) \\mid (n,k)\\in \\mathcal{C}(\\mathcal{Q}) \\right\\}\\]\n이동된 쿼리와 데이터베이스 문서 간에 일치하는 피크 좌표를 계산하려면 해당 constellation map을 교차하고 결과 집합의 크기를 결정하기만 하면 된다. 이는 다음에 의해 정의된 매칭 함수(matching function) \\(\\Delta_\\mathcal{C}:\\mathbb{Z}\\to\\mathbb{N}_0\\)를 생성한다. \\[\\Delta_\\mathcal{C}(m):= \\big| (m+\\mathcal{C}(\\mathcal{Q}))\\cap  \\mathcal{C}(\\mathcal{D}) \\big|\\] for \\(m\\in\\mathbb{Z}\\).\n일반적으로 쿼리와 데이터베이스 문서가 관련이 없을 때 (동시에) 일치하는 피크 위치의 수 \\(\\Delta_\\mathcal{C}(m)\\)는 \\(|\\mathcal{C}(\\mathcal{Q})|\\)에 비해 일반적으로 작다.\n쿼리가 데이터베이스 문서에 포함된 경우에만 매칭 함수가 일부 이동 인덱스 \\(m\\in\\mathbb{Z}\\)에 대해 큰 값 \\(\\Delta_\\mathcal{C}(m)\\)를 갖는다. 이 인덱스는 쿼리 \\(\\mathcal{Q}\\)와 \\(\\mathcal{D}\\)의 일치 섹션 사이의 시간 오프셋을 나타낸다.\n다음 코드 셀에서는 작은 편차(허용 오차 매개변수 tol_freq 및 tol_time으로 지정됨)도 허용하는 일반화된 매칭 함수 \\(\\Delta_\\mathcal{C}\\)에 대한 구현을 확인한다.\n\n\\(\\Delta_\\mathcal{C}(m)\\)를 계산할 때 이동된 쿼리 \\(m+\\mathcal{C}(\\mathcal{Q})\\)는 \\(\\mathcal{C}(\\mathcal{D})\\)와 비교된다. 이는 \\(m\\) 위치에서 쿼리와 길이가 같은 \\(\\mathcal{C}(\\mathcal{D})\\) 섹션을 잘라내고, 이 섹션을 \\(\\mathcal{C}(\\mathcal{Q})\\)와 비교하는 것과 같다.\n\\(\\mathcal{Q}\\)와 \\(\\mathcal{D}\\)의 constellation map를 비교할 때 일치하는 피크의 수를 봐야 한다. 이는 \\(\\mathcal{D}\\)를 참조로, \\(\\mathcal{Q}\\)를 추정으로 볼 때 true positive(TP)의 수에 해당한다.\n비교에서는 두 개의 허용 오차 매개변수인 tol_freq 및 tol_time이 있는 match_binary_matrices_tol 함수를 사용한다. 이러한 매개변수를 \\(0\\) 값으로 설정하면 위에서 소개한 대로 \\(\\Delta_\\mathcal{C}\\) 매칭 함수가 생성된다.\n\n\\(\\mathcal{D}\\)로 Original을 사용하고 \\(\\mathcal{Q}\\)로 Noise의 세그먼트 \\([100:149]\\)(샘플에서 제공됨)를 사용하는 Beatles 예제를 보자. 다음 그림은 다양한 허용오차 매개변수에 대한 매칭 함수를 보여준다. 이러한 함수는 이동 위치 \\(m=100\\)에서 최대값을 가정한다.\n\n\ndef compute_matching_function(C_D, C_Q, tol_freq=1, tol_time=1):\n    \"\"\"Computes matching function for constellation maps\n\n    Args:\n        C_D (np.ndarray): Binary matrix used as dababase document\n        C_Q (np.ndarray): Binary matrix used as query document\n        tol_freq (int): Tolerance in frequency direction (vertical) (Default value = 1)\n        tol_time (int): Tolerance in time direction (horizontal) (Default value = 1)\n\n    Returns:\n        Delta (np.ndarray): Matching function\n        shift_max (int): Optimal shift position maximizing Delta\n    \"\"\"\n    L = C_D.shape[1]\n    N = C_Q.shape[1]\n    M = L - N\n    assert M >= 0, \"Query must be shorter than document\"\n    Delta = np.zeros(L)\n    for m in range(M + 1):\n        C_D_crop = C_D[:, m:m+N]\n        TP, FN, FP, C_AND = match_binary_matrices_tol(C_D_crop, C_Q,\n                                                      tol_freq=tol_freq, tol_time=tol_time)\n        Delta[m] = TP\n    shift_max = np.argmax(Delta)\n    return Delta, shift_max\n\n\ndist_freq = 11\ndist_time = 5\n\nfn_wav_D = path_data+wav_dict['Original']\nY_D = compute_spectrogram(fn_wav_D)\nCmap_D = compute_constellation_map(Y_D, dist_freq, dist_time)\n\nfn_wav_Q = path_data+wav_dict['Noise']\nY_Q = compute_spectrogram(fn_wav_Q)\nY_Q = Y_Q[:, 100:150]\nCmap_Q = compute_constellation_map(Y_Q, dist_freq, dist_time)\n\nDelta_0, shift_max_0 = compute_matching_function(Cmap_D, Cmap_Q, tol_freq=0, tol_time=0)\nDelta_1, shift_max_1 = compute_matching_function(Cmap_D, Cmap_Q, tol_freq=1, tol_time=1)\nDelta_2, shift_max_2 = compute_matching_function(Cmap_D, Cmap_Q, tol_freq=3, tol_time=2)\n\ny_max = Delta_2[shift_max_2] + 1\nfig, ax, line = plot_signal(Delta_0, ylim=[0, y_max], color='g',\n                                     xlabel='Shift (samples)', ylabel='Number of matching peaks', \n                                     figsize=(7, 3))\nplt.title('Matching functions for different tolerance parameters')\nax.plot(Delta_1, color='r')\nax.plot(Delta_2, color='b')\nplt.legend(['tol_freq=0, tol_time=0', 'tol_freq=1, tol_time=1', \n            'tol_freq=3, tol_time=2'], loc='upper right', framealpha=1)\nplt.show()\n\n\n\n\n\n다음 코드 셀에서는 서로다른 이동 인덱스 \\(m\\)에 대해 이동된 쿼리 \\(m+\\mathcal{C}(\\mathcal{Q})\\)와 \\(\\mathcal{C}(\\mathcal{D})\\) 간의 로컬 일치 결과를 보여준다. \\(m=0\\)에 대해 일치하는 피크가 거의 없는 반면, 최적 이동 지수 \\(m=99\\)에 대해 일치하는 피크는 많다.\n\n\ndef plot_shifted_Cmap(Cmap_D, Cmap_Q, shift=0):\n    Cmap_Q_extend = np.zeros(Cmap_D.shape) \n    Cmap_Q_extend[:, shift:shift+Cmap_Q.shape[1]] = Cmap_Q\n    TP, FN, FP, Cmap_AND = match_binary_matrices_tol(Cmap_D, Cmap_Q_extend, \n                                                     tol_freq=1, tol_time=1)\n    title=r'Matching result for shift index m=%d (TP=%d)' % (shift, TP)\n    fig, ax, im = plot_constellation_map(Cmap_AND, color='green', s=200, marker='+', title=title)\n    n, k = np.argwhere(Cmap_D == 1).T\n    ax.scatter(k, n, color='r', s=50, marker='o')\n    n, k = np.argwhere(Cmap_Q_extend == 1).T\n    ax.scatter(k, n, color='cyan', s=20, marker='o')\n    plt.legend(['Matches (TP)', 'Reference', 'Estimated'], loc='upper right', framealpha=1)\n    plt.show()\n    \nplot_shifted_Cmap(Cmap_D, Cmap_Q, shift=0)    \nplot_shifted_Cmap(Cmap_D, Cmap_Q, shift=shift_max_1) \n\n\n\n\n\n\n\n\n이제 서로 다른 버전의 Beatles 예에서 얻은 매칭 함수를 살펴보자. 이전과 마찬가지로 Original을 \\(\\mathcal{D}\\)로 사용하고 다른 버전의 세그먼트 \\([100:149]\\)(샘플에서 제공)를 \\(\\mathcal{Q}\\)로 사용한다. 다음 그림은 Original, Noise, Coding 및 Talking 쿼리에 대한 매칭 함수를 보여준다. 처음 세 개의 쿼리에 대해 예상되는 매칭 위치에서 매칭 함수에서 뚜렷한 최대값을 관찰할 수 있지만 Talking 쿼리는 식별하기가 훨씬 더 어렵다.\n\n\ndef compute_Delta(fn_wav_D, fn_wav_Q, seg_start=100, seg_end=150, dist_freq=11, \n                  dist_time=5, tol_freq=1, tol_time=1):\n    Y_D = compute_spectrogram(fn_wav_D)\n    Cmap_D = compute_constellation_map(Y_D, dist_freq=dist_freq, dist_time=dist_time)\n    Y_Q = compute_spectrogram(fn_wav_Q)\n    Y_Q = Y_Q[:, seg_start:seg_end]\n    Cmap_Q = compute_constellation_map(Y_Q, dist_freq=dist_freq, dist_time=dist_time)\n    Delta, shift_max = compute_matching_function(Cmap_D, Cmap_Q, \n                                                 tol_freq=tol_freq, tol_time=tol_time)\n    return Delta, shift_max\n\nfn_wav_Q = path_data+wav_dict['Original']   \nDelta_0, shift_max_0 = compute_Delta(fn_wav_D, fn_wav_Q)\nfn_wav_Q = path_data+wav_dict['Noise']\nDelta_1, shift_max_1 = compute_Delta(fn_wav_D, fn_wav_Q)\nfn_wav_Q = path_data+wav_dict['Coding']\nDelta_2, shift_max_2 = compute_Delta(fn_wav_D, fn_wav_Q)\nfn_wav_Q = path_data+wav_dict['Talking']\nDelta_3, shift_max_3 = compute_Delta(fn_wav_D, fn_wav_Q)\n\ny_max = Delta_0[shift_max_0] + 1\nfig, ax, line = plot_signal(Delta_0, ylim=[0, y_max], color='r',\n                                     xlabel='Shift (samples)', \n                                     ylabel='Number of matching peaks', \n                                     figsize=(8, 3))\nax.plot(Delta_1, color='orange')\nax.plot(Delta_2, color='b')\nax.plot(Delta_3, color='k', linewidth='2')\nplt.legend(['Orignal', 'Noise', 'Coding', 'Talking'], loc='upper right', framealpha=1)\nplt.show()\n\n\n\n\n\n위의 예에서 Talking 버전의 \\([100:149]\\) 세그먼트(샘플에서 제공)를 사용할 때 Original Beatles 노래의 올바른 세그먼트를 거의 식별할 수 없는 것을 확인했다. 쿼리 배열 맵의 판별력(discriminative power)을 높이는 한 가지 방법은 지속 시간을 늘리는 것이다. 이를 시연하기 위해 다음 그림에서 길이가 다른 쿼리에 대한 매칭 함수를 보여준다.\n\nSegment \\([100:149]\\) (corresponding to 2.32 seconds)\nSegment \\([100:174]\\) (corresponding to 3.48 seconds)\nSegment \\([100:199]\\) (corresponding to 4.64 seconds)\n\n가장 긴 쿼리의 경우 심하게 왜곡된 Talking 버전의 경우에도 피크 구조가 더 두드러진다.\n\n\nfn_wav_Q = path_data+wav_dict['Talking']\nDelta_0, shift_max_0 = compute_Delta(fn_wav_D, fn_wav_Q)\nDelta_1, shift_max_1 = compute_Delta(fn_wav_D, fn_wav_Q, seg_start=100, seg_end=175)\nDelta_2, shift_max_2 = compute_Delta(fn_wav_D, fn_wav_Q, seg_start=100, seg_end=200)\n\ny_max = Delta_2[shift_max_2] + 1\nfig, ax, line = plot_signal(Delta_0, ylim=[0, y_max], color='k',\n                                     xlabel='Shift (samples)', \n                                     ylabel='Number of matching peaks', \n                                     figsize=(8, 3), linewidth=3)\nax.plot(Delta_1, color='c', linewidth=2)\nax.plot(Delta_2, color='r', linewidth=1)\nplt.legend(['[100:149]', '[100:174]', '[100:199]'], loc='upper right', framealpha=1)\nplt.show()\n\n\n\n\n\nFaster(타임스케일 수정 버전이 약 10% 더 빠르게 재생됨)와 Higher(피치 이동 버전)의 두 가지 버전을 고려해보자.\n다시 Original을 \\(\\mathcal{D}\\)로 사용하고 다른 버전의 세그먼트 \\([100:174]\\)를 \\(\\mathcal{Q}\\)로 사용하면 다음 그림에서 결과 매칭 함수를 보여준다. 위에서 정의한 대로 매칭 함수는 다소 엄격한 의미에서 constellation map을 비교한다. 즉, 주파수의 체계적 이동과 전역적 시간 확장은 고려되지 않는다.\n결과적으로 지금까지 논의된 기술로는 Faster 또는 Higher와 같은 버전을 식별할 수 없다.\n\n\nfn_wav_Q = path_data+wav_dict['Original']\nDelta_0, shift_max_0 = compute_Delta(fn_wav_D, fn_wav_Q, \n                                     seg_start=100, seg_end=175, tol_freq=1, tol_time=1)\nfn_wav_Q = path_data+wav_dict['Faster']\nDelta_1, shift_max_1 = compute_Delta(fn_wav_D, fn_wav_Q, \n                                     seg_start=100, seg_end=175, tol_freq=0, tol_time=0)\nDelta_2, shift_max_1 = compute_Delta(fn_wav_D, fn_wav_Q, \n                                     seg_start=100, seg_end=175, tol_freq=1, tol_time=1)\nfn_wav_Q = path_data+wav_dict['Higher']\nDelta_3, shift_max_2 = compute_Delta(fn_wav_D, fn_wav_Q, \n                                     seg_start=100, seg_end=175, tol_freq=1, tol_time=1)\n\ny_max = Delta_0[shift_max_0] + 1\nfig, ax, line = plot_signal(Delta_0, ylim=[0, y_max], color='r',\n                                     xlabel='Shift (samples)', \n                                     ylabel='Number of matching peaks', \n                                     figsize=(8, 3))\nax.plot(Delta_1, color='orange')\nax.plot(Delta_2, color='b')\nax.plot(Delta_3, color='k', linewidth='2')\nplt.legend([r'Orignal (tol_freq=1, tol_time=1)', r'Faster (tol_freq=0, tol_time=0)', \n            r'Faster (tol_freq=1, tol_time=1)', r'Higher (tol_freq=1, tol_time=1)'], loc='upper right', framealpha=1)\nplt.show()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#인덱싱indexing",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#인덱싱indexing",
    "title": "8.2. 오디오 식별",
    "section": "인덱싱(Indexing)",
    "text": "인덱싱(Indexing)\n\n지금까지 설명한 매칭 과정에서 쿼리는 데이터베이스에 포함된 모든 문서의 모든 섹션(쿼리와 동일한 기간을 가짐)에 대해 비교되어야 한다.\n실행 시간이 문서의 수와 크기에 선형적으로 의존하는 이러한 철저한 검색 전략은 수백만 개의 녹음이 포함된 대규모 데이터베이스에는 적합하지 않다. 확장성 측면에서 검색 결과의 정확성을 희생하지 않으면서 빠른 정보 접근을 용이하게 하는 검색 전략이 필요하다.\n이러한 검색 전략은 일반적으로 적절한 조회 작업을 통해 검색 공간을 줄여 속도와 성능을 최적화하는 인덱싱 기술을 사용한다. 인덱스는 알파벳순으로 정렬된 키워드(일반적으로 해시(hash)라고 함) 모음으로 구성된 전통적인 책 색인과 유사하게 구성된다. 각 키워드에 대해 차례로 책에서 주어진 키워드의 발생을 나타내는 증가하는 페이지 번호 목록이 있다.\n오디오 지문의 맥락에서 키워드(또는 해시)는 피크(또는 그 조합)의 주파수 값(적절하게 양자화됨)인 반면, 페이지 번호는 시간 위치 및 문서 식별자에 해당한다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#피크-쌍-peak-pairs",
    "href": "posts/8. Content-Based Audio Retrieval/8.2.Audio_Identification.html#피크-쌍-peak-pairs",
    "title": "8.2. 오디오 식별",
    "section": "피크 쌍 (Peak Pairs)",
    "text": "피크 쌍 (Peak Pairs)\n\n노이즈와 왜곡에 견고하더라도, 개별 스펙트럼 피크는 고립된 방식으로 고려될 때 그다지 특징적이지 않다는 점에서 특이성(specificity)이 낮다. 결과적으로 개별 스펙트럼 피크와 주파수 스탬프를 해시 값으로 사용하면 결과 해시 목록(“페이지 번호 목록”)이 길어지고 인덱싱이 느려지는 것으로 나타난다.\n따라서 Wang이 도입한 지문 시스템의 한 가지 중요한 단계는 지문의 견고성을 희생하지 않고 해시의 특이성과 수를 증가시키는 것이다(따라서 결과 해시 목록의 길이를 줄임). 주요 아이디어는 개별 피크가 아닌 피크 쌍을 고려하여 지문 해시를 형성하는 것이다. 이를 위해 \\((n_0,k_0)\\in\\mathcal{C}(\\mathcal{D})\\) 포인트를 고정하는데 이는 그와 관련된 앵커(anchor) 포인트 및 타겟 영역(target zone) \\(T_{(n_0,k_0)}\\subset\\mathbb{Z}\\times[0:K]\\)이라고 볼 수 있다.\n타겟 영역은 앵커 포인트에 가까운 시간-주파수 평면의 작은 직사각형 영역으로 생각되어야 한다. 그런 다음, 앵커 포인트 \\((n_0,k_0)\\) 및 일부 타겟 포인트 \\((n_1,k_1)\\in T_{(n_0,k_0)}\\cap\\mathcal{C}(\\mathcal{D})\\)로 구성되는 포인트 쌍 \\(\\big((n_0,k_0),(n_1,k_1)\\big)\\)을 고려한다.\n각 쌍은 두 개의 주파수 스탬프와 두 개의 타임 스탬프의 차이로 구성된 트리플 \\((k_0,k_1,n_1-n_0)\\)을 산출한다. 단일 주파수 스탬프 대신 이러한 트리플을 해시로 사용하는 것이다.\n\n\nImage(path_img+\"FMP_C7_F07.png\", width=700)\n\n\n\n\n\n매칭 단계에서 이제 일치하는 주파수 스탬프만 고려하는 대신 이동된 쿼리와 데이터베이스 문서 간의 일치하는 트리플을 계산한다.\n인덱싱할 데이터 항목의 수가 증가하더라도(단일 주파수 스탬프 대신 트리플을 고려하여) 트리플의 훨씬 더 높은 특이성 덕분에 검색 프로세스에서 엄청난 가속을 얻는다는 것을 보여줄 수 있다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html",
    "title": "8.3. 오디오 매칭",
    "section": "",
    "text": "내용 기반 오디오 검색 중 다른 연주나 편곡이라도 쿼리와 같은 음악을 찾는 오디오 매칭(audio matching)에 대해 설명한다. CENS와 대각선 매칭(diagonal matching), 하위시퀀스(subsequence) DTW 등에 대해 다룬다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#양자화-quantization",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#양자화-quantization",
    "title": "8.3. 오디오 매칭",
    "section": "양자화 (Quantization)",
    "text": "양자화 (Quantization)\n\n이러한 정규화된 크로마그램 표현이 이미 두 녹음에서 유사한 패턴으로 나타나지만 여전히 성능별 차이가 많이 있다. 따라서 한 가지 아이디어는 추가 양자화(quantization) 및 스무딩 절차를 적용하여 로컬 템포, 아티큘레이션 및 음 실행의 변화로 인한 로컬 변동의 영향을 더 줄이는 것이다.\n이제 이러한 단계가 실제로 구현될 수 있는 방법을 보여주는 구체적인 후처리 절차에 대해 논의한다. 위에서 계산한 기본 크로마 변형으로 시작하여 맨해튼 노름(\\(\\ell^1\\)-norm)으로 각 크로마 벡터를 정규화하여 12개의 크로마 값을 더하면 1이 되게 한다.\n\\(X=(x_1,x_2,\\ldots,x_N)\\)는 정규화된 크로마 벡터 \\(x_n\\in[0,1]^{12}\\), \\(n\\in[1:N]\\)의 결과 시퀀스를 나타낸다고 하자. 이러한 각 벡터에는 0과 1 사이의 양수 항목만 있다. 다음으로 양자화(quantization) 함수 \\(Q:[0,1]\\to\\{0,1,2,3,4\\}\\)를 다음과 같이 정의한다.\n\n\\[Q(a):=\\left\\{\\begin{array}{llrcl}\n    0 & \\text{ for } & 0    & \\leq \\,\\, a\\,\\, < &0.05, \\\\\n    1 & \\text{ for } & 0.05 & \\leq \\,\\, a\\,\\, < &0.1, \\\\\n    2 & \\text{ for } & 0.1  & \\leq \\,\\, a\\,\\, < &0.2, \\\\\n    3 & \\text{ for } & 0.2  & \\leq \\,\\, a\\,\\, < &0.4, \\\\\n    4 & \\text{ for } & 0.4  & \\leq \\,\\, a\\,\\, \\leq &1. \\\\\n    \\end{array}\\right.\\]\n\n첫 번째 단계에서 각 \\(x_n\\)의 구성 요소에 \\(Q\\)를 적용하여 각 크로마 벡터 \\(x_n=(x_n(0),\\ldots,x_n(11))^\\top\\in[0,1]^{12}\\)를 양자화하여 다음을 산출한다. \\[Q(x_n):=(Q(x_n(0)),\\ldots,Q(x_n(11)))^\\top.\\]\n직관적으로 이 양자화는 해당 크로마 클래스가 예를 들어 신호의 총 에너지의 \\(40%\\) 이상을 포함하는 경우 크로마 구성 요소에 \\(4\\)의 값을 할당한다. 또한 \\(5%\\) 임계값 미만의 크로마 구성 요소는 0으로 설정되어 노이즈에 대한 견고성을 높인다.\n임계값은 소리 강도(intensity)의 로그 인식을 설명하기 위해 로그 방식으로 선택된다. 예를 들어 벡터 \\(x_n=(0.02,0.5,0.3,0.07,0.11,0,\\ldots,0)^\\top\\)은 벡터 \\(Q(x_n):=(0,4,3,1,2,0,\\ldots,0)^\\top\\)로 변환된다.\n\n\ndef quantize_matrix(C, quant_fct=None):\n    \"\"\"Quantize matrix values in a logarithmic manner (as done for CENS features)\n\n    Args:\n        C (np.ndarray): Input matrix\n        quant_fct (list): List specifying the quantization function (Default value = None)\n\n    Returns:\n        C_quant (np.ndarray): Output matrix\n    \"\"\"\n    C_quant = np.empty_like(C)\n    if quant_fct is None:\n        quant_fct = [(0.0, 0.05, 0), (0.05, 0.1, 1), (0.1, 0.2, 2), (0.2, 0.4, 3), (0.4, 1, 4)]\n    for min_val, max_val, target_val in quant_fct:\n        mask = np.logical_and(min_val <= C, C < max_val)\n        C_quant[mask] = target_val\n    return C_quant\n\n\nC1 = librosa.feature.chroma_stft(y=x1, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\nC2 = librosa.feature.chroma_stft(y=x2, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\n\nC1_Q = quantize_matrix(C1)\nC2_Q = quantize_matrix(C2)\n\ntitle1=r'$\\ell_1$-normalized chromagram (Bernstein)'\ntitle2=r'$\\ell_1$-normalized chromagram (Karajan)'\nplot_two_chromagrams(C1, C2, Fs1=Fs1, Fs2=Fs2, title1=title1, title2=title2, clim=[0, 1])\n\ntitle1='Quantized chromagram (Bernstein)'\ntitle2='Quantized chromagram (Karajan)'\nplot_two_chromagrams(C1_Q, C2_Q, Fs1=Fs1, Fs2=Fs2, title1=title1, title2=title2, clim=[0, 4])"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#스무딩-및-다운샘플링-smoothing-and-downsampling",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#스무딩-및-다운샘플링-smoothing-and-downsampling",
    "title": "8.3. 오디오 매칭",
    "section": "스무딩 및 다운샘플링 (Smoothing and Downsampling)",
    "text": "스무딩 및 다운샘플링 (Smoothing and Downsampling)\n\n두 번째 단계에서 양자화된 시퀀스 \\((Q(x_1),\\ldots,Q(x_N))\\)는 시간 차원을 따라 더욱 평활화(smoothed)된다.\n이를 위해 스무딩 윈도우(smoothing window)(예: Hann window)의 길이를 결정하는 숫자 \\(\\ell\\in\\mathbb{N}\\)를 고정한 다음, 시퀀스 \\((Q(x_1),\\ldots,Q(x_N))\\)의 12개 구성 요소 각각의 로컬 평균(window 함수에 의해 가중됨)을 고려한다. 이는 다시 음수가 아닌 항목의 \\(12\\) 차원 벡터의 시퀀스를 생성한다.\n마지막 단계에서 이 시퀀스는 \\(d\\) 배만큼 다운샘플링되고 결과 벡터는 유클리드 노름(\\(\\ell^2\\)-norm)에 따라 정규화된다. 양자화와 평활화의 두 단계는 \\(\\ell\\) 연속 벡터의 윈도우에 대한 에너지 분포의 가중 통계를 계산하는 것으로 생각할 수 있다. 따라서 결과의 피쳐를 \\(\\mathrm{CENS}^{\\ell}_{d}\\) (chroma energy normalized statistics)라고 한다.\n다음 코드 셀에서는 CENS 계산에 관련된 모든 단계를 요약하는 함수를 본다.\nCENS 피쳐의 주요 아이디어는 상대적으로 큰 윈도우에 대한 통계를 통해 트릴이나 아르페지오와 같은 음 그룹의 템포, 아티큘레이션 및 실행에서 로컬 편차를 완화한다는 것이다.\n이 효과를 설명하기 위해 다음 예에서 두 베토벤 공연에 대한 \\(\\mathrm{CENS}^{41}_{10}\\)-features의 시퀀스를 보여준다. 원래 크로마 시퀀스에 대한 피쳐 레이트 \\(10~\\mathrm{Hz}\\)에서 시작하여, \\(\\ell=41\\) 매개변수는 \\(4100~\\mathrm{msec}\\)의 윈도우 크기에 해당한다. 또한 다운샘플링 매개변수 \\(d=10\\)를 사용하면 특징 속도가 \\(1~\\mathrm{Hz}\\)(초당 하나의 기능)로 감소한다.\n원본 크로마 시퀀스와 비교할 때, 두 공연의 결과 CENS 시퀀스는 훨씬 더 높은 수준의 유사성을 보유하면서도 일부 특징적인 음악 정보를 캡처한다.\n\n\ndef compute_cens_from_chromagram(C, Fs=1, ell=41, d=10, quant=True):\n    \"\"\"Compute CENS features from chromagram\n\n    Args:\n        C (np.ndarray): Input chromagram\n        Fs (scalar): Feature rate of chromagram (Default value = 1)\n        ell (int): Smoothing length (Default value = 41)\n        d (int): Downsampling factor (Default value = 10)\n        quant (bool): Apply quantization (Default value = True)\n\n    Returns:\n        C_CENS (np.ndarray): CENS features\n        Fs_CENS (scalar): Feature rate of CENS features\n    \"\"\"\n    C_norm = normalize_feature_sequence(C, norm='1')\n    C_Q = quantize_matrix(C_norm) if quant else C_norm\n\n    C_smooth, Fs_CENS = smooth_downsample_feature_sequence(C_Q, Fs, filt_len=ell,\n                                                                     down_sampling=d, w_type='hann')\n    C_CENS = normalize_feature_sequence(C_smooth, norm='2')\n\n    return C_CENS, Fs_CENS\n\n\nC1 = librosa.feature.chroma_stft(y=x1, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\nC2 = librosa.feature.chroma_stft(y=x2, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\n\nC1_CENS, Fs1_CENS = compute_cens_from_chromagram(C1, Fs1)\nC2_CENS, Fs2_CENS = compute_cens_from_chromagram(C2, Fs2)\n\ntitle1='CENS features (Bernstein)'\ntitle2='CENS features (Karajan)'\nplot_two_chromagrams(C1_CENS, C2_CENS, Fs1=Fs1_CENS, Fs2=Fs2_CENS, title1=title1, title2=title2, clim=[0, 1])"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매개변수-설정",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매개변수-설정",
    "title": "8.3. 오디오 매칭",
    "section": "매개변수 설정",
    "text": "매개변수 설정\n\nCENS 개념은 두 가지 주요 매개변수 \\(\\ell\\in\\mathbb{N}\\) 및 \\(d\\in\\mathbb{N}\\)에 따라 \\(\\mathrm{CENS}^{\\ell}_{d}\\) 크로마 특징 군을 생성한다. 설명된 절차는 원래 크로마그램의 비용 집약적인 계산을 반복하지 않고 특징 특이성(specificity)과 해상도(resolution)를 조정하는 계산이 간단한 방법을 소개한다. 다음 그림에서는 다양한 스무딩 및 다운샘플링 매개변수를 사용하는 CENS 특징 표현을 보여준다.\n\n\ntitle1=r'$\\ell_1$-normalized chromagram (Bernstein)'\ntitle2=r'$\\ell_1$-normalized chromagram (Karajan)'\nplot_two_chromagrams(C1, C2, Fs1=Fs1, Fs2=Fs2, title1=title1, title2=title2, clim=[0, 1])\n\nparameter_set = [(9, 2), (21, 5), (41, 10)]\nfor parameter in parameter_set:\n    ell = parameter[0]\n    d = parameter[1]\n    C1_CENS, Fs1_CENS = compute_cens_from_chromagram(C1, Fs1, ell=ell, d=d)\n    C2_CENS, Fs2_CENS = compute_cens_from_chromagram(C2, Fs2, ell=ell, d=d)\n    title1=r'CENS$^{%d}_{%d}$-features (Bernstein)' % (ell, d)\n    title2=r'CENS$^{%d}_{%d}$-features (Karajan)' % (ell, d)\n    plot_two_chromagrams(C1_CENS, C2_CENS, Fs1=Fs1_CENS, Fs2=Fs2_CENS,\n                         title1=title1, title2=title2, clim=[0, 1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCENS 피쳐는 어떤 크로마그램 표현으로도 시작하여 계산할 수 있다. 예를 들어 STFT 기반 크로마그램을 사용하는 대신 다중 속도 필터 뱅크(multirate filter bank)를 기반으로 하는 크로마그램으로 시작할 수 있다.\n\n\nImage(path_img+\"FMP_C7S3_CENS_WASPAA.png\", width=700)\n\n\n\n\n\n라이브러리 LibROSA에는 CENS 기능을 계산하는 함수(librosa.feature.chroma_cens)도 포함되어 있다. 다운샘플링은 해당 함수 외부에서 수행되어야 한다(예: C_CENS[:, ::d] 사용)."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매칭-함수-matching-function",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매칭-함수-matching-function",
    "title": "8.3. 오디오 매칭",
    "section": "매칭 함수 (Matching Function)",
    "text": "매칭 함수 (Matching Function)\n\n아래의 하위 시퀀스(subsequence) 매칭 기법은 오디오 매칭(audio matching) 작업을 동기로 한다. 오디오 매칭의 목표는 짧은 쿼리 오디오 클립에 음악적으로 해당하는 모든 오디오 발췌 부분을 검색하는 것이다.\n추상적인 관점에서 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)를 쿼리 \\(\\mathcal{Q}\\) 및 문서 \\(\\mathcal{D}\\)를 각각 대표하는 두 개의 피처 시퀀스라고 하자. 쿼리의 길이 \\(N\\)는 일반적으로 데이터베이스 문서의 길이 \\(M\\)에 비해 짧다. 쿼리 \\(\\mathcal{Q}\\)가 \\(\\mathcal{D}\\)에 어떻게든 “포함”되어 있는지 확인하기 위해 시퀀스 \\(X\\)를 시퀀스 \\(Y\\)로 이동하고 \\(X\\)를 \\(Y\\)의 적절한 하위 시퀀스와 로컬에서 비교한다. 유사하거나 동등하게 \\(X\\)에 대한 작은 거리를 갖는 \\(Y\\)의 모든 하위 시퀀스는 쿼리에 대해 일치(match)하는 것으로 간주된다.\n\\(X\\)를 \\(Y\\)의 하위 시퀀스와 로컬에서 비교하는 방법에는 여러 가지가 있다. 이제 대각선 매칭(diagonal matching)이라는 간단한 절차를 소개한다.\n우선, 시퀀스 \\(X\\)와 \\(Y\\)의 크로마 벡터를 비교하기 위해 로컬 비용 측정(또는 로컬 거리 측정)을 고정해야 한다. 다음에서는 유클리드 노름과 관련하여 모든 특징이 정규화되었다고 가정하고 내적을 기반으로 거리 측정 \\(c\\)를 사용한다. \\[c(x,y) = 1- \\langle x,y\\rangle = 1 - \\sum_{k=1}^K x(k)y(k)\\]\n\n2개의 \\(K\\)차원 벡터 \\(x\\in\\mathbb{R}^K\\) 및 \\(y\\in\\mathbb{R}^K\\)\n\\(\\|x\\|_2=\\|y\\|_2=1\\)\n또한, 모든 특징 벡터가 음이 아닌(nonnegative) 항목을 갖는다고 가정하면 \\(c(x,y)\\in[0,1]\\)와 \\(c(x,y)=0\\)는 \\(x=y\\)일 때만 그렇다.\n\n동일한 길이를 공유하는 두 개의 특징 시퀀스를 비교하는 한 가지 간단한 방법은 두 시퀀스의 해당 벡터 사이의 평균 거리를 계산하는 것이다. 이렇게 하면 쿼리 시퀀스 \\(X=(x_1,\\ldots,x_N)\\)와 쿼리와 동일한 길이 \\(N\\)를 갖는 \\(Y\\)의 모든 하위 시퀀스 \\((y_{1+m},\\ldots,y_{N+m})\\)를 비교할 수 있다. \\(m\\in[0:M-N]\\)이 이동 인덱스를 나타낸다.\n이 절차는 다음과 같이 정의된 매칭 함수 \\(\\Delta_\\mathrm{Diag}:[0:M-N]\\to\\mathbb{R}\\)를 생성한다. \\[\\Delta_\\mathrm{Diag}(m) := \\frac{1}{N}\\sum_{n=1}^{N} c(x_n,y_{n+m})\\]\n이제 이 매칭 함수가 계산되는 방식을 약간 재구성한다. \\(\\mathbf{C}\\in\\mathbb{R}^{N\\times M}\\)가 다음과 같이 주어진 비용 매트릭스라고 하자. \\[\\mathbf{C}(n,m):=c(x_n,y_m)\\] for \\(n\\in[1:N]\\) and \\(m\\in[1:M]\\)\n그러면 다음 그림과 같이 \\(\\mathbf{C}\\) 행렬의 대각선을 합산하여 \\(\\Delta_\\mathrm{Diag}(m)\\) 값을 얻는다(쿼리 길이에 의한 정규화까지). 이는 이 절차가 “대각선” 매칭으로 불리는 이유를 설명한다.\n\n\nImage(path_img+\"FMP_C7_F11.png\", width=600)\n\n\n\n\n\n다음 코드 셀에서는 대각선 매칭 절차를 구현하고 생성한 합성 시퀀스 \\(X\\) 및 \\(Y\\)에 적용한다. 다음 예에서 시퀀스 \\(Y\\)는 \\(X\\)와 유사한 5개의 하위시퀀스를 포함한다(각각 위치 \\(m=20\\), \\(40\\), \\(60\\), \\(80\\), \\(100\\)에서 시작).\n\n\\(m=20\\)에서 시작하는 첫 번째 항목은 \\(X\\)의 정확한 복사본이다.\n\\(m=40\\) 및 \\(m=60\\)의 발생은 \\(X\\)의 노이즈 버전이다.\n\\(m=80\\)의 발생은 \\(X\\)의 확장된(느린) 버전이다.\n\\(m=100\\)의 발생은 \\(X\\)의 압축된(빠른) 버전이다.\n\n다음 그림에서 볼 수 있듯이 매칭 함수 \\(\\Delta_\\mathrm{Diag}\\)는 예상 위치에서 로컬 최소값을 나타낸다. \\(m=20\\)의 첫 번째 최소값은 0이지만 \\(m=40\\) 및 \\(m=60\\)의 다음 두 최소값은 여전히 두드러진다(일치 값은 0에 가깝다). 그러나 늘림과 압축으로 인해 대각선 매칭 절차는 \\(m=80\\) 및 \\(m=100\\)에서 마지막 두 개의 하위시퀀스를 잘 캡처할 수 없다.\n\n\ndef scale_tempo_sequence(X, factor=1):\n    \"\"\"Scales a sequence (given as feature matrix) along time (second dimension)\n\n    Args:\n        X (np.ndarray): Feature sequences (given as K x N matrix)\n        factor (float): Scaling factor (resulting in length \"round(factor * N)\"\") (Default value = 1)\n\n    Returns:\n        X_new (np.ndarray): Scaled feature sequence\n        N_new (int): Length of scaled feature sequence\n    \"\"\"\n    N = X.shape[1]\n    t = np.linspace(0, 1, num=N, endpoint=True)\n    N_new = np.round(factor * N).astype(int)\n    t_new = np.linspace(0, 1, num=N_new, endpoint=True)\n    X_new = scipy.interpolate.interp1d(t, X, axis=1)(t_new)\n    return X_new, N_new\n\n\ndef cost_matrix_dot(X, Y):\n    \"\"\"Computes cost matrix via dot product\n\n    Args:\n        X (np.ndarray): First sequence (K x N matrix)\n        Y (np.ndarray): Second sequence (K x M matrix)\n\n    Returns:\n        C (np.ndarray): Cost matrix\n    \"\"\"\n    return 1 - np.dot(X.T, Y)\n\n\ndef matching_function_diag(C, cyclic=False):\n    \"\"\"Computes diagonal matching function\n\n    Args:\n        C (np.ndarray): Cost matrix\n        cyclic (bool): If \"True\" then matching is done cyclically (Default value = False)\n\n    Returns:\n        Delta (np.ndarray): Matching function\n    \"\"\"\n    N, M = C.shape\n    assert N <= M, \"N <= M is required\"\n    Delta = C[0, :]\n    for n in range(1, N):\n        Delta = Delta + np.roll(C[n, :], -n)\n    Delta = Delta / N\n    if cyclic is False:\n        Delta[M-N+1:M] = np.inf\n    return Delta\n\n\n# Create snythetic example for sequences X and Y\nN = 15\nM = 130\nfeature_dim = 12\nnp.random.seed(2)\nX = np.random.random((feature_dim, N))\nY = np.random.random((feature_dim, M))\nY[:, 20:20+N] = X\nY[:, 40:40+N] = X + 0.5 * np.random.random((feature_dim, N))\nY[:, 60:60+N] = X + 0.8 * np.random.random((feature_dim, N))\nX_slow, N_slow = scale_tempo_sequence(X, factor=1.25)\nY[:, 80:80+N_slow] = X_slow\nX_fast, N_fast = scale_tempo_sequence(X, factor=0.8)\nY[:, 100:100+N_fast] = X_fast\nY = librosa.util.normalize(Y, norm=2)\nX = librosa.util.normalize(X, norm=2)\n\n# Compute cost matrix and matching function\nC = cost_matrix_dot(X, Y)\nDelta = matching_function_diag(C)\n\n\n# Visualization\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.02], \n                                          'height_ratios': [1, 1]}, figsize=(8, 4))  \ncmap = compressed_gray_cmap(alpha=-10, reverse=True)\nplot_matrix(C, title='Cost matrix', xlabel='Time (samples)', ylabel='Time (samples)', \n                     ax=[ax[0, 0], ax[0, 1]], colorbar=True, cmap=cmap)\n\nplot_signal(Delta, ax=ax[1,0], xlabel='Time (samples)', ylabel='',\n                     title = 'Matching function', color='k')\nax[1, 0].grid()\nax[1, 1].axis('off')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#검색-절차-retrieval-procedure",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#검색-절차-retrieval-procedure",
    "title": "8.3. 오디오 매칭",
    "section": "검색 절차 (Retrieval Procedure)",
    "text": "검색 절차 (Retrieval Procedure)\n\n이제 쿼리 조각과 유사한 모든 일치 항목을 검색하기 위해 매칭 함수를 적용하는 방법에 대해 설명한다.\n다음에서는 데이터베이스가 단일 문서 \\(\\mathcal{D}\\)(예: 모든 문서 시퀀스 연결)로 표현된다고 가정한다. \\(\\mathcal{Q}\\)와 \\(\\mathcal{D}\\) 사이의 최상의 매칭을 결정하기 위해 매칭 함수 \\(\\Delta_\\mathrm{Diag}\\)를 최소화하는 인덱스 \\(m^\\ast\\in[0:M-N]\\)를 찾기만 하면 된다. \\[m^\\ast := \\underset{m\\in[0:M-N]}{\\mathrm{argmin}} \\,\\,\\Delta_\\mathrm{Diag}(m)\\]\n최상의 일치는 다음의 하위시퀀스로 주어진다. \\[Y(1+m^\\ast:N+m^\\ast) := (y_{1+m^\\ast},\\ldots,y_{N+m^\\ast})\\]\n더 많은 일치 항목을 얻기 위해 가장 일치하는 이웃을 제외한다. 예를 들어, \\(m\\in [m^\\ast-\\rho:m^\\ast+\\rho]\\cap [0:M-N]\\)에 대해 \\(\\Delta_\\mathrm{Diag}(m)=\\infty\\)를 설정하는 식으로 \\(m^\\ast\\) 주변의 \\(\\rho= \\lfloor N/2 \\rfloor\\) 이웃을 제외할 수 있다.\n이렇게 하면 후속의 일치 항목이 쿼리 길이의 절반 이상 겹치지 않는다. 후속의 일치 항목을 찾기 위해 특정 수의 일치 항목을 얻거나 지정된 거리 임계값을 초과할 때까지 후자의 절차를 반복한다.\n다음 코드 셀에서 이 검색 절차를 구현한다. \\(\\rho\\) 매개변수 외에도 일치하는 값을 제한하기 위한 \\(\\tau\\) 매개변수(즉, \\(\\Delta_\\mathrm{Diag}(m^\\ast)\\leq \\tau\\)가 필요함)와 검색할 최대 일치 항목 수를 지정한다.\n\n\ndef mininma_from_matching_function(Delta, rho=2, tau=0.2, num=None):\n    \"\"\"Derives local minima positions of matching function in an iterative fashion\n\n    Args:\n        Delta (np.ndarray): Matching function\n        rho (int): Parameter to exclude neighborhood of a matching position for subsequent matches (Default value = 2)\n        tau (float): Threshold for maximum Delta value allowed for matches (Default value = 0.2)\n        num (int): Maximum number of matches (Default value = None)\n\n    Returns:\n        pos (np.ndarray): Array of local minima\n    \"\"\"\n    Delta_tmp = Delta.copy()\n    M = len(Delta)\n    pos = []\n    num_pos = 0\n    rho = int(rho)\n    if num is None:\n        num = M\n    while num_pos < num and np.sum(Delta_tmp < tau) > 0:\n        m = np.argmin(Delta_tmp)\n        pos.append(m)\n        num_pos += 1\n        Delta_tmp[max(0, m - rho):min(m + rho, M)] = np.inf\n    pos = np.array(pos).astype(int)\n    return pos\n\n\ndef matches_diag(pos, Delta_N):\n    \"\"\"Derives matches from positions in the case of diagonal matching\n\n    Args:\n        pos (np.ndarray or list): Starting positions of matches\n        Delta_N (int or np.ndarray or list): Length of match (a single number or a list of same length as Delta)\n\n    Returns:\n        matches (np.ndarray): Array containing matches (start, end)\n    \"\"\"\n    matches = np.zeros((len(pos), 2)).astype(int)\n    for k in range(len(pos)):\n        s = pos[k]\n        matches[k, 0] = s\n        if isinstance(Delta_N, int):\n            matches[k, 1] = s + Delta_N - 1\n        else:\n            matches[k, 1] = s + Delta_N[s] - 1\n    return matches\n\n\ndef plot_matches(ax, matches, Delta, Fs=1, alpha=0.2, color='r', s_marker='o', t_marker=''):\n    \"\"\"Plots matches into existing axis\n\n    Args:\n        ax: Axis\n        matches: Array of matches (start, end)\n        Delta: Matching function\n        Fs: Feature rate (Default value = 1)\n        alpha: Transparency pramaeter for match visualization (Default value = 0.2)\n        color: Color used to indicated matches (Default value = 'r')\n        s_marker: Marker used to indicate start of matches (Default value = 'o')\n        t_marker: Marker used to indicate end of matches (Default value = '')\n    \"\"\"\n    y_min, y_max = ax.get_ylim()\n    for (s, t) in matches:\n        ax.plot(s/Fs, Delta[s], color=color, marker=s_marker, linestyle='None')\n        ax.plot(t/Fs, Delta[t], color=color, marker=t_marker, linestyle='None')\n        rect = patches.Rectangle(((s-0.5)/Fs, y_min), (t-s+1)/Fs, y_max, facecolor=color, alpha=alpha)\n        ax.add_patch(rect)\n\n\npos = mininma_from_matching_function(Delta, rho=N//2, tau=0.12, num=None)\nmatches = matches_diag(pos, N)\n\nfig, ax, line = plot_signal(Delta, figsize=(8, 2), xlabel='Time (samples)', \n                                     title = 'Matching function with retrieved matches', \n                                     color='k')\nax.grid()\nplot_matches(ax, matches, Delta)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#다중-쿼리를-이용한-매칭-함수-matching-function-using-multiple-queries",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#다중-쿼리를-이용한-매칭-함수-matching-function-using-multiple-queries",
    "title": "8.3. 오디오 매칭",
    "section": "다중 쿼리를 이용한 매칭 함수 (Matching Function Using Multiple Queries)",
    "text": "다중 쿼리를 이용한 매칭 함수 (Matching Function Using Multiple Queries)\n\n이 기본적인 매칭 절차는 쿼리의 템포가 매칭할 구간 내 템포와 대략적으로 일치하는 경우에 잘 작동한다. 그러나 앞의 예에서도 알 수 있듯이 대각선 일치는 데이터베이스 하위시퀀스가 늘려지거나 압축된 버전의 쿼리일 때 문제가 된다. 이러한 템포 차이를 보완하기 위해 다중 쿼리 전략을 적용할 수 있다. 아이디어는 다음과 같다.\n\n다양한 템포를 시뮬레이트하는 스케일링 작업을 적용하여 여러 버전의 쿼리를 생성한다.\n대각선 매칭을 사용하여 스케일링된 각 버전에 대해 별도의 매칭 함수를 계산한다.\n모든 결과 매칭 함수를 최소화하여 단일 매칭 함수를 생성한다.\n\n이 아이디어는 SSM의 경로 구조를 향상시키기 위한 다중 필터링 접근 방식(multiple-filtering approach)과 유사하다. 다음 구현에서는 예상되는 상대 템포 차이의 범위를 샘플링하는 \\(\\Theta\\) 집합을 도입한다. 음악 검색에서 일치하는 섹션 간의 상대적인 템포 차이가 \\(50\\)퍼센트보다 큰 경우는 거의 발생하지 않는다. 따라서 \\(\\Theta\\)는 대략 \\(-50\\) ~ \\(+50\\) 퍼센트의 템포 변화를 커버하도록 선택할 수 있다. 예를 들어, 집합 \\(\\Theta=\\{0.66,0.81,1.00,1.22,1.50\\}\\)(로그 간격의 템포 매개변수 포함)는 대략 \\(−50\\)에서 \\(+50\\) 퍼센트까지의 템포 변화를 포함한다. (이 집합은 compute_tempo_rel_set 함수로 계산할 수 있다.)\n\n\ndef matching_function_diag_multiple(X, Y, tempo_rel_set=[1], cyclic=False):\n    \"\"\"Computes diagonal matching function using multiple query strategy\n\n    Args:\n        X (np.ndarray): First sequence (K x N matrix)\n        Y (np.ndarray): Second sequence (K x M matrix)\n        tempo_rel_set (np.ndarray): Set of relative tempo values (scaling) (Default value = [1])\n        cyclic (bool): If \"True\" then matching is done cyclically (Default value = False)\n\n    Returns:\n        Delta_min (np.ndarray): Matching function (obtained by from minimizing over several matching functions)\n        Delta_N (np.ndarray): Query length of best match for each time position\n        Delta_scale (np.ndarray): Set of matching functions (for each of the scaled versions of the query)\n    \"\"\"\n    M = Y.shape[1]\n    num_tempo = len(tempo_rel_set)\n    Delta_scale = np.zeros((num_tempo, M))\n    N_scale = np.zeros(num_tempo)\n    for k in range(num_tempo):\n        X_scale, N_scale[k] = scale_tempo_sequence(X, factor=tempo_rel_set[k])\n        C_scale = cost_matrix_dot(X_scale, Y)\n        Delta_scale[k, :] = matching_function_diag(C_scale, cyclic=cyclic)\n    Delta_min = np.min(Delta_scale, axis=0)\n    Delta_argmin = np.argmin(Delta_scale, axis=0)\n    Delta_N = N_scale[Delta_argmin]\n    return Delta_min, Delta_N, Delta_scale\n\n\ntempo_rel_set = [0.66, 0.81, 1.00, 1.22, 1.50]\ncolor_set = ['b', 'c', 'gray', 'r', 'g']\nnum_tempo = len(tempo_rel_set)\n\nDelta_min, Delta_N, Delta_scale = matching_function_diag_multiple(X, Y,  tempo_rel_set=tempo_rel_set,\n                                                                  cyclic=False)\n\nfor k in range(num_tempo):\n    plot_signal(Delta_scale[k,:], figsize=(8, 2), xlabel='Time (samples)',\n                         title = 'Matching function with scaling factor %.2f' % tempo_rel_set[k], \n                         color=color_set[k], ylim=[0, 0.3])\n    plt.grid()\n    \nfig, ax, line = plot_signal(Delta_min, figsize=(8, 2), xlabel='Time (samples)',\n                     title = 'Matching function', color='k', ylim=[0,0.3], linewidth=3, label='min')\nax.grid()\nfor k in range(num_tempo):\n    ax.plot(Delta_scale[k, :], linewidth=1, color=color_set[k], label=tempo_rel_set[k])\n                     \nplt.legend(loc='lower right', framealpha=1);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n다중 쿼리 매칭 함수에서 확장되거나 압축된 쿼리 버전에 해당하는 하위시퀀스는 이제 값이 0에 훨씬 가까운 로컬 최소값으로 표시된다. 결과적으로, 위의 검색 절차(동일한 매개변수 설정 사용)는 이제 예상되는 일치 항목을 생성한다. 일치하는 하위 시퀀스의 길이는 고려된 모든 쿼리 버전에 대해 최소 일치 값을 생성하는 확장된 쿼리에서 파생된다. 따라서 아래 그림에서도 표시된 것처럼 길이는 원본(확장되지 않은) 쿼리의 길이 \\(N\\)과 다를 수 있다.\n\n\npos = mininma_from_matching_function(Delta_min, rho=N//2, tau=0.12, num=None)\nmatches = matches_diag(pos, Delta_N)\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.02], \n                                          'height_ratios': [3, 3]}, figsize=(8, 4))  \ncmap = compressed_gray_cmap(alpha=-10, reverse=True)\nplot_matrix(C, title='Cost matrix', xlabel='Time (samples)', ylabel='Time (samples)', \n                     ax=[ax[0, 0], ax[0, 1]], colorbar=True, cmap=cmap)\n\nplot_signal(Delta_min, ax=ax[1, 0], xlabel='Time (samples)', \n                     title = 'Matching function with retrieved matches', color='k')\n\nax[1,0].grid()\nplot_matches(ax[1, 0], matches, Delta_min)\nax[1,1].axis('off')\nplt.tight_layout()\n\n\n\n\n\n대각선 매칭에 대한 대안으로 동적 시간 워핑(DTW)의 변형을 사용하여 로컬 템포 변형을 처리할 수 있는 대안을 소개한다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#전역-정렬과-하위시퀀스-정렬",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#전역-정렬과-하위시퀀스-정렬",
    "title": "8.3. 오디오 매칭",
    "section": "전역 정렬과 하위시퀀스 정렬",
    "text": "전역 정렬과 하위시퀀스 정렬\n\nDTW(Dynamic Time Warping)에 대한 포스트에서 두 개의 특징 시퀀스를 비교할 때 템포 차이를 어떻게 처리할 수 있는지 연구한 바 있다. 워핑 경로의 개념을 기반으로 DTW 알고리즘을 사용하여 두 시퀀스 간의 최적 전역 정렬(global alignment)을 계산했다.\n오디오 매칭 시나리오에서는 정렬 작업이 약간 다르다. 주어진 두 시퀀스 간의 전체 정렬을 찾는 대신 목표는 더 짧은 시퀀스에 최적으로 맞는 긴 시퀀스 내의 하위 시퀀스를 찾는 것이다.\n\n\nImage(path_img+ \"FMP_C7_F13+.png\", width=800)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#문제의-공식화",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#문제의-공식화",
    "title": "8.3. 오디오 매칭",
    "section": "문제의 공식화",
    "text": "문제의 공식화\n\n\\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)가 특징 공간 \\(\\mathcal{F}\\)에 대한 두 개의 특징 시퀀스라고 가정하자. 길이 \\(M\\)는 길이 \\(N\\)보다 훨씬 크다. 또한, \\(c:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\)를 로컬 비용의 측정이라고 하고, \\(\\mathbf{C}\\)를 \\(\\mathbf{C}(n,m)=c(x_n,y_m)\\) (for \\(n\\in[1:N]\\), \\(m\\in[1:M]\\))로 주어진 결과 비용 행렬이라고 하자.\n\\(a,b\\in[1:M]\\)와 \\(a\\leq b\\)의 두 인덱스에 대해, \\(Y\\)의 하위 시퀀스를 나타내는 다음 표기를 사용한다. \\[Y(a:b):=(y_a,y_{a+1},\\ldots,y_b)\\]\n전역 DTW 거리를 기반으로 매칭 문제는 다음과 같은 최적화 작업으로 공식화할 수 있다. \\(X\\)까지의 DTW 거리를 최소화하는 \\(Y\\)의 하위 시퀀스(\\(Y\\)의 모든 가능한 하위 시퀀스에서)를 찾는다. 즉, 다음에 의해 정의된 인덱스를 결정하는 것이다. \\[(a^\\ast,b^\\ast) := \\underset{(a,b): 1\\leq a\\leq b\\leq M}{\\mathrm{argmin}}\n  \\mathrm{DTW}\\big( X\\,,\\, Y(a:b)\\big).\\]"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#하위시퀀스-dtw-알고리즘",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#하위시퀀스-dtw-알고리즘",
    "title": "8.3. 오디오 매칭",
    "section": "하위시퀀스 DTW 알고리즘",
    "text": "하위시퀀스 DTW 알고리즘\n\n\\((a^\\ast,b^\\ast)\\) 찾기에는 두 가지 종류의 최적화 단계가 포함된다.\n\n먼저 \\(Y\\)의 모든 가능한 하위 시퀀스를 고려하여 최적의 하위 시퀀스를 찾아야 한다.\n둘째, 각 하위시퀀스에 대해 \\(X\\)까지의 DTW 거리를 계산해야 하며, 여기에는 최적 워핑 경로의 비용을 결정하기 위한 최적화가 포함된다.\n\n인덱스 \\(a^\\ast\\) 및 \\(b^\\ast\\)와 \\(X\\) 및 하위 시퀀스 \\(Y(a^\\ast:b^\\ast)\\) 사이의 모든 최적 정렬, 모두 단일 최적화 프레임워크 내에서 계산할 수 있다. 이를 위해 원래 DTW 알고리즘의 약간의 수정만 필요하다. 기본 아이디어는 \\(X\\)와의 정렬에서 \\(Y\\)의 시작과 끝에 생략을 허용하는 것이다.\n원래 DTW 알고리즘과 마찬가지로 \\(\\mathbf{D}\\)로 표시되는 \\(N\\times M\\) 누적 비용 행렬을 정의한다. 이 행렬의 첫 번째 열은 다음과 같이 설정하여 초기화된다. \\[\\mathbf{D}(n,1):=\\sum_{k=1}^{n} \\mathbf{C}(k,1)\\] for \\(n\\in [1:N]\\).\n그러나 \\(\\mathbf{D}\\)의 첫 번째 행은 이제 다음과 같이 초기화된다. \\[\\mathbf{D}(1,m):= \\mathbf{C}(1,m)\\] for \\(m\\in [1:M]\\).\n이 초기화(initialization)는 비용을 누적하지 않고 시퀀스 \\(Y\\)의 모든 위치에서 시작할 수 있게 하여 \\(X\\)와 일치할 때 \\(Y\\)의 시작 부분을 건너뛸 수 있게 한다. \\(\\mathbf{D}\\)의 나머지 값은 원래 DTW 알고리즘에서와 같이 재귀적으로 정의된다. \\[\\mathbf{D}(n,m) = \\mathbf{C}(n,m) + \\mathrm{min}\\big\\{ \\mathbf{D}(n-1,m-1), \\mathbf{D}(n-1,m), \\mathbf{D}(n,m-1) \\big\\}\\] for \\(n\\in[2:N]\\) and \\(m\\in[2:M]\\).\n마지막으로, 전역 DTW 거리를 얻기 위해 계수 \\(\\mathbf{D}(N,M)\\)만 보는 것이 아니라, 두 번째 수정은 전체 마지막 행 \\(\\mathbf{D}(N,m)\\) (for \\(m\\in[1:M]\\))을 고려하는 것이다. 이 행에서 인덱스 \\(b^\\ast\\)는 다음과 같이 결정될 수 있다. \\[b^\\ast = \\underset{b\\in[1:M]}{\\mathrm{argmin}} \\,\\,\\mathbf{D}(N,b)\\]\n이 행에서 비용-최소화 인덱스를 선택하면(원래 DTW 접근 방식에서와 같이 마지막 인덱스를 사용하는 대신), \\(X\\)와 일치할 때 \\(Y\\)의 끝을 건너뛸 수 있게 한다.\n시작 인덱스 \\(a^\\ast\\)는 행렬 \\(\\mathbf{D}(N,m)\\)에서 직접 읽을 수 없다. \\(a^\\ast\\)를 결정하려면 고전적인 DTW에서와 같이 역추적(backtracking) 절차를 적용하여 최적의 워핑 경로를 구성해야 한다. 그러나 이번에는 (\\(q_1=(N,M)\\) 대신) \\(q_1=(N,b^\\ast)\\)로 시작하고 \\(\\mathbf{D}\\)의 첫 번째 행이 일부 요소 \\(q_L=(1,m)\\), \\(m\\in[1:M]\\)(\\(q_L=(1,1))\\) 대신)에 도달하면 멈춘다. 인덱스 \\(a^\\ast\\in[1:M]\\)는 이 인덱스 \\(m\\)에 의해 결정된다. 또한 경로 \\((q_L,q_{L-1},\\ldots,q_1)\\)는 시퀀스 \\(X\\)와 하위 시퀀스 \\(Y(a^\\ast:b^\\ast)\\) 사이의 최적 워핑 경로를 정의한다.\n하위 시퀀스 DTW 알고리즘은 다음 표에 의해 구체화된다.\n\n\nImage(path_img+\"FMP_C7_E06.png\", width=600)\n\n\n\n\n\n이제 위에서 설명한 하위시퀀스 DTW 알고리즘을 구현한다. 실례로, 실수의 두 시퀀스와 차이의 절대값(1차원 유클리드 거리)을 비용 측정으로 간주한다. 즉, 피처 공간 \\(\\mathcal{F}=\\mathbb{R}\\) 및 \\(c(x,y):=|x-y|\\) for \\(x,y\\in \\mathcal{F}\\)가 있다.\n\n\nX = np.array([3, 0, 6])\nY = np.array([2, 4, 0, 4, 0, 0, 5, 2])\nN = len(X)\nM = len(Y)\n\nplt.figure(figsize=(6, 2))\nplt.plot(X, c='k', label='$X$')\nplt.plot(Y, c='b', label='$Y$')\nplt.legend(loc='lower right')\nplt.tight_layout()\nplt.show()\n\nprint('Sequence X =', X)\nprint('Sequence Y =', Y)\n\n\n\n\nSequence X = [3 0 6]\nSequence Y = [2 4 0 4 0 0 5 2]\n\n\n\nC =  compute_cost_matrix(X, Y, metric='euclidean')\nprint('Cost matrix C =', C, sep='\\n')\n\nCost matrix C =\n[[1. 1. 3. 1. 3. 3. 2. 1.]\n [2. 4. 0. 4. 0. 0. 5. 2.]\n [4. 2. 6. 2. 6. 6. 1. 4.]]\n\n\n\n다음으로 동적 프로그래밍을 사용하여 누적 비용 행렬 \\(D\\)와 \\(D\\)의 마지막 행에 있는 비용 최소화 인덱스 \\(b^\\ast\\)를 계산한다.\n\n\ndef compute_accumulated_cost_matrix_subsequence_dtw(C):\n    \"\"\"Given the cost matrix, compute the accumulated cost matrix for\n    subsequence dynamic time warping with step sizes {(1, 0), (0, 1), (1, 1)}\n\n    Args:\n        C (np.ndarray): Cost matrix\n\n    Returns:\n        D (np.ndarray): Accumulated cost matrix\n    \"\"\"\n    N, M = C.shape\n    D = np.zeros((N, M))\n    D[:, 0] = np.cumsum(C[:, 0])\n    D[0, :] = C[0, :]\n    for n in range(1, N):\n        for m in range(1, M):\n            D[n, m] = C[n, m] + min(D[n-1, m], D[n, m-1], D[n-1, m-1])\n    return D\n\n\nD =  compute_accumulated_cost_matrix_subsequence_dtw(C)\nprint('Accumulated cost matrix D =', D, sep='\\n')\nb_ast = D[-1, :].argmin()\nprint('b* =', b_ast)\nprint('Accumulated cost D[N, b*] = ', D[-1, b_ast])\n\nAccumulated cost matrix D =\n[[1. 1. 3. 1. 3. 3. 2. 1.]\n [3. 5. 1. 5. 1. 1. 6. 3.]\n [7. 5. 7. 3. 7. 7. 2. 6.]]\nb* = 6\nAccumulated cost D[N, b*] =  2.0\n\n\n\n마지막으로 역추적을 사용하여 최적의 하위 시퀀스 \\(Y(a^\\ast:b^\\ast)\\)의 인덱스 \\(a^\\ast\\)를 결정하는 최적 워핑 경로 \\(P^\\ast\\)를 도출한다.\n\n\ndef compute_optimal_warping_path_subsequence_dtw(D, m=-1):\n    \"\"\"Given an accumulated cost matrix, compute the warping path for\n    subsequence dynamic time warping with step sizes {(1, 0), (0, 1), (1, 1)}\n\n    Args:\n        D (np.ndarray): Accumulated cost matrix\n        m (int): Index to start back tracking; if set to -1, optimal m is used (Default value = -1)\n\n    Returns:\n        P (np.ndarray): Optimal warping path (array of index pairs)\n    \"\"\"\n    N, M = D.shape\n    n = N - 1\n    if m < 0:\n        m = D[N - 1, :].argmin()\n    P = [(n, m)]\n\n    while n > 0:\n        if m == 0:\n            cell = (n - 1, 0)\n        else:\n            val = min(D[n-1, m-1], D[n-1, m], D[n, m-1])\n            if val == D[n-1, m-1]:\n                cell = (n-1, m-1)\n            elif val == D[n-1, m]:\n                cell = (n-1, m)\n            else:\n                cell = (n, m-1)\n        P.append(cell)\n        n, m = cell\n    P.reverse()\n    P = np.array(P)\n    return P\n\n\nP = compute_optimal_warping_path_subsequence_dtw(D)\nprint('Optimal warping path P =', P.tolist())\na_ast = P[0, 1]\nb_ast = P[-1, 1]\nprint('a* =', a_ast)\nprint('b* =', b_ast)\nprint('Sequence X =', X)\nprint('Sequence Y =', Y)\nprint('Optimal subsequence Y(a*:b*) =', Y[a_ast:b_ast+1])\nprint('Accumulated cost D[N, b_ast]= ', D[-1, b_ast])\n\nOptimal warping path P = [[0, 3], [1, 4], [1, 5], [2, 6]]\na* = 3\nb* = 6\nSequence X = [3 0 6]\nSequence Y = [2 4 0 4 0 0 5 2]\nOptimal subsequence Y(a*:b*) = [4 0 0 5]\nAccumulated cost D[N, b_ast]=  2.0\n\n\n\n최적의 워핑 경로(빨간색 점으로 표시됨)와 함께 하위시퀀스 DTW 방법의 비용 매트릭스 \\(C\\) 및 누적 비용 매트릭스 \\(D\\)를 시각화한다.\n\n\ncmap = compressed_gray_cmap(alpha=-10, reverse=True)\n\nplt.figure(figsize=(10, 1.8))\nax = plt.subplot(1, 2, 1)\nplot_matrix_with_points(C, P, linestyle='-', ax=[ax], aspect='equal',\n                                  clim=[0, np.max(C)], cmap=cmap, title='$C$ with optimal warping path',\n                                  xlabel='Sequence Y', ylabel='Sequence X')\n\nax = plt.subplot(1, 2, 2)\nplot_matrix_with_points(D, P, linestyle='-', ax=[ax], aspect='equal',\n                                  clim=[0, np.max(D)], cmap=cmap, title='$D$ with optimal warping path',\n                                  xlabel='Sequence Y', ylabel='Sequence X')\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매칭-함수-matching-function-1",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#매칭-함수-matching-function-1",
    "title": "8.3. 오디오 매칭",
    "section": "매칭 함수 (Matching Function)",
    "text": "매칭 함수 (Matching Function)\n\n최적 인덱스 \\(b^\\ast\\)를 표시하는 것 외에도 \\(\\mathbf{D}\\)의 마지막 행(아래 그림의 맨 위 행)은 더 많은 정보를 제공한다.\n임의의 \\(m\\in[1:M]\\)에 대한 각 항목 \\(\\mathbf{D}(N,m)\\)은 위치 \\(m\\)에서 끝나는 최적의 하위 시퀀스 \\(Y\\)에 \\(X\\)를 정렬하는 총 비용을 나타낸다.\n누적 비용을 쿼리의 \\(N\\) 길이로 정규화하는 다음의 설정을 통해 매칭 함수 \\(\\Delta_\\mathrm{DTW}:[1:M]\\to\\mathbb{R}\\)를 정의할 수 있다. \\[\\Delta_\\mathrm{DTW}(m) := \\frac{1}{N}\\,\\, \\mathbf{D}(N,m)\\] for \\(m\\in[1:M]\\)\n0에 가까운 \\(\\Delta_\\mathrm{DTW}\\)의 각 로컬 최소값 \\(b\\in[1:M]\\)은 \\(X\\)로 DTW 거리가 작은 하위 시퀀스 \\(Y(a:b)\\)의 끝 위치를 나타낸다. 시작 인덱스 \\(a\\in[1:M]\\), 그리고 이 하위시퀀스와 \\(X\\) 사이의 최적 정렬은 \\(q_1=(N,b)\\) 셀로 시작하는 역추적 절차를 통해 얻을 수 있다.\n\n\nDelta = D[-1, :] / N \n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.02], \n                                          'height_ratios': [1, 1]}, figsize=(6, 4))  \ncmap = compressed_gray_cmap(alpha=-10, reverse=True)\nplot_matrix(D, title=r'Accumulated cost matrix $\\mathbf{D}$', xlabel='Time (samples)',\n                     ylabel='Time (samples)',  ax=[ax[0, 0], ax[0, 1]], colorbar=True, cmap=cmap)\nrect = patches.Rectangle((-0.45, 1.48), len(Delta)-0.1, 1, linewidth=3, edgecolor='r', facecolor='none')\nax[0, 0].add_patch(rect)\n\nplot_signal(Delta, ax=ax[1, 0], xlabel='Time (samples)', ylabel='', ylim=[0, np.max(Delta)+1], \n                     title = r'Matching function $\\Delta_\\mathrm{DTW}$', color='k')\nax[1, 0].set_xlim([-0.5, len(Delta)-0.5])\nax[1, 0].grid()\nax[1, 1].axis('off')\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#대각선-매칭과의-비교",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#대각선-매칭과의-비교",
    "title": "8.3. 오디오 매칭",
    "section": "대각선 매칭과의 비교",
    "text": "대각선 매칭과의 비교\n\n하위시퀀스 DTW를 이용한 매칭함수 \\(\\Delta_\\mathrm{DTW}\\)는 대각선 매칭으로 얻은 매칭함수 \\(\\Delta_\\mathrm{Diag}\\)를 일종의 일반화한 것으로 볼 수 있다. 그러나 \\(\\Delta_\\mathrm{Diag}\\)와 \\(\\Delta_\\mathrm{DTW}\\) 사이에는 중요한 차이점이 있다.\n\n\\(\\Delta_\\mathrm{Diag}\\)는 일치하는 섹션의 시작 위치를 나타내지만 \\(\\Delta_\\mathrm{DTW}\\)는 일치하는 섹션의 끝 위치를 나타낸다. 간단히 말해서 대각선 매칭에서 \\(X\\) 쿼리는 정방향으로 처리되는 반면 DTW 기반 접근 방식에서는 역방향으로 처리된다.\n또한 DTW 기반 접근 방식은 매칭 구간을 쿼리와 대각선으로 정렬하는 것이 아니라 매칭과 쿼리 사이의 시간적 편차를 처리할 수 있는 워핑(warping) 작업으로 처리한다.\n\n이 효과는 쿼리 시퀀스 \\(X\\)의 길이가 \\(N=3\\)이고 가장 잘 일치하는 하위시퀀스 \\(Y(3:6)\\)의 길이가 \\(4\\)인 위의 실행 예제에서 확인 할 수 있다. DTW 기반 매칭 절차는 이러한 시간적 차이를 설명할 수 있다. 그러나 대각매칭을 사용하면 일치하는 서브시퀀스의 길이는 쿼리 \\(X\\)와 동일한 길이 \\(N\\)를 갖도록 강제된다. 이는 DTW 기반 매칭에서와 다른 최적의 하위 시퀀스로 이어진다.\n\n\nN = len(X)\nM = len(Y)\nC =  compute_cost_matrix(X, Y, metric='euclidean')\n\n# Subsequence DTW\nD =  compute_accumulated_cost_matrix_subsequence_dtw(C)\nDelta_DTW = D[-1, :] / N \nP_DTW = compute_optimal_warping_path_subsequence_dtw(D)\na_ast = P[0, 1]\nb_ast = P[-1, 1]\n\n# Diagonal matching\nDelta_Diag = matching_function_diag(C)\nm = np.argmin(Delta_Diag)\nP_Diag = []\nfor n in range(N): \n    P_Diag.append((n, m+n))\nP_Diag = np.array(P_Diag)\nmatches_Diag = [(m, N)]\n\n\n# Visualization\nfig, ax = plt.subplots(2, 4, gridspec_kw={'width_ratios': [1, 0.05, 1, 0.05], \n                                          'height_ratios': [1, 1]}, \n                       constrained_layout=True, figsize=(9, 4))  \ncmap = compressed_gray_cmap(alpha=-10, reverse=True)\nplot_matrix_with_points(C, P_DTW, linestyle='-', ax=[ax[0, 0], ax[0, 1]],\n                                  clim=[0, np.max(C)], cmap=cmap, title='$C$ with optimal warping path',\n                                  xlabel='Sequence Y', ylabel='Sequence X')\nplot_signal(Delta_DTW, ax=ax[1,0], xlabel='Time (samples)', ylabel='', ylim=[0, 5],\n                     title=r'Matching function $\\Delta_\\mathrm{DTW}$', color='k')\nax[1, 0].set_xlim([-0.5, len(Delta)-0.5])\nax[1, 0].grid()\nax[1, 0].plot(b_ast, Delta_DTW[b_ast], 'ro')\nax[1, 0].add_patch(patches.Rectangle((a_ast-0.5, 0), b_ast-a_ast+1, 7, facecolor='r', alpha=0.2))\nax[1, 1].axis('off')\n\nplot_matrix_with_points(C, P_Diag, linestyle='-', ax=[ax[0, 2], ax[0, 3]],\n                                  clim=[0, np.max(C)], cmap=cmap, title='$C$ with optimal diagonal path',\n                                  xlabel='Sequence Y', ylabel='Sequence X')\nplot_signal(Delta_Diag, ax=ax[1, 2], xlabel='Time (samples)', ylabel='', \n                     ylim=[0, 5], title = r'Matching function $\\Delta_\\mathrm{Diag}$', color='k')\nax[1, 2].set_xlim([-0.5, len(Delta)-0.5])\nax[1, 2].grid()\nax[1, 2].plot(m, Delta_Diag[m], 'ro')\nax[1, 2].add_patch(patches.Rectangle((m-0.5, 0), N, 7, facecolor='r', alpha=0.2))\nax[1, 3].axis('off');"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#step-size-조건",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#step-size-조건",
    "title": "8.3. 오디오 매칭",
    "section": "Step Size 조건",
    "text": "Step Size 조건\n\n논의된 DTW의 하위 시퀀스 변형은 기존 DTW와 동일한 방식으로 수정될 수 있다. 특히, \\(\\Sigma=\\{(1,0),(0,1),(1,1)\\}\\) 세트를 대체하여 단계 크기(step size) 조건을 변경할 수 있다. 예를 들어 \\(\\Sigma=\\{(1,1)\\}\\) 세트를 사용하면 DTW 기반 매칭은 기본적으로 정방향 처리가 아닌 역방향 처리를 제외하고 대각선 매칭으로 축소된다. 일반적으로 \\(\\Sigma=\\{(1,0),(0,1),(1,1)\\}\\) 세트를 사용하면 정렬 경로가 크게 저하될 수 있다. 극단적인 경우 \\(X\\) 시퀀스는 \\(Y\\)의 단일 요소에 할당될 수 있다.\n따라서 특정 응용에서는 완전한 유연성을 갖춘 DTW 기반 매칭과 강한 대각선 매칭 사이의 타협점인 \\(\\Sigma=\\{(2,1),(1,2),(1,1)\\}\\) 집합을 사용하는 것이 유리할 수 있다.\n\n\ndef compute_accumulated_cost_matrix_subsequence_dtw_21(C):\n    \"\"\"Given the cost matrix, compute the accumulated cost matrix for\n    subsequence dynamic time warping with step sizes {(1, 1), (2, 1), (1, 2)}\n\n    Args:\n        C (np.ndarray): Cost matrix\n\n    Returns:\n        D (np.ndarray): Accumulated cost matrix\n    \"\"\"\n    N, M = C.shape\n    D = np.zeros((N + 1, M + 2))\n    D[0:1, :] = np.inf\n    D[:, 0:2] = np.inf\n\n    D[1, 2:] = C[0, :]\n\n    for n in range(1, N):\n        for m in range(0, M):\n            if n == 0 and m == 0:\n                continue\n            D[n+1, m+2] = C[n, m] + min(D[n-1+1, m-1+2], D[n-2+1, m-1+2], D[n-1+1, m-2+2])\n    D = D[1:, 2:]\n    return D\n\n\ndef compute_optimal_warping_path_subsequence_dtw_21(D, m=-1):\n    \"\"\"Given an accumulated cost matrix, compute the warping path for\n    subsequence dynamic time warping with step sizes {(1, 1), (2, 1), (1, 2)}\n\n    Args:\n        D (np.ndarray): Accumulated cost matrix\n        m (int): Index to start back tracking; if set to -1, optimal m is used (Default value = -1)\n\n    Returns:\n        P (np.ndarray): Optimal warping path (array of index pairs)\n    \"\"\"\n    N, M = D.shape\n    n = N - 1\n    if m < 0:\n        m = D[N - 1, :].argmin()\n    P = [(n, m)]\n\n    while n > 0:\n        if m == 0:\n            cell = (n-1, 0)\n        else:\n            val = min(D[n-1, m-1], D[n-2, m-1], D[n-1, m-2])\n            if val == D[n-1, m-1]:\n                cell = (n-1, m-1)\n            elif val == D[n-2, m-1]:\n                cell = (n-2, m-1)\n            else:\n                cell = (n-1, m-2)\n        P.append(cell)\n        n, m = cell\n    P.reverse()\n    P = np.array(P)\n    return P\n\n\nC =  compute_cost_matrix(X, Y, metric='euclidean')\nD = compute_accumulated_cost_matrix_subsequence_dtw_21(C)\nP = compute_optimal_warping_path_subsequence_dtw_21(D)\n\nplt.figure(figsize=(9, 1.8))\nax = plt.subplot(1, 2, 1)\nplot_matrix_with_points(C, P, linestyle='-', ax=[ax], aspect='equal',\n                                  clim=[0, np.max(C)], cmap=cmap, title='$C$ with optimal warping path',\n                                  xlabel='Sequence Y', ylabel='Sequence X')\n\nax = plt.subplot(1, 2, 2)\nD_max = np.nanmax(D[D != np.inf])\nplot_matrix_with_points(D, P, linestyle='-', ax=[ax], aspect='equal',\n                                  clim=[0, D_max], cmap=cmap, title='$D$ with optimal warping path', \n                                  xlabel='Sequence Y', ylabel='Sequence X')\nfor x, y in zip(*np.where(np.isinf(D))):\n    plt.text(y, x, '$\\infty$', horizontalalignment='center', verticalalignment='center')\n    \nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#librosa-구현",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#librosa-구현",
    "title": "8.3. 오디오 매칭",
    "section": "LibROSA 구현",
    "text": "LibROSA 구현\n\nLibROSA의 DTW 함수 librosa.sequence.dtw는 원본 DTW와 하위시퀀스 DTW를 모두 구현할 수 있다. 후자의 경우 subseq=True 매개변수를 설정해야 한다. 이 함수를 사용하면 로컬 가중치 및 전역 제약 조건과 같은 추가 매개변수뿐만 아니라 임의의 단계 크기 조건을 지정할 수 있다.\n다음에서는 librosa의 DTW 구현을 비교한다. 이를 위해 베토벤 교향곡 5번의 시작 부분을 살펴본다.\n\n쿼리는 처음 다섯 소절의 번스타인 녹음으로 구성된다.\n데이터베이스 문서는 처음 21마디의 카라얀 버전으로 구성되어 있다.\n매칭은 CENS-feature 시퀀스를 기반으로 수행된다.\n\n\n\ndef check_matrices(M1, M2, label='Matrices'):\n    if (M1.shape != M2.shape):\n        print(label, 'have different shape!', (M1.shape, M2.shape))\n    elif not np.allclose(M1, M2):\n        print(label, 'are numerical different!')\n    else:\n        print(label, 'are equal.')        \n\n\nN_feat = 4410\nH_feat = 2205\n\nfn1 = 'FMP_C7_Audio_Beethoven_Op067-01-001-005_Bernstein.wav'\nfn2 = 'FMP_C7_Audio_Beethoven_Op067-01-001-021_Karajan.wav'\n\nx1, Fs = librosa.load(path_data+fn1)\nx2, Fs = librosa.load(path_data+fn2)\nC1 = librosa.feature.chroma_stft(y=x1, sr=Fs, tuning=0, norm=None, hop_length=H_feat, n_fft=N_feat)\nC2 = librosa.feature.chroma_stft(y=x2, sr=Fs, tuning=0, norm=None, hop_length=H_feat, n_fft=N_feat)\nell = 21\nd = 5\nX, Fs_cens = compute_cens_from_chromagram(C1, ell=21, d=5)\nY, Fs_cens = compute_cens_from_chromagram(C2, ell=21, d=5)\nN, M = X.shape[1], Y.shape[1]\n\nC_FMP = compute_cost_matrix(X, Y, 'euclidean')\n\nD_FMP = compute_accumulated_cost_matrix_subsequence_dtw(C_FMP)\nP_FMP = compute_optimal_warping_path_subsequence_dtw(D_FMP)\n    \nsigma = np.array([[1, 0], [0, 1], [1, 1]])\n\nD_librosa, P_librosa = librosa.sequence.dtw(C=C_FMP, step_sizes_sigma=sigma, subseq=True, \n                                            backtrack=True)\nP_librosa = P_librosa[::-1, :]\n\ncheck_matrices(D_librosa, D_FMP, 'D matrices')\ncheck_matrices(P_librosa, P_FMP, 'Warping paths')\n    \nfig, ax = plt.subplots(2, 1, figsize=(9, 4))\ntitle='LibROSA implementation with step size condition $\\Sigma=\\{(1, 0), (0, 1), (1, 1)\\}$'\nplot_matrix_with_points(D_librosa, P_librosa, ax=[ax[0]], cmap=cmap, \n                                  xlabel='Sequence Y (Karajan)', ylabel='Sequence X (Bernstein)', \n                                  title=title,\n                                  marker='o', linestyle='-')\ntitle='libfmp implementation with step size condition $\\Sigma=\\{(1, 0), (0, 1), (1, 1)\\}$'\nplot_matrix_with_points(D_FMP, P_FMP, ax=[ax[1]], cmap=cmap,\n                                  xlabel='Sequence Y (Karajan)', ylabel='Sequence X (Bernstein)',\n                                  title=title,\n                                  marker='o', linestyle='-')\nplt.tight_layout()\n\nD matrices are equal.\nWarping paths are equal.\n\n\n\n\n\n\nC_FMP = compute_cost_matrix(X, Y, 'euclidean')\nD_FMP = compute_accumulated_cost_matrix_subsequence_dtw_21(C_FMP)\nP_FMP = compute_optimal_warping_path_subsequence_dtw_21(D_FMP)\n    \nsigma = np.array([[2, 1], [1, 2], [1, 1]]) # 다른 step size 조건\n\nD_librosa, P_librosa = librosa.sequence.dtw(C=C_FMP, step_sizes_sigma=sigma, subseq=True, \n                                            backtrack=True)\n\nP_librosa = P_librosa[::-1, :]\n\ncheck_matrices(D_librosa, D_FMP, 'D matrices')\ncheck_matrices(P_librosa, P_FMP, 'Warping paths')\n    \nfig, ax = plt.subplots(2, 1, figsize=(9, 4))\ntitle='LibROSA implementation with step size condition $\\Sigma=\\{(2, 1), (1, 2), (1, 1)\\}$'\nplot_matrix_with_points(D_librosa, P_librosa, ax=[ax[0]], cmap=cmap,\n                                  xlabel='Sequence Y (Karajan)', ylabel='Sequence X (Bernstein)',\n                                  title=title,\n                                  marker='o', linestyle='-')\ntitle='libfmp implementation with step size condition $\\Sigma=\\{(2, 1), (1, 2), (1, 1)\\}$'\nplot_matrix_with_points(D_FMP, P_FMP, ax=[ax[1]], cmap=cmap,\n                                  xlabel='Sequence Y (Karajan)', ylabel='Sequence X (Bernstein)',\n                                  title=title,\n                                  marker='o', linestyle='-')\nplt.tight_layout()\n\nD matrices are equal.\nWarping paths are equal."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#전체-절차",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#전체-절차",
    "title": "8.3. 오디오 매칭",
    "section": "전체 절차",
    "text": "전체 절차\n\n오디오 매칭 시나리오에서 쿼리 오디오 조각 \\(\\mathcal{Q}\\) 및 데이터베이스의 녹음 컬렉션이 주어진다. 일반성을 잃지 않고(예: 모든 녹음을 연결하여) 이 컬렉션이 단일 문서 \\(\\mathcal{D}\\)로 표현된다고 가정한다.\n일반적인 매칭 방식은 다음과 같은 방식으로 진행된다.\n\n첫 번째 단계: 쿼리 \\(\\mathcal{Q}\\) 및 문서 \\(\\mathcal{D}\\)는 일련의 오디오 특징(features)으로 변환된다(예: 각각 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)). 쿼리 시퀀스의 길이 \\(N\\)는 일반적으로 데이터베이스 시퀀스의 길이 \\(M\\)보다 훨씬 짧다. 사용된 특징은 곡별 특성(예: 화성 및 멜로디 측면)을 캡처하는 동시에 연주별 변형(예: 로컬 템포, 아티큘레이션, 음표 실행 및 악기 연주)에 불변이어야 한다. 예를 들어 CENS 특징을 사용할 수 있다.\n두 번째 단계: 특징 시퀀스 \\(X\\) 및 \\(Y\\)를 기반으로 \\(X\\)와 유사한 \\(Y\\)의 하위시퀀스를 식별하려고 시도한다. 이를 위해 DTW의 대각선 매칭 하위시퀀스 변형과 같은 기술을 사용할 수 있다. 두 경우 모두 매칭 곡선 \\(\\Delta:[0:M-1]\\to\\mathbb{R}\\)를 얻는다. 0점에 가까운 \\(\\Delta\\)의 모든 로컬 최소값의 위치는 \\(X\\)와 유사한 \\(Y\\)의 하위 시퀀스를 가리킨다.\n세 번째 단계: \\(\\Delta\\)의 로컬 최소값을 선택하기 위한 적절한 전략을 사용하여 매칭 절차의 결과를 구성하는 매칭 하위시퀀스(매치(mathces) 라고 함)의 순위 목록을 도출한다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#구현-예시",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#구현-예시",
    "title": "8.3. 오디오 매칭",
    "section": "구현 예시",
    "text": "구현 예시\n\n다음에서는 위에서 설명한 대로 오디오 매칭 절차의 구체적인 구현 예를 제공한다. 최종 결과에 상당한 영향을 미칠 수 있는 많은 대안(예: 특징 표현, 정렬 전략 및 최소 선택과 관련하여)과 다양한 매개변수 선택이 있다.\n\n특징(feature) 표현으로 compute_cens_from_file 함수로 계산된 CENS 특징을 사용한다. 특히 \\(10~\\mathrm{Hz}\\)의 해상도를 가진 STFT 기반의 크로마 피처를 시작으로 해상도 \\(2~\\mathrm{Hz}\\)의 \\(\\mathrm{CENS}^{21}_{5}\\)-features를 사용한다.\n그런 다음 하위 시퀀스 DTW를 사용하여 매칭 함수를 계산한다. 구현에서는 단계 크기 조건 \\(\\Sigma=\\{(2, 1), (1, 2), (1, 1)\\}\\)와 함께 compute_matching_function_dtw 함수를 사용한다.\n매칭 전략은 위의 대각선 매칭에 설명된 간단한 반복 검색 절차를 따라 매칭 함수의 로컬 최소값을 식별한다. DTW의 경우 이러한 로컬 최소값은 일치하는 하위시퀀스의 끝 위치이다. 각 매치의 시작 위치를 얻으려면 최적의 워핑 경로(compute_optimal_warping_path_subsequence_dtw_21)를 계산하기 위한 역추적 전략을 적용해야 한다.\n\n\n\ndef compute_cens_from_file(fn_wav, Fs=22050, N=4410, H=2205, ell=21, d=5):\n    \"\"\"Compute CENS features from file\n\n    Args:\n        fn_wav (str): Filename of wav file\n        Fs (scalar): Feature rate of wav file (Default value = 22050)\n        N (int): Window size for STFT (Default value = 4410)\n        H (int): Hop size for STFT (Default value = 2205)\n        ell (int): Smoothing length (Default value = 21)\n        d (int): Downsampling factor (Default value = 5)\n\n    Returns:\n        X_CENS (np.ndarray): CENS features\n        L (int): Length of CENS feature sequence\n        Fs_CENS (scalar): Feature rate of CENS features\n        x_duration (float): Duration (seconds) of wav file\n    \"\"\"\n    x, Fs = librosa.load(fn_wav, sr=Fs)\n    x_duration = x.shape[0] / Fs\n    X_chroma = librosa.feature.chroma_stft(y=x, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\n    X_CENS, Fs_CENS = compute_cens_from_chromagram(X_chroma, Fs=Fs/H, ell=ell, d=d)\n    L = X_CENS.shape[1]\n    return X_CENS, L, Fs_CENS, x_duration\n\n\ndef compute_matching_function_dtw(X, Y, stepsize=2):\n    \"\"\"Compute CENS features from file\n\n    Args:\n        X (np.ndarray): Query feature sequence (given as K x N matrix)\n        Y (np.ndarray): Database feature sequence (given as K x M matrix)\n        stepsize (int): Parameter for step size condition (1 or 2) (Default value = 2)\n\n    Returns:\n        Delta (np.ndarray): DTW-based matching function\n        C (np.ndarray): Cost matrix\n        D (np.ndarray): Accumulated cost matrix\n    \"\"\"\n    C = cost_matrix_dot(X, Y)\n    if stepsize == 1:\n        D = compute_accumulated_cost_matrix_subsequence_dtw(C)\n    if stepsize == 2:\n        D = compute_accumulated_cost_matrix_subsequence_dtw_21(C)\n    N, M = C.shape\n    Delta = D[-1, :] / N\n    return Delta, C, D\n\n\ndef matches_dtw(pos, D, stepsize=2):\n    \"\"\"Derives matches from positions for DTW-based strategy\n\n    Args:\n        pos (np.ndarray): End positions of matches\n        D (np.ndarray): Accumulated cost matrix\n        stepsize (int): Parameter for step size condition (1 or 2) (Default value = 2)\n\n    Returns:\n        matches (np.ndarray): Array containing matches (start, end)\n    \"\"\"\n    matches = np.zeros((len(pos), 2)).astype(int)\n    for k in range(len(pos)):\n        t = pos[k]\n        matches[k, 1] = t\n        if stepsize == 1:\n            P = compute_optimal_warping_path_subsequence_dtw(D, m=t)\n        if stepsize == 2:\n            P = compute_optimal_warping_path_subsequence_dtw_21(D, m=t)\n        s = P[0, 1]\n        matches[k, 0] = s\n    return matches\n    \n    \ndef compute_plot_matching_function_DTW(fn_wav_X, fn_wav_Y, fn_ann, \n                                       ell=21, d=5, stepsize=2, tau=0.2, num=5, ylim=[0,0.35]):\n    ann, _ = read_structure_annotation(fn_ann)\n    color_ann = {'Theme': [0, 0, 1, 0.1], 'Match': [0, 0, 1, 0.2]}\n    X, N, Fs_X, x_duration = compute_cens_from_file(fn_wav_X, ell=ell, d=d)\n    Y, M, Fs_Y, y_duration = compute_cens_from_file(fn_wav_Y, ell=ell, d=d)\n    Delta, C, D = compute_matching_function_dtw(X, Y, stepsize=stepsize)\n    pos = mininma_from_matching_function(Delta, rho=2*N//3, tau=tau, num=num)\n    matches = matches_dtw(pos, D, stepsize=stepsize)\n\n    fig, ax = plt.subplots(2, 1, gridspec_kw={'width_ratios': [1], \n                                              'height_ratios': [1, 1]}, figsize=(8, 4))\n    cmap = compressed_gray_cmap(alpha=-10, reverse=True)\n    plot_matrix(C, Fs=Fs_X, ax=[ax[0]], ylabel='Time (seconds)',\n                         title='Cost matrix $C$ with ground truth annotations (blue rectangles)', \n                         colorbar=False, cmap=cmap)\n    plot_segments_overlay(ann, ax=ax[0], alpha=0.2, time_max=y_duration, \n                                   colors = color_ann, print_labels=False)\n\n    title = r'Matching function $\\Delta_\\mathrm{DTW}$ with matches (red rectangles)'\n    plot_signal(Delta,  ax=ax[1], Fs=Fs_X, color='k', title=title, ylim=ylim)\n    ax[1].grid()\n    plot_matches(ax[1], matches, Delta, Fs=Fs_X, s_marker='', t_marker='o')\n    plt.tight_layout()  \n    plt.show()\n\n\n베토벤 5번 교향곡\n\n베토벤 예제에 대한 매칭 절차를 적용한다. 쿼리로 첫 번째 테마(첫 번째 \\(21\\) 소절)의 번스타인 녹음을 사용한다. 제시부의 반복과 요약에서 약간의 음악적 수정과 함께 테마가 다시 한 번 나타난다. 데이터베이스 문서에 관해서는 세 가지 다른 버전, 번스타인과 카라얀이 지휘한 오케스트라 버전 두 개와 셰르바코프가 연주한 리스트의 피아노 편곡 버전을 고려한다.\n각 버전에 대해 비용 매트릭스 \\(C\\)를 테마에 대해 수동으로 주석이 달린 ground-truth 주석과 함께 시각화한다(버전당 3회 발생). 또한 상위 5개의 일치 항목과 결과의 매칭 함수를 보여준다\n\n\nImage(path_img+\"FMP_C7_Audio_Beethoven_Op067-01-001-021_Sibelius-Piano.png\", width=700)\n\n\n\n\n\nfn_wav_all = ['FMP_C7_Audio_Beethoven_Op067-01_Bernstein.wav',\n              'FMP_C7_Audio_Beethoven_Op067-01_Karajan.wav',\n              'FMP_C7_Audio_Beethoven_Op067-01_Scherbakov.wav']\n\nfn_ann_all = ['FMP_C7_Audio_Beethoven_Op067-01_Bernstein_Theme.csv',\n              'FMP_C7_Audio_Beethoven_Op067-01_Karajan_Theme.csv',\n              'FMP_C7_Audio_Beethoven_Op067-01_Scherbakov_Theme.csv']\n\nnames_all = ['Bernstein', 'Karajan', 'Scherbakov (piano version)']\n\nfn_wav_X = 'FMP_C7_Audio_Beethoven_Op067-01_Bernstein_Theme_1.wav'\n\nprint('--- QUERY: ---')\nipd.display(Audio(path_data+fn_wav_X))\n\nprint('--- DOCUMENTS: ---')\nfor i in range(3):\n    print(names_all[i])\n    #ipd.display(Audio(path_data+fn_wav_all[i])) # 용량 문제로 프린트 안함\n\n--- QUERY: ---\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n--- DOCUMENTS: ---\nBernstein\nKarajan\nScherbakov (piano version)\n\n\n\nfor f in range(3):\n    print('=== Query X: Bernstein (Theme 1); Database Y:', names_all[f],' ===')\n    compute_plot_matching_function_DTW(path_data+fn_wav_X,  path_data+fn_wav_all[f], path_data+fn_ann_all[f])\n\n=== Query X: Bernstein (Theme 1); Database Y: Bernstein  ===\n\n\n\n\n\n=== Query X: Bernstein (Theme 1); Database Y: Karajan  ===\n\n\n\n\n\n=== Query X: Bernstein (Theme 1); Database Y: Scherbakov (piano version)  ===\n\n\n\n\n\n\n그림에서 다음과 같은 관찰을 할 수 있다.\n\n매칭 함수는 세 가지 버전 모두에서 테마의 처음 두 발생을 명확하게 보여준다.\n악기의 차이로 인해 Scherbakov 피아노 버전에서 Karajan 오케스트라 버전보다 Bernstein 오케스트라 쿼리를 식별하기가 훨씬 더 어렵다.\n음악적 차이로 인해 제시부의 반복에서 주제의 두 번째 발생보다 요약에서 주제의 세 번째 발생을 식별하는 것이 훨씬 더 어렵다.\n\n\n\n\n쇼스타코비치 예\n\n두 번째 예로 쇼스타코비치의 Jazz Suite No.2 중 두 번째 왈츠를 들어보자. 이 곡은 \\(A_1A_2BA_3A_4\\) 형식으로, 여기서 \\(A\\) 부분은 \\(38\\) 소절로 구성되어 있으며 서로 다른 악기로 4번(\\(A_1\\), \\(A_2\\), \\(A_3\\) 및 \\(A_4\\) 부분) 나타난다. \\(A_1\\) 부분은 색소폰과 목악기로, \\(A_2\\) 부분은 현악기로, \\(A_3\\) 부분은 트롬본과 금관악기로, 마지막으로 \\(A_4\\) 부분은 투티 버전으로 멜로디를 연주한다.\n쿼리 \\(\\mathcal{Q}\\)로 Chailly 해석의 \\(A_1\\)의 \\(16\\)-measure 테마를 사용한다. 데이터베이스 문서로 Chailly 전체와 Yablonsky 전체 녹음을 각각 사용한다. 다음 그림에서는 위의 베토벤 예에서와 동일한 매칭 절차 및 매개 변수 설정을 사용한다.\n\n\nfn_wav_all = ['FMP_C7_Audio_Shostakovich_Waltz-02_Chailly.wav',\n              'FMP_C7_Audio_Shostakovich_Waltz-02_Yablonsky.wav']\n\nfn_ann_all = ['FMP_C7_Audio_Shostakovich_Waltz-02_Chailly_Theme.csv',\n              'FMP_C7_Audio_Shostakovich_Waltz-02_Yablonsky_Theme.csv']\n\nnames_all = ['Chailly', 'Yablonsky']        \n\nfn_wav_X = 'FMP_C7_Audio_Shostakovich_Waltz-02_Chailly_Theme_1.wav'\n\nprint('--- QUERY (saxophone, wood instrument): ---')\nipd.display(Audio(path_data+fn_wav_X))\n\nprint('--- DOCUMENTS: ---')\nfor i in range(2):\n    print(names_all[i])\n    #ipd.display(Audio(path_data+fn_wav_all[i])) # 용량 문제로 프린트 안함\n\n--- QUERY (saxophone, wood instrument): ---\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n--- DOCUMENTS: ---\nChailly\nYablonsky\n\n\n\nfor f in range(2):\n    print('=== Query X: Chailly (A1, 16 measures); Database Y:', names_all[f],' ===')\n    compute_plot_matching_function_DTW(path_data+fn_wav_X,  path_data+fn_wav_all[f], path_data+fn_ann_all[f], ylim=[0, 0.25])\n\n=== Query X: Chailly (A1, 16 measures); Database Y: Chailly  ===\n\n\n\n\n\n=== Query X: Chailly (A1, 16 measures); Database Y: Yablonsky  ===\n\n\n\n\n\n\n결과를 보면,\n\n두 버전의 4개 항목은 각각 상위 4개 일치 항목으로 나타난다.\nYablonsky 버전이 Chailly 버전보다 빠르지만 이러한 템포 변화는 DTW 기반 매칭 전략에 의해 성공적으로 처리된다.\n두 버전 모두에서 \\(A_3\\)(트롬본 버전)의 발생이 가장 큰 \\(\\Delta_\\mathrm{DTW}\\) 거리를 가진다. 이는 저음 악기(예: 트롬본)의 스펙트럼이 일반적으로 진동 및 스미어링과 같은 현상을 나타내어 “잡음이 많은” CENS 특징을 초래한다는 사실 때문이다.\n\n매칭 절차의 한계를 나타내기 위해 다음으로 \\(\\mathcal{Q}\\) 쿼리로 \\(A_3\\)(트롬본 버전)의 첫 번째 \\(16\\) 소절을 사용하고 실험을 반복한다. Yablonsky 버전의 \\(A_3\\) 테마는 올바르게 식별할 수 있었지만 이제 나머지 항목을 식별하기가 훨씬 더 어려워졌다. 결과적으로 false-positive 및 false-negative 매칭이 증가한다.\n\n\nfn_wav_X = 'FMP_C7_Audio_Shostakovich_Waltz-02_Chailly_Theme_3.wav'\n\nprint('--- QUERY (trombone): ---')\nipd.display(Audio(path_data+fn_wav_X))\n\n--- QUERY (trombone): ---\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfor f in range(2):\n    print('=== Query X: Chailly (A3, 16 measures); Database Y:',names_all[f],' ===')\n    compute_plot_matching_function_DTW(path_data+fn_wav_X,  path_data+fn_wav_all[f], path_data+fn_ann_all[f], ylim=[0,0.25])\n\n=== Query X: Chailly (A3, 16 measures); Database Y: Chailly  ===\n\n\n\n\n\n=== Query X: Chailly (A3, 16 measures); Database Y: Yablonsky  ===\n\n\n\n\n\n\n매칭 결과의 품질은 또한 쿼리 길이에 따라 결정적으로 달라진다. 짧은 기간의 쿼리는 일반적으로 낮은 특이성으로 인해 가짜 일치가 많이 발생하는 반면, 쿼리 길이를 늘려 특이성을 높이면 일반적으로 일치하는 횟수가 줄어든다.\n\\(\\mathcal{Q}\\) 쿼리로 Chailly 기록의 \\(A_3\\)의 \\(32\\) measures(\\(16\\) 대신)을 고려하여 이를 설명한다.\n\n\nfn_wav_X = 'FMP_C7_Audio_Shostakovich_Waltz-02_Chailly_Theme_3_32.wav'\nfn_ann_all = ['FMP_C7_Audio_Shostakovich_Waltz-02_Chailly_Theme_32.csv',\n              'FMP_C7_Audio_Shostakovich_Waltz-02_Yablonsky_Theme_32.csv']\n\nprint('--- QUERY (trombone - 32 measures): ---')\nipd.display(Audio(path_data+fn_wav_X))\n\n--- QUERY (trombone - 32 measures): ---\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfor f in range(2):\n    print('=== Query X: Chailly (A3, 32 measures); Database Y:',names_all[f],' ===')\n    compute_plot_matching_function_DTW(path_data+fn_wav_X, path_data+fn_wav_all[f], path_data+fn_ann_all[f], ylim=[0, 0.2])\n\n=== Query X: Chailly (A3, 32 measures); Database Y: Chailly  ===\n\n\n\n\n\n=== Query X: Chailly (A3, 32 measures); Database Y: Yablonsky  ==="
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#조옮김-불변-매칭-함수-transposition-invariant-matching-function",
    "href": "posts/8. Content-Based Audio Retrieval/8.3.Audio_Matching.html#조옮김-불변-매칭-함수-transposition-invariant-matching-function",
    "title": "8.3. 오디오 매칭",
    "section": "조옮김 불변 매칭 함수 (transposition-invariant matching function)",
    "text": "조옮김 불변 매칭 함수 (transposition-invariant matching function)\n\n검색 응용에서 오디오 발췌 부분이 다른 음악 키로 재생되더라도 식별할 수 있어야 되는 경우가 있다. 이제 쿼리와 일치하는 데이터베이스 섹션 간에 가능한 조옮김를 처리하는 방법에 대해 설명한다.\n예를 들어 Zager와 Evans의 “In the Year 2525”라는 노래를 생각해 보자(5.music structure에서 다룸). 이 노래는 \\(IV_1V_2V_3V_4V_5V_6V_7BV_8O\\)의 전체적인 음악적 구조를 가지고 있다. \\(I\\) 부분으로 표현되는 느린 인트로로 시작하며 \\(V\\) 부분으로 표현되는 노래의 절은 8번 나온다. 처음 4개의 벌스 섹션은 동일한 음악 키에 있지만 \\(V_5\\) 및 \\(V_6\\)는 위쪽으로 1반음 조옮김되고 \\(V_7\\) 및 \\(V_8\\)는 위쪽으로 2개 반음 조옮김된다.\n\n\nImage(\"../img/5.music_structure_analysis/FMP_C4_F13_ZagerEvans_InTheYear2525.png\", width=600)\n\n\n\n\n\n\\(12\\)차원 크로마 축을 따라 크로마 특징을 순환적으로 이동하여 조옮김 시뮬레이션을 한다.\n\ncyclic shift operator \\(\\rho:\\mathbb{R}^{12} \\to \\mathbb{R}^{12}\\):\n\n\\(\\rho(x):=(x(11),x(0),x(1),\\ldots,x(10))^\\top\\)\nfor \\(x=(x(0),x(1),\\ldots,x(10),x(11))^\\top\\in\\mathbb{R}^{12}\\).\n\n\n\\(\\rho\\)를 연속적으로 적용하면 \\(i\\in[0:11]\\)에 대한 12개의 서로 다른 순환 이동 \\(\\rho^i\\)를 얻을 수 있다. 또한 \\(\\rho^i(X)=(\\rho^i(x_1),\\rho^i(x_2),\\ldots,\\rho^i(x_N))\\)를 \\(i\\in[0:11]\\)에 대한 쿼리 \\(X\\)라고 하자. 그런 다음 12개의 시퀀스 \\(\\rho^i(X)\\) 각각을 별도의 쿼리로 사용하여 데이터베이스에서 매칭 항목을 검색한다. 이를 위해 먼저 각 \\(\\rho^i(X)\\) 및 \\(Y\\)에 대해 \\(\\Delta^{i}\\)와 같은 별도의 매칭 함수를 계산한다.\n조옮김 불변 매칭 함수(transposition-invariant matching function) \\(\\Delta^\\mathrm{TI}\\)는 다음을 설정하여 얻는다. \\[\\Delta^\\mathrm{TI}(m):= \\min_{i\\in [0:11]} \\Delta^{i}(m)\\] for \\(m\\in[0:M-1]\\).\n이제 Zager와 Evans의 노래 “In the Year 2525”를 통해 첫 절 섹션 \\(V_1\\)을 쿼리 \\(X\\)로 사용하고, 전체 노래를 데이터베이스 시퀀스 \\(Y\\)로 사용한다. 이전과 같이 DTW 기반 매칭 접근 방식에 대해 동일한 구현을 사용한다. \\(\\Delta^{0}=\\Delta_\\mathrm{DTW}\\)이다. 시각화에서는 \\(i=0,1,2,3\\)에 대한 매칭 함수 \\(\\Delta^{i}\\)와 조옮김 불변 일치 함수 \\(\\Delta^\\mathrm{TI}\\)를 보여준다. 처음 8개의 로컬 최소값 \\(\\Delta^\\mathrm{TI}\\)는 8개의 절 섹션의 끝 위치를 올바르게 나타낸다.\n\n\ndef compute_matching_function_dtw_ti(X, Y, cyc=np.arange(12), stepsize=2):\n    \"\"\"Compute transposition-invariant matching function\n\n    Args:\n        X (np.ndarray): Query feature sequence (given as K x N matrix)\n        Y (np.ndarray): Database feature sequence (given as K x M matrix)\n        cyc (np.nda(rray): Set of cyclic shift indices to be considered (Default value = np.arange(12))\n        stepsize (int): Parameter for step size condition (1 or 2) (Default value = 2)\n\n    Returns:\n        Delta_TI (np.ndarray): Transposition-invariant matching function\n        Delta_ind (np.ndarray): Cost-minimizing indices\n        Delta_cyc (np.ndarray): Array containing all matching functions\n    \"\"\"\n    M = Y.shape[1]\n    num_cyc = len(cyc)\n    Delta_cyc = np.zeros((num_cyc, M))\n    for k in range(num_cyc):\n        X_cyc = np.roll(X, k, axis=0)\n        Delta_cyc[k, :], C, D = compute_matching_function_dtw(X_cyc, Y, stepsize=stepsize)\n    Delta_TI = np.min(Delta_cyc, axis=0)\n    Delta_ind = np.argmin(Delta_cyc, axis=0)\n    return Delta_TI, Delta_ind, Delta_cyc\n\n\nfn_wav = 'FMP_C4_F13_ZagerEvans_InTheYear2525.wav'\nfn_ann = 'FMP_C4_F13_ZagerEvans_InTheYear2525.csv'\nfn_wav_X = 'FMP_C7_Audio_ZagerEvans_InTheYear2525_Part-V1.wav'\n\n#ipd.display(Audio(path_data+fn_wav))\n\n\nann, _ = read_structure_annotation(path_data+fn_ann)\nann_color = {'I': 'white', 'V1': 'red', 'V2': 'red', 'V3': 'red', 'V4': 'red', 'V5': 'green', 'V6': 'green',\n             'V7': 'blue', 'B': 'white', 'V8': 'blue', 'O': 'gray', '': 'white'}\n\nX, N, Fs_X, x_duration = compute_cens_from_file(path_data+fn_wav_X, ell=21, d=5)\nY, M, Fs_Y, y_duration = compute_cens_from_file(path_data+fn_wav, ell=21, d=5)\n\nDelta_TI, Delta_ind, Delta_cyc = compute_matching_function_dtw_ti(X, Y)\npos = mininma_from_matching_function(Delta_TI, rho=2*N//3, tau=0.1, num=8)\n\n\nfig, ax = plt.subplots(6, 1, figsize=(7, 8), gridspec_kw={'height_ratios': [1, 1, 1, 1, 1, 0.25]}) \n\ncolor_set = ['red', 'green', 'blue', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray', 'gray']\nfor k in range(4):\n    plot_signal(Delta_cyc[k,:], ax=ax[k], xlabel='', ylabel = r'$\\Delta^{%d}$' % k,\n                         color=color_set[k], ylim=[0, 0.3])\n    ax[k].grid()\n\nfor k in range(12):\n    plot_signal(Delta_cyc[k,:], ax=ax[4], color=color_set[k], ylim=[0, 0.3])\n    \nplot_signal(Delta_TI, ax=ax[4], color='k', linewidth='3', ylim=[0, 0.3],\n                     ylabel = r'$\\Delta^{\\mathrm{TI}}$', xlabel='')\nax[4].grid()    \nplot_segments(ann, ax=ax[5], nontime_axis=False, adjust_nontime_axislim=False,\n                       colors=ann_color, alpha=0.25)\nax[4].plot(pos, Delta_TI[pos], 'ro')\nax[5].set_xlabel('Time (seconds)')\nplt.tight_layout()\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S2_CENS.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S2_DiagonalMatching.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S2_SubsequenceDTW.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S2_AudioMatching.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html",
    "title": "8.4. 버전 식별",
    "section": "",
    "text": "내용 기반 오디오 검색 중 커버 곡이나 리믹스 등 같은 음악의 다른 버전을 인식하는 버전 식별(version identification)에 대해 설명한다. 공통 하위시퀀스 매칭, 매칭의 평가 지표 등에 대해 다룬다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#정렬-시나리오",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#정렬-시나리오",
    "title": "8.4. 버전 식별",
    "section": "정렬 시나리오",
    "text": "정렬 시나리오\n\n우선 다양한 정렬(alignment) 시나리오와 그 이면의 원칙에 대해 간략하게 살펴보자. 시퀀스 정렬의 일반적인 목적은 주어진 두 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)이 공유하는 유사성 영역을 식별하는 것이다. 동시에, 일치하는 영역의 요소가 대응된다.\n일반적으로 정렬의 두 범주인 전역(global) 정렬과 로컬(local) 정렬을 구분할 수 있다. 전역의 경우, 정렬이 두 시퀀스의 전체 길이에 걸쳐 있도록 강제한다. 대조적으로 로컬의 경우에 일반적으로 광범위하게 발산하는 더 긴 시퀀스 내에서 유사성 영역을 식별하려고 시도한다.\n\n동적 시간 워핑(DTW)을 다룰 때 워핑 경로의 개념에 의해 수학적으로 모델링된 전역적 정렬 기술을 접한 적이 있다. 이 정의의 경계 조건(boundary condition)은 두 개의 지정된 시퀀스 \\(X\\) 및 \\(Y\\)가 전역적으로 정렬되도록 했다. 또한 단계 크기 조건(step size condition)을 적절하게 수정하여 정렬의 연속성 정도를 조정할 수 있었다.\n오디오 매칭에 사용되는 하위 시퀀스(subsequence) DTW에서 두 시퀀스 \\(X\\) 및 \\(Y\\)는 다른 방법을 사용해 다뤄졌다. 시퀀스 \\(X\\)는 전체적으로 정렬되어야 했지만 시퀀스 \\(Y\\)의 하위 시퀀스만 일치하는 대응 항목으로 충분했다. 따라서 이 시나리오에는 혼합된 전역-로컬 정렬 접근 방식이 필요했다. 기술적으로 시퀀스 \\(Y\\)에 대한 경계 조건을 완화하여 비용 없이 정렬에서 접미사(suffix)와 접두사(prefix)를 생략할 수 있도록 하는 것이었다.\n\n이 포스트에서는 \\(X\\) 및 \\(Y\\) 두 시퀀스 모두에 대해 매칭 하위 시퀀스만 식별되는 로컬 정렬 문제를 처리한다. 매칭을 계산하기 위해 시퀀스 \\(X\\) 및 \\(Y\\) 모두에 대한 경계 조건이 제거된다. 또한 두 번째 기술적 수정이 필요하다. 앞의 두 시나리오에서는 적어도 시퀀스 \\(X\\)의 측면에 전역적 제약이 있는 경우 비용 최소화 경로를 고려하여 정렬을 계산했다. 이제 전역 제약 조건이 없는 로컬 시나리오에서 이러한 접근 방식은 빈 정렬로 이어진다. 따라서 양수 점수(관련 정보) 및 음수 점수(관련 없는 정보)가 있는 점수 매트릭스를 기반으로 점수 최대화 정렬을 살펴봄으로써 다른 최적화 기준을 도입한다.\n고려된 경계 조건 외에도 정렬 결과의 특성에 결정적으로 영향을 미치는 다른 요소가 있다. 예를 들어 부분 매칭(partial matching)에서는 단계 크기 조건을 제거하고 훨씬 더 약한 단조성 조건으로 대체한다.\n다음 그림은 서로 다른 정렬 시나리오 간의 개념적 차이점을 요약한 것이다.\n\n\nImage(path_img+\"FMP_C7_F23.png\" ,width=500)"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#공통-하위시퀀스-매칭-문제의-공식화",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#공통-하위시퀀스-매칭-문제의-공식화",
    "title": "8.4. 버전 식별",
    "section": "공통 하위시퀀스 매칭 문제의 공식화",
    "text": "공통 하위시퀀스 매칭 문제의 공식화\n\n버전 식별(version identification)을 위해 로컬 정렬 문제가 필요하며, 이 때 쿼리와 데이터베이스 문서가 특정 부분에서 유사한 톤 진행을 공유한다고 가정하지만 일치가 발생하는 유사성 정도와 기간 또는 위치는 알지 못한다.\n이 맥락에서 매칭 작업은 다음과 같이 공식화될 수 있다. 특징 공간에 대해 시퀀스 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)가 주어지면 피처 공간 \\(\\mathcal{F}\\)에서 \\(X\\) 내의 하위 시퀀스와 \\(Y\\) 내의 하위 시퀀스가 가능한 한 유사하도록 두 하위 시퀀스를 찾는다. 또한 이 두 하위시퀀스의 비교에서 시간적 변형을 처리할 수 있어야 한다.\n하위시퀀스 DTW의 경우와 마찬가지로 이 작업은 최적화 문제의 형태로 표현할 수 있으며 동적 프로그래밍을 사용하여 효율적으로 해결할 수 있다. 그러나 주어진 문제에 대해 다른 관점을 가정할 필요가 있다. 하위 시퀀스 DTW에서는 쿼리 \\(X\\)와 일치하는 \\(Y\\)의 비용 최소화 하위 시퀀스를 찾았다. 이 최적화 기준은 전체 쿼리 시퀀스 \\(X\\)가 \\(Y\\)의 하위 시퀀스와 일치하도록 강제되었기 때문에 작동했다.\n이제 \\(X\\)의 하위 시퀀스만 \\(Y\\) 중 하나와 일치해야 한다고 가정하면, 빈 하위 시퀀스라는 간단한 방법이 있다. 실제로 두 개의 빈 시퀀스를 일치시키면 전체 비용이 0이 된다. 이는 비용 행렬 \\(\\mathbf{C}\\)에 음수 값이 없다고 가정할 때 최적의 솔루션이다. 하지만 이는 원하는 바가 아니다. 직관적으로 말하면, 최소한의 비용으로 일치시킬 수 있는 하위 시퀀스를 찾고 있을 뿐만 아니라 특정 관련성이 있는 긴 하위 시퀀스도 찾고 있다. 즉, 동시에 두 가지 원칙의 균형을 맞춰야 한다.\n\n전체적인 매칭 비용을 최소화하고,\n하위 시퀀스의 길이를 최대화한다.\n\n이 문제를 해결하기 위해 비용 매트릭스 대신 점수(score) 행렬(또는 유사성(similarity) 행렬)을 사용하여 양의(positive) 관점을 가정한다. 그런 다음 비용을 최소화하는 하위시퀀스를 식별하는 대신 점수를 최대화하는 하위 시퀀스를 찾는다. 하위 시퀀스의 비교에 대한 추가 제약은 적절한 단계 크기(step size) 조건뿐만 아니라 음의 페널티 값을 도입하여 부과한다.\n유사성 측정 \\(s:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\)를 고정하면 다음을 설정하여 \\(N\\times M\\) 점수 행렬을 계산할 수 있다. \\[\\mathbf{S}(n,m):=s(x_n,y_m)\\] for \\(n\\in[1:N]\\) and \\(m\\in[1:M]\\)\n이 점수 행렬의 속성은 자기 유사성 행렬의 경우에 대해서 설명했던 것과 동일한 향상을 사용하여 더욱 향상될 수 있다. 특히 임계값 파라미터 \\(\\tau>0\\)와 페널티 파라미터 \\(\\delta\\leq 0\\)에 대한 임계값 적용은 중요한 단계이다.\n관련 유사성 관계를 나타낼 수 있는 셀은 양의 점수를 갖는 반면 다른 모든 셀에는 음의 점수를 부여하는 방식으로 점수 행렬이 구성된다. 이 속성은 가능한 큰 점수를 누적하는 경로 구성 요소를 찾으려고 시도하는 다음의 절차에서 중요하다. 이러한 경로는 주로 \\(\\mathbf{S}\\)의 양수 부분에 있으며 음수 점수의 셀을 통과하는 것을 피한다.\n공통 하위 시퀀스를 찾는 문제를 더 공식화하려면 두 피쳐 시퀀스를 비교할 때 시간적 변형을 설명하는 경로 개념이 필요하다. 자기 유사성 행렬의 맥락에서와 같이 경로는 시퀀스 \\(P=((n_1,m_1), \\ldots,(n_L,m_L))\\)로 정의된다. 이 때 셀 \\((n_\\ell,m_\\ell)\\in[1:N]\\times[1:M]\\), $ $이며, 허용되는 단계 크기의 집합 \\(\\Sigma\\)에 대해 \\((n_{\\ell+1},m_{\\ell+1}) -(n_\\ell,m_\\ell)\\in \\Sigma\\)를 만족한다.\n따라서 \\(\\Sigma=\\{(0,1),(1,0),(1,1)\\}\\)를 선택하면 이 정의는 경계 조건을 생략한 것 빼고는 워핑 경로에 대한 정의와 동일하다. 그 이유는 시퀀스 \\(X\\) 및 \\(Y\\)를 전체적으로 정렬하지 않고 하위 시퀀스만 정렬하기 때문이다. 경로 \\(P\\)에 대해 유도된(induced) 세그먼트 \\(\\pi_1(P):=[n_1:n_L]\\) 및 \\(\\pi_2(P):=[m_1:m_L]\\)를 연결한다. \\(P\\)의 점수 \\(\\sigma(P)\\)는 \\(\\sigma(P) := \\sum_{\\ell=1}^L s(n_\\ell,m_\\ell)\\)로 정의된다.\n이러한 정의를 사용하여 가능한 모든 경로(임의의 시작 및 끝 위치 포함)에서 점수 최대화 경로 \\(P^\\ast := \\underset{P}{\\mathrm{argmax}} \\,\\,\\sigma(P)\\)를 찾는 최적화 작업을 진행한다. \\(X\\) 및 \\(Y\\)의 가장 잘 일치하는 두개의 하위 시퀀스는 유도된 세그먼트 \\(\\pi_1(P^\\ast)\\) 및 \\(\\pi_2(P^\\ast)\\)에 의해 각각 주어진다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#동적-프로그래밍을-사용한-최적화-알고리즘",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#동적-프로그래밍을-사용한-최적화-알고리즘",
    "title": "8.4. 버전 식별",
    "section": "동적 프로그래밍을 사용한 최적화 알고리즘",
    "text": "동적 프로그래밍을 사용한 최적화 알고리즘\n\n점수 극대화 경로는 DTW 알고리즘과 비슷하게 동적 프로그램(DP)로 계산될 수 있다. 이를 위해 \\(N\\times M\\) 누적 점수 행렬 (accumulated score matrix) \\(\\mathbf{D}\\)를 다음과 같이 정의한다. \\[\\mathbf{D}(n,m) := \\max\\{\\sigma(P) \\mid \\mbox{$P$ is path ending at $(n,m)$ }\\}\\] for \\(n\\in[1:N]\\) and \\(m\\in[1:M]\\).\n다시 말해, \\(\\mathbf{D}(n,m)\\)는 임의의 셀에서 시작하여 \\((n,m)\\)에서 끝나는 경로로 누적된 가능한 최대 점수이다. 이 정의에 따르면, 빈 경로 \\(P=\\emptyset\\)는 점수 \\(\\sigma(P)=0\\)의 경로로 생각될 수 있다. \\(n=1\\) 혹은 \\(m=1\\)의 셀 \\((n,m)\\)에 대해 다음을 얻을 수 있다. \\[\\mathbf{D}(1,1)=\\max\\big\\{0,\\mathbf{S}(1,1)\\big\\}\\] \\[\\mathbf{D}(n,1)=\\max\\big\\{0,\\mathbf{D}(n-1,1)+\\mathbf{S}(n,1)\\big\\} \\mbox{ for } n\\in [2:N]\\] \\[\\mathbf{D}(1,m)=\\max\\big\\{0,\\mathbf{D}(1,m-1)+\\mathbf{S}(1,m)\\big\\}  \\mbox{ for } m\\in [2:M]\\]\n\n이는 재귀(recursion)에 대한 바운더리를 구성한다\n\n\\(n\\in [2:N]\\)와 \\(m\\in [2:M]\\)에 대해서는 다음을 통해 \\(\\mathbf{D}\\)를 계산할 수 있다. \\[\\mathbf{D}(n,m)= \\max\\left\\{\n           \\begin{array}{l}\n           0, \\\\\n           \\mathbf{D}(n-1,m-1) + \\mathbf{S}(n,m), \\\\\n           \\mathbf{D}(n-1,m) + \\mathbf{S}(n,m), \\\\\n           \\mathbf{D}(n,m-1)+ \\mathbf{S}(n,m) \\end{array}\\right.\\]\n이 재귀는 두 가지 면에서 DTW에 사용되는 것과 다르다.\n\n첫째, 초과 비용을 최소화하는 대신 초과(음수일 수 있음) 점수를 최대화한다.\n두 번째 차이점은 최대화에 0값이 포함된다는 것이다. 이를 통해 잠재적으로 음의 점수를 누적하지 않고도 모든 위치에서 경로를 시작할 수 있다. 이는 시퀀스 \\(X\\)와 \\(Y\\)를 비교할 때 시작 부분을 건너뛸 수 있다는 아이디어를 실현한다.\n\n고려된 경로에 경계 제약을 두지 않기 때문에, 점수 최대화 경로 \\(P^\\ast\\)는 임의의 셀에서 끝날 수 있다. 따라서 가능한 모든 경로에서 최대 점수를 얻으려면 \\(\\mathbf{D}\\)의 최대 항목을 살펴봐야 한다. \\[\\mathbf{D}^\\mathrm{max}:=\\sigma(P^\\ast)=\\max_{(n,m)\\in[1:N]\\times[1:M]}\\mathbf{ D}(n,m).\\]\n일반적으로 최대값을 갖는 \\(\\mathbf{D}\\) 항목이 여러 개 있을 수 있다. 최적 경로 \\(P^\\ast\\)의 끝 위치를 정의하는 \\(q_1\\)와 같은 항목 중 하나로 시작한다. 전체 경로는 DTW에서와 마찬가지로 역추적(backtracking)을 통해 얻는다.\n그러나 이번에는 역추적의 정지 조건(stop condition)이 다르다. \\(q_1,q_2,\\ldots,q_\\ell\\)을 반복적으로 결정된 셀이라고 하면, 역추적은 셀 \\(q_\\ell=(a,b)\\)이 \\(\\mathbf{D}(a ,b)=0\\) 또는 \\(q_\\ell=(1,1)\\)에 도달하면 멈춘다.\n첫 번째 경우 \\(q_\\ell=(a,b)\\) 셀은 비양수(nonpositive) 점수 \\(\\mathbf{S}(a,b)\\leq 0\\)이므로 제외된다. 이렇게 하면 최적의 경로 \\(P^\\ast=(q_{\\ell-1},\\ldots,q_1)\\)가 생성된다. 두 번째 경우 경로는 \\(\\mathbf{S}(1,1)>0\\)인 경우 \\(q_\\ell=(1,1)\\) 셀로 시작하거나, \\(\\mathbf{S}(1,1)\\leq 0\\)인 경우 셀 \\(q_{\\ell-1}\\)로 시작한다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#간단한-예",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#간단한-예",
    "title": "8.4. 버전 식별",
    "section": "간단한 예",
    "text": "간단한 예\n\n이 절차를 설명하기 위해 두 시퀀스 \\(X=(x_1,\\ldots,x_5)\\) 및 \\(Y=(y_1,\\ldots,y_6)\\)에 대한 예를 보자. 시퀀스와 로컬 유사성 측정을 명시적으로 지정하는 대신 이 예에서 유사성 행렬 \\(\\mathbf{S}\\)가 직접 주어진다고 가정한다. 다음 그림은 \\(\\mathbf{S}\\) 및 결과 누적 점수 행렬 \\(\\mathbf{D}\\)를 보여준다. \\(\\mathbf{D}\\) 행렬은 \\((3,5)\\) 셀에서 최대값을 가정한다. 역추적은 점수 최대화 경로 \\(P^\\ast = ((1,3),(2,3),(2,4),(3,5))\\), \\(\\sigma(P^\\ast)= 5\\)를 얻는다. 유도된 두 개의 세그먼트는 \\(\\pi_1(P^\\ast)=[1:3]\\) 및 \\(\\pi_2(P^\\ast)=[3:5]\\)이며, 이는 하위 시퀀스 \\((x_1,x_2,x_3 )\\) 및 \\((y_3,y_4,y_5)\\)를 얻는다.\n\n\nImage(path_img+\"FMP_C7_F20.png\", width=600)\n\n\n\n\n\n# Example from Figure 7.20, [Müller, FMP, Springer 2015]\nS = np.array([[1,  -2,  1,  1,  0, -2],\n              [0,  -2,  1,  2, -2,  1],\n              [0,   1, -2, -2,  1, -2],\n              [-2,  1, -2,  1, -2, -2],\n              [-2, -2,  1, -2,  1,  0]])\n\nprint('Score matrix S = ', S, sep='\\n')\n\nScore matrix S = \n[[ 1 -2  1  1  0 -2]\n [ 0 -2  1  2 -2  1]\n [ 0  1 -2 -2  1 -2]\n [-2  1 -2  1 -2 -2]\n [-2 -2  1 -2  1  0]]\n\n\n\ndef compute_accumulated_score_matrix_common_subsequence(S):\n    \"\"\"Given the score matrix, compute the accumulated score matrix\n    for common subsequence matching with step sizes {(1, 0), (0, 1), (1, 1)}\n\n    Args:\n        S (np.ndarray): Score matrix\n\n    Returns:\n        D (np.ndarray): Accumulated score matrix\n    \"\"\"\n    N, M = S.shape\n    D = np.zeros((N, M))\n\n    D[0, 0] = max(0, S[0, 0])\n\n    for n in range(1, N):\n        D[n, 0] = max(0, D[n-1, 0] + S[n, 0])\n\n    for m in range(1, M):\n        D[0, m] = max(0, D[0, m-1] + S[0, m])\n\n    for n in range(1, N):\n        for m in range(1, M):\n            D[n, m] = max(0, D[n-1, m-1] + S[n, m], D[n-1, m] + S[n, m], D[n, m-1] + S[n, m])\n\n    return D\n\n\nD = compute_accumulated_score_matrix_common_subsequence(S)\nDmax = np.max(D)\nn, m = divmod(np.argmax(D), D.shape[1])\nprint('Accumulated score matrix D = ', D, sep='\\n')\nprint('Maximal accumulated score Dmax = ', Dmax)\nprint('Maximizing cell (n,m) = (%d,%d)' % (n, m))\n\nAccumulated score matrix D = \n[[1. 0. 1. 2. 2. 0.]\n [1. 0. 2. 4. 2. 3.]\n [1. 2. 0. 2. 5. 3.]\n [0. 3. 1. 3. 3. 3.]\n [0. 1. 4. 2. 4. 4.]]\nMaximal accumulated score Dmax =  5.0\nMaximizing cell (n,m) = (2,4)\n\n\n\n마지막으로 역추적을 사용하여 최적 경로 \\(P^\\ast\\)와 유도된 세그먼트 \\(\\pi_1(P^\\ast)\\)와 \\(\\pi_2(P^\\ast)\\)를 얻는다.\n\n\ndef compute_optimal_path_common_subsequence(D, cellmax=True, n=0, m=0):\n    \"\"\"Given an accumulated score matrix, compute the score-maximizing path\n    for common subsequence matching with step sizes {(1, 0), (0, 1), (1, 1)}\n\n    Args:\n        D (np.ndarray): Accumulated score matrix\n        cellmax (bool): If \"True\", score-maximizing cell will be computed (Default value = True)\n        n (int): Index (first axis) of cell for backtracking start; only used when cellmax=False (Default value = 0)\n        m (int): Index (second axis) of cell for backtracking start; only used when cellmax=False (Default value = 0)\n\n    Returns:\n        P (np.ndarray): Score-maximizing path (array of index pairs)\n    \"\"\"\n    if cellmax:\n        # n, m = np.unravel_index(np.argmax(D), D.shape)  # doesn't work with jit\n        n, m = divmod(np.argmax(D), D.shape[1])\n    P = [(n, m)]\n\n    while ((n, m) != (0, 0) and (D[n, m] != 0)):\n        if n == 0:\n            cell = (0, m-1)\n        elif m == 0:\n            cell = (n-1, 0)\n        else:\n            val = max(D[n-1, m-1], D[n-1, m], D[n, m-1])\n            if val == D[n-1, m-1]:\n                cell = (n-1, m-1)\n            elif val == D[n-1, m]:\n                cell = (n-1, m)\n            else:\n                cell = (n, m-1)\n        P.append(cell)\n        n, m = cell\n    if (D[n, m] == 0):\n        del P[-1]\n    P.reverse()\n    P = np.array(P)\n    return P\n\n\ndef get_induced_segments(P):\n    \"\"\"Given a path, compute the induces segments\n\n    Args:\n        P (np.ndarray): Path (list of index pairs)\n\n    Returns:\n        seg_X (np.ndarray): Induced segment of first sequence\n        seg_Y (np.ndarray): Induced segment of second sequence\n    \"\"\"\n    seg_X = np.arange(P[0, 0], P[-1, 0] + 1)\n    seg_Y = np.arange(P[0, 1], P[-1, 1] + 1)\n    return seg_X, seg_Y\n\n\nP = compute_optimal_path_common_subsequence(D)\nseg_X, seg_Y = get_induced_segments(P)\n\nprint('Optimal path P =', P, sep='\\n')\nprint('Induced segment for X:', seg_X)\nprint('Induced segment for Y:', seg_Y)\n\nOptimal path P =\n[[0 2]\n [0 3]\n [1 3]\n [2 4]]\nInduced segment for X: [0 1 2]\nInduced segment for Y: [2 3 4]\n\n\n\n# a sanity check\n\nscore_P = sum(S[n, m] for (n, m) in P)\nprint('Total score of optimal path:', score_P)\nprint('Maximal accumulated score Dmax = ', Dmax)\n\nTotal score of optimal path: 5\nMaximal accumulated score Dmax =  5.0\n\n\n\nplt.figure(figsize=(9, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(S, cmap='PuOr', origin='lower', aspect='equal')\nplt.plot(P[:, 1], P[:, 0], marker='o', color='r')\nplt.clim([np.min(S), np.max(S)])\nplt.colorbar()\nplt.title('$\\mathbf{S}$ with optimal path')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.subplot(1, 2, 2)\nplt.imshow(D, cmap='gray_r', origin='lower', aspect='equal')\nplt.plot(P[:, 1], P[:, 0], marker='o', color='r')\nplt.clim([0, np.max(D)])\nplt.colorbar()\nplt.title('$\\mathbf{D}$ with optimal path')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#부분-매칭-partial-matching",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#부분-매칭-partial-matching",
    "title": "8.4. 버전 식별",
    "section": "부분 매칭 (Partial Matching)",
    "text": "부분 매칭 (Partial Matching)\n\n단계 크기(step size) 조건을 훨씬 더 약한 단조성(monotonicity) 조건으로 대체하여 정렬 요구 사항을 더 완화할 수 있다. 이에 대해 부분 매칭(parital matching)이라는 최적화 문제를 생각할 수 있다.\n이 문제를 공식화하고 동적 프로그래밍을 기반으로 효율적인 알고리즘을 보도록 한다. \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)가 특징 공간 \\(\\mathcal{F}\\)에 대한 두 시퀀스라고 가정한다. \\(X\\)와 \\(Y\\) 사이의 길이 \\(L\\in\\mathbb{N}_0\\)의 부분 매칭은 셀 \\((n_\\ell,m_\\ell)\\in[1:N]\\times[1:M]\\), \\(\\ell\\in[1:L]\\)의 시퀀스 \\(P=((n_1,m_1), \\ldots,(n_L, m_L))\\)로 정의되며, 강하게 단조 증가한다: \\[n_1<n_2<\\ldots < n_L \\quad\\mbox{and}\\quad m_1< m_2< \\ldots < m_L\\]\n유사성 척도 \\(s:\\mathcal{F}\\times\\mathcal{F}\\to \\mathbb{R}\\)가 주어지면 유사성 행렬 \\(\\mathbf{S}\\)를 \\(\\mathbf{S}(n ,m):=s(x_{n},y_{m})\\)로 정의한다. 그런 다음 부분 매칭 \\(P\\)의 총 점수 \\(\\sigma(P)\\)는 \\(\\sigma(P) := \\sum_{\\ell = 1}^{L} \\mathbf{S}(n_\\ell,m_\\ell)\\)로 구체화된다.\n\n\nImage(path_img+\"FMP_C7_E10.png\", width=600)\n\n\n\n\n\n“nested” 루프에서 점수 값 \\(\\mathbf{S}(n,m)\\)는 DTW 알고리즘과 달리 대각선 스텝 크기의 경우에만 추가된다. 여기서 비용 값 \\(\\mathbf{C}(n,m)\\)는 세 스텝 크기 모두의 경우에 추가된다. 또한 누적 점수 \\(\\mathbf{D}(n,m)\\)를 산출하는 최적의 부분 매칭은 \\(\\mathbf{D}(n,m)\\)를 산출하는 최적의 워핑 경로가 항상 \\((n,m)\\) 셀로 끝나는 DTW 알고리즘과 달리 반드시 셀 \\((n,m)\\)에서 끝나지는 않는다.\n다음 코드 셀에서는 부분 매칭 절차에 대한 참조 구현을 제공한다.\n\n\ndef compute_partial_matching(S):\n    \"\"\"Given the score matrix, compute the accumulated score matrix\n    for partial matching\n\n    Args:\n        S (np.ndarray): Score matrix\n\n    Returns:\n        D (np.ndarray): Accumulated score matrix\n        P (np.ndarray): Partial match (array of index pairs)\n    \"\"\"\n    N, M = S.shape\n    D = np.zeros((N+1, M+1))\n    for n in range(1, N+1):\n        for m in range(1, M+1):\n            D[n, m] = max(D[n, m-1], D[n-1, m], D[n-1, m-1] + S[n-1, m-1])\n\n    P = []\n    n = N\n    m = M\n    while (n > 0) and (m > 0):\n        if D[n, m] == D[n, m-1]:\n            m = m - 1\n        elif D[n, m] == D[n-1, m]:\n            n = n - 1\n        else:\n            P.append((n-1, m-1))\n            n = n - 1\n            m = m - 1\n    P.reverse()\n    P = np.array(P)\n    return D, P\n\n\nD, P = compute_partial_matching(S)\n\nplt.figure(figsize=(9, 3))\nplt.subplot(1, 2, 1)\nplt.imshow(S, cmap='PuOr', origin='lower', aspect='equal')\nplt.plot(P[:, 1], P[:, 0], marker='o', color='r', linestyle='')\nplt.clim([np.min(S), np.max(S)])\nplt.colorbar()\nplt.title('$\\mathbf{S}$ with optimal match')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.subplot(1, 2, 2)\nplt.imshow(D, cmap='gray_r', origin='lower', aspect='equal')\nplt.plot(P[:, 1]+1, P[:, 0]+1, marker='o', color='r', linestyle='')\nplt.clim([0, np.max(D)])\nplt.colorbar()\nplt.title('$\\mathbf{D}$ with optimal match')\nplt.xlabel('Sequence Y')\nplt.ylabel('Sequence X')\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#점수-행렬의-계산",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#점수-행렬의-계산",
    "title": "8.4. 버전 식별",
    "section": "점수 행렬의 계산",
    "text": "점수 행렬의 계산\n\n첫 번째 단계에서 쿼리와 데이터베이스 문서는 \\(X=(x_1,x_2,\\ldots,x_N)\\) 및 \\(Y=(y_1,y_2,\\ldots,y_M)\\)과 같이 크로마 기반 특징 시퀀스로 변환된다. 뉘앙스를 혼합하고 싶기 때문에 CENS 특징과 같이 매끄럽고 정규화된 크로마 변형을 사용하는 것이 좋다.\n버전 식별에서 오디오 매칭과 유사하게 \\(2~\\mathrm{Hz}\\)(초당 두 개의 크로마 벡터)의 피쳐 레이트(feature rate)는 견고성과 특이성 간의 좋은 트레이드 오프 관계를 구성한다.\n시퀀스 \\(X\\) 및 \\(Y\\)가 주어졌을 때, 최대한 유사한 \\(X\\) 내의 하위 시퀀스와 \\(Y\\) 내의 하위 시퀀스를 찾는다. 이를 위해 로컬 정렬을 계산한다. 이 점수 최대화 정렬 기법을 사용하려면 양인 점수를 가진 셀은 \\(X\\)와 \\(Y\\) 사이의 잠재적 관계를, 음의 점수를 가진 셀은 관련 없는 정보를 인코딩하는 점수 행렬을 입력해야 한다.\n이러한 점수 행렬을 구성하기 위해 자기유사성 행렬(SSM)을 구성하는 과정에서 보았던 것과 유사한 기술을 적용한다.\n\n시퀀스 \\(X\\) 및 \\(Y\\)의 요소를 쌍으로 비교하여 첫 번째 점수 행렬을 얻는다. 정규화된 CENS 벡터에 적용된 내적을 로컬 유사성 측정으로 사용한다. 이 행렬에서 유사도가 높은 경로는 유사한 하위 시퀀스를 나타낸다.\n정보를 향상시키기 위해 경로 향상 전략을 적용한다.\n다른 음악 키를 설명하기 위해 조옮김 불변 SSM에서 사용된 아이디어를 적용할 수 있다.\n양수 값과 음수 값 사이의 균형을 얻기 위해 정규화 및 페널티와 함께 임계값 절차를 적용한다. 특히 가장 높은 값을 가진 셀의 \\(15\\) 퍼센트를 유지하는 임계값(thresh=0.15)을 위해 상대 전략(strategy = relative)을 사용한다. 또한 0과 1 사이의 양수 범위를 재조정한다(scale=1). 다른 모든 셀은 관련이 없는 것으로 간주되며 음수 점수 값(penalty=-2)으로 설정된다.\n\n다음 코드 셀에서 “Day Tripper”라는 노래에 대해 표시되는 점수 행렬을 계산한다. 세로축은 원래 Beatles 버전에 해당하고 가로축은 Ocean Color Scene의 커버 버전에 해당한다.\n\n\ndef compute_sm_from_wav(x1, x2, Fs, N=4410, H=2205, ell=21, d=5, L_smooth=12,\n                        tempo_rel_set=np.array([0.66, 0.81, 1, 1.22, 1.5]),\n                        shift_set=np.array([0]), strategy='relative', scale=True,\n                        thresh=0.15, penalty=-2.0, binarize=False):\n    \"\"\"Compute a similarity matrix (SM)\n\n    Args:\n        x1 (np.ndarray): First signal\n        x2 (np.ndarray): Second signal\n        Fs (scalar): Sampling rate of WAV files\n        N (int): Window size for computing STFT-based chroma features (Default value = 4410)\n        H (int): Hop size for computing STFT-based chroma features (Default value = 2205)\n        ell (int): Smoothing length for computing CENS features (Default value = 21)\n        d (int): Downsampling factor for computing CENS features (Default value = 5)\n        L_smooth (int): Length of filter for enhancing SM (Default value = 12)\n        tempo_rel_set (np.ndarray): Set of relative tempo values for enhancing SM\n            (Default value = np.array([0.66, 0.81, 1, 1.22, 1.5]))\n        shift_set (np.ndarray): Set of shift indices for enhancing SM (Default value = np.array([0]))\n        strategy (str): Thresholding strategy for thresholding SM ('absolute', 'relative', 'local')\n            (Default value = 'relative')\n        scale (bool): If scale=True, then scaling of positive values to range [0,1] for thresholding SM\n            (Default value = True)\n        thresh (float): Treshold (meaning depends on strategy) (Default value = 0.15)\n        penalty (float): Set values below treshold to value specified (Default value = -2.0)\n        binarize (bool): Binarizes final matrix (positive: 1; otherwise: 0) (Default value = False)\n\n    Returns:\n        X (np.ndarray): CENS feature sequence for first signal\n        Y (np.ndarray): CENS feature sequence for second signal\n        Fs_feature (scalar): Feature rate\n        S_thresh (np.ndarray): Similarity matrix\n        I (np.ndarray): Index matrix\n    \"\"\"\n    # Computation of CENS features\n    C1 = librosa.feature.chroma_stft(y=x1, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\n    C2 = librosa.feature.chroma_stft(y=x2, sr=Fs, tuning=0, norm=1, hop_length=H, n_fft=N)\n    Fs_C = Fs / H\n    X, Fs_feature = compute_cens_from_chromagram(C1, Fs_C, ell=ell, d=d)\n    Y, Fs_feature = compute_cens_from_chromagram(C2, Fs_C, ell=ell, d=d)\n\n    # Compute enhanced SM\n    S, I = compute_sm_ti(X, Y, L=L_smooth,  tempo_rel_set=tempo_rel_set,\n                                   shift_set=shift_set, direction=2)\n    S_thresh = threshold_matrix(S, thresh=thresh, strategy=strategy,\n                                          scale=scale, penalty=penalty, binarize=binarize)\n    return X, Y, Fs_feature, S_thresh, I\n\n\nfn1 = 'FMP_C7_F19_TheBeatles_DayTripper_TheBeatles.wav'\nfn2 = 'FMP_C7_F19_TheBeatles_DayTripper_OceanColourScene.wav'\n\nx1, Fs = librosa.load(path_data+fn1)\nx2, Fs = librosa.load(path_data+fn2)\n\npenalty=-2\ntempo_rel_set=np.array([0.8, 1, 1.2])\nL_smooth = 20\nX, Y, Fs_X, S, I = compute_sm_from_wav(x1, x2, Fs, tempo_rel_set = tempo_rel_set,\n                                              L_smooth=L_smooth, penalty=penalty)\n\ncmap_penalty = colormap_penalty(penalty=penalty)\n\nfigsize=(8, 4)\nplot_matrix(S, figsize=figsize, cmap=cmap_penalty, \n                     Fs=Fs_X, Fs_F=Fs_X, aspect='equal',\n                     title='Score matrix $\\mathbf{S}$', \n                     xlabel='Time (seconds) [Ocean Colour Scene]', \n                     ylabel='Time (seconds) [Beatles]')\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#공통-하위시퀀스-매칭",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#공통-하위시퀀스-매칭",
    "title": "8.4. 버전 식별",
    "section": "공통 하위시퀀스 매칭",
    "text": "공통 하위시퀀스 매칭\n\n이 점수 매트릭스에서 \\(\\mathbf{S}\\)의 양수 부분에 있는 많은 경로 구성 요소를 명확하게 확인 할 수 있다. 반면 커버 버전의 시작과 끝 부분에는 경로 같은 구조가 없다. 위에서 언급했듯이 처음 50초 동안 밴드 멤버들은 원래 비틀즈 버전과 음색(tonal) 관계가 없도록 청중과 상호 작용하고 대화한다.\n다음 코드 셀에서는 유사성이 높은 잠재적으로 긴 경로를 식별하기 위해 공통 하위시퀀스 매칭(common subsequence matching)를 사용한다. 그림은 점수 행렬 \\(\\mathrm{S}\\)와 누적 점수 행렬 \\(\\mathrm{D}\\)를 각각 최적의 경로와 함께 보여준다.\n\n\nD = compute_accumulated_score_matrix_common_subsequence(S)\nDmax = np.max(D)\nn, m = divmod(np.argmax(D), D.shape[1])\nP = compute_optimal_path_common_subsequence(D)\nseg_X, seg_Y = get_induced_segments(P)\n\nfigsize = (8, 4)\nplot_matrix(S, figsize=figsize, cmap=cmap_penalty, Fs=1, Fs_F=1, aspect='equal',\n                     title='Score matrix $\\mathbf{S}$ with optimal path', \n                     xlabel='Time (frames) [Ocean Colour Scene]', \n                     ylabel='Time (frames) [Beatles]')\nplt.plot(P[:, 1], P[:, 0], marker='.', color='r')\nplt.tight_layout()\n\nfigsize = (8, 4)\nplot_matrix(D, figsize=figsize, cmap='gray_r', Fs=1, Fs_F=1, aspect='equal',\n                     title='Accumulated score matrix $\\mathbf{D}$ with optial path', \n                     xlabel='Time (frames) [Ocean Colour Scene]', \n                     ylabel='Time (frames) [Beatles]')\nplt.plot(P[:, 1], P[:, 0], marker='.', color='r')\nplt.tight_layout()\nplt.show()\n\nprint('Maximal accumulated score Dmax = %.2f' % Dmax)\nprint('Maximizing cell (n, m) = (%d, %d)' % (n, m))\nprint('Induced segment for X: [%d:%d]' % (seg_X[0], seg_X[-1]))\nprint('Induced segment for Y: [%d:%d]' % (seg_Y[0], seg_Y[-1]))\n\n\n\n\n\n\n\nMaximal accumulated score Dmax = 135.52\nMaximizing cell (n, m) = (165, 247)\nInduced segment for X: [13:165]\nInduced segment for Y: [109:247]\n\n\n\n이전 그림에서 축은 프레임으로 주어진다. \\(\\mathbf{S}\\)에서 가장 긴 연속 경로는 셀 \\((13,109)\\)에서 시작하여 셀 \\((165,247)\\)에서 끝나는 대각선 방향이다.\n\\(\\mathbf{D}\\)의 최대 항목은 다음과 같다. \\[\\mathbf{D}^\\mathrm{max} = \\mathbf{D}(165,247) = 135.52\\]\n일치하는 공통 하위시퀀스는 \\(X[13:165]\\)(비틀즈 버전의 경우) 및 \\(Y[109:247]\\)(커버 버전의 경우)이다. 피쳐 레이트가 \\(2~\\mathrm{Hz}\\)인 것을 고려하면 비틀즈 녹음의 섹션은 대략 \\(76~\\mathrm{sec}\\)의 길이를 갖는 반면, 커버 곡에서 정렬된 섹션의 길이는 \\(69~\\mathrm{sec}\\)이다. 이것은 다시 원래 버전의 템포가 Ocean Color Scene의 커버 버전보다 느리다는 것을 나타낸다.\n다음 코드 셀에서 이번에는 초 단위로 지정된 축을 사용하여 최적의 경로와 함께 점수 행렬을 다시 시각화한다. 유도된 세그먼트는 녹색 선으로 표시된다.\n\n\nfigsize = (8, 4)\nplot_matrix(S, figsize=figsize, cmap=cmap_penalty, Fs=Fs_X, Fs_F=Fs_X, aspect='equal',\n                     title='Score matrix $\\mathbf{S}$ with optimal path', \n                     xlabel='Time (seconds) [Ocean Colour Scene]', \n                     ylabel='Time (seconds) [Beatles]')\nplt.plot(P[:, 1] / Fs_X, P[:, 0] / Fs_X, marker='.', color='r')\n\nstart_X, start_Y = P[0, :] / Fs_X\nend_X, end_Y = P[-1, :] / Fs_X\n\nplt.plot([0, 0], [start_X, end_X], c='g', linewidth=7)\nplt.plot([start_Y, end_Y], [0, 0], c='g', linewidth=7)\n\nplt.plot([0, start_Y], [start_X, start_X], c='r', linestyle=':')\nplt.plot([0, end_Y], [end_X, end_X], c='r', linestyle=':')\n\nplt.plot([start_Y, start_Y], [0, start_X], c='r', linestyle=':')\nplt.plot([end_Y, end_Y], [0, end_X], c='r', linestyle=':')\nplt.tight_layout()\nplt.show()\n\nprint('Induced segment of the original version (Beatles):')\nipd.display(Audio(x1[int(start_X * Fs):int(end_X * Fs)], rate=Fs))\nprint('Induced segment of the cover version (Ocean Colour Scene):')\nipd.display(Audio(x2[int(start_Y * Fs):int(end_Y * Fs)], rate=Fs))\n\n\n\n\nInduced segment of the original version (Beatles):\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nInduced segment of the cover version (Ocean Colour Scene):\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#문서-수준-검색",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#문서-수준-검색",
    "title": "8.4. 버전 식별",
    "section": "문서-수준 검색",
    "text": "문서-수준 검색\n\n버전 식별과 같은 문서 수준(document-level) 검색 시나리오에서 목표는 조각이 아닌 전체 문서를 검색하는 것이다(문서 수준 유사성 점수가 공통 하위 시퀀스의 로컬 일치를 기반으로 할 수 있음).\n다음에서 데이터베이스는 \\(K\\)개의 문서, \\({\\mathcal{D}_1,\\mathcal{D}_2,\\ldots,\\mathcal{D}_K\\ }\\)의 집합으로 구성되어 있다고 가정한다. 문서 \\(\\mathcal{D}_k\\)는 식별자 \\(k\\in[1:K]\\)와 관련된다.\n또한 쿼리 문서 \\(\\mathcal{Q}\\)가 주어지면 각 \\(k\\in[1:K]\\)에 대해 \\(\\gamma(\\mathcal{Q},\\mathcal{D}_k)\\in\\mathbb{R}\\) 값을 산출하는 유사성 측도가 있다고 가정한다.\n이러한 값을 기반으로 유사성 값의 내림차순으로 데이터베이스 문서를 순위(즉, 정렬)매길 수 있다. 이와 같이 주어진 쿼리와 가장 유사한 검색 결과가 사용자에게 표시되는 결과 목록의 앞부분에 나타난다. 최상위(top rank)는 검색 결과 1위(즉, 유사도가 가장 높은 문서)의 검색 결과이다.\n또한 \\(k,\\ell \\in[1:K]\\)에 대해 \\(\\gamma(\\mathcal{Q},\\mathcal {D}_k)>\\gamma(\\mathcal{Q},\\mathcal{D}_\\ell)\\)이면 \\(\\mathcal{D}_k\\)가 \\(\\mathcal{D}_\\ell\\)보다 높은 순위를 갖는다고 한다.\n\\(\\gamma(\\mathcal{Q},\\mathcal{D}_k)=\\gamma(\\mathcal{Q},\\mathcal{D}_\\ell)\\)의 경우, \\(\\mathcal{D}_k\\)는 \\(k<\\ell\\)일 때 \\(\\mathcal{D}_\\ell\\)보다 순위가 높다고 말한다. 수학적으로 순위는 내림차순으로 데이터베이스 문서를 정렬하는 순열 \\(\\rho_\\mathcal{Q}:[1:K]\\to[1:K]\\)로 지정할 수 있다.\n\n\\[\\gamma(\\mathcal{Q},\\mathcal{D}_{\\rho_\\mathcal{Q}(1)}) \\geq\n           \\gamma(\\mathcal{Q},\\mathcal{D}_{\\rho_\\mathcal{Q}(2)}) \\geq\n           \\ldots \\geq\n           \\gamma(\\mathcal{Q},\\mathcal{D}_{\\rho_\\mathcal{Q}(K)})\\]\n\n이 표기법을 사용하여 최상위 순위는 인덱스 값 \\(1\\)(식별자 \\(\\rho_\\mathcal{Q}(1)\\)가 있는 문서에 해당)로 표시되고 가장 낮은 순위는 인덱스값 \\(K\\)(식별자가 \\(\\rho_\\mathcal{Q}(K)\\)인 문서에 해당)로 표시된다."
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#연관성-함수-relevance-function",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#연관성-함수-relevance-function",
    "title": "8.4. 버전 식별",
    "section": "연관성 함수 (Relevance Function)",
    "text": "연관성 함수 (Relevance Function)\n\n다음에서는 정밀도(precision)와 재현율(recall)에 기반한 몇 가지 일반적인 평가 척도의 주요 아이디어를 소개한다.\n\\(\\mathcal{I}:=[1:K]\\)가 현재 시나리오에서 데이터베이스 문서의 식별자인 항목(item)의 집합이라고 하자. 평가를 위해 연관(relevant) (또는 양성) 항목의 참조 주석을 사용할 수 있다고 가정한다.\n\\(\\mathcal{I}_\\mathcal{Q}:=\\mathcal{I}^\\mathrm{Ref}_+\\subseteq \\mathcal{I}\\)를 주어진 쿼리 문서 \\(\\mathcal{Q}\\)에 의존하는 연관 항목의 집합이라고 하자. 이 시나리오에서 검색 시스템은 정렬된 문서 목록을 반환한다.\n순위 검색 결과를 평가할 수 있는 한 가지 가능성은 순위 매개변수 \\(r\\in[1:K]\\)에 따라 precision 및 recall 값의 전체 계열을 고려하는 것이다.\n이후의 편의를 위해 순위가 \\(r\\)인 문서가 연관성 있을 때 \\(1\\) 값을 가정하고 그렇지 않으면 \\(0\\) 값을 가정하는 연관성 함수 \\(\\chi_\\mathcal{Q}:[1:K]\\to\\{0,1\\}\\)를 정의한다. 수학적으로 다음과 같이 표현할 수 있다. \\[\\chi_\\mathcal{Q}(r):=\n  \\left\\{\\begin{array}{ll}\n  1 \\quad \\mbox{if}\\quad \\rho_\\mathcal{Q}(r) \\in \\mathcal{I}_\\mathcal{Q},\\\\\n  0 \\quad \\mbox{if}\\quad \\rho_\\mathcal{Q}(r) \\in \\mathcal{I} \\setminus \\mathcal{I}_\\mathcal{Q}.\n  \\end{array}\\right.\\]"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#precision-recall-pr-곡선",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#precision-recall-pr-곡선",
    "title": "8.4. 버전 식별",
    "section": "Precision-Recall (PR) 곡선",
    "text": "Precision-Recall (PR) 곡선\n\n그런 다음 랭크 \\(r\\in[1:K]\\)의 precision \\(\\mathrm{P}_\\mathcal{Q}(r)\\)와 recall \\(\\mathrm{R}_\\mathcal{Q}(r)\\)를 다음과 같이 정의한다. \\[\\mathrm{P}_\\mathcal{Q}(r) := \\frac{1}{r} \\,\\,\\sum_{k=1}^r \\chi_\\mathcal{Q}(k), \\\\\n\\mathrm{R}_\\mathcal{Q}(r) := \\frac{1}{|\\mathcal{I}_\\mathcal{Q}|} \\,\\,\\sum_{k=1}^ r \\chi_\\mathcal{Q}(k).\\]\n\\(\\{(\\mathrm{P}_\\mathcal{Q}(r),\\mathrm{R}_\\mathcal{Q}(r)) \\mid r\\in[1:K] \\}\\)는 한 축은 precision를 나타내고 다른 축은 recall을 나타내는 2차원 평면에서 시각화된다. 연속된 점들을 결합하면 소위 Precision-recall 곡선 또는 PR 곡선이 생성된다.\n\n\n간단한 예\n\n정의를 설명하기 위해 \\(\\mathcal{I}:=[1:K]\\) 및 \\(K=10\\) 집합으로 인덱싱된 문서가 있는 예를 고려해보자. 주어진 쿼리 \\(\\mathcal{Q}\\)에 대해 연관 항목 집합이 \\(\\mathcal{I}_\\mathcal{Q}=\\{2,7,8,9\\}\\)라고 가정한다.\n\n첫 번째 표에서 \\(k\\in[1:K]\\)에 대한 문서 식별자와 유사성 점수 \\(\\gamma(\\mathcal{Q},\\mathcal{D}_k)\\in\\mathbb{R}\\)를 나타낸다. 여기에서 각 문서에 대한 순위 \\(\\rho_\\mathcal{Q}(k)\\in[1:K]\\)를 도출할 수 있다.\n두 번째 표에서 문서가 순위에 따라 정렬되어 있으며, 순위, 문서 식별자 및 문서의 연관성을 나타낸다. 이로부터 마지막 두 열에 표시된 대로 \\(r\\in[1:K]\\)에서의 \\(\\mathrm{P}_\\mathcal{Q}(r)\\) 값과 \\(\\mathrm{R}_\\mathcal{Q}(r)\\) 값을 계산할 수 있다.\n마지막으로 그림은 결과 PR 곡선을 보여준다.\n\n\n\nImage(path_img+\"FMP_C7_F21.png\", width=600)\n\n\n\n\n\n이 예에서 연관 항목은 순위 위치 \\(r\\in\\{1,2,4,8\\}\\)에서 발생한다. 예를 들어 상위 3개 일치 항목에는 \\(\\mathrm{P}_\\mathcal{Q}(3)=2/3\\) 및 \\(\\mathrm{R}_\\mathcal{Q}(3)=2/4\\)를 산출하는, 4개의 관련 문서 중 2개가 포함되어 있다. 일반적으로 PR 곡선에는 톱니 모양이 있다. \\(r^\\mathrm{th}\\) 문서가 관련이 있으면 정밀도와 재현율이 모두 증가한다. 관련이 없는 경우 재현율은 동일하게 유지되지만 정밀도는 떨어진다. 또한 \\(\\mathrm{P}_\\mathcal{Q}(r)=1\\) 값은 모든 상위 \\(r\\) 일치 항목이 관련됨을 의미한다.\n\n\ndef plot_PR_curve(P_Q, R_Q, figsize=(3, 3)):\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    plt.plot(R_Q, P_Q, linestyle='--', marker='o', color='k', mfc='r')\n    plt.xlim([0, 1.1])\n    plt.ylim([0, 1.1])\n    ax.set_aspect('equal', 'box')\n    plt.title('PR curve')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid()\n    plt.tight_layout()\n    return fig, ax\n\n\n# Set up data\nI = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nscore = np.array([8, 52, 22, 10, 12, 34, 11, 27, 72, 18])\nI_Q = np.array([2, 7, 8, 9])\n\n# Compute rank and sort documents according to rank \nK = len(I)\nindex_sorted = np.flip(np.argsort(score))\nI_sorted = I[index_sorted]\nrank = np.argsort(index_sorted) + 1 \nrank_sorted =  np.arange(1, K+1)\n\n# Compute relevance function X_Q (indexing starts with zero)\nX_Q = np.zeros(K, dtype=bool)\nfor i in range(K):\n    if I_sorted[i] in I_Q:\n        X_Q[i] = True\n\n# Compute precision and recall values (indexing starts with zero)\nM = len(I_Q)\nP_Q = np.zeros(K)\nR_Q = np.zeros(K)\nfor i in range(K):\n    r = rank_sorted[i]\n    P_Q[i] = np.sum(X_Q[:r]) / r\n    R_Q[i] = np.sum(X_Q[:r]) / M\n\n# Arrange output as tables\ndf1 = pd.DataFrame({'ID': I, 'Score': score, 'Rank': rank})\ndf2 = pd.DataFrame({'Rank': rank_sorted, 'ID': I_sorted, \n                    '$\\chi_\\mathcal{Q}$': X_Q, \n                    'P(r)': P_Q, \n                    'R(r)': R_Q})\nfig, ax = plot_PR_curve(P_Q, R_Q)\n\n# Visualize tables and figure in floating box\n\nipd.display(df1)\nipd.display(df2)\n\n\n\n\n\n  \n    \n      \n      ID\n      Score\n      Rank\n    \n  \n  \n    \n      0\n      1\n      8\n      10\n    \n    \n      1\n      2\n      52\n      2\n    \n    \n      2\n      3\n      22\n      5\n    \n    \n      3\n      4\n      10\n      9\n    \n    \n      4\n      5\n      12\n      7\n    \n    \n      5\n      6\n      34\n      3\n    \n    \n      6\n      7\n      11\n      8\n    \n    \n      7\n      8\n      27\n      4\n    \n    \n      8\n      9\n      72\n      1\n    \n    \n      9\n      10\n      18\n      6\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      Rank\n      ID\n      $\\chi_\\mathcal{Q}$\n      P(r)\n      R(r)\n    \n  \n  \n    \n      0\n      1\n      9\n      True\n      1.000000\n      0.25\n    \n    \n      1\n      2\n      2\n      True\n      1.000000\n      0.50\n    \n    \n      2\n      3\n      6\n      False\n      0.666667\n      0.50\n    \n    \n      3\n      4\n      8\n      True\n      0.750000\n      0.75\n    \n    \n      4\n      5\n      3\n      False\n      0.600000\n      0.75\n    \n    \n      5\n      6\n      10\n      False\n      0.500000\n      0.75\n    \n    \n      6\n      7\n      5\n      False\n      0.428571\n      0.75\n    \n    \n      7\n      8\n      7\n      True\n      0.500000\n      1.00\n    \n    \n      8\n      9\n      4\n      False\n      0.444444\n      1.00\n    \n    \n      9\n      10\n      1\n      False\n      0.400000\n      1.00\n    \n  \n\n\n\n\n\n\n\n\n\nPR Curve의 break-even point\n\n특정 쿼리의 경우 PR 곡선은 순위 검색 결과의 전반적인 품질에 대해 좋은 인상을 준다. 그러나 전체 정밀도 및 재현율 값을 처리하는 것은 상당히 번거로울 수 있다. 따라서 이 PR 값 계열을 전체 검색 성능에 대한 몇 가지 특성 정보를 포함하는 단일 평가 측정으로 줄이는 경우가 많다. 그러한 척도 중 하나는 PR 곡선의 break-even point로, 정밀도가 재현율과 동일할 때의 양의 값으로 정의된다.\n위 예시에서 break-even point은 \\(\\mathrm{P}_\\mathcal{Q}(4)=\\mathrm{R}_\\mathcal{Q}(4)=0.75\\)이다. 순위 목록의 상위 \\(|\\mathcal{I}_\\mathcal{Q}|\\) 항목 중 관련 문서가 하나 이상 있는 경우에만 break-even point가 존재한다.\n또한 이 경우 break-even point은 순위 \\(r=|\\mathcal{I}_\\mathcal{Q}|\\)의 정밀도(또는 재현율)에 해당한다. \\[\\mathrm{P}_\\mathcal{Q}(|\\mathcal{I}_\\mathcal{Q}|) = \\mathrm{R}_\\mathcal{Q}(|\\mathcal{I}_\\mathcal {Q}|)\\]\n또 다른 척도로 PR곡선의 정밀도와 재현율에 해당하는 F-측정값(F-measure)을 고려할 수 있다. \\[\\mathrm{F}_\\mathcal{Q}(r) = 2\\cdot \\frac{\\mathrm{P}_\\mathcal{Q}(r)\\cdot \\mathrm{R}_\\mathcal{Q}(r)}{\\mathrm{P}_\\mathcal{Q}( r) + \\mathrm{R}_\\mathcal{Q}(r)}\\] 를 \\(\\mathrm{P}_\\mathcal{Q}(r)\\) 및 \\(\\mathrm{R}_\\mathcal{Q}(r)\\)의 F-측정값이라고 하자.\n\n\\(\\mathrm{P}_\\mathcal{Q}(r)=0\\) 및 \\(\\mathrm{R}_\\mathcal{Q}(r)=0\\)인 경우 \\(\\mathrm{F }_\\mathcal{Q}(r)=0\\)이다.\n\n그런 다음 PR 곡선의 최대 F-측정값은 다음과 같이 정의된다. \\[\\mathrm{F}_\\mathcal{Q}^\\mathrm{max} := \\max_{r\\in[1:K]} \\mathrm{F}_\\mathcal{Q}(r)\\]\n위의 예에서 최대 F-측정값은 \\(\\mathrm{F}_\\mathcal{Q}^\\mathrm{max}=0.75\\)이며 break-even point와 같다. 그러나 일반적으로 두 측정값이 일치할 필요는 없다.\n\n\n# Break-even point\nBEP = P_Q[M-1]\n# Maximal F-measure\nsum_PR = P_Q + R_Q\nsum_PR[sum_PR==0] = 1 # Avoid division by zero\nF_Q = 2 * (P_Q * R_Q) / sum_PR\nF_max = F_Q.max()\n\n# Figure\nfig, ax = plot_PR_curve(P_Q, R_Q, figsize=(4, 4))\nax.plot(BEP, BEP, color='green', marker='o', fillstyle='none', markersize=15)\nax.set_title('PR curve with break-even point (green circle)')\nplt.show()\n\nprint('Break-even point = %.2f' % BEP)\nprint('F_Q = ', np.round(F_Q, 2))\nprint('F_max = %.2f' % F_max)\n\n\n\n\nBreak-even point = 0.75\nF_Q =  [0.4  0.67 0.57 0.75 0.67 0.6  0.55 0.67 0.62 0.57]\nF_max = 0.75"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#average-precision",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#average-precision",
    "title": "8.4. 버전 식별",
    "section": "Average Precision",
    "text": "Average Precision\n\n이제 특정 순위 위치에서 정밀도 값을 보고 그 결과를 평균화하는 평가 척도를 고려한다. 평균 정밀도(average precision) \\(\\overline{\\mathrm{P}}_\\mathcal{Q}\\)는 다음과 같이 정의된다. \\[\\overline{\\mathrm{P}}_\\mathcal{Q}:= \\frac{1}{|\\mathcal{I}_\\mathcal{Q}|} \\sum_{r=1}^K \\mathrm{P }_\\mathcal{Q}(r)\\chi_\\mathcal{Q}(r)\\]\n이 정의에서 정밀도 \\(\\mathrm{P}_\\mathcal{Q}(r)\\)는 \\(\\chi_\\mathcal{Q}(r)=1\\)인 경우에만 고려된다. 즉, 재현율 수준이 변경되는 순위에서만 평균이 계산된다. 예를 들어, 위의 검색 결과의 경우 평균 정밀도는 다음과 같다. \\[\\overline{\\mathrm{P}}_\\mathcal{Q} = \\frac{1}{4} (1 + 1 + 0.75 + 0.5) = 0.8125.\\]\nbreak-even point와 최대 F-measure의 경우, 평균 정밀도는 0과 1 사이이며 모든 관련 문서가 최상위에 있는 경우에만 값 1을 가정한다. 평균 정밀도의 한 가지 장점은 다른 두 가지 척도와 달리 전체 순위 목록을 고려한다는 것이다. 예를 들어, 위의 예에서 \\(\\chi_\\mathcal{Q}(8)\\) 및 \\(\\chi_\\mathcal{Q}(9)\\) 값을 교환하면(따라서 관련 문서의 순위가 한 단계 낮아짐) break-even point와 최대 F-측정은 변경되지 않은 반면, 평균 정밀도는 \\(\\bar{\\mathrm{P}}_\\mathcal{Q} = (1 + 1 + 0.75 + 0.44)/4 = 0.7975\\)로 감소한다.\n\n\nP_average = np.sum(P_Q * X_Q) / len(I_Q)\nprint('Average precision =', P_average)\n\nAverage precision = 0.8125"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#mean-average-precision",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#mean-average-precision",
    "title": "8.4. 버전 식별",
    "section": "Mean Average Precision",
    "text": "Mean Average Precision\n\n지금까지 단일 쿼리 문서 \\(\\mathcal{Q}\\)에 대한 평가 방법을 살펴보았다. 실제로 검색 시스템의 성능을 평가할 때 주어진 애플리케이션 시나리오 내에서 사용자가 필요로 하는 일반적인 정보를 반영하는 다양한 쿼리를 사용해야 한다. 단일 평가의 숫자를 얻기 위해 이러한 모든 값에 대한 평균을 취하여 쿼리 종속 값을 결합하는 경우가 많다.\n예를 들어 평균 정밀도의 경우를 살펴보자. \\(\\{\\mathcal{Q}_1,\\ldots,\\mathcal{Q}_J\\}\\)를 평가에서 고려할 쿼리 문서 집합이라고 하자. 그런 다음 각 \\(j\\in[1:J]\\)에 대해 \\(\\overline{\\mathrm{P}}_{\\mathcal{Q}_j}\\) 값을 얻는다.\nMean average precision 또는 MAP은 다음과 같이 정의된다. \\[\\overline{\\mathrm{P}} := \\frac{1}{J} \\sum_{j=1}^J \\overline{\\mathrm{P}}_{\\mathcal{Q}_j}\\]"
  },
  {
    "objectID": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#구현",
    "href": "posts/8. Content-Based Audio Retrieval/8.4.Version_Identification.html#구현",
    "title": "8.4. 버전 식별",
    "section": "구현",
    "text": "구현\n\ndef compute_prf_metrics(I, score, I_Q):\n    \"\"\"Compute precision, recall, F-measures and other\n    evaluation metrics for document-level retrieval\n\n    Args:\n        I (np.ndarray): Array of items\n        score (np.ndarray): Array containing the score values of the times\n        I_Q (np.ndarray): Array of relevant (positive) items\n\n    Returns:\n        P_Q (float): Precision\n        R_Q (float): Recall\n        F_Q (float): F-measures sorted by rank\n        BEP (float): Break-even point\n        F_max (float): Maximal F-measure\n        P_average (float): Mean average\n        X_Q (np.ndarray): Relevance function\n        rank (np.ndarray): Array of rank values\n        I_sorted (np.ndarray): Array of items sorted by rank\n        rank_sorted (np.ndarray): Array of rank values sorted by rank\n    \"\"\"\n    # Compute rank and sort documents according to rank\n    K = len(I)\n    index_sorted = np.flip(np.argsort(score))\n    I_sorted = I[index_sorted]\n    rank = np.argsort(index_sorted) + 1\n    rank_sorted = np.arange(1, K+1)\n\n    # Compute relevance function X_Q (indexing starts with zero)\n    X_Q = np.isin(I_sorted, I_Q)\n\n    # Compute precision and recall values (indexing starts with zero)\n    M = len(I_Q)\n    P_Q = np.cumsum(X_Q) / np.arange(1, K+1)\n    R_Q = np.cumsum(X_Q) / M\n\n    # Break-even point\n    BEP = P_Q[M-1]\n    # Maximal F-measure\n    sum_PR = P_Q + R_Q\n    sum_PR[sum_PR == 0] = 1  # Avoid division by zero\n    F_Q = 2 * (P_Q * R_Q) / sum_PR\n    F_max = F_Q.max()\n    # Average precision\n    P_average = np.sum(P_Q * X_Q) / len(I_Q)\n\n    return P_Q, R_Q, F_Q, BEP, F_max, P_average, X_Q, rank, I_sorted, rank_sorted\n\n\n# excercise 7.14\n\nI = np.array([1, 2, 3, 4, 5, 6, 7, 8])\nscore = np.array([0.7, 2.6, 3.6, 3.5, 3.2, 3.7, 1.5, 3.1])\nI_Q = np.array([2, 3, 4, 8])\n\noutput = compute_prf_metrics(I, score, I_Q)\nP_Q, R_Q, F_Q, BEP, F_max, P_average, X_Q, rank, I_sorted, rank_sorted = output\n\n# Arrange output as tables\nscore_sorted = np.flip(np.sort(score))\ndf = pd.DataFrame({'Rank': rank_sorted, 'ID': I_sorted,\n                   'Score': score_sorted,\n                   '$\\chi_\\mathcal{Q}$': X_Q, \n                   'P(r)': P_Q, \n                   'R(r)': R_Q,\n                   'F(r)': F_Q})\nfig, ax = plot_PR_curve(P_Q, R_Q, figsize=(3,3))\nax.plot(BEP, BEP, color='green', marker='o', fillstyle='none', markersize=15)\nax.set_title('PR curve')\n\n# Visualize tables and figure in floating box\nipd.display(df)\n\nprint('Break-even point = %.2f' % BEP)\nprint('F_max = %.2f' % F_max)\nprint('Average precision =', np.round(P_average, 5))\n\n\n\n\n\n  \n    \n      \n      Rank\n      ID\n      Score\n      $\\chi_\\mathcal{Q}$\n      P(r)\n      R(r)\n      F(r)\n    \n  \n  \n    \n      0\n      1\n      6\n      3.7\n      False\n      0.000000\n      0.00\n      0.000000\n    \n    \n      1\n      2\n      3\n      3.6\n      True\n      0.500000\n      0.25\n      0.333333\n    \n    \n      2\n      3\n      4\n      3.5\n      True\n      0.666667\n      0.50\n      0.571429\n    \n    \n      3\n      4\n      5\n      3.2\n      False\n      0.500000\n      0.50\n      0.500000\n    \n    \n      4\n      5\n      8\n      3.1\n      True\n      0.600000\n      0.75\n      0.666667\n    \n    \n      5\n      6\n      2\n      2.6\n      True\n      0.666667\n      1.00\n      0.800000\n    \n    \n      6\n      7\n      7\n      1.5\n      False\n      0.571429\n      1.00\n      0.727273\n    \n    \n      7\n      8\n      1\n      0.7\n      False\n      0.500000\n      1.00\n      0.666667\n    \n  \n\n\n\n\nBreak-even point = 0.50\nF_max = 0.80\nAverage precision = 0.60833\n\n\n\n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S3_CommonSubsequence.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S3_VersionIdentification.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C7/C7S3_Evaluation.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "",
    "text": "오디오의 하모니(Harmonic) 부분과 타악기(Percussive) 부분을 분리하는 HPS와 HRPS, 그리고 신호의 재구성 방법을 설명한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#화성음과-타악기-소리",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#화성음과-타악기-소리",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "화성음과 타악기 소리",
    "text": "화성음과 타악기 소리\n\n음악 사운드는 음향 품질이 다른 광범위한 소리 구성 요소로 구성될 수 있다. 특히 화성음(harmonic)과 타악기(percussive)의 두 가지 넓은 범주의 소리를 고려한다.\n\n화성음(harmonic sound)는 피치 사운드로 인식하는 것으로, 멜로디와 화음(chord)을 듣게 만드는 것이다. 화성음의 원형은 스펙트로그램 표현의 가로선에 해당하는 정현파(sinusoid)의 음향적 실현이다. 바이올린 소리는 우리가 화음이라고 생각하는 또 다른 전형적인 예이다. 다시 말하지만, 스펙트로그램에서 관찰된 대부분의 구조는 수평적 특성을 가진다(비록 잡음과 같은 구성 요소와 혼합되어 있음에도 불구하고).\n반면에 타악기 소리(percussive sound)는 우리가 충돌, 노크, 박수 또는 클릭 등으로 인식하는 것이다. 드럼 스트로크 사운드 또는 음악 톤의 어택 단계에서 발생하는 트랜지언트가 더 일반적인 예이다. 충격음의 원형은 스펙트로그램 표현의 수직선에 해당하는 임펄스(impulse)의 음향적 구현이다.\n\n다음 예에서는 바이올린 녹음, 캐스터네츠 녹음 및 이 두 녹음의 중첩에 대한 스펙트로그램 표현(대수 압축 사용)을 보여준다. 바이올린 소리의 경우 연주되는 음의 기본 주파수의 정수배인 고조파(harmonics)에 해당하는 서로 위에 쌓인 수평선을 관찰할 수 있다.\n\n\ndef compute_plot_spectrogram(x, Fs=22050, N=4096, H=2048, ylim=None,\n                     figsize =(5, 2), title='', log=False):\n    N, H = 1024, 512\n    X = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann', \n                     center=True, pad_mode='constant')\n    Y = np.abs(X)**2\n    if log:\n        Y_plot = np.log(1 + 100 * Y)\n    else:\n        Y_plot = Y\n    plot_matrix(Y_plot, Fs=Fs/H, Fs_F=N/Fs, title=title, figsize=figsize)\n    if ylim is not None:\n        plt.ylim(ylim)\n    plt.tight_layout()\n    plt.show()\n    return Y\n\n\nfn_wav = 'FMP_C8_F02_Long_Violin.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nY = compute_plot_spectrogram(x, Fs=Fs, title = 'Violin', ylim=[0, 3000], log=1)\nipd.display(Audio(data=x, rate=Fs))\n\nfn_wav = 'FMP_C8_F02_Long_Castanets.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nY = compute_plot_spectrogram(x, Fs=Fs, title = 'Castanets', ylim=[0, 3000], log=1)\nipd.display(Audio(data=x, rate=Fs))\n\nfn_wav = 'FMP_C8_F02_Long_CastanetsViolin.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nY = compute_plot_spectrogram(x, Fs=Fs, title = 'Mix', ylim=[0, 3000], log=1)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hps-과정",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hps-과정",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "HPS 과정",
    "text": "HPS 과정\n\nHPS(Harmonic-Percussive Separation) 의 목표는 주어진 오디오 신호를 두 부분으로 분해하는 것이다. 하나는 화성음로 구성되고 다른 하나는 타악기로 구성된다. 이 작업은 소리의 이벤트가 실제로 화음인지 타악기인지 종종 불분명하기 때문에 다소 모호하다. 실제로 백색소음이나 박수소리 등 화성음도 타악기도 아닌 소리가 많다.\n다음 그림에 HPS의 절차가 나와있다. 주어진 신호의 스펙트로그램 표현을 수평 방향(시간에 따라)으로 필터링하여 타악기 이벤트를 억제하면서 하모닉 이벤트를 향상시킨다. 마찬가지로 스펙트로그램은 수직 방향(주파수를 따라)으로 필터링되어 타악기 이벤트를 강화하고 하모닉 이벤트를 억제한다. 2개의 필터링된 스펙트로그램은 시간-주파수(time-frequency) 마스크(mask) 를 생성하는 데 사용되며, 그런 다음 원본 스펙트로그램에 적용된다. 마스킹된 스펙트로그램 표현에서 역 STFT를 적용하여 신호의 하모닉 및 타악기 부분을 얻는다.\n\n\nImage(path_img+\"FMP_C8_F03.png\", width=400)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#표기법",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#표기법",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "표기법",
    "text": "표기법\n\n다음에서 \\(x:\\mathbb{Z}\\to\\mathbb{R}\\)를 샘플된 오디오 신호의 이산 시간 표현이라고 하자. 목표는 \\(x\\)를 하모닉 신호 \\(x^\\mathrm{h}:\\mathbb{Z}\\to\\mathbb{R}\\)와 타악기 신호 \\(x^\\mathrm{p}:\\mathbb{Z}\\to\\mathbb{R}\\)로 분해하는 것이다. \\[x = x^\\mathrm{h} + x^\\mathrm{p}\\]\n첫번째 단계에서 신호 \\(x\\)의 이산 STFT \\(\\mathcal{X}\\)을 계산한다. 편의를 위해 다음의 정의를 반복한다. \\[\\mathcal{X}(n,k):= \\sum_{r=0}^{N-1} x(r + n H)w(r)\\exp(-2\\pi ikr/N)\\]\n\n이 때, \\(w:[0:N-1]\\to\\mathbb{R}\\)는 길이 \\(N\\)과 홉(hop) 크기 매개변수 \\(H\\)의 적절한 윈도우(window) 함수이다.\n\n이후 단계에서 경계(boundary)에 대한 고려를 피하기 위해 시간 및 주파수 방향에서 행렬 \\(\\mathcal{X}\\)의 적절한 제로 패딩(zero-padding)을 적용하여 \\(n\\in\\mathbb{Z}\\) 및 \\(k\\in\\mathbb{Z}\\)를 가정할 수 있다. \\(\\mathcal{X}\\)로부터 power 스펙트로그램 \\(\\mathcal{Y}\\)를 도출한다. \\[\\mathcal{Y}(n,k):= |\\mathcal{X}(n,k)|^2\\]"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#중앙값median-필터링",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#중앙값median-필터링",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "중앙값(Median) 필터링",
    "text": "중앙값(Median) 필터링\n\n다음 단계에서는 \\({\\mathcal Y}\\)를 필터링하여 하모니 강화(harmonically enhanced) 스펙트로그램 \\(\\tilde{{\\mathcal Y}}_\\mathrm{h}\\) 및 타악기 강화(percussively enhanced) 스펙트로그램 \\(\\tilde{{\\mathcal Y}}_\\mathrm{p}\\)를 계산한다.\n이를 위해 중앙값 필터링 개념을 적용한다. 유한한 숫자 목록의 중앙값은 숫자의 절반이 값보다 낮고 절반이 그보다 높은 속성을 가진 숫자 값이다.\n\\(A = (a_1,a_2,\\ldots,a_L)\\)를 오름차순으로 정렬하면 \\(\\tilde{A}=(\\tilde{a}_1,\\tilde{a}_2,\\ldots,\\tilde{a}_L)\\) (\\(\\ell<m\\) , \\(\\ell,m\\in[1:L]\\)에 대해 \\(\\tilde{a}_\\ell\\leq\\tilde{a}_m\\))가 생성된다. 그러면 중앙값 \\(\\mu_{1/2}(A)\\)은 다음과 같이 정의된다. \\[\\mu_{1/2}(A) :=  \\begin{cases}\\tilde{a}_{(L+1)/2}, \\ \\mbox{for $L$ being odd,} \\\\\n  (\\tilde{a}_{L/2} + \\tilde{a}_{L/2+1})/2,  \\ \\mbox{otherwise} \\end{cases}\\]\n중앙값은 실수 시퀀스에 로컬 방식으로 적용될 수 있다. 이를 위해 시퀀스의 주어진 요소를 중앙값으로 대체한다. 이는 \\(L\\in\\mathbb{N}\\) 길이의 중앙값 필터 개념으로 이어진다.\n\\(A=(a_n\\mid n\\in\\mathbb{Z})\\)가 실수 \\(a_n\\in\\mathbb{R}\\)의 시퀀스이고 \\(L\\in\\mathbb{N}\\)가 정수라고 가정한다. 그러면 시퀀스 \\(\\mu_{1/2}^L[A]\\)는 다음과 같이 정의된다. \\[\\mu_{1/2}^L [A] (n) = \\mu_{1/2}\\big((a_{n-(L-1)/2},\\ldots, a_{n+(L -1)/2})\\big)\\]\n예를 들어 \\(A=(\\ldots,0,5,3,2,8,2,0,\\ldots)\\) 시퀀스를 고려해보자. 여기서 \\(A\\)는 표시된 값을 벗어나면 0이라고 가정한다. \\(L=3\\)를 사용하여 \\(\\mu_{1/2}^L[A]=(\\ldots,0,3,3,3,2,2,0,\\ldots)\\)를 얻는다.\nscipy-패키지의 signal.medfilt 함수는 중간 필터를 입력 신호에 적용하며 길이는 매개변수 kernel_size(홀수 정수로 가정)에 의해 결정된다. 제로 패딩을 적용하면 출력 신호의 크기가 입력과 동일하다.\n\n\nA = np.array([5.,3,2,8,2])\nfilter_len = 3\nA_result = signal.medfilt(A, kernel_size=filter_len)\nprint('A        = ', A)\nprint('A_result = ', A_result)\n\nA        =  [5. 3. 2. 8. 2.]\nA_result =  [3. 3. 3. 2. 2.]\n\n\n\n수직 / 수평 중앙값 필터링\n\n이 시나리오에서는 스펙트로그램 \\(\\mathcal{Y}\\)에 중앙값 필터링(median filtering)의 개념을 두가지 방법으로 적용한다. 한번은 \\(\\mathcal{Y}\\)의 행(row)을 고려하여 수평으로, 한번은 \\(\\mathcal{Y}\\)의 열(column)을 고려하여 수직으로 한다.\n이렇게 하면 각각 \\(\\tilde{\\mathcal{Y}}^\\mathrm{h}\\) 및 \\(\\tilde{\\mathcal{Y}}^\\mathrm{p}\\)로 표시되는 두 개의 필터링된 스펙트로그램이 생성된다.\n보다 정확하게는 \\(L^\\mathrm{h}\\) 및 \\(L^\\mathrm{p}\\)를 홀수 길이 매개변수로 두고, 다음을 정의할 수 있다. \\[\\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k):=\\mu_{1/2}((\\mathcal{Y}(n-(L^\\mathrm{h}-1)/2,k),\\ldots, \\mathcal{Y}(n+(L^\\mathrm{h}-1)/2,k))),\\] \\[\\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k):=\\mu_{1/2}((\\mathcal{Y}(n,k-(L^\\mathrm{p}-1)/2),\\ldots, \\mathcal{Y}(n,k+(L^\\mathrm{p}-1)/2)))\\] for \\(n,k\\in\\mathbb{Z}\\) (\\(\\mathcal{Y}\\)의 제로패딩 가정)\n\n\ndef median_filter_horizontal(x, filter_len):\n    \"\"\"Apply median filter in horizontal direction\n\n    Args:\n        x (np.ndarray): Input matrix\n        filter_len (int): Filter length\n\n    Returns:\n        x_h (np.ndarray): Filtered matrix\n    \"\"\"\n    return signal.medfilt(x, [1, filter_len])\n\n\ndef median_filter_vertical(x, filter_len):\n    \"\"\"Apply median filter in vertical direction\n\n    Args:\n        x: Input matrix\n        filter_len (int): Filter length\n\n    Returns:\n        x_p (np.ndarray): Filtered matrix\n    \"\"\"\n    return signal.medfilt(x, [filter_len, 1])\n\n\ndef plot_spectrogram_hp(Y_h, Y_p, Fs=22050, N=4096, H=2048, figsize =(10, 2), \n                         ylim=None, clim=None, title_h='', title_p='', log=False):\n    if log: \n        Y_h_plot = np.log(1 + 100 * Y_h)\n        Y_p_plot = np.log(1 + 100 * Y_p)\n    else: \n        Y_h_plot = Y_h\n        Y_p_plot = Y_p\n    plt.figure(figsize=figsize)\n    ax = plt.subplot(1,2,1)\n    plot_matrix(Y_h_plot, Fs=Fs/H, Fs_F=N/Fs, ax=[ax], clim=clim,\n                         title=title_h, figsize=figsize)\n    if ylim is not None:\n        ax.set_ylim(ylim)\n        \n    ax = plt.subplot(1,2,2)\n    plot_matrix(Y_p_plot, Fs=Fs/H, Fs_F=N/Fs, ax=[ax], clim=clim,\n                         title=title_p, figsize=figsize)\n    if ylim is not None:\n        ax.set_ylim(ylim)\n  \n    plt.tight_layout()\n    plt.show()\n\n\n예를 들어 캐스터네츠 클릭(타악기 성분)이 중첩된 바이올린 녹음(화성 성분)의 예를 보자. 중앙값 필터를 가로 방향으로 적용하면 가로 구조가 더 뚜렷해지고 세로 구조가 사라진다. 수직 방향으로 중앙값 필터를 적용할 경우에도 유사한 개선 효과를 얻을 수 있는데, 이번에는 타악기 구조에 대해 그렇다.\n\n\nfn_wav = 'FMP_C8_F02_Long_CastanetsViolin.wav'\nx, Fs = librosa.load(path_data+fn_wav, mono=True)\nN, H = 1024, 512\nX = librosa.stft(y=x, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\nY = np.abs(X)**2\n\nL_set = np.array([[5,5],[23,9],[87,47]])\nnum = L_set.shape[0]\nfor m in range(num):\n    L_h = L_set[m,0]\n    L_p = L_set[m,1]\n    Y_h = median_filter_horizontal(Y, L_h)\n    Y_p = median_filter_vertical(Y, L_p)\n    title_h = r'Horizontal filtering ($L^h=%d$)'%L_h\n    title_p = r'Vertical filtering ($L^p=%d$)'%L_p\n    plot_spectrogram_hp(Y_h, Y_p, Fs=Fs, N=N, H=H, \n            title_h=title_h, title_p=title_p, ylim=[0, 3000], log=True)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#binary-마스킹-soft-마스킹",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#binary-마스킹-soft-마스킹",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "Binary 마스킹, Soft 마스킹",
    "text": "Binary 마스킹, Soft 마스킹\n\n두 개의 필터링된 스펙트로그램 \\(\\tilde{\\mathcal{Y}}^\\mathrm{h}\\) 및 \\(\\tilde{\\mathcal{Y}}^\\mathrm{p}\\)는 신호의 하모닉 및 타악기 구성에 직접 적용되지 않는다. 대신 먼저 두 개의 마스크를 생성하는 데 사용되며, 그런 다음 원래 스펙트로그램에서 원하는 구성 요소를 “펀칭 아웃”(“punching out”)하는 데 사용된다.\n\\(\\tilde{\\mathcal{Y}}^\\mathrm{h}\\) 및 \\(\\tilde{\\mathcal{Y}}^\\mathrm{p}\\)에서 도출할 수 있는 다양한 유형의 시간-주파수 마스크가 있다. 첫 번째 유형은 바이너리 마스크(binary mask)라고 하며 각 시간-주파수 빈에 값 1 또는 값 0이 할당된다. 이진법의 경우 다음을 설정하여 두 마스크를 정의한다. \\[\\mathcal{M}^\\mathrm{h}(n,k) :=\n\\begin{cases}\n1, & \\text{if } \\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k) \\geq \\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k), \\\\\n0, & \\text{otherwise,}\n\\end{cases}\\] \\[\\mathcal{M}^\\mathrm{p}(n,k) :=\n\\begin{cases}\n1, & \\text{if } \\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k) < \\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k), \\\\\n0, & \\text{otherwise}\n\\end{cases}\\]\n이분법의 결정 대신 스펙트럼 계수의 크기를 비교할 때 상대적 가중치를 고려할 수 있다. 이는 소프트 마스크(soft mask)라고도 알려진 또 다른 유형의 마스크이다. 이 경우 설정을 통해 두 개의 마스크를 정의한다. \\[\\mathcal{M}^\\mathrm{h}(n,k) := \\frac{\\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k)+\\varepsilon/2 }{\\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k) +\\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k)+\\varepsilon},\\] \\[\\mathcal{M}^\\mathrm{p}(n,k) := \\frac{\\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k)+\\varepsilon/2}{ \\tilde{\\mathcal{Y}}^\\mathrm{h}(n,k) + \\tilde{\\mathcal{Y}}^\\mathrm{p}(n,k)+\\varepsilon}\\] for \\(n,k\\in\\mathbb{Z}\\)\n\n0으로 나누지 않도록 작은 양수 \\(\\varepsilon>0\\)를 추가한다.\n\n(바이너리 또는 소프트) 시간-주파수 마스크는 각 시간 주파수 빈이 해당 구성 요소에 속하는 정도를 나타낸다. 구성요소를 얻기 위해 pointwise 곱셈을 통해 원본 스펙트로그램에 마스크를 적용한다. 하모닉 및 타악기 마스크의 경우 두 개의 마스크 버전 \\(\\mathcal{Y}^\\mathrm{h}\\) 및 \\(\\mathcal{Y}^\\mathrm{p}\\)가 생성되며, 다음과 같이 정의된다. \\[\\mathcal{Y}^\\mathrm{h}(n,k) := \\mathcal{M}^\\mathrm{h}(n,k) \\cdot \\mathcal{Y}(n,k),\\] \\[\\mathcal{Y}^\\mathrm{p}(n,k) := \\mathcal{M}^\\mathrm{p}(n,k) \\cdot \\mathcal{Y}(n,k)\\] for \\(n,k\\in\\mathbb{Z}\\)\n바이너리 마스크의 경우 마스크 값이 1이면 스펙트로그램의 값이 유지되고 마스크 값이 0이면 억제된다. 즉, \\(\\mathcal{Y}\\)의 모든 시간-주파수 빈이 \\(\\mathcal{Y}^\\mathrm{h}\\) 또는 \\(\\mathcal{Y}^\\mathrm{p}\\)에 할당된다.\n소프트 마스크의 경우 이 할당은 마스킹 가중치로 표현된다. 이러한 종류의 스펙트럼 조작은 Wiener 필터링이라고도 하며 통계적 디지털 신호 처리에서 중요한 개념이다.\n\n\nL_h = 23\nL_p = 9\nY_h = median_filter_horizontal(Y, L_h)\nY_p = median_filter_vertical(Y, L_p)\ntitle_h = r'Horizontal filtering ($L^h=%d$)'%L_h\ntitle_p = r'Vertical filtering ($L^p=%d$)'%L_p\nplot_spectrogram_hp(Y_h, Y_p, Fs=Fs, N=N, H=H, \n        title_h=title_h, title_p=title_p, ylim=[0, 3000], log=True)\n\nM_binary_h = np.int8(Y_h >= Y_p)\nM_binary_p = np.int8(Y_h < Y_p)\ntitle_h = r'Horizontal binary mask'\ntitle_p = r'Vertical binary mask'\nplot_spectrogram_hp(M_binary_h, M_binary_p, Fs=Fs, N=N, H=H, clim=[0,1],\n        title_h=title_h, title_p=title_p, ylim=[0, 3000])\n\neps = 0.00001\nM_soft_h = (Y_h + eps/2)/(Y_h + Y_p + eps)\nM_soft_p = (Y_p + eps/2)/(Y_h + Y_p + eps)\ntitle_h = r'Horizontal soft mask'\ntitle_p = r'Vertical soft mask'\nplot_spectrogram_hp(M_soft_h, M_soft_p, Fs=Fs, N=N, H=H, clim=[0,1],\n        title_h=title_h, title_p=title_p, ylim=[0, 3000])"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#신호-재구성-signal-reconstruction",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#신호-재구성-signal-reconstruction",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "신호 재구성 (Signal Reconstruction)",
    "text": "신호 재구성 (Signal Reconstruction)\n\n지금까지 신호의 스펙트로그램 \\(\\mathcal{Y}\\)를 \\(\\mathcal{Y}^\\mathrm{h}\\) 및 \\(\\mathcal{Y}^\\mathrm{p}\\) 두 구성요소로 분해했다.\n두 개의 시간 영역 신호 \\(x^\\mathrm{h}\\) 및 \\(x^\\mathrm{p}\\)를 얻는 가장 편리한 방법은 두 마스크를 원래 STFT \\(\\mathcal{X}\\)에 직접 적용하여, 2개의 마스킹된 복소수 STFT \\(\\mathcal{X}^\\mathrm{h}\\) 및 \\(\\mathcal{X}^\\mathrm{p}\\)을 생성하는 것이다.: \\[\\mathcal{X}^\\mathrm{h}(n,k) := \\mathcal{M}^\\mathrm{h}(n,k) \\cdot \\mathcal{X}(n,k),\\] \\[\\mathcal{X}^\\mathrm{p}(n,k) := \\mathcal{M}^\\mathrm{p}(n,k) \\cdot \\mathcal{X}(n,k)\\] for \\(n,k\\in\\mathbf{Z}\\).\n그런 다음 시간 영역 신호 \\(x^\\mathrm{h}\\) 및 \\(x^\\mathrm{p}\\)를 얻기 위해 마스킹된 STFT에 역 STFT를 적용한다. 하지만 몇가지 문제가 있다.\n\n첫째, \\(\\mathcal{X}^\\mathrm{h}\\) 및 \\(\\mathcal{X}^\\mathrm{p}\\) 두 구성 요소에 대해 \\(\\mathcal{X}\\)의 동일한 위상 정보를 사용하는 것은 서로 다른 신호 구성요소 사이의 가능한 위상 간섭(phase interference)에 대해 설명하지 않는다. 일반적으로 서로 다른 신호 구성 요소에 대한 일관된 위상 정보의 추정은 매우 어렵다.\n두 번째 문제는 STFT 조작(예: 마스크 적용)이 일관된 시간 영역 신호의 재구성에 문제를 일으킬 수 있다는 사실이다. 뒤의 신호 재구성에 대해 더 자세히 다룰 것이다.\n\n다음 예제에서는 바이너리 마스킹의 경우만 고려한다. 마스킹된 STFT에서 신호를 재구성하기 위해 librosa.istft 함수를 사용한다.\n\n\nX_h = X * M_binary_h\nX_p = X * M_binary_p\n\nx_h = librosa.istft(X_h, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\nx_p = librosa.istft(X_p, hop_length=H, win_length=N, window='hann', center=True, length=x.size)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#매개변수의-물리적-해석",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#매개변수의-물리적-해석",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "매개변수의 물리적 해석",
    "text": "매개변수의 물리적 해석\n\n지금까지 논의된 HPS 절차에는 STFT의 윈도우 길이 \\(N\\), 홉 크기 \\(H\\), 중앙값 필터링에 사용되는 필터 길이 \\(L^\\mathrm{h}\\) 및 \\(L^\\mathrm{p}\\) 등 선택해야 할 매개변수가 많다. \\(L^\\mathrm{h}\\) 및 \\(L^\\mathrm{p}\\) 매개변수는 인덱스(즉, 프레임 및 빈)로 지정되었다. 의미 있는 방식으로 이러한 매개변수를 선택하려면 각각 초와 헤르츠 측면에서 물리적 해석을 이해해야 한다.\n입력 신호 \\(x\\)의 샘플링 레이트 \\(F_\\mathrm{s}\\)와 프레임 길이 \\(N\\) 및 홉 크기 \\(H\\)가 주어지면 다음 함수는 초 단위로 주어진 필터 길이와 Hertz를 인덱스로 주어진 필터 길이로 변환한다. 중앙값 필터링의 경우 필터 길이가 홀수 정수인지 확인해야 한다.\n\n\ndef convert_l_sec_to_frames(L_h_sec, Fs=22050, N=1024, H=512):\n    \"\"\"Convert filter length parameter from seconds to frame indices\n\n    Args:\n        L_h_sec (float): Filter length (in seconds)\n        Fs (scalar): Sample rate (Default value = 22050)\n        N (int): Window size (Default value = 1024)\n        H (int): Hop size (Default value = 512)\n\n    Returns:\n        L_h (int): Filter length (in samples)\n    \"\"\"\n    L_h = int(np.ceil(L_h_sec * Fs / H))\n    return L_h\n\n\ndef convert_l_hertz_to_bins(L_p_Hz, Fs=22050, N=1024, H=512):\n    \"\"\"Convert filter length parameter from Hertz to frequency bins\n\n    Args:\n        L_p_Hz (float): Filter length (in Hertz)\n        Fs (scalar): Sample rate (Default value = 22050)\n        N (int): Window size (Default value = 1024)\n        H (int): Hop size (Default value = 512)\n\n    Returns:\n        L_p (int): Filter length (in frequency bins)\n    \"\"\"\n    L_p = int(np.ceil(L_p_Hz * N / Fs))\n    return L_p\n\n\ndef make_integer_odd(n):\n    \"\"\"Convert integer into odd integer\n\n    Args:\n        n (int): Integer\n\n    Returns:\n        n (int): Odd integer\n    \"\"\"\n    if n % 2 == 0:\n        n += 1\n    return n\n\n\nFs, N, H = 22050, 1024, 512\nprint('L_h(%.1f sec) = %d' % (0.5, make_integer_odd(convert_l_sec_to_frames(0.5, Fs=Fs, N=N, H=H)) ))\nprint('L_p(%.1f Hz) = %d' % (600, make_integer_odd(convert_l_hertz_to_bins(600, Fs=Fs, N=N, H=H)) ))\n\nL_h(0.5 sec) = 23\nL_p(600.0 Hz) = 29"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hps-구현",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hps-구현",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "HPS 구현",
    "text": "HPS 구현\n\n다음 코드 셀에서는 전체 HPS 절차를 구현하는 함수 hps를 사용한다.\n\n\ndef hps(x, Fs, N, H, L_h, L_p, L_unit='physical', mask='binary', eps=0.001, detail=False):\n    \"\"\"Harmonic-percussive separation (HPS) algorithm\n\n    Args:\n        x (np.ndarray): Input signal\n        Fs (scalar): Sampling rate of x\n        N (int): Frame length\n        H (int): Hopsize\n        L_h (float): Horizontal median filter length given in seconds or frames\n        L_p (float): Percussive median filter length given in Hertz or bins\n        L_unit (str): Adjusts unit, either 'pyhsical' or 'indices' (Default value = 'physical')\n        mask (str): Either 'binary' or 'soft' (Default value = 'binary')\n        eps (float): Parameter used in soft maskig (Default value = 0.001)\n        detail (bool): Returns detailed information (Default value = False)\n\n    Returns:\n        x_h (np.ndarray): Harmonic signal\n        x_p (np.ndarray): Percussive signal\n        details (dict): Dictionary containing detailed information; returned if ``detail=True``\n    \"\"\"\n    assert L_unit in ['physical', 'indices']\n    assert mask in ['binary', 'soft']\n    # stft\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\n    # power spectrogram\n    Y = np.abs(X) ** 2\n    # median filtering\n    if L_unit == 'physical':\n        L_h = convert_l_sec_to_frames(L_h_sec=L_h, Fs=Fs, N=N, H=H)\n        L_p = convert_l_hertz_to_bins(L_p_Hz=L_p, Fs=Fs, N=N, H=H)\n    L_h = make_integer_odd(L_h)\n    L_p = make_integer_odd(L_p)\n    Y_h = signal.medfilt(Y, [1, L_h])\n    Y_p = signal.medfilt(Y, [L_p, 1])\n\n    # masking\n    if mask == 'binary':\n        M_h = np.int8(Y_h >= Y_p)\n        M_p = np.int8(Y_h < Y_p)\n    if mask == 'soft':\n        eps = 0.00001\n        M_h = (Y_h + eps / 2) / (Y_h + Y_p + eps)\n        M_p = (Y_p + eps / 2) / (Y_h + Y_p + eps)\n    X_h = X * M_h\n    X_p = X * M_p\n\n    # istft\n    x_h = librosa.istft(X_h, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n    x_p = librosa.istft(X_p, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n\n    if detail:\n        return x_h, x_p, dict(Y_h=Y_h, Y_p=Y_p, M_h=M_h, M_p=M_p, X_h=X_h, X_p=X_p)\n    else:\n        return x_h, x_p\n\n\n이제 다양한 매개변수 \\(N\\), \\(H\\), \\(L^\\mathrm{h}\\) 및 \\(L^\\mathrm{p}\\)의 역할을 예를 통해 보자. 먼저 다른 설정을 사용하여 HPS 절차를 적용한다. 그런 다음 Chopin의 Prelude Op. 28 No. 4 녹음으로 동일한 실험을 수행한다.\n네 가지 매개변수의 상호 작용으로 인해 결과 구성 요소의 음질을 예측하기가 쉽지 않다. \\(L^\\mathrm{h}\\)를 증가시키면 절차가 “하모니적으로 더 엄격해지며” 하모닉에서 타악기 구성 요소로의 “흐름”으로 이어진다. \\(L^\\mathrm{p}\\)를 증가시키면 반대 현상이 발생한다. 음질은 절대값보다 \\(L^\\mathrm{h}\\)와 \\(L^\\mathrm{p}\\) 사이의 상대 관계(relation)에 따라 달라진다. 윈도우 길이 \\(N\\)도 결정적인 영향을 미친다. 예를 들어 \\(N\\)을 높이면 하모닉 성분을 분리하는 데 도움이 될 수 있지만 타악기 성분에 “번짐”이 발생합한다.\n\n\ndef generate_audio_tag_html_list(list_x, Fs, width=\"150\", height=\"40\"):\n    \"\"\"Generates audio tag for html needed to be shown in table\n\n    Args:\n        list_x (list): List of waveforms\n        Fs (scalar): Sample rate\n        width (str): Width in px (Default value = '150')\n        height (str): Height in px (Default value = '40')\n\n    Returns:\n        audio_tag_html_list (list): List of HTML strings with audio tags\n    \"\"\"\n    audio_tag_html_list = []\n    for i in range(len(list_x)):\n        audio_tag = ipd.Audio(list_x[i], rate=Fs)\n        audio_tag_html = audio_tag._repr_html_().replace('\\n', '').strip()\n        audio_tag_html = audio_tag_html.replace('<audio ',\n                                                '<audio style=\"width: '+width+'px; height: '+height+'px;\"')\n        audio_tag_html_list.append(audio_tag_html)\n    return audio_tag_html_list\n\n\ndef experiment_hps_parameter(fn_wav, param_list):\n    \"\"\"Script for running an HPS experiment over a parameter list, such as ``[[1024, 256, 0.1, 100], ...]``\n\n    Args:\n        fn_wav (str): Path to wave file\n        param_list (list): List of parameters\n    \"\"\"\n    Fs = 22050\n    x, Fs = librosa.load(fn_wav, sr=Fs)\n\n    list_x = []\n    list_x_h = []\n    list_x_p = []\n    list_N = []\n    list_H = []\n    list_L_h_sec = []\n    list_L_p_Hz = []\n    list_L_h = []\n    list_L_p = []\n\n    for param in param_list:\n        N, H, L_h_sec, L_p_Hz = param\n        print('N=%4d, H=%4d, L_h_sec=%4.2f, L_p_Hz=%3.1f' % (N, H, L_h_sec, L_p_Hz))\n        x_h, x_p = hps(x, Fs=Fs, N=N, H=H, L_h=L_h_sec, L_p=L_p_Hz)\n        L_h = convert_l_sec_to_frames(L_h_sec=L_h_sec, Fs=Fs, N=N, H=H)\n        L_p = convert_l_hertz_to_bins(L_p_Hz=L_p_Hz, Fs=Fs, N=N, H=H)\n        list_x.append(x)\n        list_x_h.append(x_h)\n        list_x_p.append(x_p)\n        list_N.append(N)\n        list_H.append(H)\n        list_L_h_sec.append(L_h_sec)\n        list_L_p_Hz.append(L_p_Hz)\n        list_L_h.append(L_h)\n        list_L_p.append(L_p)\n\n    html_x = generate_audio_tag_html_list(list_x, Fs=Fs)\n    html_x_h = generate_audio_tag_html_list(list_x_h, Fs=Fs)\n    html_x_p = generate_audio_tag_html_list(list_x_p, Fs=Fs)\n\n    pd.options.display.float_format = '{:,.1f}'.format\n    pd.set_option('display.max_colwidth', None)\n    df = pd.DataFrame(OrderedDict([\n        ('$N$', list_N),\n        ('$H$', list_H),\n        ('$L_h$ (sec)', list_L_h_sec),\n        ('$L_p$ (Hz)', list_L_p_Hz),\n        ('$L_h$', list_L_h),\n        ('$L_p$', list_L_p),\n        ('$x$', html_x),\n        ('$x_h$', html_x_h),\n        ('$x_p$', html_x_p)]))\n    df.index = np.arange(1, len(df) + 1)\n    ipd.display(ipd.HTML(df.to_html(escape=False, index=False))) \n\n\nparam_list = [\n    [1024, 256, 0.1, 100],\n    [1024, 256, 0.1, 1000],\n    [1024, 256, 0.8, 100],\n    [8192, 256, 0.1, 100]]    \n\n\nfn_wav = 'FMP_C8_F02_Long_CastanetsViolin.wav'\nprint('=============================================================')\nprint('Experiment for ',\"Castanets Violin\")\nexperiment_hps_parameter(path_data+fn_wav, param_list)\n\nfn_wav = 'FMP_C8_F27_Chopin_Op028-04_minor.wav'\n#print('=============================================================')\n#print('Experiment for ',\"Chopin Op28-04\", param_list)\n#experiment_hps_parameter(path_data+fn_wav, param_list) \n\n=============================================================\nExperiment for  Castanets Violin\nN=1024, H= 256, L_h_sec=0.10, L_p_Hz=100.0\nN=1024, H= 256, L_h_sec=0.10, L_p_Hz=1000.0\nN=1024, H= 256, L_h_sec=0.80, L_p_Hz=100.0\nN=8192, H= 256, L_h_sec=0.10, L_p_Hz=100.0\n\n\n\n\n  \n    \n      $N$\n      $H$\n      $L_h$ (sec)\n      $L_p$ (Hz)\n      $L_h$\n      $L_p$\n      $x$\n      $x_h$\n      $x_p$\n    \n  \n  \n    \n      1024\n      256\n      0.1\n      100\n      9\n      5\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      1024\n      256\n      0.1\n      1000\n      9\n      47\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      1024\n      256\n      0.8\n      100\n      69\n      5\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      8192\n      256\n      0.1\n      100\n      9\n      38\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\n\n# 다른 예시\n\ndef experiment_hps_pieces(wav_list, path, piece_list, \n                          Fs = 22050, N=1024, H=256, L_h_sec=0.2, L_p_Hz=500):\n    \"\"\"Script for running experiment over list of different pieces\n    \"\"\"      \n    list_x = []\n    list_x_h = []\n    list_x_p = []\n    for wav in wav_list:    \n        print(wav)\n        x, Fs = librosa.load(path_data+wav, sr=Fs)\n        x_h, x_p = hps(x, Fs=Fs, N=N, H=H, L_h=L_h_sec, L_p=L_p_Hz)\n        list_x.append(x)\n        list_x_h.append(x_h)\n        list_x_p.append(x_p)\n\n    html_x = generate_audio_tag_html_list(list_x, Fs=Fs)\n    html_x_h = generate_audio_tag_html_list(list_x_h, Fs=Fs)\n    html_x_p = generate_audio_tag_html_list(list_x_p, Fs=Fs)\n\n    pd.options.display.float_format = '{:,.1f}'.format    \n    pd.set_option('display.max_colwidth', None)    \n    df = pd.DataFrame(OrderedDict([\n        ('Piece', piece_list),  \n        ('x', html_x),                                   \n        ('x_h', html_x_h),\n        ('x_p', html_x_p)]))\n\n    df.index = np.arange(1, len(df) + 1)\n    ipd.display(ipd.HTML(df.to_html(escape=False, index=False)))    \n\n\nwav_list = (\n'FMP_C8_F02_Long_CastanetsViolinApplause.wav',    \n'FMP_C8_F02_VibratoImpulsesNoise.wav',    \n'FMP_C8_Audio_Bornemark_StopMessingWithMe-Excerpt_SoundCloud_mix.wav'\n)\n\npiece_list = (  \n'Violin + Castanets + Applause',    \n'Vibrato + Impulses + Noise',    \n'Bornemark, Stop Messing With Me'\n)\n\n#experiment_hps_pieces(wav_list, path_data, piece_list)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hprs-구현",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#hprs-구현",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "HPRS 구현",
    "text": "HPRS 구현\n\n이제 위의 바이올린, 박수, 캐스터네츠 녹음을 중첩하여 HRPS 절차를 시도한다. 다음 그림은 바이너리 마스크 \\(\\mathcal{M}^\\mathrm{h}\\), \\(\\mathcal{M}^\\mathrm{r}\\) 및 \\(\\mathcal{M}^\\mathrm{p}\\)를 보여준다.\n화성 신호 \\(x^\\mathrm{h}\\)는 대부분 바이올린에 해당하는 반면 나머지 신호 \\(x^\\mathrm{r}\\)는 박수 소리의 대부분을 포착한다. 타악기 신호 \\(x^\\mathrm{p}\\)에는 캐스터네츠가 포함되어 있지만 신호가 상당히 왜곡되어 있으며 여전히 많은 박수 성분이 포함되어 있다. 신호 왜곡의 또 다른 이유는 신호 재구성 단계에서 도입된 위상 아티팩트(phase artifacts) 때문이기도 하다. 이러한 아티팩트는 특히 타악기 구성 요소에서 들을 수 있다.\n\n\nfn_wav ='FMP_C8_F02_Long_CastanetsViolinApplause.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nN = 1024\nH = 512\nL_h_sec = 0.2\nL_p_Hz = 500\nbeta = 2\nx_h, x_p, x_r, D = hrps(x, Fs=Fs, N=N, H=H, \n                        L_h=L_h_sec, L_p=L_p_Hz, beta=beta, detail=True)\n\n\nylim = [0, 3000]\nplt.figure(figsize=(10,3))\nax = plt.subplot(1,3,1)\nplot_matrix(D['M_h'], Fs=Fs/H, Fs_F=N/Fs, ax=[ax], clim=[0,1],\n                     title='Horizontal binary mask')\nax.set_ylim(ylim)\n\nax = plt.subplot(1,3,2)\nplot_matrix(D['M_r'], Fs=Fs/H, Fs_F=N/Fs, ax=[ax], clim=[0,1],\n                     title='Residual binary mask')\nax.set_ylim(ylim)\n\nax = plt.subplot(1,3,3)\nplot_matrix(D['M_p'], Fs=Fs/H, Fs_F=N/Fs, ax=[ax], clim=[0,1],\n                     title='Vertical binary mask')\nax.set_ylim(ylim)\n\nplt.tight_layout()\nplt.show()\n\nhtml_x_h = generate_audio_tag_html_list([x_h], Fs=Fs, width='220')\nhtml_x_r = generate_audio_tag_html_list([x_r], Fs=Fs, width='220')\nhtml_x_p = generate_audio_tag_html_list([x_p], Fs=Fs, width='220')\n\npd.options.display.float_format = '{:,.1f}'.format    \npd.set_option('display.max_colwidth', None)    \ndf = pd.DataFrame(OrderedDict([     \n    ('$x_h$', html_x_h),                                   \n    ('$x_r$', html_x_r),\n    ('$x_p$', html_x_p)]))\nipd.display(ipd.HTML(df.to_html(escape=False, header=False, index=False)))\n\n\n\n\n\n\n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#분리-계수separation-factor의-영향",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#분리-계수separation-factor의-영향",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "분리 계수(separation factor)의 영향",
    "text": "분리 계수(separation factor)의 영향\n\n분리 계수 \\(\\beta\\)는 분해를 조정하는 데 사용할 수 있다. \\(\\beta=1\\)의 경우는 원래의 HP 분해로 축소된다. \\(\\beta\\)를 증가시키면 \\(x^\\mathrm{h}\\) 및 \\(x^\\mathrm{p}\\) 구성 요소의 재구성에 더 적은 시간-주파수 빈이 할당되는 반면, 더 많은 시간-주파수 빈이 잔차 성분 \\(x^\\mathrm{r}\\)의 재구성에 사용된다.\n직관적으로 \\(\\beta\\) 매개변수가 클수록 \\(x^\\mathrm{h}\\) 및 \\(x^\\mathrm{p}\\) 구성 요소의 화성 및 타악기 특성이 더 명확해진다. 매우 큰 \\(\\beta\\)의 경우 잔차 신호 \\(x^\\mathrm{r}\\)는 전체 신호 \\(x\\)를 포함하는 경향이 있다.\n\n\nImage(path_img+\"FMP_C8_E05_HRP_beta.png\", width=500)\n\n\n\n\n\ndef experiment_hrps_parameter(fn_wav, param_list):\n    \"\"\"Script for running an HRPS experiment over a parameter list, such as ``[[1024, 256, 0.1, 100], ...]``\n    Args:\n        fn_wav (str): Path to wave file\n        param_list (list): List of parameters\n    \"\"\"\n    x, Fs = librosa.load(fn_wav)\n\n    list_x = []\n    list_x_h = []\n    list_x_p = []\n    list_x_r = []\n    list_N = []\n    list_H = []\n    list_L_h_sec = []\n    list_L_p_Hz = []\n    list_L_h = []\n    list_L_p = []\n    list_beta = []\n\n    for param in param_list:\n        N, H, L_h_sec, L_p_Hz, beta = param\n        print('N=%4d, H=%4d, L_h_sec=%4.2f, L_p_Hz=%3.1f, beta=%3.1f' % (N, H, L_h_sec, L_p_Hz, beta))\n        x_h, x_p, x_r = hrps(x, Fs=Fs, N=1024, H=512, L_h=L_h_sec, L_p=L_p_Hz, beta=beta)\n        L_h = convert_l_sec_to_frames(L_h_sec=L_h_sec, Fs=Fs, N=N, H=H)\n        L_p = convert_l_hertz_to_bins(L_p_Hz=L_p_Hz, Fs=Fs, N=N, H=H)\n        list_x.append(x)\n        list_x_h.append(x_h)\n        list_x_p.append(x_p)\n        list_x_r.append(x_r)\n        list_N.append(N)\n        list_H.append(H)\n        list_L_h_sec.append(L_h_sec)\n        list_L_p_Hz.append(L_p_Hz)\n        list_L_h.append(L_h)\n        list_L_p.append(L_p)\n        list_beta.append(beta)\n\n    html_x = generate_audio_tag_html_list(list_x, Fs=Fs)\n    html_x_h = generate_audio_tag_html_list(list_x_h, Fs=Fs)\n    html_x_p = generate_audio_tag_html_list(list_x_p, Fs=Fs)\n    html_x_r = generate_audio_tag_html_list(list_x_r, Fs=Fs)\n\n    pd.options.display.float_format = '{:,.1f}'.format\n    pd.set_option('display.max_colwidth', None)\n    df = pd.DataFrame(OrderedDict([\n        ('$N$', list_N),\n        ('$H$', list_H),\n        ('$L_h$ (sec)', list_L_h_sec),\n        ('$L_p$ (Hz)', list_L_p_Hz),\n        ('$L_h$', list_L_h),\n        ('$L_p$', list_L_p),\n        ('$\\\\beta$', list_beta),\n        ('$x$', html_x),\n        ('$x_h$', html_x_h),\n        ('$x_r$', html_x_r),\n        ('$x_p$', html_x_p)]))\n\n    df.index = np.arange(1, len(df) + 1)\n    ipd.display(ipd.HTML(df.to_html(escape=False, index=False)))\n\n\nparam_list = [\n    [1024, 256, 0.2, 500, 1.1],\n    [1024, 256, 0.2, 500, 2],\n    [1024, 256, 0.2, 500, 4],   \n    [1024, 256, 0.2, 500, 32],      \n]\n\nfn_wav = 'FMP_C8_F02_Long_CastanetsViolinApplause.wav'\nprint('=============================================================')\nprint('Experiment for ',\"Castanets Violin Applause\")\nexperiment_hrps_parameter(path_data+fn_wav, param_list)\n\n=============================================================\nExperiment for  Castanets Violin Applause\nN=1024, H= 256, L_h_sec=0.20, L_p_Hz=500.0, beta=1.1\nN=1024, H= 256, L_h_sec=0.20, L_p_Hz=500.0, beta=2.0\nN=1024, H= 256, L_h_sec=0.20, L_p_Hz=500.0, beta=4.0\nN=1024, H= 256, L_h_sec=0.20, L_p_Hz=500.0, beta=32.0\n\n\n\n\n  \n    \n      $N$\n      $H$\n      $L_h$ (sec)\n      $L_p$ (Hz)\n      $L_h$\n      $L_p$\n      $\\beta$\n      $x$\n      $x_h$\n      $x_r$\n      $x_p$\n    \n  \n  \n    \n      1024\n      256\n      0.2\n      500\n      18\n      24\n      1.1\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      1024\n      256\n      0.2\n      500\n      18\n      24\n      2.0\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      1024\n      256\n      0.2\n      500\n      18\n      24\n      4.0\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n    \n      1024\n      256\n      0.2\n      500\n      18\n      24\n      32.0\n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#cascaded-hrps",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#cascaded-hrps",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "Cascaded HRPS",
    "text": "Cascaded HRPS\n\nHRPS 절차는 더 확장될 수 있다.예를 들어, cascaded harmonic-residual-percussive(CHRPS) 절차가 있다.\n먼저 큰 분리 계수(예: \\(\\beta=5\\))를 사용하여 신호를 화성(H), 잔차(R) 및 타악기(P) 성분으로 분리한다. 그런 다음 더 작은 분리 계수(예: \\(\\beta=3\\))를 사용하여 첫 번째 단계의 잔차 성분을 추가로 구성한다.\n보다 일반적으로, 분리 계수 \\(\\beta_1 > \\beta_2 > \\ldots >\\beta_B\\)가 감소하는 \\(B\\in\\mathbb{R}\\) 계단식 단계를 사용하여 CHRP 절차는 \\((2B+1)\\) 구성요소 신호를 생성한다. 합계는 입력 신호 \\(x\\)와 같다(작은 오류까지).\n필요한 모든 계단식 단계가 완료되면 각 요소의 신호가 화성에서 잔차를 거쳐 타악기 순으로 축에 정렬된다.\n\n다음 그림은 \\(B = 3\\)를 사용하여 이 절차를 보여준다. 이 경우 CHRP 절차는 H, RH, RRH, RRR, RRP, RP, P의 7개의 구성 요소 신호를 생성한다.\n\nImage(path_img+\"FMP_C8_E05_HRP-cascaded.png\", width=400)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#chrp-피쳐",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#chrp-피쳐",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "CHRP 피쳐",
    "text": "CHRP 피쳐\n\n음악 이벤트 밀도(density) 및 음악 구조 분석과 같은 작업 등으로부터 동기를 부여받아 이제 중간-수준(mid-level) 특징 표현을 소개한다.\n이를 위해 슬라이딩 윈도우 기법을 사용하여 \\(2B+1\\) 성분 신호 각각에 대한 로컬 에너지를 계산한다. 이를 위해 윈도우 길이가 \\(N\\in\\mathbb{N}\\)이고 홉 크기 \\(H\\in\\mathbb{N}\\)를 사용한다.\n\\(M\\in\\mathbb{N}\\)을 에너지 프레임의 수라고 한다. 그런 다음, 모든 구성 요소 신호의 로컬 에너지 값을 \\(m\\in[1:M]\\)에 대해 \\(v_m\\in\\mathbb{R}^{2B+1}\\)가 되도록 특징 행렬 \\(V=(v_1,\\ldots,v_M)\\)에 쌓는다. 또한 \\(v_m\\) 열은 (예: \\(\\ell^1\\)-norm)을 사용하여 정규화될 수 있다.\n각 \\(v_m\\in[0,1]^{2B+1}\\)는 각 프레임 \\(m\\in[1:M]\\)에 대한 구성요소 전체의 에너지 분포를 나타낸다.\n다음 예에서는 겹치지 않는 5개의 사운드 샘플(캐스터네츠, 스네어 롤, 박수, 스타카토 현 및 레가토 바이올린)로 구성된 신호에 대한 CHRP 특징 매트릭스를 보여준다.\n캐스터네츠는 충격 에너지가 가장 높고 P 성분에 잘 국한되어 있다. 스네어 롤은 주로 RP와 RRR로 구성된다. 실제로 스네어 드럼은 타악기적인 어택과 시끄럽고 음색이 강한 감쇠 곡선(드럼의 튜닝 주파수에 따라 다름)을 가지고 있다. 빠르게 연속적으로 치면 노이지한 디케이(decay) 꼬리가 타악기 온셋보다 우세하다. 박수는 RRR을 중심으로 하며 잔여 영역에 잘 국한된다. 스타카토 현은 이 연주 기법에서 나타나는 시끄러운 공격에 해당하는 추가 RH 구성 요소와 함께 주로 하모닉하다. 바이올린 레가토는 H 구성 요소로 제한된다. 안정적인 화성 신호 특성이 다른 모든 구성 요소를 지배하기 때문이다.\n\n\nipd.display(Image(path_img+ \"FMP_C8_E05_CHRP_Ex-Ramp.png\", width=400))\n# ipd.display(Audio(path_data+\"FMP_C8_PLS-Fig-5-2_PRH-RampSynthetic.wav\"))\n\n\n\n\n\n다음 예는 패러디들(paradiddles) 이라고 알려진 스네어 연주 기법에서 타악기에서 잔차로 에너지가 이동하는 것을 보여준다. 다음 그림에서는 스네어 드럼에서 연주되는 패러디들의 파형을 보여준다.\n처음에 속도가 증가(\\(0\\)-\\(40\\) 초)하고 그 다음 속도가 감소(\\(40\\)-\\(75\\) 초)하는 연주이다. 결과 CHRP 특징 행렬에서 특정 온셋 주파수 또는 재생 속도에 도달(약 \\(25\\)초 즈음)한 후 남은 P 에너지가 얼마나 적은지 알 수 있다. 이것은 소음과 같은 꼬리가 상대적 근접에 도달하고 개별 타악기 온셋을 압도하여 특징 행렬의 나머지 구성 요소 주위에 에너지를 집중시키기 때문이다.\n\n\nipd.display(Image(path_img+ \"FMP_C8_E05_CHRP_Ex-Snare.png\", width=400))\n#ipd.display(Audio(path_data+\"FMP_C8_PLS-Fig-5-3_Paradiddle.wav\"))"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#표기-및-문제-공식",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#표기-및-문제-공식",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "표기 및 문제 공식",
    "text": "표기 및 문제 공식\n\n\\(x:\\mathbb{Z}\\to\\mathbb{R}\\)를 이산 시간 신호라고 하고, \\(\\mathcal{X}\\)를 \\(N\\in\\mathbb{N}\\) 길이의 윈도우 함수 \\(w:[0:N-1]\\to\\mathbb{R}\\)와 홉 크기 매개변수 \\(H\\in\\mathbb{N}\\)에 기반한 STFT라고 하자.\n원래 신호는 조건 \\(\\sum_{n\\in\\mathbb{Z}} w(r-nH)\\not= 0\\)가 성립할 때 \\(\\mathcal{X}\\)로부터 완벽하게 재구성할 수 있다. \\(\\mathcal{X}^\\mathrm{mod}\\)가 주어진 MSTFT라고 가정하자. 수정된 경우의 재구성에서 다음 절차를 적용하는 것이 간단해 보인다. 첫 번째 단계에서 \\(\\mathcal{X}^\\mathrm{mod}\\)의 각 열에 역 DFT를 적용하여 다음의 결과를 얻는다. \\[(v_n(0),\\ldots, v_n(N-1))^\\top := \\mathrm{DFT}_N^{-1} \\Big((\\mathcal{X}^\\mathrm{mod}(n, 0),\\ldots, \\mathcal{X}^\\mathrm{mod}d(n,N-1))^\\top\\Big)\\] for \\(n\\in\\mathbb{Z}\\)\n또한 \\(r\\in\\mathbb{Z}\\setminus[0:N-1]\\)에 대해 \\(v_n(r):=0\\)로 설정한다. 그런 다음 중첩-추가(overlap-add) 기술을 적용하여 \\(x^\\mathrm{Rec}:\\mathbb{Z}\\to\\mathbb{R}\\) 신호를 다음과 같이 설정하여 정의한다. \\[x^\\mathrm{Rec}(r) := \\frac{\\sum_{n\\in\\mathbb{Z}} v_n(r-nH)}{\\sum_{n\\in\\mathbb{Z}} w(r-nH)}\\] for \\(r\\in\\mathbb{Z}\\).\n신호 \\(x^\\mathrm{Rec}\\)에는 문제가 있다. 일반적으로 신호 \\(x^\\mathrm{Rec}\\)의 STFT \\(\\mathcal{X}^\\mathrm{Rec}\\)는 수정된 STFT \\(\\mathcal{X}^\\mathrm{mod}\\)와 동일하지 않다. 그 이유는 \\(x^\\mathrm{Rec}\\)에 윈도우잉을 적용할 때 결과적으로 윈도우화된 섹션 \\(x^\\mathrm{Rec}_n\\)이 일반적으로 \\(v_n\\)과 일치하지 않기 때문이다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#예시",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#예시",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "예시",
    "text": "예시\n\n이 상황은 다음 그림으로 설명된다. 첫 번째 행에는 신호 \\(x\\), 세 개의 윈도우 섹션 \\(x_n\\) 및 크기(magnitude) STFT의 결과 열이 \\(\\mathcal{X}\\)로 표시된다. 두 번째 행에서 크기 STFT가 수정되고(가운데 열이 0으로 설정되어, 신호의 스파이크가 제거됨) 재구성된 신호 \\(v_n\\) 및 \\(x^\\mathrm{Rec}\\)가 얻어진다. 세 번째 행은 신호 \\(x^\\mathrm{Rec}\\), 윈도우 영역 \\(x^\\mathrm{Rec}_n\\) 및 크기 STFT \\(\\mathcal{X}^\\mathrm{Rec}\\)를 보여준다.\n\n\nImage(path_img+\"FMP_C8_F08.png\", width=500)\n\n\n\n\n\n이 그림에서 STFT \\(\\mathcal{X}^\\mathrm{Rec}\\)는 MSTFT \\(\\mathcal{X}^\\mathrm{Mod}\\)와 일치하지 않는다. 이는 STFT 계산에 사용되는 시간이동된 분석 창이 인접한 창과 겹치기 때문이다. 예를 들어 \\(\\mathcal{X}^\\mathrm{Rec}\\)의 두 번째 프레임을 계산하면 첫 번째 및 세 번째 창의 정보도 포함된다.\n직관적으로 재구성에서 overlap-add 절차를 사용하면 이전 프레임과 이후 프레임의 정보가 현재 프레임에 재도입된다. \\(v_n\\) 및 \\(x^\\mathrm{Rec}_n\\) 신호가 다를 수 있지만 이러한 신호에 대한 각각의 합계는 동일한 신호 \\(x^\\mathrm{Rec}\\)를 산출한다.\n\n\n# Signal\nL = 64\nt = np.arange(L)/L\nomega = 8\nx = np.sin(2 * np.pi * omega * t)\nx[31] = +1.5\nx[32] = -1.5\nN = 32\nH = N//2\nw_type = 'hann'\nw = signal.get_window(w_type, N) \n\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=False)\nplt.figure(figsize=(9,2.5))\nax = plt.subplot(1,3,1)\nplot_matrix(np.abs(X),ax=[ax], \n                xlabel='Time (samples)', ylabel='Frequency (bins)',                      \n                title=r'STFT $\\mathcal{X}$')\n\nX_mod = X\nX_mod[:,1]=0\nax = plt.subplot(1,3,2)\nplot_matrix(np.abs(X_mod),ax=[ax],\n                xlabel='Time (frames)', ylabel='Frequency (bins)', \n                title=r'MSTFT $\\mathcal{X}^\\mathrm{Mod}$')\n\nx_rec = librosa.istft(X_mod, hop_length=H, win_length=N, window='hann', center=False, length=L)\nX_rec = librosa.stft(x_rec, n_fft=N, hop_length=H, win_length=N, window='hann', pad_mode='constant', center=False)\nax = plt.subplot(1,3,3)\nplot_matrix(np.abs(X_rec),ax=[ax], \n                xlabel='Time (frames)', ylabel='Frequency (bins)', \n                title=r'STFT $\\mathcal{X}^\\mathrm{Rec}$')\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(8,2))\nplt.subplot(1,2,1)\nplt.plot(x, 'k')\nplt.xlim([0,L-1])\nplt.ylim([-1.6, 1.6]) \nplt.title(r'Original signal $x$')\nplt.xlabel('Time (samples)')\n\nplt.subplot(1,2,2)\nplt.plot(x_rec, 'k')\nplt.xlim([0,L-1])\nplt.ylim([-1.6, 1.6]) \nplt.title(r'Reconstructed signal $x^\\mathrm{Rec}$ from $\\mathcal{X}^\\mathrm{Mod}$')\nplt.xlabel('Time (samples)')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#최적화-문제로서의-재구성",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#최적화-문제로서의-재구성",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "최적화 문제로서의 재구성",
    "text": "최적화 문제로서의 재구성\n\n재구성된 신호 \\(x^\\mathrm{Rec}\\)의 STFT가 지정된 MSTFT와 일치하지 않을 수 있음을 봤다. 즉 재구성 방법에 관계없이 STFT가 주어진 MSTFT와 일치하는 시간 영역 신호가 존재하지 않을 수 있다.\n따라서 적절하게 정의된 거리 측정과 관련하여 STFT가 MSTFT에 가능한 한 근접한 신호를 추정하는 방법을 찾아야 한다.\n주어진 MSTFT \\(\\mathcal{X}^\\mathrm{Mod}\\)와 신호 \\(x'\\)의 STFT \\(\\mathcal{X}'\\) 사이의 거리를 측정하기 위해 평균 제곱 오차(mean square error) \\(\\Delta(\\mathcal{X}^\\mathrm{Mod},\\mathcal{X}')\\)를 도입하며 다음과 같이 정의된다. \\[\\Delta(\\mathcal{X}^\\mathrm{Mod},\\mathcal{X}'):=\\sum_{n\\in\\mathbb{Z}}\\,\\,\\sum_{k\\in[0 :N-1]}|\\mathcal{X}^\\mathrm{Mod}(n,k)-\\mathcal{X}'(n,k)|^2.\\]\n목표는 STFT \\(\\mathcal{X}^\\ast\\)가 가능한 모든 신호 \\(x'\\)에 대해 오차를 최소화하는 신호 \\(x^\\ast\\)를 찾는 것이다. \\[x^\\ast := \\underset{x'}{\\mathrm{argmin}} \\Delta(\\mathcal{X}^\\mathrm{Mod},\\mathcal{X}')\\]\n이 최적화 문제는 다음과 같은 방법으로 풀 수 있다. \\[x^\\ast(r) = \\frac{\\sum_{n\\in\\mathbb{Z}} w(r-nH)v_n(r-nH)}{\\sum_{n\\in\\mathbb{Z} } w(r-nH)^2}\\]\n이 절차는 본질적으로 이전에 설명한 중첩-추가(overlap-add) 기술과 유사하다. 가장 큰 차이점은 최적의 솔루션에서 \\(v_n\\) 신호가 오버레이 되고 추가되기 전에 분석 윈도우에 표시된다는 것이다. 또한 추가 윈도우잉은 제곱 윈도우의 합으로 정규화하여 보상된다.\n자세한 내용과 대체 절차는 Griffin과 Lim의 원본 논문을 참조하자.\n\nDaniel W. Griffin and Jae S. Lim (1984) “Signal estimation from modified short-time Fourier transform”. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#크로마-특징-향상",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#크로마-특징-향상",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "크로마 특징 향상",
    "text": "크로마 특징 향상\n\n첫 번째 예로 HPS 절차의 출력을 사용하여 크로마 기반 오디오 특징(feature)을 향상시키는 방법을 살펴보자. 크로마 특징은 음악의 선율 및 화성 속성이 중요한 작업을 위해 설계되었다. 이러한 특성은 일반적으로 적은 수의 크로마 대역에 신호 에너지가 집중되어 반영된다.\n크로마 기반 특징을 계산할 때 스펙트로그램을 피치 하위 밴드으로 분해하는 것으로 시작한다. 이 분해에서 스펙트로그램의 수평 구조는 몇 개의 대역에서 신호 에너지의 높은 집중을 초래하는 반면 수직 구조는 평평한 에너지 분포를 초래한다.\n따라서 크로마 표현을 개선하는 한 가지 방법은 먼저 HPS를 적용하여 신호 \\(x\\)를 하모니 성분 \\(x^\\mathrm{h}\\) 및 타악기 성분 \\(x^\\mathrm{p}\\)로 분해하는 것이다. 그런 다음, 하모니 성분 \\(x^\\mathrm{h}\\)만을 기준으로 크로마 특징을 계산한다. 이러한 HPS 기반 전처리 단계의 효과는 다음 그림에 설명되며, 크기(magnitude) STFT(첫 번째 행)와 원래 신호 \\(x\\), 하모니 성분 \\(x^\\mathrm{h}\\) 및 타악기 성분 \\(x^\\mathrm{p}\\)에 대한 결과 크로마 표현(두 번째 행)을 보여준다.\n\n\n# Computation of harmonic and percussive componentn signals\nFs = 11025\n#fn_wav = 'FMP_C8_F02_Medi_CastanetsViolin.wav'\nfn_wav = 'FMP_C8_F02_Long_CastanetsViolin.wav'\nx, Fs = librosa.load(path_data+fn_wav, sr=Fs)\nN, H = 512, 256 \nL_h_sec=0.2\nL_p_Hz=500\nx_h, x_p = hps(x, Fs=Fs, N=N, H=H, L_h=L_h_sec, L_p=L_p_Hz)\n\n\n# Computation of spectrograms and chromgrams\nplt.figure(figsize=(12, 4))    \n\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\nY = np.log(1 + 10 * np.abs(X)) \nC = librosa.feature.chroma_stft(S=Y, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\nax = plt.subplot(2,3,1)\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax], title='Mix', clim=[0,5]);\nax = plt.subplot(2,3,4)\nplot_chromagram(C, Fs=Fs/H, ax=[ax], clim=[0,70], chroma_yticks=[0,4,7,11]);\n\nX_h = librosa.stft(x_h, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\nY_h = np.log(1 + 10 * np.abs(X_h)) \nC_h = librosa.feature.chroma_stft(S=Y_h, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\nax = plt.subplot(2,3,2)\nplot_matrix(Y_h, Fs=Fs/H, Fs_F=N/Fs, ax=[ax], title='Harmonic', clim=[0,5]);\nax = plt.subplot(2,3,5)\nplot_chromagram(C_h, Fs=Fs/H, ax=[ax], clim=[0,70], chroma_yticks=[0,4,7,11]);\n\nX_p = librosa.stft(x_p, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\nY_p = np.log(1 + 10 * np.abs(X_p)) \nC_p = librosa.feature.chroma_stft(S=Y_p, sr=Fs, tuning=0, norm=None, hop_length=H, n_fft=N)\nax = plt.subplot(2,3,3)\nplot_matrix(Y_p, Fs=Fs/H, Fs_F=N/Fs, ax=[ax], title='Percussive', clim=[0,5]);\nax = plt.subplot(2,3,6)\nplot_chromagram(C_p, Fs=Fs/H, ax=[ax], clim=[0,70], chroma_yticks=[0,4,7,11]);\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n물론, 이 파이프라인에서는 크로마 특징을 계산하기 전에 시간 영역 신호 \\(x^\\mathrm{h}\\)를 재구성할 필요가 없다. 대신, 로그 주파수 및 크로마 표현을 도출하기 위해 마스킹된 크기 STFT \\(\\mathcal{Y}^\\mathrm{h}\\)를 직접 사용할 수 있다.\n또한 HP 분리 단계는 로그 압축(logarithmic compression) 또는 시간 평활화(temporal smoothing) 와 같은 추가 향상 전략과 쉽게 결합될 수 있다. 그러나 다양한 강화 전략은 서로 영향을 미치거나 유사한 목적을 달성할 수도 있다. 예를 들어 시간적 평활화는 타악기 성분의 영향을 줄이는 것을 목표로 하여 HP 분리 기반 전처리와 유사한 효과를 생성한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#노벨티-표현-향상",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#노벨티-표현-향상",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "노벨티 표현 향상",
    "text": "노벨티 표현 향상\n\n두 번째 예로서 온셋 감지 작업을 고려해 보자.\n일반적으로 전체 주파수 스펙트럼에 걸쳐 퍼지는 일시적인 사운드 구성 요소와 함께 온셋 시작이 자주 발생한다. 따라서 HPS 절차는 온셋 감지를 적용하기 전에 수직 시간-주파수 패턴을 향상시키는 데 사용될 수 있다.\n다음 예는 이러한 접근 방식의 효과를 보여준다. 여기서 간단한 에너지 기반 참신 함수가 원래 신호뿐만 아니라 하모니 성분 \\(x^\\mathrm{h}\\) 및 타악기 성분 \\(x^\\mathrm{p}\\)에 적용된다.\n스펙트럼 변화에 기반한 다른 온셋 감지는 이전의 harmonic-percussive 분해로부터 같은 정도로 이익을 얻지 못할 수 있다. 그 이유는 스펙트럼 변화의 계산이 인접한 스펙트럼 벡터의 열 방향 차이를 고려하여 타악기(수직) 구조의 일부 향상을 이미 포함하고 있기 때문이다.\n\n\nfig, ax = plt.subplots(2, 4, gridspec_kw={'width_ratios': [1, 1,  1, 0.05], \n                                          'height_ratios': [1.5, 1]}, \n                       constrained_layout=True, figsize=(12, 4))    \n\n# Plotting spectrograms\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0]], colorbar=None, title='Mix', clim=[0,5]);\nplot_matrix(Y_h, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,1]], colorbar=None, title='Harmonic', clim=[0,5]);\nplot_matrix(Y_p, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,2],ax[0,3]], title='Percussive', clim=[0,5]);\n\n# Computation and plotting of novelty curves\nN, H = 2048, 256     \nnov, Fs_nov = compute_novelty_energy(x, Fs=Fs,  N=N, H=H)\nplot_signal(nov, Fs=Fs_nov, ax=ax[1,0]);\n\nnov, Fs_nov = compute_novelty_energy(x_h, Fs=Fs,  N=N, H=H)\nplot_signal(nov, Fs=Fs_nov, ax=ax[1,1]);\n\nnov, Fs_nov = compute_novelty_energy(x_p, Fs=Fs,  N=N, H=H)\nplot_signal(nov, Fs=Fs_nov, ax=ax[1,2]);\nax[1,3].set_axis_off()\nplt.show()"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#시간-척도-수정-향상-improving-time-scale-modification",
    "href": "posts/9. Musically Informed Audio Decomposition/9.1.Harmonic–Percussive_Separation.html#시간-척도-수정-향상-improving-time-scale-modification",
    "title": "9.1. 화성-타악 분리 (HPS)",
    "section": "시간-척도 수정 향상 (Improving Time-Scale Modification)",
    "text": "시간-척도 수정 향상 (Improving Time-Scale Modification)\n\nTime-scale modification(TSM)은 피치를 변경하지 않고 오디오 신호의 재생 속도를 높이거나 낮추는 작업이다.\n음악 신호의 TSM에서 주요 문제는 타악기 트랜지언트(transient) 현상이 종종 지각적으로 저하된다는 것이다. 이러한 저하를 방지하기 위해 일부 TSM 접근 방식은 입력 신호에서 트랜지언트 현상을 명시적으로 식별하고 특수한 방식으로 처리하길 시도한다. 그러나 이러한 접근은 두 가지 이유로 문제가 있다. 첫째, 트랜지언트 감지의 오류는 최종 TSM 결과에 즉각적인 영향을 미치며, 둘째, 트랜지언트의 인지적 보존은 어려운 작업이다.\n이 문제를 해결하기 위해 Driedger et al.의 HPS 단계를 적용하여 신호를 화성음 구성 요소와 일반적으로 트랜지언트를 포함하는 타악기 구성 요소로 분할한다. 화성음(하모니) 성분은 큰 프레임 크기를 사용하는 위상 보코더(phase vocoder) 접근 방식으로 수정되는 반면, 노이즈와 같은 타악기 성분은 간단한 시간-영역 중첩-추가 기법(time-domain overlap-add) 으로 수정된다. 명시적인 트랜지언트 감지 없이 트랜지언트를 높은 수준으로 보존하는 짧은 프레임 크기를 사용한다.\n다음 그림은 HP 기반 TSM 절차를 보여준다.\n\n\nImage(path_img+\"FMP_C8_F09_TSM.png\", width=400)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html",
    "title": "9.2. 멜로디 추출",
    "section": "",
    "text": "멜로디 추출에 대해 알아보고 그 예를 살펴본다. 또한 순간 주파수 추정, 돌출(salience) 표현, 기본 주파수 추적 등을 설명한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#순간-주파수-if",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#순간-주파수-if",
    "title": "9.2. 멜로디 추출",
    "section": "순간 주파수 (IF)",
    "text": "순간 주파수 (IF)\n\n우선 주파수 표현 및 측정의 주요 아이디어를 상기해보자. 다음 형식의 복소수 지수 함수를 고려한다. \\[\\mathbf{exp}_{\\omega,\\varphi}:\\mathbb{R}\\to\\mathbb{C}, \\quad \\mathbf{exp}_\\omega(t):= \\mathrm{exp}\\big (2\\pi i(\\omega t - \\varphi)\\big)\\]\n\n주파수 매개변수 \\(\\omega\\in\\mathbb{R}\\)(\\(\\mathrm{Hz}\\)로 측정) 및 위상 매개변수 \\(\\varphi\\)(\\(360^\\circ\\)의 각도에 해당하는 \\(1\\)로 정규화된 라디안으로 측정\n\n\\(\\varphi=0\\)인 경우, \\[\\mathbf{exp}_{\\omega} := \\mathbf{exp}_{\\omega,0}\\]\n시간 매개변수 \\(t\\)를 균일하게 증가시키면, 지수 함수는 단위 원(unit circle) 주위의 원 운동(circular motion)을 설명한다. 실제 및 가상 축에 투영하면 두 개의 정현파 운동(sinusoidal motions)(코사인 및 사인 함수로 설명됨)이 생성된다.\n원형 운동을 균일하게 회전하는 바퀴로 생각하면 주파수 매개변수 \\(\\omega\\)는 단위 시간당 회전 수(이 경우 1초 지속 시간)에 해당한다. 즉, 주파수는 회전율로 해석할 수 있다.\n이 해석을 기반으로 임의의 시간 간격 \\([t_1,t_2]\\) 및 \\(t_1<t_2\\) 동안 회전하는 바퀴와 주파수 값을 연결할 수 있다. 이를 위해 시간 \\(t_1\\)에서의 각도 위치 \\(\\varphi_1\\)와 시간 \\(t_2\\)에서의 각도 위치 \\(\\varphi_2\\)를 측정한다. 이는 다음 그림에 설명되어 있다.\n\n\nImage(path_img+\"FMP_C8_F12.png\", width=300)\n\n\n\n\n\n주파수는 시간 간격의 길이 \\(t_2-t_1\\)로 나눈 각도 위치의 변화 \\(\\varphi_2-\\varphi_1\\)로 정의된다.\n극한의 경우, 시간 간격이 임의로 작아지면 다음과 같이 주어진 순간 주파수 \\(\\omega_{t_1}\\)를 얻는다. \\[\\omega_{t_1}:=\\lim_{t_2\\to t_1}\\frac{\\varphi_2-\\varphi_1}{t_2-t_1}\\]"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#위상-예측-오류",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#위상-예측-오류",
    "title": "9.2. 멜로디 추출",
    "section": "위상 예측 오류",
    "text": "위상 예측 오류\n\n주파수 값 \\(\\omega\\in\\mathbb{R}\\)과 두 개의 시간 인스턴스(\\(t_1\\in\\mathbb{R}\\), \\(t_2\\in\\mathbb{R}\\))를 고정하는 시간-연속적 관점을 가정해 보자.\n나중에 STFT 매개변수와 관련된 특정 값을 선택할 것이다. 신호 \\(x\\)를 윈도우 버전의 분석 함수 \\(\\mathbf{exp}_\\omega\\)(하나는 \\(t_1\\)에, 다른 하나는 \\(t_2\\)에 위치)와 연관시키면, 두 개의 복소 푸리에 계수를 얻는다.\n\\(\\varphi_1\\)와 \\(\\varphi_2\\)를 각각 이 두 계수의 위상이라 하자.\n신호 \\(x\\)가 주파수 \\(\\omega\\)의 강한 주파수 성분을 포함하는 경우, \\(\\varphi_1\\) 및 \\(\\varphi_2\\)의 두 위상은 다음과 같은 방식으로 일치해야 한다. : 시간 위치 \\(t_1\\)에서 각도 위치 \\(\\varphi_1\\)를 가정하는 주파수 \\(\\omega\\)의 회전(rotation)이 시간 \\(t_2\\)에서 위상 \\(\\varphi^\\mathrm{Pred}:=\\varphi_1 + \\omega\\cdot \\Delta t\\)를 가져야 한다.(\\(\\Delta t:=t_2-t_1\\))\n따라서 신호 \\(x\\)가 함수 \\(\\mathbf{exp}_\\omega\\)와 유사하게 동작하는 경우 \\(\\varphi_2\\approx\\varphi^\\mathrm{Pred}\\)이어야 한다. \\(x\\) 신호가 \\(\\mathbf{exp}_\\omega\\)보다 약간 느리게 진동(oscillate)하는 경우 \\(x\\) 신호에 대한 인스턴스 \\(t_1\\)에서 인스턴스 \\(t_2\\)까지의 위상 증분은 프로토타입 진동 \\(\\mathbf{exp}_\\omega\\)에 대한 것보다 작다. 결과적으로 \\(t_2\\)에서 측정된 위상 \\(\\varphi_2\\)는 예측 위상 \\(\\varphi^\\mathrm{Pred}\\)보다 작다. \\(x\\)가 \\(\\mathbf{exp}_\\omega\\)보다 약간 빠르게 진동하는 세 번째 경우에서 \\(\\varphi_2\\) 위상은 예측 위상 \\(\\varphi^\\mathrm{Pred}\\)보다 크다. 세 가지 경우를 다음 그림으로 설명한다.\n\n\nImage(path_img+\"FMP_C8_F13.png\", width=300)\n\n\n\n\n\n\\(\\varphi_2\\)와 \\(\\varphi^\\mathrm{Pred}\\)의 차이를 측정하기 위해 다음과 같이 정의된 예측 오류를 도입한다. \\[\\varphi^\\mathrm{Err}:=\\Psi(\\varphi_2-\\varphi^\\mathrm{Pred})\\]\n이 정의에서 \\(\\Psi:\\mathbb{R}\\to\\left[-0.5,0.5\\right]\\)는 principal argument function이며, 적절한 정수 값을 더하거나 빼서 위상차를 \\([-0.5,0.5]\\) 범위로 매핑한다. 이는 위상 차이를 계산할 때 불연속성을 방지한다.\n예측 오류는 신호 \\(x\\)에 대한 정제된 주파수 추정 \\(\\mathrm{IF}(\\omega)\\)를 얻기 위해 주파수 값 \\(\\omega\\)를 수정하는 데 사용할 수 있다. \\[\\mathrm{IF}(\\omega) :=  \\omega + \\frac{\\varphi^\\mathrm{Err}}{\\Delta t}\\]\n이 값은 순간 주파수(instantaneous frequency)(IF)라고도 하며 초기 주파수 \\(\\omega\\)의 조정으로 생각할 수 있다. 엄밀히 말하면, 순간 빈도는 단일 시간 인스턴스를 참조하는 것이 아니라 전체 시간 간격 \\([t_1,t_2]\\)를 참조한다. 그러나 실제로 이 간격은 일반적으로 매우 작게 선택된다(대략 몇 밀리초(milliseconds))."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#stft-주파수-분해능-개선",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#stft-주파수-분해능-개선",
    "title": "9.2. 멜로디 추출",
    "section": "STFT 주파수 분해능 개선",
    "text": "STFT 주파수 분해능 개선\n\n이제 이산 STFT의 주파수 분해능을 개선하기 위해 순간 주파수 개념을 적용한다. 극좌표 표현을 사용하여 푸리에 계수 \\(\\mathcal{X}(n,k)\\in\\mathbb{C}\\)를 다음과 같이 쓸 수 있다. \\[\\mathcal{X}(n,k)= |\\mathcal{X}(n,k)|\\mathrm{exp}(2\\pi i\\varphi(n,k))\\] for \\(\\varphi(n,k)\\in[0,1)\\)\n프로토타입 진동의 경우, 주파수 매개변수 \\(k\\in[0:N/2]\\)에 의해 결정되는 주파수를 사용한다. \\[\\omega = F_\\mathrm{coef}(k) = \\frac{k\\cdot F_\\mathrm{s}}{N}.\\]\n또한 두 시간 인스턴스는 이전 프레임과 현재 프레임의 위치에 따라 결정된다. \\[t_1=T_\\mathrm{coef}(n-1)=\\frac{(n-1)\\cdot H}{F_\\mathrm{s}},\\] \\[\\quad t_2=T_\\mathrm{coef}(n)=\\frac{n\\cdot H}{F_\\mathrm{s}}\\]\n마지막으로 이러한 시간 인스턴스에서 측정된 위상은 STFT에서 얻은 것이다. \\[\\varphi_1=\\varphi(n-1,k)\\] \\[\\varphi_2=\\varphi(n,k)\\]\n이로부터 위의 방정식들을 사용하고 약간의 재정렬을 수행하여 순간 주파수를 얻는다. \\[F_\\mathrm{coef}^\\mathrm{IF}(k,n):=\\mathrm{IF}(\\omega) = \\left( k + \\kappa(k,n) \\right)\\cdot\\frac{F_\\mathrm{s}}{N}\\] bin offset \\(\\kappa(k,n) = \\frac{N}{H}\\cdot\\Psi\\left(\\varphi(n,k)-\\varphi(n-1,k) - \\frac{k\\cdot H}{N}\\right)\\)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#구현",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#구현",
    "title": "9.2. 멜로디 추출",
    "section": "구현",
    "text": "구현\n\n다음 코드 셀에서는 주어진 STFT의 모든 시간-주파수 빈에 대한 순간 주파수(IF)를 추정하는 구현을 제공한다.\n\n입력은 샘플링 속도 Fs, 윈도우 길이 N, 홉 크기 H , 그리고 차원 K(주파수 빈 수) 및 L(프레임 수)의 복소수 값 STFT 행렬 X로 구성된다.\n출력은 각 시간-주파수 빈에 대한 IF의 추정치를 포함하는 실수 값 행렬 F_coef_IF이다. F_coef_IF도 (K \\(\\times\\) L)-행렬이 되도록 패딩을 적용한다.\n구현에서 IF는 행렬 연산을 사용하여 모든 시간-주파수 빈에 대해 동시에 계산된다.\n\n\n\ndef principal_argument(v):\n    \"\"\"Principal argument function\n\n    Args:\n        v (float or np.ndarray): Value (or vector of values)\n\n    Returns:\n        w (float or np.ndarray): Principle value of v\n    \"\"\"\n    w = np.mod(v + 0.5, 1) - 0.5\n    return w\n\n\ndef compute_if(X, Fs, N, H):\n    \"\"\"Instantenous frequency (IF) estamation\n\n    Args:\n        X (np.ndarray): STFT\n        Fs (scalar): Sampling rate\n        N (int): Window size in samples\n        H (int): Hop size in samples\n\n    Returns:\n        F_coef_IF (np.ndarray): Matrix of IF values\n    \"\"\"\n    phi_1 = np.angle(X[:, 0:-1]) / (2 * np.pi)\n    phi_2 = np.angle(X[:, 1:]) / (2 * np.pi)\n\n    K = X.shape[0]\n    index_k = np.arange(0, K).reshape(-1, 1)\n    # Bin offset (FMP, Eq. (8.45))\n    kappa = (N / H) * principal_argument(phi_2 - phi_1 - index_k * H / N)\n    # Instantaneous frequencies (FMP, Eq. (8.44))\n    F_coef_IF = (index_k + kappa) * Fs / N\n\n    # Extend F_coef_IF by copying first column to match dimensions of X\n    F_coef_IF = np.hstack((np.copy(F_coef_IF[:, 0]).reshape(-1, 1), F_coef_IF))\n\n    return F_coef_IF"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#if-시각화",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#if-시각화",
    "title": "9.2. 멜로디 추출",
    "section": "IF 시각화",
    "text": "IF 시각화\n\n이제 IF 추정 절차에 대한 더 깊은 통찰력을 제공하는 시각화를 소개한다. 예를 들어, 피아노로 연주되는 \\(\\mathrm{C4}\\)의 단일 음 또는 녹음을 고려하자. 이 음은 중심 주파수가 \\(261.6~\\mathrm{Hz}\\), MIDI 피치 \\(p=60\\)에 해당한다. 다음 그림은 오디오 신호의 파형과 스펙트로그램 표현을 보여준다.\n\n\nfn_wav = 'FMP_C6_F04_NoteC4_Piano.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nipd.display(Audio(data=x,rate=Fs))\n\nN = 2048\nH = 64\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\nY = np.log(1+ 10*np.abs(X))\nK = X.shape[0]\nL = X.shape[1]\nylim = [0,1500]\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [1, 2]}, figsize=(5, 4))        \n\nplot_signal(x, Fs, ax=ax[0,0])\nax[0,1].set_axis_off()\nplot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[1,0],ax[1,1]], colorbar=True)\nax[1,0].set_ylim(ylim)\nplt.tight_layout()\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\nIF 추정의 출력은 주파수 값 \\(F_\\mathrm{coef}^\\mathrm{IF}(k,n)\\in\\mathbb{R}\\)(Hertz로 제공됨)이다(\\(k\\in[0:K-1]\\), \\(n\\in[0:L-1]\\)). 이 값은 STFT의 \\(k\\)번째 푸리에 계수와 관련된 (프레임 독립적인) 주파수 값 \\(F_\\mathrm{th}\\)의 개선이다.\n다음 그림에서 \\(F_\\mathrm{coef}\\) 및 \\(F_\\mathrm{coef}^\\mathrm{IF}\\)에 의해 주어진 \\((K\\times N)\\)-행렬을 색상 코드 형태로 표시한다. 또한 Hertz로 지정된 bin offset인 차이를 시각화한다. \\[F_\\mathrm{coef}^\\mathrm{IF}-F_\\mathrm{coef} = \\kappa \\cdot\\frac{N}{F_\\mathrm{s}}\\]\n스펙트로그램을 시각화(크기(magnitude)가 색상으로 구분됨)하는 것과 반대로 이번에는 주파수 값을 시각화(헤르츠로 표시됨)한다.\n\n\ndef plot_IF(F_coef_X, F_coef_IF, ylim, figsize=(10,3)):\n    plt.figure(figsize=figsize)\n    ax = plt.subplot(1,3,1)\n    plot_matrix(F_coef_X, Fs=Fs/H, Fs_F=N/Fs, cmap='PuOr', clim=ylim, ax=[ax],\n            title=r'$F_\\mathrm{coef}$')\n    plt.ylim(ylim)\n    ax = plt.subplot(1,3,2)\n    plot_matrix(F_coef_IF, Fs=Fs/H, Fs_F=N/Fs, cmap='PuOr', clim=ylim, ax=[ax],\n            title=r'$F_\\mathrm{coef}^\\mathrm{IF}$')\n    plt.ylim(ylim)\n    ax = plt.subplot(1,3,3)\n    plot_matrix(F_coef_IF-F_coef_X, Fs=Fs/H, Fs_F=N/Fs, cmap='seismic', ax=[ax],\n            title=r'$F_\\mathrm{coef}^\\mathrm{IF}-F_\\mathrm{coef}$')\n    plt.ylim(ylim)\n    plt.tight_layout()\n    \nF_coef_IF = compute_if(X, Fs, N, H)\nF_coef = np.arange(K) * Fs / N\nF_coef_X = np.ones((K,L)) * F_coef.reshape(-1,1)\nplot_IF(F_coef_X, F_coef_IF, ylim)    \n\n\n\n\n\n이 시각화에서 행렬 \\(F_\\mathrm{coef}\\)와 \\(F_\\mathrm{coef}^\\mathrm{IF}\\)는 상당히 유사해 보인다. 그러나 차이를 볼 때 음표 \\(\\mathrm{C4}\\) (\\(261.6~\\mathrm{Hz}\\))의 기본 주파수와 그 고조파의 영역에서 주파수 값의 조정을 명확하게 볼 수 있다.\n고조파 바로 아래의 주파수 값은 위로 조정되고(빨간색) 고조파 바로 위의 주파수 값은 아래로 보정된다(파란색).\n다음 그림에서는 주파수 값에 대한 색상 코딩을 조정하면서 기본 주파수 \\(261.6~\\mathrm{Hz}\\) 부근을 확대한다. 이 시각화는 IF 추정 절차가 \\(261.6~\\mathrm{Hz}\\) 부근의 모든 주파수 계수를 정확히 해당 주파수에 할당함을 보여준다.\n\n\n# Compute a neighborhood around center frequency of C4 (p=60)\npitch = 60\nfreq_fund = 2 ** ((pitch - 69) / 12) * 440\nfreq_0 = freq_fund-150\nfreq_1 = freq_fund+150\nylim = [freq_0, freq_1]\n\nplot_IF(F_coef_X, F_coef_IF, ylim)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#홉-크기에-따라서",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#홉-크기에-따라서",
    "title": "9.2. 멜로디 추출",
    "section": "홉 크기에 따라서",
    "text": "홉 크기에 따라서\n\n추정된 순간 주파수의 품질은 다음의 두 연속된 프레임 사이의 길이에 따라 달라진다. \\[\\Delta t=t_2-t_1 = T_\\mathrm{coef}(n) - T_\\mathrm{coef}(n-1) = H/F_\\mathrm{s}\\]\n따라서 이산 STFT에 적용할 때 작은 홉 크기 \\(H\\)를 사용하는 것이 유리하다. 이는 다음 그림에 설명되어 있다. 단, 단점은 작은 홉 크기를 사용하면 이산 STFT를 계산하기 위한 계산 비용이 증가한다는 것이다.\n\n\nimport time\n\ndef compute_plot_IF(x, N, H):\n    \n    # Compute STFT\n    start = time.time()\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Y = np.log(1+ 10*np.abs(X))\n    end = time.time()\n    print('Runtime (STFT): %.8f seconds' % (end - start))    \n    K = X.shape[0]\n    L = X.shape[1]\n    \n    # Compute IF\n    start = time.time()\n    F_coef_IF = compute_if(X, Fs, N, H)\n    end = time.time()\n    print('Runtime (IF): %.8f seconds' % (end - start))    \n    \n    # Plot\n    F_coef = np.arange(K) * Fs / N\n    F_coef_X = np.ones((K,L)) * F_coef.reshape(-1,1)\n    plot_IF(F_coef_X, F_coef_IF, ylim) \n    plt.show()\n    \n    \nN, H = 2048, 64\nprint('Instanteneous frequency estimation using N=%d and H=%d'%(N,H))\ncompute_plot_IF(x, N, H)\n\nN, H = 2048, 256\nprint('Instanteneous frequency estimation using N=%d and H=%d'%(N,H))\ncompute_plot_IF(x, N, H)\n\nN, H = 2048, 1024\nprint('Instanteneous frequency estimation using N=%d and H=%d'%(N,H))\ncompute_plot_IF(x, N, H)\n\nInstanteneous frequency estimation using N=2048 and H=64\nRuntime (STFT): 0.03986549 seconds\nRuntime (IF): 0.15757895 seconds\n\n\n\n\n\nInstanteneous frequency estimation using N=2048 and H=256\nRuntime (STFT): 0.00999880 seconds\nRuntime (IF): 0.03590417 seconds\n\n\n\n\n\nInstanteneous frequency estimation using N=2048 and H=1024\nRuntime (STFT): 0.00299215 seconds\nRuntime (IF): 0.00797868 seconds"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#로그-주파수-스펙트로그램-경우",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#로그-주파수-스펙트로그램-경우",
    "title": "9.2. 멜로디 추출",
    "section": "로그-주파수 스펙트로그램 경우",
    "text": "로그-주파수 스펙트로그램 경우\n\n로그-주파수 스펙트로그램을 생각해보자. \\(x\\)는 \\(F_\\mathrm{s}\\) 레이트로 샘플링되고 윈도우 길이 \\(N\\in\\mathbb{N}\\) 및 홉 크기 \\(H\\in\\mathbb{N}\\)를 사용한 STFT \\(\\mathcal{X}\\) 오디오 신호를 나타낸다. 주파수 지수 \\(k\\in[0:N/2]\\)는 \\(F_\\mathrm{coef}(k) := \\frac{k\\cdot F_\\mathrm{s}}{N}\\) (헤르츠로 주어짐)에 해당한다.\n로그 주파수 스펙트로그램을 얻기 위한 한 가지 전략은 피치 매개변수 \\(p\\in[0:127]\\)에 대한 집합 \\(P(p) := \\{k:F_\\mathrm{MIDI}(p-0.5) \\leq F_\\mathrm{coef}(k) < F_\\mathrm{MIDI}(p+0.5)\\}\\)에 관한 STFT 계수를 풀링(pool)하거나 비닝(bin)하는 것이다.\n중심 주파수는 $ F_(p) = 2^{(p-69)/12} $로 주어진다.\n피치를 고정하고 결과 피치 밴드에 있는 모든 주파수를 찾는 대신, 주어진 주파수로 피치 인덱스를 할당하는 \\(\\mathrm{Bin}:\\mathbb{R}\\to\\mathbb{Z}\\) 매핑을 정의할 수도 있다.: \\[\\mathrm{Bin}(\\omega) := \\left\\lfloor 12\\cdot\\log_2\\left(\\frac{\\omega}{440}\\right)+69.5\\right\\rfloor.\\]\n이 함수를 이용하여 비닝(binning)을 다음과 같이 표현할 수 있다. \\[P(p) := \\{k: \\mathrm{Bin}(F_\\mathrm{coef}(k))=p\\}\\]\n여기에서 풀링을 통해 로그 주파수 스펙트로그램 \\(\\mathcal{Y}_\\mathrm{LF}:\\mathbb{Z}\\times [0:127]\\)를 얻는다. \\[\\mathcal{Y}_\\mathrm{LF}(n,p) := \\sum_{k \\in P(p)}{|\\mathcal{X}(n,k)|^2}\\]\n\n\ndef f_coef(k, Fs, N):\n    \"\"\"STFT center frequency\n    Args:\n        k (int): Coefficient number\n        Fs  (scalar): Sampling rate in Hz\n        N (int): Window length in samples\n    Returns:\n        freq (float): STFT center frequency\n    \"\"\"\n    return k * Fs / N\n\n\ndef frequency_to_bin_index(F, R=10.0, F_ref=55.0):\n    \"\"\"| Binning function with variable frequency resolution\n    | Note: Indexing starts with 0 (opposed to [FMP, Eq. (8.49)])\n    Args:\n        F (float): Frequency in Hz\n        R (float): Frequency resolution in cents (Default value = 10.0)\n        F_ref (float): Reference frequency in Hz (Default value = 55.0)\n    Returns:\n        bin_index (int): Index for bin (starting with index 0)\n    \"\"\"\n    bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n    return bin_index\n\n\ndef p_bin(b, freq, R=10.0, F_ref=55.0):\n    \"\"\"Computes binning mask [FMP, Eq. (8.50)]\n    Args:\n        b (int): Bin index\n        freq (float): Center frequency\n        R (float): Frequency resolution in cents (Default value = 10.0)\n        F_ref (float): Reference frequency in Hz (Default value = 55.0)\n    Returns:\n        mask (float): Binning mask\n    \"\"\"\n    mask = frequency_to_bin_index(freq, R, F_ref) == b\n    mask = mask.reshape(-1, 1)\n    return mask\n\n\ndef compute_y_lf_bin(Y, Fs, N, R=10.0, F_min=55.0, F_max=1760.0):\n    \"\"\"Log-frequency Spectrogram with variable frequency resolution using binning\n    Args:\n        Y (np.ndarray): Magnitude spectrogram\n        Fs (scalar): Sampling rate in Hz\n        N (int): Window length in samples\n        R (float): Frequency resolution in cents (Default value = 10.0)\n        F_min (float): Lower frequency bound (reference frequency) (Default value = 55.0)\n        F_max (float): Upper frequency bound (is included) (Default value = 1760.0)\n    Returns:\n        Y_LF_bin (np.ndarray): Binned log-frequency spectrogram\n        F_coef_hertz (np.ndarray): Frequency axis in Hz\n        F_coef_cents (np.ndarray): Frequency axis in cents\n    \"\"\"\n    # [FMP, Eq. (8.51)]\n    B = frequency_to_bin_index(np.array([F_max]), R, F_min)[0] + 1\n    F_coef_hertz = 2 ** (np.arange(0, B) * R / 1200) * F_min\n    F_coef_cents = np.arange(0, B*R, R)\n    Y_LF_bin = np.zeros((B, Y.shape[1]))\n\n    K = Y.shape[0]\n    freq = f_coef(np.arange(0, K), Fs, N)\n    freq_lim_idx = np.where(np.logical_and(freq >= F_min, freq <= F_max))[0]\n    freq_lim = freq[freq_lim_idx]\n    Y_lim = Y[freq_lim_idx, :]\n\n    for b in range(B):\n        coef_mask = p_bin(b, freq_lim, R, F_min)\n        Y_LF_bin[b, :] = (Y_lim*coef_mask).sum(axis=0)\n    return Y_LF_bin, F_coef_hertz, F_coef_cents\n\n\n다음 코드 셀은 Freischütz예의 로그-주파수 스펙토그램을 나타낸다.\n\n\nY_LF, F_coef_hertz, F_coef_cents = compute_y_lf_bin(Y, Fs, N, R=100, F_min=32.703, F_max=11025)  \n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)         \n\nplot_matrix(Y_LF, Fs=Fs/H, Fs_F=1, ax=[ax[0]], ylabel='Frequency (pitch)',\n                     title='LF-spectrogram (semitone resolution)', \n                     colorbar=True, cmap=cmap);\nax[0].set_ylim([33, 110])\nplot_matrix(Y_LF, Fs=Fs/H, Fs_F=1, ax=[ax[1]], ylabel='Frequency (pitch)',\n                     title='', colorbar=True, cmap=cmap);\nax[1].set_ylim(ylim_zoom_pitch)\nax[1].set_xlim(xlim_zoom)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#refined-binning",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#refined-binning",
    "title": "9.2. 멜로디 추출",
    "section": "Refined Binning",
    "text": "Refined Binning\n\n이제 보다 일반적인 빈 할당(bin assignment)을 고려하여 로그 주파수 스펙트로그램의 정의를 확장한다.\n이를 위해 \\(\\omega_\\mathrm{ref}\\in\\mathbb{R}\\)를 빈 인덱스 \\(1\\)에 할당할 참고 주파수라 하자. 또한, \\(R\\in\\mathbb{R}\\)(센트 단위)를 대수적(loagrithmically)으로 간격을 둔 주파수 축의 원하는 분해능(resolution)이라고 하자.\n그러면 주파수 \\(\\omega\\in\\mathbb{R}\\)(Hertz로 표시)에 대해 빈(bin) 인덱스 \\(\\mathrm{Bin}(\\omega)\\)는 다음과 같이 정의된다. \\[\\mathrm{Bin}(\\omega) := \\left\\lfloor \\frac{1200}{R} \\cdot\\log_2\\left(\\frac{\\omega}{\\omega_\\mathrm{ref}}\\right)+1.5\\right\\rfloor\\]\n예를 들어, \\(R=100\\)는 빈당 \\(100\\) 센트(1반음)의 해상도로 주파수 축의 세분화를 산출한다. \\(R=10\\)을 사용하면 각 빈이 \\(10\\) 센트(반음의 10분의 1)에 해당하도록 더 미세하게 세분화된다.\n빈 매핑 함수를 기반으로 위의 로그 주파수 스펙트로그램의 정의를 확장해보자. 참조 주파수 \\(\\omega_\\mathrm{ref}\\) 및 해상도 \\(R\\)을 고정하고 \\(B\\in\\mathbb{N}\\)를 고려할 빈의 수로 둔다. 각 빈 인덱스 \\(b\\in[1:B]\\)에 대해 다음의 집합을 정의한다. \\[P(b) := \\left\\{k: \\mathrm{Bin}\\left(F_\\mathrm{coef}{(k)}\\right)=b\\right\\}\\]\n또한 스펙트로그램 \\(\\mathcal{Y} = |\\mathcal{X}(n,k)|^2\\)(또는 이것의 로그 압축 버전)에서 시작하여, 각 프레임 인덱스 \\(n\\in\\mathbb{Z}\\)와 빈 인덱스 \\(b\\in[1:B]\\)에 대해 $ (n,b) := {k P(b)}{(n,k)}$를 설정한다.\n이 정제된 비닝 전략은 다음 코드 셀에서 구현된다. 다음 사항에 유의하자.\n\n위에서 계산한 로그 압축 스펙트로그램을 입력으로 사용한다.\n\\(\\omega_\\mathrm{min}=55~\\mathrm{Hz}\\)(피치 \\(p=33\\)에 해당)와 \\(\\omega_\\mathrm{max}=1760~\\mathrm{Hz}\\)(피치 \\(p=93\\)에 해당) 사이의 주파수만 고려된다. 이 범위는 5옥타브(\\(6000\\) 센트에 해당)를 포함한다.\n비닝 인덱스 \\(b\\in[1:B]\\)의 경우 파이썬 구현에서 인덱스 0으로 인덱싱을 시작하기 때문에, 주어진 알고리즘 설명과 관련하여 인덱스 이동을 -1로 이끈다.\n참조 주파수는 \\(\\omega_\\mathrm{ref}= \\omega_\\mathrm{min}\\)로 설정되고 능해도는 \\(R=50\\) 센트로 설정된다.\n로그 주파수 스펙트로그램의 시각화에서 주파수 축은 센트 단위로 지정된다(\\(0\\) 센트는 \\(\\omega_\\mathrm{ref}=55~\\mathrm{Hz}\\)에 해당됨).\n특히 낮은 주파수의 경우 STFT에 의해 도입된 선형 주파수 그리드로 인해 빈이 비어 있을 수 있다. 빈(empty) 빈(bin) 문제는 주파수 그리드 해상도를 높이거나 보간 기술을 사용하여 해결할 수 있다.\n\n\n\nR = 50\nF_min = 55.0\nF_max = 1760.0\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)    \n\nY_LF_bin, F_coef_hertz, F_coef_cents = compute_y_lf_bin(Y, Fs, N, R, F_min=F_min, F_max=F_max)\n\n\nplot_matrix(Y_LF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[0]], ylabel='Frequency (cents)',\n                     title='LF-spectrogram (R = 50 cents)', colorbar=True, cmap=cmap);\n\nplot_matrix(Y_LF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[1]], ylabel='Frequency (cents)',\n                     title='', colorbar=True, cmap=cmap);\n\nylim_zoom_cents = [3200, 5700] \nxlim_zoom = [0,1]\nax[1].set_xlim(xlim_zoom)\nax[1].set_ylim(ylim_zoom_cents)\nplt.tight_layout()"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#순간-주파수-사용",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#순간-주파수-사용",
    "title": "9.2. 멜로디 추출",
    "section": "순간 주파수 사용",
    "text": "순간 주파수 사용\n\n위에서 소개한 단순 비닝 접근 방식에서는 \\(\\mathcal{Y}\\)의 선형 간격 주파수 정보가 비선형, 로그 방식으로 확장된다. 이것은 특히 \\(\\mathcal{Y}_\\mathrm{LF}\\)의 아래쪽 부분에서 볼 수 있는 주파수 방향의 아티팩트(예: 가로 흰색 줄무늬)를 초래한다. 이제 순간 주파수를 사용하여 이 문제를 완화할 수 있는 방법에 대해 논의해본다.\n중심 주파수 \\(F_\\mathrm{coef}(k)\\)를 사용하는 대신, 집합 \\(P^\\mathrm{IF}(b,n) := \\left\\{k: \\mathrm{Bin}\\left(F_\\mathrm{coef}^\\mathrm{IF}(k, n)\\right)=b\\right\\}\\) (\\(b\\in[1:B]\\), \\(n\\in\\mathbb{Z}\\))을 정의하는 정제된(refined) 주파수 추정치 \\(F_\\mathrm{coef}^\\mathrm{IF}(k,n)\\)를 사용한다.\n이 새로운 빈 할당에서 다음을 설정하여 정제된 로그 주파수 스펙트로그램 \\(\\mathcal{Y}_\\mathrm{LF}^\\mathrm{IF}\\)를 도출한다. \\[\\mathcal{Y}_\\mathrm{LF}^\\mathrm{IF}(n,b) := \\sum_{k \\in P^\\mathrm{IF}(b,n)}{\\mathcal{Y} (n,k)}\\] (각 프레임 인덱스 \\(n\\in\\mathbb{Z}\\) 및 빈 인덱스 \\(b\\in[1:B]\\)에 대해)\n이 수정의 효과는 다음 코드 셀에 설명되어 있다. 순간 주파수를 사용하면 STFT의 선형 주파수 그리드가 초래한 일부 문제가 완화된다. 특히 시간-주파수 패턴은 스펙트럼 계수가 단일 고조파 소스(source)에 명확하게 할당될 수 있을 때 더 선명해진다.\n\n\ndef p_bin_if(b, F_coef_IF, R=10.0, F_ref=55.0):\n    \"\"\"Computes binning mask for instantaneous frequency binning [FMP, Eq. (8.52)]\n\n    Args:\n        b (int): Bin index\n        F_coef_IF (float): Instantaneous frequencies\n        R (float): Frequency resolution in cents (Default value = 10.0)\n        F_ref (float): Reference frequency in Hz (Default value = 55.0)\n\n    Returns:\n        mask (np.ndarray): Binning mask\n    \"\"\"\n    mask = frequency_to_bin_index(F_coef_IF, R, F_ref) == b\n    return mask\n\n\ndef compute_y_lf_if_bin(X, Fs, N, H, R=10, F_min=55.0, F_max=1760.0, gamma=0.0):\n    \"\"\"Binned Log-frequency Spectrogram with variable frequency resolution based on instantaneous frequency\n\n    Args:\n        X (np.ndarray): Complex spectrogram\n        Fs (scalar): Sampling rate in Hz\n        N (int): Window length in samples\n        H (int): Hopsize in samples\n        R (float): Frequency resolution in cents (Default value = 10)\n        F_min (float): Lower frequency bound (reference frequency) (Default value = 55.0)\n        F_max (float): Upper frequency bound (Default value = 1760.0)\n        gamma (float): Logarithmic compression factor (Default value = 0.0)\n\n    Returns:\n        Y_LF_IF_bin (np.ndarray): Binned log-frequency spectrogram using instantaneous frequency\n        F_coef_hertz (np.ndarray): Frequency axis in Hz\n        F_coef_cents (np.ndarray): Frequency axis in cents\n    \"\"\"\n    # Compute instantaneous frequencies\n    F_coef_IF = compute_if(X, Fs, N, H)\n    freq_lim_mask = np.logical_and(F_coef_IF >= F_min, F_coef_IF < F_max)\n    F_coef_IF = F_coef_IF * freq_lim_mask\n\n    # Initialize ouput array and compute frequency axis\n    B = frequency_to_bin_index(np.array([F_max]), R, F_min)[0] + 1\n    F_coef_hertz = 2 ** (np.arange(0, B) * R / 1200) * F_min\n    F_coef_cents = np.arange(0, B*R, R)\n    Y_LF_IF_bin = np.zeros((B, X.shape[1]))\n\n    # Magnitude binning\n    if gamma == 0:\n        Y = np.abs(X) ** 2\n    else:\n        Y = np.log(1 + np.float32(gamma)*np.abs(X))\n    for b in range(B):\n        coef_mask = p_bin_if(b, F_coef_IF, R, F_min)\n\n        Y_LF_IF_bin[b, :] = (Y * coef_mask).sum(axis=0)\n    return Y_LF_IF_bin, F_coef_hertz, F_coef_cents\n\n\nR = 50\nF_min = 55.0\nF_max = 1760.0\nY_LF_IF_bin, F_coef, F_coef_cents = compute_y_lf_if_bin(X, Fs, N, H, R=R, \n                                                        F_min=F_min, F_max=F_max, gamma=1)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)  \nplot_matrix(Y_LF_IF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[0]], ylabel='Frequency (cents)',\n                     title=r'IF-based LF-spectrogram ($R = %0.0f$ cents)'%R, colorbar=True, cmap=cmap);\n\nplot_matrix(Y_LF_IF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[1]], ylabel='Frequency (cents)',\n                     title='', colorbar=True, cmap=cmap);\nax[1].set_xlim(xlim_zoom)\nax[1].set_ylim(ylim_zoom_cents)\nplt.tight_layout()\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n\n\n\n\n\n\n다음 그림에서는 \\(R=10\\) 센트의 해상도를 사용하는 IF 기반 로그 주파수 스펙트로그램을 보여준다. 이 높은 주파수 해상도에서도 빈(empty) 주파수 빈으로 인한 아티팩트는 거의 보이지 않는다.\n\n\nR = 10\nY_LF_IF_bin, F_coef, F_coef_cents = compute_y_lf_if_bin(X, Fs, N, H, R=R, \n                                                        F_min=F_min, F_max=F_max, gamma=1)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize) \nplot_matrix(Y_LF_IF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[0]], ylabel='Frequency (cents)', \n                     title='IF-based LF-spectrogram ($R = %0.0f$ cents)'%R, colorbar=True, cmap=cmap);\n\nplot_matrix(Y_LF_IF_bin, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[1]], ylabel='Frequency (cents)', \n                     title='', colorbar=True, cmap=cmap);\nax[1].set_xlim(xlim_zoom)\nax[1].set_ylim(ylim_zoom_cents)\nplt.tight_layout()\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#고조파-합산-harmonic-summation",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#고조파-합산-harmonic-summation",
    "title": "9.2. 멜로디 추출",
    "section": "고조파 합산 (Harmonic Summation)",
    "text": "고조파 합산 (Harmonic Summation)\n\n음악 톤과 같은 소리 이벤트는 (대략) 기본 주파수의 정수배인 고조파와 함께 기본 주파수와 연관되어 있다. 따라서 녹음된 멜로디의 스펙트로그램 표현은 일반적으로 서로 위에 쌓인 주파수 궤적의 전체 계열을 나타낸다. 크로마 시간-주파수 패턴의 여러 모양을 활용하여 스펙트로그램 표현을 개선할 수 있다.\n방법은 고조파 합산(harmonic summation)이라고도 하는 기술로, 적절하게 가중된 합계를 형성하여 주파수와 해당 고조파를 공동으로 고려하는 것이다.\n\\(H\\in\\mathbb{N}\\)를 합계에서 고려할 고조파 수라고 하자. 그런 다음 스펙트로그램 표현 \\(\\mathcal{Y}\\)가 주어지면 다음을 설정하여 고조파 합 스펙트로그램 \\(\\tilde{\\mathcal{Y}}\\)를 정의한다. \\[\\tilde{\\mathcal{Y}}(n,k) := \\sum_{h=1}^{H} \\alpha^{h-1} \\cdot \\mathcal{Y}(n,k\\cdot h)\\] for \\(n,k\\in\\mathbb{Z}\\) (\\(\\mathcal{Y}\\)가 주파수 방향에서 적절하게 제로-패딩 되어 있다고 가정)\n합계에서 고조파는 가중 매개변수 \\(\\alpha \\in [0, 1]\\)를 사용하여 지수(exponentially)가중 될 수 있다.\n\n\ndef harmonic_summation(Y, num_harm=10, alpha=1.0):\n    \"\"\"Harmonic summation for spectrogram [FMP, Eq. (8.54)]\n\n    Args:\n        Y (np.ndarray): Magnitude spectrogram\n        num_harm (int): Number of harmonics (Default value = 10)\n        alpha (float): Weighting parameter (Default value = 1.0)\n\n    Returns:\n        Y_HS (np.ndarray): Spectrogram after harmonic summation\n    \"\"\"\n    Y_HS = np.zeros(Y.shape)\n    Y_zero_pad = np.vstack((Y, np.zeros((Y.shape[0]*num_harm, Y.shape[1]))))\n    K = Y.shape[0]\n    for k in range(K):\n        harm_idx = np.arange(1, num_harm+1)*(k)\n        weights = alpha ** (np.arange(1, num_harm+1) - 1).reshape(-1, 1)\n        Y_HS[k, :] = (Y_zero_pad[harm_idx, :] * weights).sum(axis=0)\n    return Y_HS\n\n\nY_HS = harmonic_summation(Y, num_harm=10)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)        \n\nplot_matrix(Y_HS, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0]], title='Spectrogram after harmonic summation', colorbar=True, cmap=cmap);\nax[0].set_ylim([0, 5000])\nplot_matrix(Y_HS, Fs=Fs/H, Fs_F=N/Fs, ax=[ax[1]], title='', colorbar=True, cmap=cmap);\nax[1].set_ylim(ylim_zoom)\nax[1].set_xlim(xlim_zoom)\n\nplt.tight_layout()"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#돌출-표현",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#돌출-표현",
    "title": "9.2. 멜로디 추출",
    "section": "돌출 표현",
    "text": "돌출 표현\n\n\\(\\mathcal{Y}_\\mathrm{LF}\\) 또는 \\(\\mathcal{Y}_\\mathrm{LF}^\\mathrm{IF}\\)와 같은 로그 주파수 스펙트로그램 표현에 유사한 구조를 적용할 수 있다.\n단, 이 경우 고조파 합산의 수정이 필요하다. 특히, 로그 주파수 영역에서 작업할 때 주파수와 그것의 고조파 간의 관계는 곱셈이 아니라 덧셈이다. 예를 들어, 로그 주파수 스펙트로그램 \\(\\mathcal{Y}_\\mathrm{LF}^\\mathrm{IF}\\)의 경우 고조파 합계 버전 \\(\\mathcal{Z}:=\\tilde{ \\mathcal{Y}}_\\mathrm{LF}^\\mathrm{IF}\\)를 다음과 같이 설정하여 얻는다. \\[\\mathcal{Z}(n,b) := \\sum_{h=1}^{H} \\alpha^{h-1} \\cdot \\mathcal{Y}_\\mathrm{LF}^\\mathrm{ IF}\\left(n,b+ \\left\\lfloor \\frac{1200}{R}\\log_2(h)\\right\\rfloor\\right)\\]\n톤 주파수 성분의 돌출성(salience)을 강조하여 \\(\\mathcal{Z}\\)는 돌출(salience) 표현이라고도 한다.\n다음 코드 셀은 로그 주파수 표현을 위한 고조파 합계를 구현한다.\n\n\ndef harmonic_summation_lf(Y_LF_bin, R, num_harm=10, alpha=1.0):\n    \"\"\"Harmonic summation for log-frequency spectrogram [FMP, Eq. (8.55)]\n\n    Args:\n        Y_LF_bin (np.ndarray): Log-frequency spectrogram\n        R (float): Frequency resolution in cents\n        num_harm (int): Number of harmonics (Default value = 10)\n        alpha (float): Weighting parameter (Default value = 1.0)\n\n    Returns:\n        Y_LF_bin_HS (np.ndarray): Log-frequency spectrogram after harmonic summation\n    \"\"\"\n    Y_LF_bin_HS = np.zeros(Y_LF_bin.shape)\n    pad_len = int(np.floor(np.log2(num_harm) * 1200 / R))\n    Y_LF_bin_zero_pad = np.vstack((Y_LF_bin, np.zeros((pad_len, Y_LF_bin.shape[1]))))\n    B = Y_LF_bin.shape[0]\n    for b in range(B):\n        harmonics = np.arange(1, num_harm+1)\n        harm_idx = b + np.floor(np.log2(harmonics) * 1200 / R).astype(np.int64)\n        weights = alpha ** (np.arange(1, num_harm+1) - 1).reshape(-1, 1)\n        Y_LF_bin_HS[b, :] = (Y_LF_bin_zero_pad[harm_idx, :] * weights).sum(axis=0)\n    return Y_LF_bin_HS\n\n\nY_LF_IF_bin_HS = harmonic_summation_lf(Y_LF_IF_bin, num_harm=10, R=R)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)   \n\nplot_matrix(Y_LF_IF_bin_HS, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[0]], ylabel='Frequency (cents)', \n                     title='IF-based LF-spectrogram after harmonic summation', colorbar=True, cmap=cmap);\n\nplot_matrix(Y_LF_IF_bin_HS, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[1]], ylabel='Frequency (cents)', \n                     title='', colorbar=True, cmap=cmap);\nax[1].set_xlim(xlim_zoom)\nax[1].set_ylim(ylim_zoom_cents)\nplt.tight_layout()\n\n\n\n\n\n시각화를 보면 고조파 합산의 효과가 크지 않은 것 처럼 보인다. 최종 결과에 상당한 영향을 미칠 수 있는 많은 매개 변수가 있다. 특히, IF 기반 선명화(sharpening)와 결합된 높은 주파수 분해능은 고조파 관련 빈에서 작은 편차로 인해 고조파 합산에 문제를 일으킬 수 있다.\n고조파 합산의 견고성을 높이는 간단한 방법은 주파수 축을 따라 스무딩(smoothing) 단계를 도입하는 것이다. 다음 그림에 표시된 대로 이 단계는 결과 돌출 표현에 상당한 영향을 미칠 수 있다.\n다음 코드 셀은 최종 돌출 표현을 계산하기 위한 구현을 제공하여 전체 절차를 요약한다.\n\n\ndef compute_salience_rep(x, Fs, N, H, R, F_min=55.0, F_max=1760.0, num_harm=10, freq_smooth_len=11, alpha=1.0,\n                         gamma=0.0):\n    \"\"\"Salience representation [FMP, Eq. (8.56)]\n\n    Args:\n        x (np.ndarray): Audio signal\n        Fs (scalar): Sampling frequency\n        N (int): Window length in samples\n        H (int): Hopsize in samples\n        R (float): Frequency resolution in cents\n        F_min (float): Lower frequency bound (reference frequency) (Default value = 55.0)\n        F_max (float): Upper frequency bound (Default value = 1760.0)\n        num_harm (int): Number of harmonics (Default value = 10)\n        freq_smooth_len (int): Filter length for vertical smoothing (Default value = 11)\n        alpha (float): Weighting parameter (Default value = 1.0)\n        gamma (float): Logarithmic compression factor (Default value = 0.0)\n\n    Returns:\n        Z (np.ndarray): Salience representation\n        F_coef_hertz (np.ndarray): Frequency axis in Hz\n        F_coef_cents (np.ndarray): Frequency axis in cents\n    \"\"\"\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\n    Y_LF_IF_bin, F_coef_hertz, F_coef_cents = compute_y_lf_if_bin(X, Fs, N, H, R, F_min, F_max, gamma=gamma)\n    # smoothing\n    Y_LF_IF_bin = ndimage.convolve1d(Y_LF_IF_bin, np.hanning(freq_smooth_len), axis=0, mode='constant')\n    Z = harmonic_summation_lf(Y_LF_IF_bin, R=R, num_harm=num_harm, alpha=alpha)\n    return Z, F_coef_hertz, F_coef_cents\n\n\nZ, F_coef_hertz, F_coef_cents = compute_salience_rep(x, Fs, N=1024, H=128, R=10, \n                                                     num_harm=10, freq_smooth_len=11, alpha=1, gamma=1)\n\nfig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [2, 1]}, figsize=figsize)   \n\nplot_matrix(Z, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[0]], ylabel='Frequency (cents)',\n                     title='Salience respresentation', colorbar=True, cmap=cmap);\n\nplot_matrix(Z, Fs=Fs/H, F_coef=F_coef_cents, ax=[ax[1]], ylabel='Frequency (cents)', \n                     title='', colorbar=True, cmap=cmap);\nax[1].set_xlim(xlim_zoom)\nax[1].set_ylim(ylim_zoom_cents)\nplt.tight_layout()\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n\n\n\n\n\n\n이 그림에서 볼 수 있듯이 합산 프로세스는 지배적인 톤 구성 요소를 증폭하는 반면 순간 주파수 추정에 의해 도입된 노이즈와 같은 아티팩트가 감쇠된다.\n단, 단점은 프로세스가 특히 스펙트로그램의 낮은 주파수 범위에 나타나는 고스트(ghost) 요소를 생성한다는 것이다(예: 오른쪽 플롯에서 주파수 \\(3500\\) 센트 주변의 비브라토 참조). 그러나 이것은 지배 주파수 궤적(predominant frequency trajectories)만을 찾는 경우에는 큰 문제가 되지 않는다.\n요약하면, 돌출(salience) 표현 \\(\\mathcal{Z}\\)의 구성은 몇 가지 가정과 관찰에 의해 동기가 부여된다.\n\n첫째, 로그 주파수 비닝은 주파수의 로그 인식과 음 피치의 음악적 개념을 설명한다.\n둘째, 순간 주파수를 사용하면 주파수 추정의 정확도가 향상된다.\n셋째, 고조파 합산은 규칙적인 간격의 주파수 성분을 증폭시킨다. 이것은 톤의 에너지가 기본 주파수에 포함되어 있을 뿐만 아니라 전체 고조파 스펙트럼에 퍼져 있다는 사실을 설명한다.\n\n게다가, 우리는 고조파 합산 이전의 추가 스무딩 단계가 최종 돌출 표현에서 상당한 개선을 가져올 수 있음을 확인했다.\n일반적으로 고해상도로 샘플링된 데이터에 로컬 연산을 적용할 때 데이터의 작은 편차나 이상값은 상당한 성능 저하로 이어질 수 있다. 이러한 상황에서 추가 필터링 단계(예: 가우스 커널 또는 중앙값 필터링을 사용한 컨볼루션)는 일부 문제를 완화하는 데 도움이 될 수 있다.\n위에서 본 경우 스무딩은 고조파 합산을 적용하기 전에 주파수 빈의 작은 편차의 균형을 맞추는 데 도움이 되었다. 노벨티 기반 경계 감지와 관련하여 유사한 전략을 적용했다. 여기서 노벨티 함수 \\(\\Delta_\\mathrm{Structure}\\)를 계산하기 위해서는 연속되는 특징들 간의 지역적 차이를 계산하기 전에 추가적인 필터링이 유리했다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#주파수-궤적-frequency-trajectory",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#주파수-궤적-frequency-trajectory",
    "title": "9.2. 멜로디 추출",
    "section": "주파수 궤적 (Frequency Trajectory)",
    "text": "주파수 궤적 (Frequency Trajectory)\n\n일반적으로 멜로디(melody)는 일관된 개체를 형성하고 특정 음악적 아이디어를 표현하는 음악 톤의 선형적 연속(succession)으로 정의될 수 있다. 음악 처리의 다른 많은 개념과 마찬가지로 멜로디의 개념은 다소 모호하다. 이 포스트에서는 음악이 오디오 녹음의 형태로 제공되는 시나리오(심볼릭 음악 표현이 아님)를 고려한다.\n또한 음의 순서를 추정하는 것이 아니라 음의 피치에 해당하는 주파수 값의 순서를 결정하는 것이 목표이다. 연속적인 주파수 글라이드(glides) 및 변조(modulation)를 캡처할 수 있는 시간 경과에 따른 이러한 주파수 경로를 주파수 궤적(trajectory) 이라고 한다. 특히 멜로디 음표의 기본 주파수 값(F0 값이라고도 함)에 관심이 있다. 그 결과의 궤적은 F0-궤적이라고도 한다.\n수학적으로 F0-궤적을 다음의 함수로 모델링한다. \\[\\eta:\\mathbb{R}\\to\\mathbb{R}\\cup\\{\\ast\\}\\]\n\n이는 각 시점 \\(t\\in\\mathbb{R}\\)(초 단위)에 주파수 값 \\(\\eta(t)\\in\\mathbb{R}\\)(Hertz 단위) 또는 기호 \\(\\eta(n)=\\ast\\)를 할당한다.\n\\(\\eta(t)=\\ast\\)은 이 시점에서 멜로디 성분에 해당하는 F0 값이 없다는 뜻이다.\n\n예시로 위에서 봤던 “Der Freischütz” - Carl Maria von Weber를 보자. 악보 표현에서 주요 멜로디는 가사와 함께 표시되어 있다. 소프라노 가수의 퍼포먼스에서 멜로디는 F0-값의 궤적에 해당한다. 표시된 기호적 표현과 반대로, 몇몇 음은 부드럽게 연결되어 있다. 더욱이 비브라토에 따른 뚜렷한 주파수 변조를 되려 관찰 할 수 있다.\n\n\nipd.display(Image(path_img+\"FMP_C8_F10a.png\", width=300))\nipd.display(Audio(path_data+\"FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40.wav\"))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n다음 그림에서는 가수의 F0-궤적(주석 파일에서 읽음)을 시각화한다. 하나는 선형 주파수 축(Hertz로 주어짐)과 로그 주파수 축(참고 주파수\\(\\omega_\\mathrm{ref}=55~\\mathrm{Hz}\\) 에 상대적인 센트로 주어짐)으로 한 번 표시된다.\n\n\ndef hz_to_cents(F, F_ref=55.0):\n    \"\"\"Converts frequency in Hz to cents\n\n    Args:\n        F (float or np.ndarray): Frequency value in Hz\n        F_ref (float): Reference frequency in Hz (Default value = 55.0)\n\n    Returns:\n        F_cent (float or np.ndarray): Frequency in cents\n    \"\"\"\n    F_cent = 1200 * np.log2(F / F_ref)\n    return F_cent\n\n\ndef cents_to_hz(F_cent, F_ref=55.0):\n    \"\"\"Converts frequency in cents to Hz\n\n    Args:\n        F_cent (float or np.ndarray): Frequency in cents\n        F_ref (float): Reference frequency in Hz (Default value = 55.0)\n\n    Returns:\n        F (float or np.ndarray): Frequency in Hz\n    \"\"\"\n    F = F_ref * 2 ** (F_cent / 1200)\n    return F\n\n\n# Load audio\nfn_wav = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40.wav'\n\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\nipd.Audio(x, rate=Fs)\n\n# Read in the F0 trajectory\nfn_traj = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40_F0-user-Book.csv'\ntraj_df = pd.read_csv(path_data+fn_traj,sep=';', keep_default_na=False, header=0)\ntraj = traj_df.values\n\nfig, ax = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1, 2, 2]}, figsize=(6,5))\nplot_signal(x, Fs, ax=ax[0], xlabel='');\nax[0].set_ylim((-1,1));\nax[0].set_ylabel('Amplitude')\nax[0].set_xlabel('Time (seconds)')\n\ntraj_plot = traj[traj[:, 1]>0]\nax[1].plot(traj_plot[:, 0], traj_plot[:, 1], color='r', markersize=4, marker='.', linestyle='');\nax[1].set_ylim((55,880));\nax[1].set_yticks([55, 220, 440, 660, 880])\nax[1].set_xlim((0, x_duration));\nax[1].set_ylabel('Frequency (Hertz)')\nax[1].set_xlabel('Time (seconds)')\nplt.tight_layout()\n\nax[2].plot(traj_plot[:, 0], hz_to_cents(traj_plot[:, 1]), color='r', markersize=4, marker='.', linestyle='');\nax[2].set_ylim((2400, 4800));\nax[2].set_yticks([2400, 3600, 4800])\nax[2].set_xlim((0, x_duration));\nplt.xlabel('Time (seconds)')\nax[2].set_ylabel('Frequency (Cents)')\nplt.tight_layout()\n\n\n\n\n\nSonification\n\ndef sonify_trajectory_with_sinusoid(traj, audio_len, Fs=22050, amplitude=0.3, smooth_len=11):\n    \"\"\"Sonification of trajectory with sinusoidal\n\n    Args:\n        traj (np.ndarray): F0 trajectory (time in seconds, frequency in Hz)\n        audio_len (int): Desired audio length in samples\n        Fs (scalar): Sampling rate (Default value = 22050)\n        amplitude (float): Amplitude (Default value = 0.3)\n        smooth_len (int): Length of amplitude smoothing filter (Default value = 11)\n\n    Returns:\n        x_soni (np.ndarray): Sonification\n    \"\"\"\n    # unit confidence if not specified\n    if traj.shape[1] < 3:\n        confidence = np.zeros(traj.shape[0])\n        confidence[traj[:, 1] > 0] = amplitude\n    else:\n        confidence = traj[:, 2]\n\n    # initialize\n    x_soni = np.zeros(audio_len)\n    amplitude_mod = np.zeros(audio_len)\n\n    # Computation of hop size\n    # sine_len = int(2 ** np.round(np.log(traj[1, 0]*Fs) / np.log(2)))\n    sine_len = int(traj[1, 0] * Fs)\n\n    t = np.arange(0, sine_len) / Fs\n    phase = 0\n\n    # loop over all F0 values, insure continuous phase\n    for idx in np.arange(0, traj.shape[0]):\n        cur_f = traj[idx, 1]\n        cur_amp = confidence[idx]\n\n        if cur_f == 0:\n            phase = 0\n            continue\n\n        cur_soni = np.sin(2*np.pi*(cur_f*t+phase))\n        diff = np.maximum(0, (idx+1)*sine_len - len(x_soni))\n        if diff > 0:\n            x_soni[idx * sine_len:(idx + 1) * sine_len - diff] = cur_soni[:-diff]\n            amplitude_mod[idx * sine_len:(idx + 1) * sine_len - diff] = cur_amp\n        else:\n            x_soni[idx*sine_len:(idx+1)*sine_len-diff] = cur_soni\n            amplitude_mod[idx*sine_len:(idx+1)*sine_len-diff] = cur_amp\n\n        phase += cur_f * sine_len / Fs\n        phase -= 2 * np.round(phase/2)\n\n    # filter amplitudes to avoid transients\n    amplitude_mod = np.convolve(amplitude_mod, np.hanning(smooth_len)/np.sum(np.hanning(smooth_len)), 'same')\n    x_soni = x_soni * amplitude_mod\n    return x_soni\n\n\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11, amplitude=0.6)\n\n# left: x, right: sonification\nx_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))  \n\nprint('F0 sonification (mono)')\nipd.display(Audio(x_traj_mono, rate=Fs))\nprint('F0 sonification superimposed with original recording (mono)')\nipd.display(Audio( (x_traj_mono+x)/2, rate=Fs))\n#print('F0 sonification (right channel), original recording (left channel)')\n#ipd.display(Audio(x_traj_stereo, rate=Fs))\n\nF0 sonification (mono)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nF0 sonification superimposed with original recording (mono)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#돌출성-기반-주파수-추적",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#돌출성-기반-주파수-추적",
    "title": "9.2. 멜로디 추출",
    "section": "돌출성 기반 주파수 추적",
    "text": "돌출성 기반 주파수 추적\n\n종종 메인 멜로디는 주어진 오디오 신호에서 가장 강한 하모닉 소스(harmonic source)라는 의미에서 지배적(dominant)이다.\n리드 싱어나 리드 악기가 주로 멜로디를 연주하는 음악으로 제한해서 보기로 한다. 특히 하나의 음원에 연관될 수 있는 멜로디 라인은 한 번에 하나뿐이라고 가정한다.\n이 시나리오에서 메인 멜로디를 추출하기 위한 간단한 방법은 각 시간 프레임에서 가장 강한 고조파 주파수 성분을 찾는 것이다. 돌출(salience) 표현을 설계하는 목적은 강한 고조파 성분을 향상시키는 것이었다. 따라서 멜로디 추출을 위한 첫 번째 전략은 돌출 표현의 프레임별 최대값을 고려하는 것이다.\n다음에서 \\(\\mathcal{Z}\\)는 \\(\\mathcal{Z}(n,b)\\in\\mathbb{R}\\) 항목을 포함하는 돌출 표현을 나타낸다고 하자. 여기서 \\(n\\in[1:N]\\) 프레임 인덱스를 나타내고, \\(b\\in[1:B]\\)는 빈(bin) 인덱스를 나타낸다.\n\n\ndef visualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=55.0, colorbar=True, cmap='gray_r',\n                                        figsize=(7, 4), traj=None, constraint_region=None, ax=None):\n    \"\"\"Visualize salience representation with optional F0-trajectory and constraint regions\n\n    Args:\n        Z: Salience representation\n        T_coef: Time axis\n        F_coef_cents: Frequency axis in cents\n        F_ref: Reference frequency (Default value = 55.0)\n        colorbar: Show or hide colorbar (Default value = True)\n        cmap: Color map (Default value = 'gray_r')\n        figsize: Figure size (Default value = (7, 4))\n        traj: F0 trajectory (time in seconds, frequency in Hz) (Default value = None)\n        constraint_region: Constraint regions, row-format: (t_start_sec, t_end_sec, f_start_hz, f_end,hz)\n            (Default value = None)\n        ax: Handle to existing axis (Default value = None)\n\n    Returns:\n        fig: Handle to figure\n        ax: Handle to cent axis\n        ax_f: Handle to frequency axis\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    sal = ax.imshow(Z, extent=[T_coef[0], T_coef[-1], F_coef_cents[0], F_coef_cents[-1]],\n                    cmap=cmap, origin='lower', aspect='auto', interpolation='nearest')\n\n    y_ticklabels_left = np.arange(F_coef_cents[0], F_coef_cents[-1]+1, 1200)\n    ax.set_yticks(y_ticklabels_left)\n    ax.set_yticklabels(y_ticklabels_left)\n    ax.set_ylabel('Frequency (Cents)')\n\n    if colorbar:\n        plt.colorbar(sal, ax=ax, pad=0.1)\n\n    ax_f = ax.twinx()  # instantiate a second axes that shares the same y-axis\n    ax_f.set_yticks(y_ticklabels_left - F_coef_cents[0])\n    y_ticklabels_right = cents_to_hz(y_ticklabels_left, F_ref).astype(int)\n    ax_f.set_yticklabels(y_ticklabels_right)\n    ax_f.set_ylabel('Frequency (Hz)')\n\n    # plot contour\n    if traj is not None:\n        traj_plot = traj[traj[:, 1] > 0, :]\n        traj_plot[:, 1] = hz_to_cents(traj_plot[:, 1], F_ref)\n        ax.plot(traj_plot[:, 0], traj_plot[:, 1], color='r', markersize=4, marker='.', linestyle='')\n\n    # plot constraint regions\n    if constraint_region is not None:\n        for row in constraint_region:\n            t_start = row[0]  # sec\n            t_end = row[1]  # sec\n            f_start = row[2]  # Hz\n            f_end = row[3]  # Hz\n            ax.add_patch(matplotlib.patches.Rectangle((\n                t_start, hz_to_cents(f_start, F_ref)), width=t_end-t_start,\n                height=hz_to_cents(f_end, F_ref)-hz_to_cents(f_start, F_ref),\n                fill=False, edgecolor='k', linewidth=3, zorder=2))\n\n    ax.set_xlabel('Time (seconds)')\n\n    if fig is not None:\n        plt.tight_layout()\n\n    return fig, ax, ax_f\n\n\n# Computation of salience representation\nN = 1024\nH = 128\nR = 10\nF_min = 55.0\nF_max = 1760.0\nnum_harm = 8\nfreq_smooth_len = 11\nalpha = 0.9\ngamma = 0.1\n\nZ, F_coef_hertz, F_coef_cents = compute_salience_rep(x, Fs, N=N, H=H, R=R, \n                    F_min=F_min, F_max=F_max, num_harm=num_harm, \n                    freq_smooth_len=freq_smooth_len, alpha=alpha, gamma=gamma)\nT_coef = (np.arange(Z.shape[1]) * H) / Fs\n\n# Visualization\nfigsize = (10,3)\ncmap = compressed_gray_cmap(alpha=5)\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_min, figsize=figsize, cmap=cmap, \n                                    colorbar=True, ax=None)\nplt.show()\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n\n\n\n\n\n\n이산(discrete) 설정에서 주파수 궤적은 각 프레임 인덱스 \\(n\\in[1:N]\\) 에 할당되는 \\(\\eta:[1:N]\\to[1:B]\\cup\\{\\ast\\}\\) 함수로 정의된다. 이 때 각 프레임 인덱스는 빈 인덱스 \\(\\eta(n)\\in[1:B]\\) 또는 기호 \\(\\eta(n)=\\ast\\)이다.\n각 프레임에 대해 \\(\\mathcal{Z}\\)의 빈 인덱스를 최대화하면, \\(\\eta^\\mathrm{max}(n):= \\underset{b\\in[1:B]}{\\mathrm{argmax}}\\,\\, \\mathcal{Z}(n,b)\\)로 정의된 주파수 궤적 \\(\\eta^\\mathrm{max}\\)를 얻는다.\n다음 그림은 간단한 F0 궤적 추적 절차를 보여준다. 다수의 이상값과 시간적 불연속성을 관찰할 수 있다. 이 방법은 멜로디가 실제로 존재하는지 여부에 관계없이 각 시간 프레임에 F0 값을 할당한다.\n\n\n# Frequency trajectory via maximization\nindex_max = np.argmax(Z, axis=0)\ntraj = np.hstack((T_coef.reshape(-1, 1), F_coef_hertz[index_max].reshape(-1, 1)))\n\n# Visualization\nfigsize = (10,3)\ncmap = compressed_gray_cmap(alpha=5)\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_min, figsize=figsize, cmap=cmap, \n                                    colorbar=True, traj=traj, ax=None)\nplt.show()\n\n# Sonification\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11)\nx_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))  \nipd.display(Audio(x_traj_mono, rate=Fs))\n#print('F0 sonification (right channel), original recording (left channel)')\n#ipd.display(Audio(x_traj_stereo, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#연속성-제약-continuity-constraints",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#연속성-제약-continuity-constraints",
    "title": "9.2. 멜로디 추출",
    "section": "연속성 제약 (Continuity Constraints)",
    "text": "연속성 제약 (Continuity Constraints)\n\n시간적 맥락을 고려하지 않고 순전히 프레임 방식으로 주파수 궤적을 계산하면 로컬 불연속성 및 무작위 점프가 발생할 수 있다. 실제로 멜로디의 주파수 궤적은 훨씬 더 구조화되어 있다. 일반적으로 음 전환 사이에 간헐적으로 점프하면서 시간이 지남에 따라 천천히 변경된다.\n이제 시간적 유연성(temporal flexibility)(가능한 점프를 설명하기 위해)과 시간적 연속성(temporal continuity)(스무스한(smoothness) 속성을 설명하기 위해)이라는 두 가지 상충되는 조건의 균형을 맞출 수 있는 절차에 대해 논의한다.\n\\(\\mathcal{Z}\\)를 \\(N\\) 시간 프레임과 \\(B\\) 주파수 빈으로 구성된 돌출(salience) 표현이라고 하자. \\(\\mathcal{Z}(n,b)\\) 값은 시간 \\(n\\in[1:N]\\)에 빈 \\(b\\in[1:B]\\)과 관련된 주파수가 지배적인 음조에 속할 가능도(likelihood)를 나타내는 점수로 간주될 수 있다\n또한, \\(T\\in\\mathbb{R}^{B\\times B}\\)를 전이(transition) 행렬이라고 하자. 여기서 항목 \\(T(b,c)\\in\\mathbb{R}\\)가 시간 프레임 \\(n\\in[1:N-1]\\)의 빈 \\(b\\in[1:B]\\)에서 시간 프레임 \\(n\\in[1:N-1]\\)의 빈 \\(c\\in[1:B]\\)로 이동할 가능도를 나타낸다.\n이러한 전이 행렬을 정의하는 방법에는 여러 가지가 있다. \\(T\\)가 가까운 빈(예: \\(|b-c|\\)가 작을 때 \\(\\mathbf{T}(b,c)\\approx 1\\)) 사이의 전이를 선호하고 멀리 떨어진 빈 사이의 전이에는 페널티를 준다. (예: \\(|b-c|\\)가 클 때 \\(\\mathbf{T}(b,c)\\approx \\delta\\) , 작은 양수값 \\(\\delta>0\\)). 예를 들어, 큰 편차에는 무관심하면서 \\(\\pm 50\\) 센트까지의 작은 변동은 선호할 수 있다.\n다음 코드 셀에서 이러한 요구 사항을 충족하는 전이 행렬 \\(T\\)를 정의한다.\n\n\ndef define_transition_matrix(B, tol=0, score_low=0.01, score_high=1.0):\n    \"\"\"Generate transition matrix\n\n    Args:\n        B (int): Number of bins\n        tol (int): Tolerance parameter for transition matrix (Default value = 0)\n        score_low (float): Score (low) for transition matrix (Default value = 0.01)\n        score_high (float): Score (high) for transition matrix (Default value = 1.0)\n\n    Returns:\n        T (np.ndarray): Transition matrix\n    \"\"\"\n    col = np.ones((B,)) * score_low\n    col[0:tol+1] = np.ones((tol+1, )) * score_high\n    T = scipy.linalg.toeplitz(col)\n    return T\n\n\n# Compute transition matrix\nT = define_transition_matrix(B=51, tol=5)\n\n# Plot\ncolorList = np.array([[1,0.9,0.9,1],[0,0,0,1]])\ncmap = ListedColormap(colorList) \nplt.figure(figsize=(4,3))\nim = plt.imshow(T, origin='lower', cmap=cmap)\nplt.title('Transition matrix')\nplt.xlabel('Bin index')\nplt.ylabel('Bin index')\nax_cb = plt.colorbar(im)\nax_cb.set_ticks(np.array([0.25, 0.75]))\nax_cb.set_ticklabels(np.array([np.min(T), np.max(T)])) \n\n\n\n\n\n돌출 표현 \\(\\mathcal{Z}\\)와 전이 우도(transition likelihood) 행렬 \\(\\mathbf{T}\\)가 주어지면 각 궤적 \\(\\eta:[1:N]\\to[1:B]\\)에 총점(total score) \\(\\sigma(\\eta)\\)을 연결할 수 있다. \\[\\sigma(\\eta) := \\mathcal{Z}(1,\\eta(1)) \\cdot \\prod_{n=2}^{N} \\Big(\\mathbf{T}(\\eta(n- 1),\\eta(n))\\cdot \\mathcal{Z}(n,\\eta(n))\\Big)\\]\n원하는 궤적 \\(\\eta^\\mathrm{DP}\\)는 점수 최대화 궤적으로 정의된다. \\[\\eta^\\mathrm{DP} := \\underset{\\eta}{\\mathrm{argmax}} \\,\\, \\sigma(\\eta)\\]\n동적 프로그래밍(Viterbi 알고리즘과 유사)을 통해 최대 총점을 계산할 수 있다. 또한 적절한 역추적 절차를 적용하여 궤적 \\(\\eta^\\mathrm{DP}\\)를 얻을 수 있다.\n많은 요소의 곱을 계산할 때 수치적 불안정성(예: 곱이 0에 접근하는 언더플로(underflow) 문제)에 자주 직면하게 된다. 따라서 실제로는 곱이 합이 되는 로그 도메인에서 최적화 접근 방식을 구현해야 한다. 이러한 접근 방식은 다음 구현에서 사용된다.\n\n\ndef compute_trajectory_dp(Z, T):\n    \"\"\"Trajectory tracking using dynamic programming\n\n    Args:\n        Z: Salience representation\n        T: Transisition matrix\n\n    Returns:\n        eta_DP (np.ndarray): Trajectory indices\n    \"\"\"\n    B, N = Z.shape\n    eps_machine = np.finfo(np.float32).eps\n    Z_log = np.log(Z + eps_machine)\n    T_log = np.log(T + eps_machine)\n\n    E = np.zeros((B, N))\n    D = np.zeros((B, N))\n    D[:, 0] = Z_log[:, 0]\n\n    for n in np.arange(1, N):\n        for b in np.arange(0, B):\n            D[b, n] = np.max(T_log[b, :] + D[:, n-1]) + Z_log[b, n]\n            E[b, n-1] = np.argmax(T_log[b, :] + D[:, n-1])\n\n    # backtracking\n    eta_DP = np.zeros(N)\n    eta_DP[N-1] = int(np.argmax(D[:, N-1]))\n\n    for n in np.arange(N-2, -1, -1):\n        eta_DP[n] = E[int(eta_DP[n+1]), n]\n\n    return eta_DP.astype(np.int64)\n\n\n# Frequency trajectory via DP-based optimization\nB, N = Z.shape\nT = define_transition_matrix(B, tol=5)\nindex_DP =  compute_trajectory_dp(Z, T)\n\ntraj = np.hstack((T_coef.reshape(-1, 1), F_coef_hertz[index_DP].reshape(-1, 1)))\n\n# Visualization\nfigsize = (10,3)\ncmap = compressed_gray_cmap(alpha=10)\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_min, \n                                    figsize=figsize, cmap=cmap, \n                                    colorbar=True, traj=traj, ax=None)\nplt.show()\n\n# Sonification\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11)\nx_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))\nipd.display(Audio(x_traj_mono, rate=Fs))\n#print('F0 sonification (right channel), original recording (left channel)')\n#ipd.display(Audio(x_traj_stereo, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n그림은 Freischütz 예의 결과를 보여준다. 단순힌 최대화 접근 방식과 비교해서, 전이 우도에 의해 도입된 연속성 제약 덕분에 대부분의 이상값이 제거되었다. 단, 반주가 멜로디보다 강한 조성 성분을 나타낼 경우 여전히 트래킹 오류가 발생한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#악보-정보-제약-score-informed-constraints",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#악보-정보-제약-score-informed-constraints",
    "title": "9.2. 멜로디 추출",
    "section": "악보 정보 제약 (Score-Informed Constraints)",
    "text": "악보 정보 제약 (Score-Informed Constraints)\n\n연속성 제약을 통합하는 것 외에 또 다른 전략은 F0-추적 프로세스를 지원하기 위해 멜로디 진행에 대한 추가 음악적 지식을 활용하는 것이다. 예를 들어, 가수의 음역(소프라노, 알토, 테너, 베이스)을 알면 예상되는 F0 값의 검색 범위를 좁힐 수 있다. 또는 멜로디가 실제로 존재하는 경우와 존재하지 않는 경우에 대한 정보를 가지고 비멜로디 프레임에 대해 모든 F0 값을 ‘\\(\\ast\\)’ 기호로 설정할 수 있다.\n보다 일반적으로, 위에서 설명한 추가 지식을 사용하여 시간-주파수 평면 내의 제약 영역(constraint regions)을 정의할 수 있다. 그런 다음 이러한 지정된 영역에서만 F0 추적이 수행된다.\n이러한 접근 방식의 예로, 주어진 음악 녹음의 기초가 되는 악보 표현의 가용성을 가정하는 악보 정보 절차(score-informed procedure)에 대해 논의하자. 악보에서 제공하는 추가 악기 및 음표 정보는 멜로디의 F0 궤적 추정을 도울 수 있다.\n\n첫 번째 단계에서 악보와 주어진 오디오 녹음을 정렬해야 한다. 이는 수동적인 주석 프로세스에서 수행되거나 음악 동기화 기술을 사용하여 (반)자동으로 수행될 수 있다.\n그런 다음 정렬된 악보 정보를 사용하여 멜로디에서 발생하는 음에 대한 시간-주파수 평면의 직사각형 제약 영역을 정의할 수 있다. 이러한 각 영역의 수평 위치와 너비는 음표의 물리적 시작 시간과 예상 지속 시간에 해당하는 반면, 수직 위치와 높이는 음표의 피치와 중심 주파수에서 예상되는 주파수 편차를 설명한다.\n마지막으로, 각각의 제한 영역에 대해 위의 절차를 적용하여 음표별(notewise) 주파수 궤적을 계산할 수 있다. 음표의 제약 영역의 시간적 중첩이 없다고 가정하면, 악보 정보가 포함된 단일 궤적 \\(\\eta^\\mathrm{Score}\\)를 형성하기 위해 음표 궤적을 조합할 수 있다. \\(n\\in[1:N]\\) 프레임이 제약 영역에 포함되지 않는 경우 \\(\\eta^\\mathrm{Score}(n):=\\ast\\)를 설정한다.\n\n다음 코드 셀에서 악보 정보 절차의 구현을 볼 수 있다.\n\n제약 영역을 정의하는 데 사용되는 정렬된 악보 정보는 주석 파일로부터 읽는다.\n악보 정보를 제약 영역으로 변환할 때 중심 주파수 주변의 주파수 방향(센트 단위로 지정)에 대한 허용 오차를 사용한다.\n값 ‘\\(\\ast\\)’(소위 무성 프레임을 나타냄)는 빈 인덱스 -1을 사용하여 인코딩된다.\n\n\n\ndef convert_ann_to_constraint_region(ann, tol_freq_cents=300.0):\n    \"\"\"Convert score annotations to constraint regions\n\n    Args:\n        ann (list): Score annotations [[start_time, end_time, MIDI_pitch], ...\n        tol_freq_cents (float): Tolerance in pitch directions specified in cents (Default value = 300.0)\n\n    Returns:\n        constraint_region (np.ndarray): Constraint regions\n    \"\"\"\n    tol_pitch = tol_freq_cents / 100\n    freq_lower = 2 ** ((ann[:, 2] - tol_pitch - 69)/12) * 440\n    freq_upper = 2 ** ((ann[:, 2] + tol_pitch - 69)/12) * 440\n    constraint_region = np.concatenate((ann[:, 0:2],\n                                        freq_lower.reshape(-1, 1),\n                                        freq_upper.reshape(-1, 1)), axis=1)\n    return constraint_region\n\n\ndef compute_trajectory_cr(Z, T_coef, F_coef_hertz, constraint_region=None,\n                          tol=5, score_low=0.01, score_high=1.0):\n    \"\"\"Trajectory tracking with constraint regions\n\n    Args:\n        Z (np.ndarray): Salience representation\n        T_coef (np.ndarray): Time axis\n        F_coef_hertz (np.ndarray): Frequency axis in Hz\n        constraint_region (np.ndarray): Constraint regions, row-format: (t_start_sec, t_end_sec, f_start_hz, f_end_hz)\n            (Default value = None)\n        tol (int): Tolerance parameter for transition matrix (Default value = 5)\n        score_low (float): Score (low) for transition matrix (Default value = 0.01)\n        score_high (float): Score (high) for transition matrix (Default value = 1.0)\n\n    Returns:\n        eta (np.ndarray): Trajectory indices, unvoiced frames are indicated with -1\n    \"\"\"\n    # do tracking within every constraint region\n    if constraint_region is not None:\n        # initialize contour, unvoiced frames are indicated with -1\n        eta = np.full(len(T_coef), -1)\n\n        for row_idx in range(constraint_region.shape[0]):\n            t_start = constraint_region[row_idx, 0]  # sec\n            t_end = constraint_region[row_idx, 1]  # sec\n            f_start = constraint_region[row_idx, 2]  # Hz\n            f_end = constraint_region[row_idx, 3]  # Hz\n\n            # convert start/end values to indices\n            t_start_idx = np.argmin(np.abs(T_coef - t_start))\n            t_end_idx = np.argmin(np.abs(T_coef - t_end))\n            f_start_idx = np.argmin(np.abs(F_coef_hertz - f_start))\n            f_end_idx = np.argmin(np.abs(F_coef_hertz - f_end))\n\n            # track in salience part\n            cur_Z = Z[f_start_idx:f_end_idx+1, t_start_idx:t_end_idx+1]\n            T = define_transition_matrix(cur_Z.shape[0], tol=tol,\n                                         score_low=score_low, score_high=score_high)\n            cur_eta = compute_trajectory_dp(cur_Z, T)\n\n            # fill contour\n            eta[t_start_idx:t_end_idx+1] = f_start_idx + cur_eta\n    else:\n        T = define_transition_matrix(Z.shape[0], tol=tol, score_low=score_low, score_high=score_high)\n        eta = compute_trajectory_dp(Z, T)\n\n    return eta\n\n\n# Read score annotations and convert to constraint regions\n\nfn_score = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40_CR-score.csv'\nann_score_df = pd.read_csv(path_data+fn_score, sep=';', keep_default_na=False, header=0)\nprint('Score annotations:')\nipd.display(ann_score_df)\nann_score = ann_score_df.values\nconstraint_region = convert_ann_to_constraint_region(ann_score, tol_freq_cents=300)\nprint('Constraint regions:')\nprint(constraint_region)\n\nScore annotations:\n\n\n\n\n\n\n  \n    \n      \n      start_time\n      end_time\n      MIDI_pitch\n    \n  \n  \n    \n      0\n      0.0\n      0.9\n      76\n    \n    \n      1\n      0.9\n      1.6\n      68\n    \n    \n      2\n      1.6\n      1.9\n      68\n    \n    \n      3\n      1.9\n      2.6\n      69\n    \n    \n      4\n      2.6\n      3.1\n      71\n    \n    \n      5\n      3.1\n      3.4\n      73\n    \n    \n      6\n      3.4\n      4.2\n      71\n    \n    \n      7\n      4.2\n      4.8\n      71\n    \n  \n\n\n\n\nConstraint regions:\n[[  0.           0.9        554.36526195 783.99087196]\n [  0.9          1.6        349.22823143 493.88330126]\n [  1.6          1.9        349.22823143 493.88330126]\n [  1.9          2.6        369.99442271 523.2511306 ]\n [  2.6          3.1        415.30469758 587.32953583]\n [  3.1          3.4        466.16376152 659.25511383]\n [  3.4          4.2        415.30469758 587.32953583]\n [  4.2          4.8        415.30469758 587.32953583]]\n\n\n\n# Frequency trajectory using constraint regions\nindex_CR = compute_trajectory_cr(Z, T_coef, F_coef_hertz, constraint_region, tol=5)\ntraj = np.hstack((T_coef.reshape(-1, 1), F_coef_hertz[index_CR].reshape(-1, 1)))\ntraj[index_CR==-1, 1] = 0  # set unvoiced frames to zero\n\n# Visualization\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_min, figsize=figsize, cmap=cmap, \n                                    constraint_region=constraint_region, colorbar=True, traj=traj, ax=None)\nplt.show()\n\n# Sonification\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11)\nx_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))  \nipd.display(Audio(x_traj_mono, rate=Fs))\n#print('F0 sonification (right channel), original recording (left channel)')\n#ipd.display(Audio(x_traj_stereo, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n악보-정보 제약 영역이 정확하지 않은 경우(예: 동기화 부정확성 또는 악보와 오디오 녹음 간의 편차로 인해) 절차에 문제가 발생하여 주파수 궤적이 손상될 수 있다. 이러한 오류는 제약 영역을 적절하게 조정하여 수정할 수 있다.\n다음 예는 결과 F0 궤적과 함께 사용자-최적화(user-optimzied) 제약 영역을 보여준다.\n\n\n# Read constraint regions\nfn_region = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40_CR-user.csv'\nconstraint_region_df = pd.read_csv(path_data+fn_region, sep=';', keep_default_na=False, header=0)\nconstraint_region = constraint_region_df.values\n\nindex_CR = compute_trajectory_cr(Z, T_coef, F_coef_hertz, constraint_region, tol=5)\ntraj = np.hstack((T_coef.reshape(-1, 1), F_coef_hertz[index_CR].reshape(-1, 1)))\ntraj[index_CR==-1, 1] = 0  # set unvoiced frames to zero\n\n# Visualization\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_min, \n                                    figsize=figsize, cmap=cmap, \n                                    constraint_region=constraint_region, \n                                    colorbar=True, traj=traj, ax=None)\nplt.show()\n\n# Sonification\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11)\nx_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))  \nipd.display(ipd.Audio(x_traj_mono, rate=Fs))\n#print('F0 sonification (right channel), original recording (left channel)')\n#ipd.display(ipd.Audio(x_traj_stereo, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체-과정-예시",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체-과정-예시",
    "title": "9.2. 멜로디 추출",
    "section": "전체 과정: 예시",
    "text": "전체 과정: 예시\n\nF0-궤적의 (제약-기반) 전체 계산 절차를 보자.\n예로 Sven Bornemark의 노래 “Stop Messing With Me”의 발췌 부분에 적용한다. 발췌 부분에서 남성 가수가 드럼, 피아노, 기타 반주로 멜로디를 연주한다. 언제 가수가 노래하고, 노래하는 피치의 범위가 어떻게 되는지 인코딩하는 제약 영역이 이미 주어져있는 경우를 고려한다.\n\n\ndef compute_traj_from_audio(x, Fs=22050, N=1024, H=128, R=10.0, F_min=55.0, F_max=1760.0,\n                            num_harm=10, freq_smooth_len=11, alpha=0.9, gamma=0.0,\n                            constraint_region=None, tol=5, score_low=0.01, score_high=1.0):\n    \"\"\"Compute F0 contour from audio signal\n\n    Args:\n        x (np.ndarray): Audio signal\n        Fs (scalar): Sampling frequency (Default value = 22050)\n        N (int): Window length in samples (Default value = 1024)\n        H (int): Hopsize in samples (Default value = 128)\n        R (float): Frequency resolution in cents (Default value = 10.0)\n        F_min (float): Lower frequency bound (reference frequency) (Default value = 55.0)\n        F_max (float): Upper frequency bound (Default value = 1760.0)\n        num_harm (int): Number of harmonics (Default value = 10)\n        freq_smooth_len (int): Filter length for vertical smoothing (Default value = 11)\n        alpha (float): Weighting parameter for harmonics (Default value = 0.9)\n        gamma (float): Logarithmic compression factor (Default value = 0.0)\n        constraint_region (np.ndarray): Constraint regions, row-format: (t_start_sec, t_end_sec, f_start_hz, f_end,hz)\n            (Default value = None)\n        tol (int): Tolerance parameter for transition matrix (Default value = 5)\n        score_low (float): Score (low) for transition matrix (Default value = 0.01)\n        score_high (float): Score (high) for transition matrix (Default value = 1.0)\n\n    Returns:\n        traj (np.ndarray): F0 contour, time in seconds in 1st column, frequency in Hz in 2nd column\n        Z (np.ndarray): Salience representation\n        T_coef (np.ndarray): Time axis\n        F_coef_hertz (np.ndarray): Frequency axis in Hz\n        F_coef_cents (np.ndarray): Frequency axis in cents\n    \"\"\"\n    Z, F_coef_hertz, F_coef_cents = compute_salience_rep(\n        x, Fs, N=N, H=H, R=R, F_min=F_min, F_max=F_max, num_harm=num_harm, freq_smooth_len=freq_smooth_len,\n        alpha=alpha, gamma=gamma)\n\n    T_coef = (np.arange(Z.shape[1]) * H) / Fs\n    index_CR = compute_trajectory_cr(Z, T_coef, F_coef_hertz, constraint_region,\n                                     tol=tol, score_low=score_low, score_high=score_high)\n\n    traj = np.hstack((T_coef.reshape(-1, 1), F_coef_hertz[index_CR].reshape(-1, 1)))\n    traj[index_CR == -1, 1] = 0\n    return traj, Z, T_coef, F_coef_hertz, F_coef_cents\n\n\n# Load audio\nfn_wav = 'FMP_C8_Audio_Bornemark_StopMessingWithMe-Excerpt_SoundCloud_mix.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\nipd.Audio(x, rate=Fs)\n\n# Read constraint regions\nfn_region = 'FMP_C8_F11_SvenBornemark_StopMessingWithMe_CR.csv'\nconstraint_region_df = pd.read_csv(path_data+fn_region, sep=';', keep_default_na=False, header=0)\nconstraint_region = constraint_region_df.values\n\n# Compute trajectory\ntraj, Z, T_coef, F_coef_hertz, F_coef_cents = compute_traj_from_audio(x, Fs=Fs, \n                                        constraint_region=constraint_region, gamma=0.1)\n\n# Visualization\nvisualize_salience_traj_constraints(Z, T_coef, F_coef_cents, F_ref=F_coef_hertz[0], figsize=figsize, cmap=cmap, \n                                    constraint_region=constraint_region, colorbar=True, traj=traj, ax=None)\nplt.show()\n\n# Sonification\nx_traj_mono = sonify_trajectory_with_sinusoid(traj, len(x), Fs, smooth_len=11)\n#x_traj_stereo = np.vstack((x.reshape(1,-1), x_traj_mono.reshape(1,-1)))  \nprint('original recording')\nipd.display(Audio(x, rate=Fs))\nprint('F0 sonification only')\nipd.display(Audio(x_traj_mono, rate=Fs))\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n\n\n\n\n\noriginal recording\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nF0 sonification only\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#멜로디",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#멜로디",
    "title": "9.2. 멜로디 추출",
    "section": "멜로디",
    "text": "멜로디\n\n특정 노래를 설명하라고 하면 메인 멜로디를 부르거나 흥얼거리는 경우가 많다. 일반적으로 멜로디는 특정 음악적 아이디어를 표현하는 음악 톤의 선형적 연속으로 정의될 수 있다. 음색의 특수한 배치로 인해 멜로디는 하나의 일관된 실체로 인식되며, 곡의 가장 기억에 남는 요소로 듣는 이의 머리에 박히게 되며, 멜로디는 일반적으로 어떤 식으로든 두드러진다.\n악보에서 멜로디는 높은 음으로 구성되는 경우가 많고 반주는 낮은 음으로 구성되는 경우가 많다. 또는 멜로디가 특징적인 음색을 가진 일부 악기로 연주된다. 일부 연주에서 멜로디의 음은 비브라토, 트레몰로 또는 글리산도(한 음에서 다른 음으로의 연속적인 미끄러짐)와 같이 쉽게 식별할 수 있는 시간-주파수 패턴을 특징으로 한다.\n음악 처리의 다른 많은 개념과 마찬가지로 멜로디의 개념은 다소 모호하다.\n\n먼저, 음악이 오디오 녹음의 형태로 제공되는 시나리오를 고려한다(상징적 음악 표현이 아님).\n둘째, 음의 시퀀스를 추정하는 대신 음의 피치에 해당하는 F0-값의 시퀀스로 멜로디를 나타낸다.\n셋째, 멜로디가 지배적인 음악(예: 리드 싱어 또는 리드 악기 연주)으로 제한한다.\n넷째, 하나의 음원에 연관될 수 있는 멜로디 라인은 한 번에 하나만 있다고 가정한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#멜로디-추출-melody-extraction",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#멜로디-추출-melody-extraction",
    "title": "9.2. 멜로디 추출",
    "section": "멜로디 추출 (Melody Extraction)",
    "text": "멜로디 추출 (Melody Extraction)\n\n이러한 가정을 바탕으로 멜로디 추출 문제는 다음과 같은 신호 처리 작업으로 간주할 수 있다. 녹음이 주어지면 리드 보이스 또는 악기가 연주하는 음에 해당하는 우세한(predominant) F0 값의 시퀀스를 자동으로 추정하는 것이 목표이다.\n기본 절차는 다음과 같이 요약할 수 있다.\n\n먼저 돌출 표현을 계산한다.\n그런 다음 적절한 평활화 또는 점수 기반 제약 조건을 사용하여 주요 F0 값의 궤적을 도출한다.\n또한 Salamo and Gómez가 제안한 것처럼 피치 윤곽에 따라 다른 음악적 제약을 도입할 수도 있다.\n\n가정이 단순화된 제약 경우에도 멜로디 추출 문제는 쉽지 않다.\n\n많은 악기가 동시에 연주되는 음악에서는 개별 악기의 음에 특정한 시간-주파수 패턴을 부여하기가 어렵다.\n또한 공명 및 잔향 효과의 존재는 다른 음원의 중첩을 더욱 증가시킨다.\n또한 기본 주파수를 성공적으로 추정한 후에도 여전히 F0 값 중 어느 것이 주 멜로디에 속하고 반주의 일부인지 결정해야 한다.\n\nFreischütz 예를 계속하여 다음 그림에 스펙트로그램 표현과 소프라노 가수가 연주하는 멜로디의 F0 궤적을 보여준다.\n\n가수가 계속 노래하는 것은 아니다. 이는 스펙트로그램 아래에 표시된 활동 패턴(activity pattern)으로도 알 수 있다.\n일부 시점에서 가장 높은 크기의 스펙트럼 빈이 반주에 속할 수 있다. 이것이 멜로디 추정 오류의 한 가지 이유이다.\n일부 고조파는 기본 주파수보다 더 많은 에너지를 포함할 수 있으며, 이로 인해 멜로디 추정에서 옥타브 오류가 발생할 수 있다.\n\n\n\ndef plot_STFT_F0_activity(Y, traj, figsize=(7,4), ylim = [0, 4000], cmap='gray_r'):\n    fig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [10, 0.2],\n                                        'height_ratios': [10, 1]}, figsize= figsize)    \n    \n    fig_im, ax_im, im = plot_matrix(Y, Fs=Fs/H, Fs_F=N/Fs, \n                                            ax=[ax[0,0], ax[0,1]], title='', cmap=cmap);\n    ax[0,0].set_ylim(ylim)\n    traj_plot = traj[traj[:, 1] > 0, :]\n    ax[0,0].plot(traj_plot[:, 0], traj_plot[:, 1], color='r', markersize=4, marker='.', linestyle='');\n        \n    # F0 activity\n    activity = np.array(traj[:, 1] > 0).reshape(1, -1)\n    im = ax[1,0].imshow(activity, aspect='auto', cmap='bwr', clim=[-1, 1])\n    ax[1,0].set_xticks([])\n    ax[1,0].set_yticks([])\n    ax[1,1].axis('off')\n    plt.tight_layout()\n    return fig, ax\n\n\n# Load audio\nfn_wav = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40.wav'\nx, Fs = librosa.load(path_data+fn_wav)\n#ipd.display(Audio(x, rate=Fs))\n\n# Computation of STFT\nN = 2048\nH = N//4\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\nY = np.log(1 + np.abs(X))\nFs_feature = Fs/H\nT_coef = np.arange(X.shape[1]) / Fs_feature\nfreq_res = Fs / N\nF_coef = np.arange(X.shape[0]) * freq_res\n\n# Load F0 trajectory and and resample to fit to X\nfn_traj = 'FMP_C8_F10_Weber_Freischuetz-06_FreiDi-35-40_F0-user-Book.csv'\ntraj_df = pd.read_csv(path_data+fn_traj, header=0,sep=';',keep_default_na=False)\ntraj = traj_df.values\nFs_traj = 1/(traj[1,0]-traj[0,0])\ntraj_X_values = interp1d(traj[:,0], traj[:,1], kind='nearest', fill_value='extrapolate')(T_coef)\ntraj_X = np.hstack((T_coef[:,None], traj_X_values[:,None]))\n\n# Visualization\nfigsize=(7,4)\nylim = [0, 4000]\nfig, ax = plot_STFT_F0_activity(Y, traj_X, figsize=figsize, ylim=ylim)\nplt.show()\n\n# Sonification\nsoni_mono = sonify_trajectory_with_sinusoid(traj_X, len(x), Fs)\n#soni_stereo = np.vstack((x.reshape(1,-1), soni_mono.reshape(1,-1)))  # left: x, right: sonification\nipd.display(ipd.Audio(soni_mono, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체-아이디어",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체-아이디어",
    "title": "9.2. 멜로디 추출",
    "section": "전체 아이디어",
    "text": "전체 아이디어\n\n단순화된 시나리오에서 멜로디 추출의 목적은 메인 멜로디에 해당하는 F0 값의 시퀀스를 추정하는 것이다. 이와 관련된 작업을 멜로디 분리라고 하며, 음악 신호를 메인 멜로디 음성을 캡처하는 멜로디 구성 요소(melody component)와 나머지 음향 이벤트를 캡처하는 반주 구성 요소(accompaniment component)로 분해하는 것을 목표로 한다.\n멜로디 분리의 한 가지 응용은 주어진 노래에 대한 노래방 버전의 자동 생성이다. 여기에서 주요 멜로디 음성은 노래의 원래 녹음에서 제거된다. 좀 더 기술적으로 말하면, 멜로디 분리의 목적은 주어진 신호 \\(x\\)를 멜로디 성분 \\(x^\\mathrm{Mel}\\)과 반주 성분 \\(x^\\mathrm{Acc}\\)로 분해하는 것이다: \\[x = x^\\mathrm{Mel} + x^\\mathrm{Acc}\\]\n하나의 일반적인 접근법은 먼저 멜로디 추출 알고리즘을 적용하여 메인 멜로디의 F0-trajectory를 도출하는 것이다. 이러한 F0 값과 해당 고조파를 기반으로 멜로디 구성 요소에 대한 이진 마스크(binary mask)를 구성할 수 있으며 반주의 바이너리 마스크는 보완(complement)으로 정의된다.\nHPS에서 설명된 대로 마스킹 기술을 사용하여 두 개의 마스킹된 STFT \\(\\mathcal{X}^\\mathrm{Mel}\\) 및 \\(\\mathcal{X}^\\mathrm{Acc}\\)를 도출한다. 이로부터 신호 재구성 기법을 적용하여 \\(x^\\mathrm{Mel}\\) 및 \\(x^\\mathrm{Acc}\\) 신호를 얻는다.\n이제 이 멜로디 분리 절차를 구현하고 Freischütz 예에 적용해보자.\n\n\n이진 마스크\n\n주어진 F0 궤적에 대해 이진 마스크를 정의하는 방법에는 여러 가지가 있다. 고조파를 고려하여 다음 코드 셀에서 두 가지 변형을 제공한다.\n\n첫 번째 변형에서는 허용 오차 매개변수 ‘tol_bin’(주파수 빈으로 주어짐)을 지정할 수 있으며, 이는 각 주파수 빈 주위에 고정 크기의 주파수 이웃을 정의하는 데 사용된다.\n두 번째 변형에서는 허용 오차 매개변수 ‘tol_cents’(센트 단위로 주어짐)를 지정할 수 있으며, 이는 각 주파수 빈 주위에 주파수 종속 크기의 이웃을 정의하는 데 사용된다. 보다 정확하게는 이웃 크기가 중심 주파수에 따라 선형적으로 증가하며, 따라서 센트 단위로 측정할 때 일정하게 유지된다.\n\n다음 그림에서는 두 가지 변형에 대한 멜로디의 이진 마스크(및 반주에 대한 보완)를 보여준다.\n\n\ndef convert_trajectory_to_mask_bin(traj, F_coef, n_harmonics=1, tol_bin=0):\n    \"\"\"Computes binary mask from F0 trajectory\n\n    Args:\n        traj (np.ndarray): F0 trajectory (time in seconds in 1st column, frequency in Hz in 2nd column)\n        F_coef (np.ndarray): Frequency axis\n        n_harmonics (int): Number of harmonics (Default value = 1)\n        tol_bin (int): Tolerance in frequency bins (Default value = 0)\n\n    Returns:\n        mask (np.ndarray): Binary mask\n    \"\"\"\n    # Compute STFT bin for trajectory\n    traj_bin = np.argmin(np.abs(F_coef[:, None] - traj[:, 1][None, :]), axis=0)\n\n    K = len(F_coef)\n    N = traj.shape[0]\n    max_idx_harm = np.max([K, np.max(traj_bin)*n_harmonics])\n    mask_pad = np.zeros((max_idx_harm.astype(int)+1, N))\n\n    for h in range(n_harmonics):\n        mask_pad[traj_bin*h, np.arange(N)] = 1\n    mask = mask_pad[1:K+1, :]\n\n    if tol_bin > 0:\n        smooth_len = 2*tol_bin + 1\n        mask = ndimage.maximum_filter1d(mask, smooth_len, axis=0, mode='constant', cval=0, origin=0)\n\n    return mask\n\n\ndef convert_trajectory_to_mask_cent(traj, F_coef, n_harmonics=1, tol_cent=0.0):\n    \"\"\"Computes binary mask from F0 trajectory\n\n    Args:\n        traj (np.ndarray): F0 trajectory (time in seconds in 1st column, frequency in Hz in 2nd column)\n        F_coef (np.ndarray): Frequency axis\n        n_harmonics (int): Number of harmonics (Default value = 1)\n        tol_cent (float): Tolerance in cents (Default value = 0.0)\n\n    Returns:\n        mask (np.ndarray): Binary mask\n    \"\"\"\n    K = len(F_coef)\n    N = traj.shape[0]\n    mask = np.zeros((K, N))\n\n    freq_res = F_coef[1] - F_coef[0]\n    tol_factor = np.power(2, tol_cent/1200)\n    F_coef_upper = F_coef * tol_factor\n    F_coef_lower = F_coef / tol_factor\n    F_coef_upper_bin = (np.ceil(F_coef_upper / freq_res)).astype(int)\n    F_coef_upper_bin[F_coef_upper_bin > K-1] = K-1\n    F_coef_lower_bin = (np.floor(F_coef_lower / freq_res)).astype(int)\n\n    for n in range(N):\n        for h in range(n_harmonics):\n            freq = traj[n, 1] * (1 + h)\n            freq_bin = np.round(freq / freq_res).astype(int)\n            if freq_bin < K:\n                idx_upper = F_coef_upper_bin[freq_bin]\n                idx_lower = F_coef_lower_bin[freq_bin]\n                mask[idx_lower:idx_upper+1, n] = 1\n    return mask\n\n\nfigsize = (12, 2.5)\n\n# Binary masks for melody and accompaniment using bins\ntol_bin = 8\nmask_mel_bin = convert_trajectory_to_mask_bin(traj_X, F_coef, \n                                              n_harmonics=30, tol_bin=tol_bin)\nmask_acc_bin = np.ones(mask_mel_bin.shape) - mask_mel_bin\n\nplt.figure(figsize=figsize)\nax = plt.subplot(121)\ntitle = r'Binary mask for melody (tol_bin = %d)'%tol_bin\nfig_im, ax_im, im = plot_matrix(mask_mel_bin, Fs=Fs_feature, ax=[ax],\n                                         Fs_F=1/freq_res, figsize=figsize, title=title);\nplt.ylim(ylim);\nax = plt.subplot(122)\ntitle=r'Binary mask for accompaniment (tol_bin = %d)'%tol_bin\nfig, ax, im = plot_matrix(mask_acc_bin, Fs=Fs_feature, ax=[ax], \n                                   Fs_F=1/freq_res, figsize=figsize, title=title);\nplt.ylim(ylim);\nplt.tight_layout()\n\n\n\n\n\n# Binary masks for melody and accompaniment using cents\ntol_cent = 80\nmask_mel_cent = convert_trajectory_to_mask_cent(traj_X, F_coef, n_harmonics=30, tol_cent=tol_cent)\nmask_acc_cent = np.ones(mask_mel_cent.shape) - mask_mel_cent\n\nplt.figure(figsize=figsize)\nax = plt.subplot(121)\ntitle=r'Binary mask for melody (tol_cent = %d)'%tol_cent\nfig_im, ax_im, im = plot_matrix(mask_mel_cent, Fs=Fs_feature, ax=[ax], \n                                         Fs_F=1/freq_res, figsize=figsize, title=title);\nplt.ylim(ylim);\nax = plt.subplot(122)\ntitle=r'Binary mask for accompaniment (tol_cent = %d)'%tol_cent\nfig, ax, im = plot_matrix(mask_acc_cent, Fs=Fs_feature, ax=[ax], \n                                   Fs_F=1/freq_res, figsize=figsize, title=title);\nplt.ylim(ylim);\nplt.tight_layout()\n\n\n\n\n\n\n신호 재구성\n\n다음으로 마스크된 STFT \\(\\mathcal{X}^\\mathrm{Mel}\\) 및 \\(\\mathcal{X}^\\mathrm{Acc}\\) 및 신호 \\(x^\\mathrm{Mel}\\)를 구성한다. 이로부터 신호 재구성 기법을 적용하여 시간 영역 신호 \\(x^\\mathrm{Acc}\\)를 얻는다.\n다시 두 가지 마스킹 변형에 대한 결과를 보자.\n\n\ndef compute_plot_display_mel_acc(Fs, X, N, H, mask_mel, figsize=figsize):\n    mask_acc = np.ones(mask_mel.shape) - mask_mel\n    X_mel = X * mask_mel\n    X_acc = X * mask_acc\n    x_mel = librosa.istft(X_mel, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n    x_acc = librosa.istft(X_acc, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n    \n    plt.figure(figsize=figsize)\n    ax = plt.subplot(121)\n    fig_im, ax_im, im = plot_matrix(np.log(1+10 * np.abs(X_mel)), Fs=Fs_feature, ax=[ax], Fs_F=1/freq_res, figsize=figsize, \n                     title='Masked STFT for melody');\n    plt.ylim(ylim);\n    ax = plt.subplot(122)\n    fig, ax, im = plot_matrix(np.log(1+10 * np.abs(X_acc)), Fs=Fs_feature, ax=[ax], Fs_F=1/freq_res, figsize=figsize, \n                     title='Masked STFT for accompaniment');\n    plt.ylim(ylim);    \n    plt.show()\n    audio_player_list([x_mel, x_acc], [Fs, Fs], width=350, \n         columns=['Reconstructed signal for melody', 'Reconstructed signal for accompaniment'])\n\n\nprint('Melody separation (tol_bin = %d)'%tol_bin)\ncompute_plot_display_mel_acc(Fs, X, N, H, mask_mel_bin, figsize=figsize)\n\nprint('Melody separation (tol_cent = %d)'%tol_cent)\ncompute_plot_display_mel_acc(Fs, X, N, H, mask_mel_cent, figsize=figsize)\n\nmask_mel_joint = np.logical_or(mask_mel_bin, mask_mel_cent)\nprint('Melody separation (joint mask using \"logical or\" operator)')\ncompute_plot_display_mel_acc(Fs, X, N, H, mask_mel_joint, figsize=figsize)\n\nMelody separation (tol_bin = 8)\n\n\n\n\n\n\n\n  \n    \n      Reconstructed signal for melody\n      Reconstructed signal for accompaniment\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\nMelody separation (tol_cent = 80)\n\n\n\n\n\n\n\n  \n    \n      Reconstructed signal for melody\n      Reconstructed signal for accompaniment\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\nMelody separation (joint mask using \"logical or\" operator)\n\n\n\n\n\n\n\n  \n    \n      Reconstructed signal for melody\n      Reconstructed signal for accompaniment\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체과정-예시",
    "href": "posts/9. Musically Informed Audio Decomposition/9.2.Melody_Extraction.html#전체과정-예시",
    "title": "9.2. 멜로디 추출",
    "section": "전체과정: 예시",
    "text": "전체과정: 예시\n\nF0-trajectory가 주어졌다고 가정하면, 멜로디-반주 분리를 위한 전체적인 과정은 다음과 같은 코드 셀로 정리된다. 예로 위에서도 사용했던 Sven Bornemark의 “Stop Messing With Me”에서 발췌 녹음을 보자.\n\n\ndef separate_melody_accompaniment(x, Fs, N, H, traj, n_harmonics=10, tol_cent=50.0):\n    \"\"\"F0-based melody-accompaniement separation\n\n    Args:\n        x (np.ndarray): Audio signal\n        Fs (scalar): Sampling frequency\n        N (int): Window size in samples\n        H (int): Hopsize in samples\n        traj (np.ndarray): F0 traj (time in seconds in 1st column, frequency in Hz in 2nd column)\n        n_harmonics (int): Number of harmonics (Default value = 10)\n        tol_cent (float): Tolerance in cents (Default value = 50.0)\n\n    Returns:\n        x_mel (np.ndarray): Reconstructed audio signal for melody\n        x_acc (np.ndarray): Reconstructed audio signal for accompaniement\n    \"\"\"\n    # Compute STFT\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\n    Fs_feature = Fs / H\n    T_coef = np.arange(X.shape[1]) / Fs_feature\n    freq_res = Fs / N\n    F_coef = np.arange(X.shape[0]) * freq_res\n\n    # Adjust trajectory\n    traj_X_values = interp1d(traj[:, 0], traj[:, 1], kind='nearest', fill_value='extrapolate')(T_coef)\n    traj_X = np.hstack((T_coef[:, None], traj_X_values[:, None, ]))\n\n    # Compute binary masks\n    mask_mel = convert_trajectory_to_mask_cent(traj_X, F_coef, n_harmonics=n_harmonics, tol_cent=tol_cent)\n    mask_acc = np.ones(mask_mel.shape) - mask_mel\n\n    # Compute masked STFTs\n    X_mel = X * mask_mel\n    X_acc = X * mask_acc\n\n    # Reconstruct signals\n    x_mel = librosa.istft(X_mel, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n    x_acc = librosa.istft(X_acc, hop_length=H, win_length=N, window='hann', center=True, length=x.size)\n\n    return x_mel, x_acc\n\n\n# Load audio\nfn_wav = 'FMP_C8_Audio_Bornemark_StopMessingWithMe-Excerpt_SoundCloud_mix.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nx_duration = len(x)/Fs\n\nfn_wav_track_mel = 'FMP_C8_Audio_Bornemark_StopMessingWithMe-Excerpt_SoundCloud_vocals.wav'\nx_track_mel, Fs = librosa.load(path_data+fn_wav_track_mel)\n\nfn_wav_track_acc = 'FMP_C8_Audio_Bornemark_StopMessingWithMe-Excerpt_SoundCloud_accomp.wav'\nx_track_acc, Fs = librosa.load(path_data+fn_wav_track_acc)\n\n\n# Read constraint regions\nfn_region = 'FMP_C8_F11_SvenBornemark_StopMessingWithMe_CR.csv'\nconstraint_region_df = pd.read_csv(path_data+fn_region, header=0, sep=';', keep_default_na=False)\nconstraint_region = constraint_region_df.values\n\n# Compute trajectory\ntraj, Z, T_coef, F_coef_hertz, F_coef_cents = compute_traj_from_audio(x, Fs=Fs, \n                                        constraint_region=constraint_region, gamma=0.1)\n\nN = 2048\nH = N//4\nx_mel, x_acc = separate_melody_accompaniment(x, Fs=Fs, N=N, H=H, traj=traj, n_harmonics=30, tol_cent=50)\n\naudio_player_list([x_mel, x_acc], [Fs, Fs], width=350, \n    columns=['Reconstructed signal for melody', 'Reconstructed signal for accompaniment'])\n\naudio_player_list([x_track_mel, x_track_acc], [Fs, Fs], width=350, \n    columns=['Ideal signal for melody', 'Ideal signal for melody for accompaniment'])\n\nC:\\Users\\JHCho\\AppData\\Local\\Temp\\ipykernel_15196\\3605094473.py:23: RuntimeWarning: divide by zero encountered in log2\n  bin_index = np.floor((1200 / R) * np.log2(F / F_ref) + 0.5).astype(np.int64)\n\n\n\n\n  \n    \n      Reconstructed signal for melody\n      Reconstructed signal for accompaniment\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element.                \n    \n  \n\n\n\n\n\n  \n    \n      Ideal signal for melody\n      Ideal signal for melody for accompaniment\n    \n  \n  \n    \n                                              Your browser does not support the audio element.                \n                                              Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "",
    "text": "NMF 기반의 오디오 분해 방법을 살펴본다. 행렬 분해 기법인 NMF를 설명하고, 음악 정보를 활용한 스펙트로그램의 분해 등을 다룬다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-스펙트로그램-분해spectrogam-factorization",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-스펙트로그램-분해spectrogam-factorization",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "예시: 스펙트로그램 분해(Spectrogam Factorization)",
    "text": "예시: 스펙트로그램 분해(Spectrogam Factorization)\n\nNMF의 예시로, 이 기술을 사용하여 음악 녹음의 스펙트로그램을 음악적으로 의미 있는 구성 요소로 분해하는 방법을 본다.\nFrédéric Chopin의 Prélude Op.28, No.4를 예로 보자. 다음 그림은 연주의 오디오 녹음과 동기화된 악보의 피아노 롤 표현과 악보를 보여준다. 설명을 위해 음표 번호 \\(p=71\\)와 관련된 모든 정보는 빨간색 직사각형 프레임으로 강조 표시되어 있다.\n\n\nImage(path_img+\"FMP_C8_F21a-b.png\", width=400)\n\n\n\n\n\n원본 데이터 행렬 \\(V\\)는 스펙트럼 벡터의 시퀀스인 크기 STFT를 사용한다. NMF를 사용하면 이 행렬을 두 개의 비음수 행렬 \\(W\\) 및 \\(H\\)의 곱으로 분해할 수 있다.\n이상적인 경우 첫 번째 행렬 \\(W\\)는 음악에서 발생하는 음표 피치의 스펙트럼 패턴을 나타내고 두 번째 행렬 \\(H\\)는 오디오 녹음에서 이러한 스펙트럼 패턴이 활성화되는 시간 위치를 나타낸다. 다음 그림은 Chopin 예제에 대한 그러한 분해를 보여준다.\n\n\nImage(path_img+\"FMP_C8_F21c-e.png\", width=400)\n\n\n\n\n\n이 경우 행렬 \\(W\\)로 지정된 각 템플릿은 특정 피치의 음이 \\(V\\)에서 스펙트럼적으로 구현되는 방식을 반영하고, 활성화 행렬 \\(H\\)는 악보의 피아노 롤 표현과 유사하게 보인다. 그러나 실제로는 어떤 신호 속성이 학습된 요인에 의해 궁극적으로 포착되는지 예측하기 어려운 경우가 많다.\n이 분해를 더 잘 제어하기 위해 NMF가 음악적으로 의미 있는 분해를 생성하도록 제한하기 위해 추가 악보 정보를 사용할 수 있는 방법을 보기로 한다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#nmf-정의",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#nmf-정의",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "NMF 정의",
    "text": "NMF 정의\n\n\\(V \\in \\mathbb{R}_{\\ge 0}^{K \\times N}\\)가 \\(K\\in\\mathbb{N}\\) 행과 \\(N\\in\\mathbb{N}\\) 열을 가진 비음(nonnegative)행렬이라고 하자. \\(K\\) 및 \\(N\\)보다 작은 \\(R\\in\\mathbb{N}\\)가 주어지면 NMF의 목표는 두개의 비음행렬 \\(W \\in \\mathbb{R}_{\\ge 0}^{K \\times R}\\) 및 \\(H \\in \\mathbb{R}_{\\ge 0}^{R \\times N}\\)를 찾는 것이다.: \\[V \\approx W \\cdot H\\]\n앞서 언급한 바와 같이 \\(W\\)의 열 벡터는 템플릿 벡터(template vectors)라고도 하며 가중치 \\(H\\)는 활성화(activations)라고 한다.\n\\(V \\approx W \\cdot H\\)를 찾으려면 근사의 품질을 정량화하는 거리 함수를 지정해야 한다. 다음에서는 유클리드 거리를 기반으로 한다. \\(A,B\\in\\mathbb{R}^{K \\times N}\\)가 계수 \\(A_{kn}\\) 및 \\(B_{kn}\\) (\\(k\\in[1:K]\\), \\(n\\in[1:N]\\))를 갖는 두 행렬이라고 하자. 그러면 \\(A\\)와 \\(B\\) 사이의 유클리드 거리의 제곱은 다음과 같이 정의된다. \\[\\|A-B\\|^2:= \\sum_{k=1}^{K}\\sum_{n=1}^{N}(A_{kn}-B_{kn})^2\\]\n이 거리 측정을 기반으로 NMF 문제를 다음과 같이 공식화할 수 있다. 비음행렬 \\(V\\in\\mathbb{R}_{\\ge 0}^{K \\times N}\\) 및 순위 매개변수 \\(R\\)가 주어지면 , \\(W \\in \\mathbb{R}_{\\ge 0}^{K \\times R}\\) 및 \\(H \\in \\mathbb{R}_{\\ge 0}^{R \\times N}\\)에 대해 |V-WH|^2를 최소화한다. 즉, \\(\\|V-WH\\|^2\\)를 \\(W\\)와 \\(H\\)의 결합함수(joint function)로 보고 \\(W\\)와 \\(H\\)에 대한 비음성 제약 하에서 최소값을 찾는 것이 목적이다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#곱셈-업데이트-규칙-multiplicative-update-rules",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#곱셈-업데이트-규칙-multiplicative-update-rules",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "곱셈 업데이트 규칙 (Multiplicative Update Rules)",
    "text": "곱셈 업데이트 규칙 (Multiplicative Update Rules)\n\n\\(\\|V-WH\\|^2\\) 최소화 문제에 표준 경사 하강법(standard gradient descent approach)을 적용한다. 결합 최적화(joint optimization)는 매우 어려운 문제이기 때문에 먼저 요인 \\(W\\)를 고정하고 \\(H\\)에 대해 최적화한 다음, 학습된 요인 \\(H\\)를 고정하고 \\(W\\)에 대해 최적화하는 것이 하나의 아이디어이다. 이 과정은 각 단계마다 \\(W\\)와 \\(H\\)의 역할이 바뀌는 방식으로 반복된다. 표준 경사 하강법에서 업데이트 규칙은 부가적이며 음의 경사 방향으로 단계 크기(step size)를 제어하기 위해 매개변수를 선택해야 한다.\nNMF 최적화 알고리즘의 주요 트릭은 가산(additive) 업데이트 규칙이 곱셈(multiplicative) 업데이트 규칙이 되도록 이 단계 크기 매개변수를 특정 방식으로 설정하는 것이다.\n다음 표는 NMF 분해 학습을 위한 반복 알고리즘을 보여준다. 여기서 곱셈 업데이트 규칙은 행렬 표기법으로 제공된다. \\(\\odot\\) 연산자는 점별(pointwise) 곱셈을 나타내고 \\(\\oslash\\) 연산자는 점별 나눗셈을 나타낸다.\n\n\nImage(path_img+\"FMP_C8_T01.png\", width=600)\n\n\n\n\n\n곱셈(multiplicative) 업데이트 규칙은 몇가지 눈에 띄는 점이 있다.\n\n첫 번째로 행렬 시퀀스 \\(W^{(0)},W^{(1)},W^{(2)},\\ldots\\) 및 \\(H^{(0)},H^{(1)},H^{(2)},\\ldots\\)는 수렴한다. \\(W^{(\\infty)}\\) 및 \\(H^{(\\infty)}\\)로 극한 행렬을 나타내면, 정상성(stationarity) 속성은 이러한 행렬이 거리 함수 \\(\\|V-WH\\| ^2\\)읠 로컬 최소값을 형성함을 의미한다..\n둘째, 구현하기가 매우 쉽다.\n셋째, 실제로는 수렴이 다른 많은 방법에 비해 상대적으로 빠른 것으로 나타난다.\n넷째, 비음수 제약 조건이 자동으로 적용된다.\n\n실제로 비음수 행렬 \\(V\\), \\(W^{(0)}\\) 및 \\(H^{(0)}\\)부터 시작하여 위 표의 방정식 (1) 및 (2)의 모든 항목도 음수가 아니다. 모든 연산은 곱셈 또는 나눗셈이므로 행렬 \\(W^{(\\ell)}\\) 및 \\(H^{(\\ell)}\\)도 반복 전체에서 음수가 아닌 상태로 유지된다.\n지정된 정지 기준(stop criterion)이 충족될 때까지 반복이 수행된다. 예를 들어, 일부 지정 매개변수 \\(L\\in\\mathbb{N}\\)에 대해 특정 수의 반복 \\(\\ell=0,1,2,\\ldots, L\\)을 수행할 수 있다. 또 다른 정지 기준으로, 연속적으로 계산된 두 개의 템플릿 행렬과 활성화 행렬 사이의 거리를 볼 수 있다. 임계값 \\(\\varepsilon>0\\)을 지정하면 \\(\\|H^{(\\ell+1)}-H^{(\\ell)}\\|^2\\leq \\varepsilon\\) 및 \\(\\|W^{(\\ell+1)}-W^{(\\ell)}\\|^2\\leq \\varepsilon\\)일 때 반복이 중지될 수 있다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#구현",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#구현",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "구현",
    "text": "구현\n\n다음 코드 셀에서는 위에서 설명한 NMF 절차의 기본 구현을 제공한다. 다음은 몇 가지 중요한 참고 사항이다.\n\n모든 연산은 행렬 기반 연산을 사용하여 수행된다.\n0으로 나누는 것을 방지하기 위해 곱셈 업데이트 규칙의 분모에 작은 값(엡실론)이 추가된다.\n분해 \\(V\\approx WH\\)에서 각각 \\(W\\) 및 \\(H\\)의 열과 행에 대해 자유도가 있다. 예를 들어, \\(W\\) 열을 인수 \\(\\alpha\\in\\mathbb{R}\\)로 나누고 \\(WH\\) 곱을 변경하지 않고 \\(H\\)의 해당 행을 같은 인수로 곱할 수 있다. 다음 코드에서 norm=True 매개변수를 설정하면 이 사실을 사용하여 최종 템플릿 행렬 \\(W\\)의 모든 열이 최대 노름(norm)과 관련하여 정규화되도록 한다. 지정하지 않으면 행렬 \\(W\\) 및 \\(H\\)가 임의로 초기화된다.\n\n\n\ndef nmf(V, R, thresh=0.001, L=1000, W=None, H=None, norm=False, report=False):\n    \"\"\"NMF algorithm with Euclidean distance\n\n    Args:\n        V (np.ndarray): Nonnegative matrix of size K x N\n        R (int): Rank parameter\n        thresh (float): Threshold used as stop criterion (Default value = 0.001)\n        L (int): Maximal number of iteration (Default value = 1000)\n        W (np.ndarray): Nonnegative matrix of size K x R used for initialization (Default value = None)\n        H (np.ndarray): Nonnegative matrix of size R x N used for initialization (Default value = None)\n        norm (bool): Applies max-normalization of columns of final W (Default value = False)\n        report (bool): Reports errors during runtime (Default value = False)\n\n    Returns:\n        W (np.ndarray): Nonnegative matrix of size K x R\n        H (np.ndarray): Nonnegative matrix of size R x N\n        V_approx (np.ndarray): Nonnegative matrix W*H of size K x N\n        V_approx_err (float): Error between V and V_approx\n        H_W_error (np.ndarray): History of errors of subsequent H and W matrices\n    \"\"\"\n    K = V.shape[0]\n    N = V.shape[1]\n    if W is None:\n        W = np.random.rand(K, R)\n    if H is None:\n        H = np.random.rand(R, N)\n    V = V.astype(np.float64)\n    W = W.astype(np.float64)\n    H = H.astype(np.float64)\n    H_W_error = np.zeros((2, L))\n    ell = 1\n    below_thresh = False\n    eps_machine = np.finfo(np.float32).eps\n    while not below_thresh and ell <= L:\n        H_ell = H\n        W_ell = W\n        H = H * (W.transpose().dot(V) / (W.transpose().dot(W).dot(H) + eps_machine))\n        W = W * (V.dot(H.transpose()) / (W.dot(H).dot(H.transpose()) + eps_machine))\n\n        H_error = np.linalg.norm(H-H_ell, ord=2)\n        W_error = np.linalg.norm(W - W_ell, ord=2)\n        H_W_error[:, ell-1] = [H_error, W_error]\n        if report:\n            print('Iteration: ', ell, ', H_error: ', H_error, ', W_error: ', W_error)\n        if H_error < thresh and W_error < thresh:\n            below_thresh = True\n            H_W_error = H_W_error[:, 0:ell]\n        ell += 1\n    if norm:\n        for r in range(R):\n            v_max = np.max(W[:, r])\n            if v_max > 0:\n                W[:, r] = W[:, r] / v_max\n                H[r, :] = H[r, :] * v_max\n    V_approx = W.dot(H)\n    V_approx_err = np.linalg.norm(V-V_approx, ord=2)\n    return W, H, V_approx, V_approx_err, H_W_error\n\n\n\\(K=4\\) 및 \\(N=8\\)의 작은 예제 \\(V \\in \\mathbb{R}_{\\ge 0}^{K \\times N}\\)를 통해 NMF 절차를 본다. 순위(rank) 매개변수는 \\(R=2\\)로 설정된다.\n\n\ndef plot_nmf(V, W, H, V_approx, error, figsize=(10,2), aspect='auto', wr=[1, 0.5, 1, 1]): \n    fig, ax = plt.subplots(1, 4, gridspec_kw={'width_ratios': wr},\n                           figsize=figsize)    \n    cmap = 'gray_r'\n    im = ax[0].imshow(V, aspect=aspect, origin='lower', cmap=cmap, clim=[0, np.max(V)])\n    ax[0].set_title(r'$V$')\n    plt.sca(ax[0])\n    plt.colorbar(im)   \n    im = ax[1].imshow(W, aspect=aspect, origin='lower', cmap=cmap, clim=[0, np.max(W)])\n    ax[1].set_title(r'$W$')\n    plt.sca(ax[1])\n    plt.colorbar(im)\n    im = ax[2].imshow(H, aspect=aspect, origin='lower', cmap=cmap, clim=[0, np.max(H)])\n    ax[2].set_title(r'$H$')\n    plt.sca(ax[2])    \n    plt.colorbar(im)\n    im = ax[3].imshow(V_approx, aspect=aspect, origin='lower', cmap=cmap, clim=[0, np.max(V_approx)])\n    ax[3].set_title(r'$WH$ (error = %0.2f)'%error)\n    plt.sca(ax[3])    \n    plt.colorbar(im)\n    plt.tight_layout() \n    plt.show() \n    \nV = np.array([ \n    [0, 1, 2, 3, 4, 5, 6, 7], \n    [0, 1, 2, 3, 3, 2, 1, 0],\n    [0, 0, 0, 0, 0, 0, 0, 0], \n    [7, 0, 0, 0, 0, 0, 0, 0], \n    [7, 6, 5, 4, 3, 2, 1, 0]    \n             ],dtype=float)\n\nK = V.shape[0]\nN = V.shape[1]\nR = 2\nthresh = 0.001\nL = 100\n\nW_init =  np.random.rand(K,R) \nW_init = W_init/np.max(W_init)\nH_init = np.random.rand(R,N)    \n\nprint('Matrix V and randomly initialized matrices W and H')\nV_approx = W_init.dot(H_init)\nerror = np.linalg.norm(V-V_approx, ord=2)\nplot_nmf(V, W_init, H_init, V_approx, error, figsize=(12,2), \n         aspect='equal', wr=[1, 0.3, 1, 1])\n\nprint('Matrix V and matrices W and H after training')\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, \n                                   L=L, W=W_init, H=H_init, norm=1, report=0)\nplot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), \n         aspect='equal', wr=[1, 0.3, 1, 1])\n\nplt.figure(figsize=(5,2))\nnum_iter = H_W_error.shape[1]\nplt.plot(np.arange(num_iter)+1, H_W_error[0,:], 'r', label='W error')\nplt.plot(np.arange(num_iter)+1, H_W_error[1,:], 'b', label='H error')\nplt.yscale('log')\nplt.xlabel('Iteration index')\nplt.ylabel('Error (log axis)')\nplt.title('Required number of iterations: %d'%num_iter)\nplt.xlim([1, num_iter])\nplt.legend()\nplt.tight_layout()\n\nMatrix V and randomly initialized matrices W and H\n\n\n\n\n\nMatrix V and matrices W and H after training\n\n\n\n\n\n\n\n\n\n순위(rank) 매개변수에 따라\n\n마지막 예에서 NMF가 학습한 두 개의 템플릿(\\(W\\) 열)은 (계수 값 측면에서) 지배적인 \\(V\\)의 첫 번째 열과 마지막 열 각각의 특성을 캡처한다.\n예제는 \\(V\\)와 학습된 곱 \\(WH\\) 사이의 오류가 여전히 클 수 있음을 보여준다. 이는 NMF 알고리즘이 전역 최적으로 수렴하는 경우에도 같을 수 있다.\n다음 예에서는 다른 매개변수 \\(R\\)를 사용한 예를 보여준다. \\(R=1\\)를 사용하는 것은 전체 행렬 \\(V\\)를 “설명”하는 데 단일 템플릿 벡터만 사용되는 가장 제한적인 경우이다. \\(R\\)를 늘리면 \\(V\\approx WH\\)의 더 나은 근사값을 얻는다. 그러나 행렬 \\(W\\) 및 \\(H\\)는 덜 구조화되어 해석하기 더 어려워진다.\n\n\nthresh = 0.00001\nL = 100\nR_set = np.array([1, 2, 3, 4])\nfor R in R_set: \n    print('R = %d'%R)\n    W_init =  np.random.rand(K,R) \n    W_init = W_init/np.max(W_init)\n    H_init = np.random.rand(R,N)    \n    W, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, \n                                       L=L, W=W_init, H=H_init, norm=1, report=0)\n    plot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), \n             aspect='equal', wr=[1, 0.7, 1, 1])\n\nR = 1\n\n\n\n\n\nR = 2\n\n\n\n\n\nR = 3\n\n\n\n\n\nR = 4"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-c-major-scale",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-c-major-scale",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "예시: C-Major Scale",
    "text": "예시: C-Major Scale\n\n실제 오디오 신호의 크기(magnitude) 스펙트로그램에 NMF를 적용해보자. 예로 \\(\\mathrm{C4}\\) (\\(261.6~\\mathrm{Hz}\\))로 시작하여 \\(\\mathrm{C5}\\) (\\(523.2~\\mathrm{Hz}\\)).로 끝나는 C-major scale 녹음을 본다.\n\n\nImage(path_img+\"FMP_C8_Audio_C-major-scale.png\", width=300)\n\n\n\n\n\n입력 행렬 \\(V\\)로 STFT의 저주파 부분만 고려한다. 첫 번째 실험에서는 \\(R=7\\)를 사용한다.\n\n\nfn_wav = 'FMP_C3_F08_C-major-scale.wav'\nx, Fs = librosa.load(path_data+fn_wav)\nN, H = 1024, 512\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann', center=True, pad_mode='constant')\nV = np.log(1 + 10 * np.abs(X))\nV = V[0:60,:]\n\nK = V.shape[0]\nN = V.shape[1]\nR = 7\nthresh = 0.0001\nL = 200\n\nW_init =  np.random.rand(K,R) \nW_init = W_init/np.max(W_init)\nH_init = np.random.rand(R,N)    \nV_approx = W_init.dot(H_init)\nerror = np.linalg.norm(V-V_approx, ord=2)\nprint('Matrix V and randomly initialized matrices W and H')\nplot_nmf(V, W_init, H_init, V_approx, error, figsize=(12,2), wr=[1, 1, 1, 1])\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, \n                                              L=L, W=W_init, H=H_init, norm=1, report=0)\nprint('Matrix V and matrices W and H after training')\nplot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), wr=[1, 1, 1, 1])\n\nMatrix V and randomly initialized matrices W and H\n\n\n\n\n\nMatrix V and matrices W and H after training\n\n\n\n\n\n\n이 그림은 좋은 전체 근사치 \\(V\\approx WH\\)를 얻을 수 있음을 의미한다. \\(W\\)로 인코딩된 7개의 템플릿은 첫 번째(C4)와 마지막 음(C5)이 템플릿을 공유하는 C장음계의 원형 스펙트럼 벡터에 해당한다.\n이 예는 또한 \\(W\\)의 템플릿 열 순서(및 \\(H\\)의 활성화 행과 동일)에 의미 체계가 없음을 보여준다. 실제로 NMF 분해는 어떤 순서도 선호하지 않는다.\n순위 매개변수를 늘리면 일반적으로 다음 그림과 같이 더 나은 전체 근사 품질을 얻는다. 예를 들어, 시작 관련(수직) 구조와 같은 세부 사항은 작은 \\(R\\)를 사용하는 것에 비해 더 잘 재구성된다. 단, \\(W\\)와 \\(H\\)의 음악적 해석이 손실될 수 있다는 단점이 있다.\n\n\nR = 4\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, L=L, norm=1)\nprint('Matrix V and matrices W and H after training using R = %d'%R)\nplot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), wr=[1, 1, 1, 1])\n\nR = 10\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, L=L, norm=1)\nprint('Matrix V and matrices W and H after training using R = %d'%R)\nplot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), wr=[1, 1, 1, 1])\n\nR = 15\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, thresh=thresh, L=L, norm=1)\nprint('Matrix V and matrices W and H after training using R = %d'%R)\nplot_nmf(V, W, H, V_approx, V_approx_err, figsize=(12,2), wr=[1, 1, 1, 1])\n\nMatrix V and matrices W and H after training using R = 4\n\n\n\n\n\nMatrix V and matrices W and H after training using R = 10\n\n\n\n\n\nMatrix V and matrices W and H after training using R = 15"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-chopin",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#예시-chopin",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "예시 (Chopin)",
    "text": "예시 (Chopin)\n\nImage(path_img+\"FMP_C8_F21a.png\", width=300)\n\n\n\n\n\nAudio(path_data+\"FMP_C8_F27_Chopin_Op028-04_minor.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nNMF 알고리즘에 대한 입력으로 다음 코드 셀에서 계산된 로그적으로 압축된 크기 스펙트로그램을 사용한다.\n\n\n#fn_wav = 'FMP_C8_Audio_Chopin_Op028-04_SMD_beginning.wav'\nfn_wav = \"FMP_C8_F27_Chopin_Op028-04_minor.wav\"\nx, Fs = librosa.load(path_data+fn_wav)\nN_fft = 2048\nH_fft = 1024\nX = librosa.stft(x, n_fft=N_fft, hop_length=H_fft)\nV = np.log(1 + np.abs(X))\n#V = np.abs(X)\nfreq_max = 2000\n\nplot_matrix(V, Fs=Fs/H_fft, Fs_F=N_fft/Fs)\nplt.ylim([0, freq_max]);"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#랜덤-초기화-nmf",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#랜덤-초기화-nmf",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "랜덤 초기화 NMF",
    "text": "랜덤 초기화 NMF\n\n이제 임의로 초기화된 행렬 \\(W^{(0)}\\) 및 \\(H^{(0)}\\)를 반복적으로 업데이트하여 크기 스펙트로그램에 NMF 알고리즘을 적용한다. 다음 그림에는 초기화된 템플릿과 활성화 매트릭스 그리고 학습된 것 모두 표시되어 있다. 이 예에서 순위 매개변수는 \\(R=8\\)로 설정되어 음악 예에서 발생하는 8개의 서로 다른 음악 피치 각각에 대해 하나의 템플릿을 허용한다.\n\n\ndef plot_nmf_factors(W, H, V, Fs, N_fft, H_fft, freq_max, label_pitch=None,\n                     title_W='W', title_H='H', title_V='V', figsize=(13, 3)):\n    \"\"\"Plots the factors of an NMF-based spectral decomposition\n\n    Args:\n        W: Template matrix\n        H: Activation matrix\n        V: Reconstructed input matrix\n        Fs: Sampling frequency\n        N_fft: FFT length\n        H_fft: Hopsize\n        freq_max: Maximum frequency to be plotted\n        label_pitch: Labels for the different pitches (Default value = None)\n        title_W: Title for imshow of matrix W (Default value = 'W')\n        title_H: Title for imshow of matrix H (Default value = 'H')\n        title_V: Title for imshow of matrix V (Default value = 'V')\n        figsize: Size of the figure (Default value = (13, 3))\n    \"\"\"\n    R = W.shape[1]\n    N = H.shape[1]\n    # cmap = libfmp.b.compressed_gray_cmap(alpha=5)\n    cmap = 'gray_r'\n    dur_sec = (N-1) * H_fft / Fs\n    if label_pitch is None:\n        label_pitch = np.arange(R)\n\n    plt.figure(figsize=figsize)\n    plt.subplot(131)\n    plt.imshow(W, cmap=cmap, origin='lower', aspect='auto', extent=[0, R, 0, Fs/2], interpolation='nearest')\n    plt.ylim([0, freq_max])\n    plt.colorbar()\n    plt.xticks(np.arange(R) + 0.5, label_pitch)\n    plt.xlabel('Pitch')\n    plt.ylabel('Frequency (Hz)')\n    plt.title(title_W)\n\n    plt.subplot(132)\n    plt.imshow(H, cmap=cmap, origin='lower', aspect='auto', extent=[0, dur_sec, 0, R], interpolation='nearest')\n    plt.colorbar()\n    plt.yticks(np.arange(R) + 0.5, label_pitch)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Pitch')\n    plt.title(title_H)\n\n    plt.subplot(133)\n    plt.imshow(V, cmap=cmap, origin='lower', aspect='auto', extent=[0, dur_sec, 0, Fs/2], interpolation='nearest')\n    plt.ylim([0, freq_max])\n    plt.colorbar()\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Frequency (Hz)')\n    plt.title(title_V)\n\n    plt.tight_layout()\n    plt.show()\n\nK = V.shape[0]\nN = V.shape[1]\nR = 8\n\nW_init = np.random.rand(K,R)\nH_init = np.random.rand(R,N)\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, W=W_init, H=H_init, L=200, norm=True)\n#label_pitch=['54', '55', '57', '59', '63', '64', '71', '72']\nplot_nmf_factors(W_init, H_init, W_init.dot(H_init), Fs, N_fft, H_fft, freq_max)\nplot_nmf_factors(W, H, W.dot(H), Fs, N_fft, H_fft, freq_max)               \n\n\n\n\n\n\n\n\n이 예에서 알 수 있듯이 이 접근 방식에는 다양한 문제가 있다. 학습된 템플릿 행렬 \\(W\\)를 보면 주어진 템플릿이 어떤 소리나 피치에 해당하는지 명확하지 않다. 템플릿 벡터 중 일부만이 명확한 화성 구조를 나타내지만 대부분의 템플릿은 개별 음이 아닌 음의 혼합에 해당하는 것으로 보인다. 이것은 또한 악보의 피아노 롤 표현과 거의 연관될 수 없는 \\(H\\)의 활성화 패턴으로 표시된다.\n요약하면, 원래 행렬 \\(V\\)가 잘 근사될 수 있지만, 단순히 임의 초기화에 기반한 표준 NMF 접근 방식을 적용하면 명확한 음악적 의미가 결여된 분해가 생성되는 경우가 많다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#피치-정보-템플릿-제약-pitch-informed-template-constraints",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#피치-정보-템플릿-제약-pitch-informed-template-constraints",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "피치 정보 템플릿 제약 (Pitch-Informed Template Constraints)",
    "text": "피치 정보 템플릿 제약 (Pitch-Informed Template Constraints)\n\n위의 문제를 극복하기 위한 한 가지 중요한 아이디어는 \\(W\\) 및 \\(H\\)에 적절한 제약을 추가하는 것이다. 이러한 맥락에서 곱셈 업데이트 규칙은 큰 이점이 있다. 곱셈의 경우 전체 학습 과정에서 값이 0인 항목이 그대로 유지된다. 이 속성 덕분에 초기화 단계에서 적절한 항목을 0으로 설정하여 \\(W\\) 및 \\(H\\)에 특정 구조를 적용할 수 있다. 이 수정된 초기화 방법을 사용할 수 있다.\n이제 이 전략을 적용하여 \\(W\\)의 템플릿에서 고조파 구조를 강화한다. 피치 \\(p\\)를 각 템플릿 벡터에 할당한 후 중심 주파수 \\(F_\\mathrm{pitch}(p) = 2^{(p-69)/12} \\cdot 440\\)를 기본 주파수의 추정치로 사용할 수 있다. 이로부터 중심 주파수의 정수배에 위치한 고조파의 대략적인 위치를 도출할 수 있다. 정확한 주파수를 알 수 없기 때문에 이러한 위치 주변의 이웃은 나머지 항목을 0으로 설정하면서 템플릿에서 0이 아닌 값으로 초기화될 수 있다.\n\n\ndef pitch_from_annotation(annotation):\n    \"\"\"Extract set of occurring pitches from annotation\n\n    Args:\n        annotation (list): Annotation data\n\n    Returns:\n        pitch_set (np.ndarray): Set of occurring pitches\n    \"\"\"\n    pitch_all = np.array([c[2] for c in annotation])\n    pitch_set = np.unique(pitch_all)\n    return pitch_set\n\n\ndef template_pitch(K, pitch, freq_res, tol_pitch=0.05):\n    \"\"\"Defines spectral template for a given pitch\n\n    Args:\n        K (int): Number of frequency points\n        pitch (float): Fundamental pitch\n        freq_res (float): Frequency resolution\n        tol_pitch (float): Relative frequency tolerance for the harmonics (Default value = 0.05)\n\n    Returns:\n        template (np.ndarray): Nonnegative template vector of size K\n    \"\"\"\n    max_freq = K * freq_res\n    pitch_freq = 2**((pitch - 69) / 12) * 440\n    max_order = int(np.ceil(max_freq / ((1 - tol_pitch) * pitch_freq)))\n    template = np.zeros(K)\n    for m in range(1, max_order + 1):\n        min_idx = max(0, int((1 - tol_pitch) * m * pitch_freq / freq_res))\n        max_idx = min(K-1, int((1 + tol_pitch) * m * pitch_freq / freq_res))\n        template[min_idx:max_idx+1] = 1 / m\n    return template\n        \n    \ndef init_nmf_template_pitch(K, pitch_set, freq_res, tol_pitch=0.05):\n    \"\"\"Initializes template matrix for a given set of pitches\n\n    Args:\n        K (int): Number of frequency points\n        pitch_set (np.ndarray): Set of fundamental pitches\n        freq_res (float): Frequency resolution\n        tol_pitch (float): Relative frequency tolerance for the harmonics (Default value = 0.05)\n\n    Returns:\n        W (np.ndarray): Nonnegative matrix of size K x R with R = len(pitch_set)\n    \"\"\"\n    R = len(pitch_set)\n    W = np.zeros((K, R))\n    for r in range(R):\n        W[:, r] = template_pitch(K, pitch_set[r], freq_res, tol_pitch=tol_pitch)\n    return W\n\n\ndef csv_to_list(csv):\n    df = pd.read_csv(csv, header=0, sep=';', keep_default_na=False)\n    score = []\n    for i, (start, duration, pitch, velocity, label) in df.iterrows():\n        score.append([start, duration, pitch, velocity, label])\n    return score\n\n\nfn_ann = 'FMP_C8_Audio_Chopin_Op028-04_SMD_beginning.csv'\nannotation = csv_to_list(path_data+fn_ann)\npitch_set = pitch_from_annotation(annotation)\n\nK = V.shape[0]\nN = V.shape[1]\nR = pitch_set.shape[0]\nfreq_res = Fs / N_fft\nW_init = init_nmf_template_pitch(K, pitch_set, freq_res, tol_pitch=0.05)\nH_init = np.random.rand(R,N)\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, W=W_init, H=H_init, L=100, norm=True)\nplot_nmf_factors(W_init, H_init, W_init.dot(H_init), Fs, N_fft, H_fft, freq_max, label_pitch=pitch_set)\nplot_nmf_factors(W, H, W.dot(H), Fs, N_fft, H_fft, freq_max, label_pitch=pitch_set)    \n\n\n\n\n\n\n\n\n곱셈 업데이트 규칙에 기반한 학습 프로세스는 고조파 구조를 유지할 뿐만 아니라 이를 더욱 세분화한다. 동시에 활성화 매트릭스는 연주된 음의 존재와 강도를 아주 잘 반영한다. 활성 매트릭스의 수직 구조는 노트 온셋의 트랜지언트와 관련된 짧은 시간 에너지 폭발의 결과이다. 이러한 구조는 화성 정보를 제공하는 템플릿에 의해 잘 반영되지 않으며 모든 템플릿의 중첩으로 “합성”(synthesized)된다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-활성화-제약-score-informed-activation-constraints",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-활성화-제약-score-informed-activation-constraints",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "악보 정보 활성화 제약 (Score-Informed Activation Constraints)",
    "text": "악보 정보 활성화 제약 (Score-Informed Activation Constraints)\n\n추가 사전 지식이 있는 경우 이를 활용하여 분해 프로세스를 더욱 안정화할 수 있다. 이런 맥락에서 악보가 특히 중요하다. 개략적인 수준에서 악보에서 어떤 악기가 연주되고 있는지 또는 음악이 진행되는 동안 어떤 피치가 발생하는지와 같은 전역적 정보를 추출할 수 있다.\n이 예에서는 이러한 정보를 사용하여 템플릿 수를 자동으로 \\(R=8\\)로 설정할 수 있다. 또한 격리된 음표의 녹음에서 파생될 수 있는 악기 종속적 고조파 모델을 사용하여 템플릿의 초기화를 개선할 수 있다.\n보다 미세한 수준에서, 음이 실제로 연주되는 시점에 대한 로컬 정보를 활용할 수도 있다. 악보에 의해 지정된 음표 이벤트가 오디오 녹음에서 발생하는 시간 위치에 정렬되어 있다고 가정하면 특정 템플릿이 활성화될 수 있는 시간 제약을 부과할 수 있다.\n이를 위해 해당 활성화 항목을 1로 설정하여 \\(H^{(0)}\\)에서 적합한 영역을 초기화하고, 나머지 항목은 0으로 설정된다. 예를 들어 피치 \\(p=55\\)에 해당하는 활성화 항목은 대략 \\(t=2\\)에서 \\(t=7\\)까지의 시간 간격으로 제한된다(초 단위).\n\n\ndef init_nmf_activation_score(N, annotation, frame_res, tol_note=[0.2, 0.5], pitch_set=None):\n    \"\"\"Initializes activation matrix for given score annotations\n\n    Args:\n        N (int): Number of frames\n        annotation (list): Annotation data\n        frame_res (time): Time resolution\n        tol_note (list or np.ndarray): Tolerance (seconds) for beginning and end of a note (Default value = [0.2, 0.5])\n        pitch_set (np.ndarray): Set of occurring pitches (Default value = None)\n\n    Returns:\n        H (np.ndarray): Nonnegative matrix of size R x N\n        pitch_set (np.ndarray): Set of occurring pitches\n    \"\"\"\n    note_start = np.array([c[0] for c in annotation])\n    note_dur = np.array([c[1] for c in annotation])\n    pitch_all = np.array([c[2] for c in annotation])\n    if pitch_set is None:\n        pitch_set = np.unique(pitch_all)\n    R = len(pitch_set)\n    H = np.zeros((R, N))\n    for i in range(len(note_start)):\n        start_idx = max(0, int((note_start[i] - tol_note[0]) / frame_res))\n        end_idx = min(N, int((note_start[i] + note_dur[i] + tol_note[1]) / frame_res))\n        pitch_idx = np.argwhere(pitch_set == pitch_all[i])\n        H[pitch_idx, start_idx:end_idx] = 1\n    return H, pitch_set\n\n\nframe_res = H_fft/Fs\nH_init, pitch_set = init_nmf_activation_score(N, annotation, frame_res, tol_note=[0.2, 0.5])\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, R, W=W_init, H=H_init, L=100, norm=True)\nplot_nmf_factors(W_init, H_init, W_init.dot(H_init), Fs, N_fft, H_fft, freq_max, label_pitch=pitch_set)\nplot_nmf_factors(W, H, W.dot(H), Fs, N_fft, H_fft, freq_max, label_pitch=pitch_set)\n\n\n\n\n\n\n\n\n결과적으로 활성화 행렬 \\(H\\)는 동기화된 악보 정보의 대략적인 피아노 롤 표현으로 해석될 수 있다. 어떤 의미에서 동기화 단계는 NMF 기반 학습 절차에 의해 정제된 첫 번째 근사 분해를 산출하는 것으로 볼 수 있다. 예상대로 활성화 제약 조건을 템플릿 벡터에 대한 제약 조건과 결합하면 결과가 원하는 의미 체계를 더 잘 반영한다는 점에서 분해가 더욱 안정화된다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#온셋-onset-모델",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#온셋-onset-모델",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "온셋 (Onset) 모델",
    "text": "온셋 (Onset) 모델\n\n지금까지 NMF 기반 분해는 신호의 고조파 특성만을 나타냈다. 온셋과 같은 충격 속성을 설명하기 위해 이제 추가 템플릿을 도입하여 NMF 모델을 확장할 수 있는 방법을 보여준다.\n첫 번째 아이디어는 모든 피치의 온셋을 공동으로 설명하는 하나의 추가 템플릿을 사용하는 것이다. 그러나 음 시작에 대한 스펙트럼 패턴이 각 피치와 완전히 독립적이지 않을 수 있으므로(피아노의 경우처럼) 각 피치에 대해 다음과 같은 하나의 추가 온셋 템플릿을 사용한다.\n일반적으로 온셋 템플릿의 스펙트럼 모양을 예측하기는 어렵다. 따라서 고조파 템플릿과 달리 스펙트럼 제약 조건을 적용하지 않고 온셋 템플릿을 균일하게 초기화하고 학습 프로세스가 각각의 모양을 파생하도록 한다. 온셋 템플릿에 대한 의미 있는 스펙트럼 제약 조건을 찾기는 어렵지만 음 온셋의 단시간(short-time) 특성으로 인해 활성화 측면에서 상대적으로 엄격한 시간 제약 조건을 도입할 수 있다.\n이러한 각 위치 주변의 작은 이웃 내에서(동기화 부정확 가능성을 고려하여) 해당 활성화 항목을 하나로 초기화하고 나머지 항목은 모두 0으로 설정한다. 이는 시작 템플릿이 활성화될 수 있는 시점을 강력하게 제한한다.\n다음 예에서는 템플릿 및 활성화 측면에서 초기화와 함께 확장된 NMF 모델을 사용한다.\n\n\ndef init_nmf_template_pitch_onset(K, pitch_set, freq_res, tol_pitch=0.05):\n    \"\"\"Initializes template matrix with onsets for a given set of pitches\n\n    Args:\n        K (int): Number of frequency points\n        pitch_set (np.ndarray): Set of fundamental pitches\n        freq_res (float): Frequency resolution\n        tol_pitch (float): Relative frequency tolerance for the harmonics (Default value = 0.05)\n\n    Returns:\n        W (np.ndarray): Nonnegative matrix of size K x (2R) with R = len(pitch_set)\n    \"\"\"\n    R = len(pitch_set)\n    W = np.zeros((K, 2*R))\n    for r in range(R):\n        W[:, 2*r] = 0.1\n        W[:, 2*r+1] = template_pitch(K, pitch_set[r], freq_res, tol_pitch=tol_pitch)\n    return W\n\n\ndef init_nmf_activation_score_onset(N, annotation, frame_res, tol_note=[0.2, 0.5], tol_onset=[0.3, 0.1],\n                                    pitch_set=None):\n    \"\"\"Initializes activation matrix with onsets for given score annotations\n\n    Args:\n        N (int): Number of frames\n        annotation (list): Annotation data\n        frame_res (float): Time resolution\n        tol_note (list or np.ndarray): Tolerance (seconds) for beginning and end of a note (Default value = [0.2, 0.5])\n        tol_onset (list or np.ndarray): Tolerance (seconds) for beginning and end of an onset\n            (Default value = [0.3, 0.1])\n        pitch_set (np.ndarray): Set of occurring pitches (Default value = None)\n\n    Returns:\n        H (np.ndarray): Nonnegative matrix of size (2R) x N\n        pitch_set (np.ndarray): Set of occurring pitches\n        label_pitch (np.ndarray): Pitch labels for the templates\n    \"\"\"\n    note_start = np.array([c[0] for c in annotation])\n    note_dur = np.array([c[1] for c in annotation])\n    pitch_all = np.array([c[2] for c in annotation])\n    if pitch_set is None:\n        pitch_set = np.unique(pitch_all)\n    R = len(pitch_set)\n    H = np.zeros((2*R, N))\n    for i in range(len(note_start)):\n        start_idx = max(0, int((note_start[i] - tol_note[0]) / frame_res))\n        end_idx = min(N, int((note_start[i] + note_dur[i] + tol_note[1]) / frame_res))\n        start_onset_idx = max(0, int((note_start[i] - tol_onset[0]) / frame_res))\n        end_onset_idx = min(N, int((note_start[i] + tol_onset[1]) / frame_res))\n        pitch_idx = np.argwhere(pitch_set == pitch_all[i])\n        H[2*pitch_idx, start_onset_idx:end_onset_idx] = 1\n        H[2*pitch_idx+1, start_idx:end_idx] = 1\n    label_pitch = np.zeros(2*len(pitch_set),  dtype=int)\n    for k in range(len(pitch_set)):\n        label_pitch[2*k] = pitch_set[k]\n        label_pitch[2*k+1] = pitch_set[k]\n    return H, pitch_set, label_pitch\n\n\nH_init, pitch_set, label_pitch = init_nmf_activation_score_onset(N, annotation, frame_res, \n                                                    tol_note=[0.1, 0.5], tol_onset=[0.2, 0.1])\nW_init = init_nmf_template_pitch_onset(K, pitch_set, freq_res, tol_pitch=0.05)\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, 2*R, W=W_init, H=H_init, L=100, norm=True)\nplot_nmf_factors(W_init, H_init, W_init.dot(H_init), Fs, N_fft, H_fft, freq_max, label_pitch=label_pitch )\nplot_nmf_factors(W, H, W.dot(H), Fs, N_fft, H_fft, freq_max, label_pitch=label_pitch )\n\n\n\n\n\n\n\n\n그림과 같이 학습된 고조파(harmonic) 템플릿은 이전의 분해에 비해 훨씬 깔끔한 고조파 구조를 가진다. 그 이유는 대부분의 트랜지언트 광대역 에너지가 이제 온셋 템플릿에 의해 포착되기 때문이다.\n또한 대부분의 온셋 템플릿에 대해 음 이벤트 시작 시 충동의(impulse-like) 활성화 패턴을 관찰할 수 있다. 이러한 템플릿이 실제로 온셋을 나타냄을 알 수 있다. 온셋 템플릿을 자세히 살펴보면 다양한 스펙트럼 계수에 걸쳐 에너지의 확산을 관찰할 수 있다. 종종 확산(spread)은 기본 주파수와 첫 번째 고조파 주변 지역에 집중된다.\n\n결론\n\n학습 과정 내내 \\(W^{(0)}\\) 및 \\(H^{(0)}\\)에서 0으로 유지되는 항목이 없다는 점에서 도입된 제약 조건은 강(hard)하다. 따라서 잠재적인 동기화 부정확성을 설명하고 어느 정도의 유연성을 유지하려면 다소 관대한 제약 조건 영역을 사용해야 한다.\n특정 계수를 0으로 설정하여 엄격한 제약 조건을 사용하면 표준 NMF에서와 정확히 동일한 곱셈(multiplicative) 업데이트 규칙을 사용할 수 있다. 따라서 제한된 절차는 원래 접근 방식의 구현 용이성(ease of implementation) 및 계산 효율성(computationcal efficiency)을 상속한다. 이것은 NMF를 사용하는 가장 유리한 측면 중 하나이다."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-스펙트로그램-분해",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-스펙트로그램-분해",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "악보 정보 스펙트로그램 분해",
    "text": "악보 정보 스펙트로그램 분해\n\nNMF 알고리즘에 대한 입력으로 로그적으로 압축된 크기 스펙트로그램을 사용한다. 또한 악보 정보 (score-informed) 템플릿 및 활성화 제약 조건(activation constraints)이 있는 온셋 모델을 사용한다.\n다음 코드 셀에서는 악보 기반 스펙트로그램 분해 결과를 보여준다.\n\n\nfn_wav = \"FMP_C8_F27_Chopin_Op028-04_minor.wav\"\n\nfn_ann = 'FMP_C8_Audio_Chopin_Op028-04_SMD_beginning.csv'\n\nannotation = csv_to_list(path_data+fn_ann)\n\n# Input Spectrogram\nx, Fs = librosa.load(path_data+fn_wav)\nN_fft = 2048\nH_fft = 1024\nX = librosa.stft(x, n_fft=N_fft, hop_length=H_fft)\nV = np.log(1 + np.abs(X))\n#V = np.abs(X)\nfreq_res = Fs / N_fft\nfreq_max = 2000\nframe_res = H_fft/Fs\n\n\n# NMF decomposition using onset model\nK = V.shape[0]\nN = V.shape[1]\npitch_set = pitch_from_annotation(annotation)\nR = pitch_set.shape[0]\n\nH_init, pitch_set, label_pitch = init_nmf_activation_score_onset(N, annotation, frame_res, \n                                                    tol_note=[0.1, 0.5], tol_onset=[0.2, 0.1])\nW_init = init_nmf_template_pitch_onset(K, pitch_set, freq_res, tol_pitch=0.05)\n\nW, H, V_approx, V_approx_err, H_W_error = nmf(V, 2*R, W=W_init, H=H_init, L=100, norm=True)\nplot_nmf_factors(W, H, W.dot(H), Fs, N_fft, H_fft, freq_max, label_pitch=label_pitch, \n                           title_W=r'$W$', title_H=r'$H$', title_V=r'$WH$')"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-활성화-행렬-분할",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#악보-정보-활성화-행렬-분할",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "악보 정보 활성화 행렬 분할",
    "text": "악보 정보 활성화 행렬 분할\n\n\\(H\\)의 활성화 제약 조건과 악보의 음표 사이의 매핑을 사용하여 \\(H\\)를 왼손과 오른손 각각의 활성화인 두 개의 행렬 \\(H^\\mathrm{L}\\) 및 \\(H^\\mathrm{R}\\)로 분할할 수 있다. 이 두 행렬을 템플릿 행렬 \\(W\\)와 곱하면 \\(WH^\\mathrm{L}\\) 및 \\(WH^\\mathrm{R}\\) 두 행렬을 얻는다. 직관적으로 이 두 행렬은 각각 왼손과 오른손에 대해 원하는 구성 요소에 대한 추정 크기 STFT로 간주할 수 있다. 다음 코드 셀에서 활성화 매트릭스의 악보 기반 분할 구현을 제공한다.\n\n\ndef split_annotation_lh_rh(ann):\n    \"\"\"Splitting of the annotation data in left and right hand\n\n    Args:\n        ann (list): Annotation data\n\n    Returns:\n        ann_lh (list): Annotation data for left hand\n        ann_rh (list): Annotation data for right hand\n    \"\"\"\n    ann_lh = []\n    ann_rh = []\n    for a in ann:\n        if a[4] == 'lh':\n            ann_lh = ann_lh + [a]\n        if a[4] == 'rh':\n            ann_rh = ann_rh + [a]\n    return ann_lh, ann_rh\n\n\nannotation_lh, annotation_rh = split_annotation_lh_rh(annotation)\n\nH_init_L, pitch_set_L, label_pitch_L = init_nmf_activation_score_onset(N, annotation_lh, frame_res, \n                                            tol_note=[0.1, 0.5], tol_onset=[0.2, 0.1], pitch_set=pitch_set)\nH_init_R, pitch_set_R, label_pitch_R = init_nmf_activation_score_onset(N, annotation_rh, frame_res, \n                                            tol_note=[0.1, 0.5], tol_onset=[0.2, 0.1], pitch_set=pitch_set)\n\nH_L = H * H_init_L\nH_R = H * H_init_R\n\nplot_nmf_factors(W, H_R, W.dot(H_R), Fs, N_fft, H_fft, freq_max, label_pitch=label_pitch, \n                           title_W=r'$W$', title_H=r'$H^R$', title_V=r'$WH^R$')\nplot_nmf_factors(W, H_L, W.dot(H_L), Fs, N_fft, H_fft, freq_max, label_pitch=label_pitch, \n                           title_W=r'$W$', title_H=r'$H^L$', title_V=r'$WH^L$')"
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#스펙트럼-마스킹-spectral-masking",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#스펙트럼-마스킹-spectral-masking",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "스펙트럼 마스킹 (Spectral Masking)",
    "text": "스펙트럼 마스킹 (Spectral Masking)\n\n시간 영역 신호를 얻기 위한 첫 번째 아이디어는 단순히 원래 STFT \\(\\mathcal{X}\\)의 위상(phase) 정보를 사용하고 신호 재구성(signal reconstruction) 방법을 적용하여 수정된 STFT 결과를 역으로 구하는 것이다.\n그러나 NMF 기반 모델은 일반적으로 원래 크기 스펙트로그램(또는 대수적으로 압축된 버전)의 대략적인 근사값만 생성하므로 스펙트럼 뉘앙스 또는 실제 크기가 잘 캡처되지 않을 수 있다. 따라서 이러한 방식으로 재구성된 오디오 구성 요소에는 많은 부산물(artifact)이 포함될 수 있다.\n이러한 부산물 중 일부는 마스킹(masking) 기술을 사용하여 제거하거나 약화시킬 수 있다. 재구성을 위해 \\(WH^\\mathrm{L}\\) 및 \\(WH^\\mathrm{R}\\)를 직접 사용하는 대신 이러한 행렬을 사용하여 두 개의 소프트 마스크(soft mask)를 먼저 정의하는 것이 좋다. \\[M^\\mathrm{L} := (W H^\\mathrm{L})\\oslash (W H + \\varepsilon),\\] \\[M^\\mathrm{R} := (W H^\\mathrm{R})\\oslash (W H + \\varepsilon)\\] (여기서 \\(\\oslash\\) 연산자는 point-wise 나눗셈을 나타내며, 작은 양수 값 \\(\\varepsilon>0\\)을 추가하여 0으로 나눔을 방지한다.)\n그런 다음 두 개의 소프트 마스크를 원래 STFT \\(\\mathcal{X}\\)에 적용할 수 있다. 역 STFT를 사용하여 왼쪽 및 오른쪽 손의 음표에 대해 각각 원하는 두 개의 시간 영역 구성 요소 신호를 얻는다. 마스킹 기반 접근 방식을 사용하면 NMF 분해 요인에 의해 직접 캡처되지 않더라도 원본 녹음의 많은 스펙트럼 세부 정보가 보존된다. 이는 음향적으로 더 좋은 결과를 종종 낳는다. 단, 단점은 원래 오디오 데이터를 필터링함으로써 \\(WH^\\mathrm{L}\\) 및 \\(WH^\\mathrm{R}\\)에서 직접 재구성하는 것과 비교하여, 마스킹이 대상이 아닌 스펙트럼 구성 요소를 더 많이 유지할 수 있다는 것이다.\n\n\neps_machine = np.finfo(np.float32).eps\nM_L = W.dot(H_L) / (W.dot(H) + eps_machine)\nX_L = X * M_L\nx_L = librosa.istft(X_L, hop_length=H_fft, win_length=N_fft, length=x.size)\n\nM_R = W.dot(H_R) / (W.dot(H) + eps_machine)\nX_R = X * M_R\nx_R = librosa.istft(X_R, hop_length=H_fft, win_length=N_fft, length=x.size)\n\nplt.figure(figsize=(9,3))\nax = plt.subplot(1,2,1)\nplot_matrix(M_L, Fs=Fs/H_fft, Fs_F=N_fft/Fs, ax=[ax], title=r'$M^L$')\nax.set_ylim([0, freq_max]);\n\nax = plt.subplot(1,2,2)\nplot_matrix(M_R, Fs=Fs/H_fft, Fs_F=N_fft/Fs, ax=[ax], title=r'$M^R$')\nax.set_ylim([0, freq_max]);\nplt.tight_layout()\nplt.show()\n\nprint('Original signal:')\nipd.display(Audio(x, rate=Fs) )\nprint('Reconstructed signal for left hand:')\nipd.display(Audio(x_L, rate=Fs) )\nprint('Reconstructed signal for right hand:')\nipd.display(Audio(x_R, rate=Fs) )\n\n\n\n\nOriginal signal:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nReconstructed signal for left hand:\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nReconstructed signal for right hand:\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#오디오-편집",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#오디오-편집",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "오디오 편집",
    "text": "오디오 편집\n\n악보 정보의 가용성은 분리 프로세스에 활용될 수 있을 뿐만 아니라 상호 오디오 처리 응용에 대한 사용자 친화적인 액세스를 제공한다. 음별(notewise) 오디오 분해를 기반으로 동일한 조작을 주어진 오디오 녹음으로 자동 전송할 수 있다.\n\n이를 위해 원본 녹음은 조작된 음의 오디오 이벤트 및 나머지 구성 요소로 분해된다.\n이후 각 음별 오디오 이벤트의 원래 피치를 지정된 악보 조작에 따라 적절히 올리거나 내린다. 이러한 사운드 수정 기술은 피치 이동이라고도 한다.\n마지막으로 수정된 오디오 이벤트가 나머지 구성 요소에 다시 추가된다.\n\n유사한 전략을 사용하여 음의 길이나 음량을 변경하거나, 오디오 녹음에서 한 음을 완전히 제거하거나, 기존 음을 복사하고 조작하여 음을 추가할 수도 있다.\n다음의 예는 악보 기반 오디오 편집의 가능성을 보여준다. 원래 \\(\\mathrm{E}\\)-minor로 쓰여진 곡은 조표(key signature) 를 1샵에서 4샵으로 변경하여 \\(\\mathrm{E}\\)-메이저로 변환되었다. 이렇게 하면 \\(\\mathrm{C}5\\)가 \\(\\mathrm{C}^\\sharp 5\\)로, \\(\\mathrm{G}3\\)가 \\(\\mathrm{G}^\\sharp 3\\)로 이동한다.\n\n\nImage(path_img+\"FMP_C8_F27.png\", width=400)\n\n\n\n\n\nipd.display(Audio(path_data+'FMP_C8_F27_Chopin_Op028-04_minor.wav'))\nipd.display(Audio(path_data+'FMP_C8_F27_Chopin_Op028-04_major.wav'))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#nmf-오디오-모자이크",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#nmf-오디오-모자이크",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "NMF 오디오 모자이크",
    "text": "NMF 오디오 모자이크\n\nNMF가 어떻게 작동하는지 이해하기 위한 실험\n하나의 오디오 신호 \\(x_1\\)의 스펙트럼 콘텐츠와 다른 오디오 신호 \\(x_2\\)의 NMF 템포 활성화를 사용하여 오디오 신호 \\(y\\)를 합성할 것이다.\n\nStep 1: 첫번째 신호의 STFT 계산 \\(x_1\\):\n\\[X_1 \\leftarrow \\textrm{STFT}(x_1)\\]\nStep 2: 1단계에서 학습한 크기 스펙트로그램 \\(|X_1|\\)에 스펙트럼 프로필을 고정하면서 두 번째 신호 \\(x_2\\)에서 NMF를 수행하여 템포 활성화(temporal activation)를 학습한다.\n\\[H \\leftarrow \\textrm{NMF}(x_2, |X_1|)\\]\nStep 3: \\(|X_1|\\) 및 \\(H\\)를 사용하여 오디오 신호를 합성한다\n\\[y \\leftarrow \\textrm{ISTFT}(|X_1| H)\\]\nSTEP1 : Magnitude Spectrogram of Signal 1\n\nx1, sr = librosa.load('../audio/violin_c4.wav')\nipd.Audio(x1, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX1 = librosa.stft(x1)\nX1_mag, X1_phase = librosa.magphase(X1)\n\nX1_db = librosa.amplitude_to_db(X1_mag)\nplt.figure(figsize=(14, 4))\nlibrosa.display.specshow(X1_db, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x193c43b62b0>\n\n\n\n\n\nSTEP2: NMF on Signal 2\n\nx2, _ = librosa.load('../audio/58bpm.wav')\nipd.Audio(x2, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX2 = librosa.stft(x2)\nX2_mag, X2_phase = librosa.magphase(X2)\n\nX2_db = librosa.amplitude_to_db(X2_mag)\nplt.figure(figsize=(14, 4))\nlibrosa.display.specshow(X2_db, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x193c3fe8c40>\n\n\n\n\n\n\n# Cache some matrix multiplications.\nW = librosa.util.normalize(X1_mag, norm=2, axis=0)\nWTX = W.T.dot(X2_mag)\nWTW = W.T.dot(W)\n\n# Initialize H.\nH = np.random.rand(X1.shape[1], X2.shape[1])\n\n# Update H.\neps = 0.01\nfor _ in range(100):\n    H = H*(WTX + eps)/(WTW.dot(H) + eps)\n\n\nprint(H.shape)\nplt.imshow(H.T.dot(H))\nplt.show()\n\n(130, 357)\n\n\n\n\n\nStep 3: Synthesize Output Signal\n\nY_mag = W.dot(H)\n\nY_db = librosa.amplitude_to_db(Y_mag)\nplt.figure(figsize=(14, 4))\nlibrosa.display.specshow(Y_db, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x193c7885d90>\n\n\n\n\n\n\nY = Y_mag*X2_phase\ny = librosa.istft(Y)\nipd.Audio(y, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#대안-sparse-coder",
    "href": "posts/9. Musically Informed Audio Decomposition/9.3.NMF-Based_Audio_Decomposition.html#대안-sparse-coder",
    "title": "9.3. NMF 기반 오디오 분해",
    "section": "대안: sparse coder",
    "text": "대안: sparse coder\n\nfrom sklearn.decomposition import SparseCoder\n\n\nsparse_coder = SparseCoder(X1_mag.T, transform_n_nonzero_coefs=1)\nH = sparse_coder.transform(X2_mag.T)\n\nprint(H.shape)\nplt.imshow(H.T.dot(H))\nplt.show()\n\n(357, 130)\n\n\n\n\n\n\nY_mag = W.dot(H.T)\nY_db = librosa.amplitude_to_db(Y_mag)\nplt.figure(figsize=(14, 4))\nlibrosa.display.specshow(Y_db, sr=sr, x_axis='time', y_axis='log')\nplt.colorbar()\n\n<matplotlib.colorbar.Colorbar at 0x193c5a33c40>\n\n\n\n\n\n\nY = Y_mag*X2_phase\ny = librosa.istft(Y)\nipd.Audio(y, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C8/C8S3_NMFbasic.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C8/C8S3_NMFSpecFac.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C8/C8S3_NMFAudioDecomp.html\n\n\n구글 Colab 링크"
  }
]