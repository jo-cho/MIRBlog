[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\n음악정보검색(Music Information Retrieval)에 대한 포스트를 올리는 개인 블로그입니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "조조의 음악정보검색 블로그",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\n2.3. 오디오 표현 (Audio Representation)\n\n\n\n\n\n\n\n음악표현\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nCheonghyo Cho\n\n\n\n\n\n\n  \n\n\n\n\n2.1. 악보 (Sheet Music)\n\n\n\n\n\n\n\n음악표현\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nCheonghyo Cho\n\n\n\n\n\n\n  \n\n\n\n\n2.2. 기호 표현 (Symbolic Representation)\n\n\n\n\n\n\n\n음악표현\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nCheonghyo Cho\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html",
    "title": "2.1. 악보 (Sheet Music)",
    "section": "",
    "text": "음악의 표현 방법 중 악보(sheet)와 기보법(notation), 음(note), 피치(pitch), 크로마(chroma) 등에 대해 다룬다."
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#악보",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#악보",
    "title": "2.1. 악보 (Sheet Music)",
    "section": "악보",
    "text": "악보\nFull Score (전체 악보) - 위에서부터 악기별로 악보가 정렬되어 있다.\n\nImage(\"../img/2.music_representation/FMP_C1_F10.png\", width=400, height=400)\n\n\n\n\n\n예전에는 고품질의 표기를 그리는 것이 중요했으며, 이는 “music engraving”이라고 불렸다.\n하지만 요즘은 컴퓨터 소프트웨어가 악보를 그릴 수 있다. 아래는 위의 악보를 컴퓨터가 똑같이 제작한 버전의 악보이다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F10_Beethoven_Fifth-MM1-21_Sibelius-Orchestra.png\", width=500)"
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#기보법-music-notation",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#기보법-music-notation",
    "title": "2.1. 악보 (Sheet Music)",
    "section": "기보법 (Music Notation)",
    "text": "기보법 (Music Notation)\n\n오선보(staff)는 5개의 수평선들과 네 개의 공백의 집합으로, 각기 다른 음 높낮이를 표현한다.\n5선 만으로는 음의 높이를 알 수 없다. 따라서, 음의 자리를 정해주는 음자리표(clef)를 5선의 맨 앞에 그려 넣는데, 이렇게 음자리표까지 그려져 음의 자리가 정해져야 비로소 보표가 된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F04.png\", width=500)\n\n\n\n\n\n조표(key signature)란 악보에서 음자리표와 박자표 사이에 붙는 올림표나 내림표를 말하며, 음표 앞에 표기하는 임시표와는 달리 보통의 음표보다 반음이 지속적으로 높거나 낮은 상태를 나타내기 위해 사용된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F05.png\", width=500)\n\n\n\n\n\n악보는 음표(note), 쉼표(rest)로 형성되어 있다. (음표에 대한 자세한 설명은 생략한다.)\n\n\nImage(\"../img/2.music_representation/FMP_C1_F07.png\", width=500)\n\n\n\n\n\n박자표(time signature)는 악곡의 박자 종류를 가리킨다. 박자표는 모두 분수의 꼴로 쓴다\n\n\nImage(\"../img/2.music_representation/FMP_C1_F06.png\", width=500)\n\n\n\n\n\n여러 오선을 합쳐 staff system을 만들 수 있다. 다양한 악기를 동시에 연주할 때 사용된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F08.png\", width=500)\n\n\n\n\n\n템포, 다이나믹, 표현 등을 위한 설명으로 아티큘레이션(articulation)을 쓸 수 있다. 아래의 그림에 나와있다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F09.png\", width=500)"
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#음과-피치",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#음과-피치",
    "title": "2.1. 악보 (Sheet Music)",
    "section": "음과 피치",
    "text": "음과 피치\n\n피치(=음고, 음높낮이)(pitch)란 음(note)이 얼마나 높은지 낮은지를 다루는 속성이다. 피치는 음파의 기본 주파수(fundamental frequency)와 긴밀히 연관되어 있다.\n옥타브(ocatve)는 두 음의 간격을 의미하는데, 한 옥타브 높은 음은 낮은 음은 두배의 기본 주파수이다. 예를 들어 440Hz의 A와 880Hz의 A는 한 옥타브를 사이에 두고 나눠진다.\n피치 클래스(pitch class)란 옥타브를 간격으로 있는 모든 음의 집합이다. 예를 들어 C {…, C1, C2, …}는 하나의 피치 클래스, D {…, D1, D2, …}는 또다른 피치 클래스이다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_PitchClassC.png\", width=500)\n\n\n\n\n\nimport numpy as np\n\ndef generate_sinusoid_pitches(pitches=[69], dur=0.5, Fs=4000, amp=0.25):\n    \"\"\"Generation of sinusoids for a given list of MIDI pitches\n\n    Args:\n        pitches (list): List of MIDI pitches (Default value = [69])\n        dur (float): Duration (in seconds) of each sinusoid (Default value = 0.5)\n        Fs (scalar): Sampling rate (Default value = 4000)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = []\n    for p in pitches:\n        freq = 2 ** ((p - 69) / 12) * 440\n        x = np.append(x, np.sin(2 * np.pi * freq * t))\n    x = amp * x / np.max(x)\n    return x, t\n\n\nFs = 22050\n\npitches = [36,48,60,72,84,96,108]\nx, t = generate_sinusoid_pitches(pitches=pitches, Fs=Fs)\nprint('Pitch class C = {..., C1, C2, C3, C4, C5, C6, C7, ...}', flush=True)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nPitch class C = {..., C1, C2, C3, C4, C5, C6, C7, ...}\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n음계(scale)는 음악에서 피치(pitch) 순서로 된 음의 집합을 말한다. 악곡을 주로 구성하는 음을 나타내며, 음계의 종류에 따라 곡의 분위기가 달라진다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_MusicalScales.png\", width=500)\n\n\n\n\n\ndur = 0.5\nFs = 22050\n\nx_maj, t = generate_sinusoid_pitches(pitches=[60,62,64,65,67,69,71,72], Fs=Fs)\nx_min, t = generate_sinusoid_pitches(pitches=[60,62,63,65,67,68,70,72], Fs=Fs)\n\nprint('C major scale', flush=True)\nipd.display(ipd.Audio(data=x_maj, rate=Fs))\nprint('C minor scale', flush=True)\nipd.display(ipd.Audio(data=x_min, rate=Fs))\n\nC major scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nC minor scale\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n평균율(equal temperament)이란 한 옥타브를 12개의 동일한 음계 단계로 나눈 것을 의미한다.\n두 연속된 음계 사이의 차이를 반음(semitone)이라고 하는데, 이는 12음 음계의 가장 작은 간격이다. 음악인들은 이를 ’half-step’이라고도 말한다.\n12음 평균율 음계에는 12개의 피치 클래스가 있다. 서양 음악 표기법에서 이러한 피치 클래스는 알파벳과 임시표(accidental)를 결합하여 표시된다. 7개의 피치 클래스(C 장조에 해당)는 문자 C, D, E, F, G, A 및 B로 표시된다. 이러한 피치 클래스는 피아노 건반의 흰색 건반에 해당된다. 나머지 5개의 피치 등급은 피아노 건반의 검은 건반에 해당하며 알파벳과 임시표(♯ ,♭)의 조합으로 표시된다. 샵(♯)은 음을 반음 올리고 플랫(♭)은 반음 내린 것으로 음 이름 뒤에 표시된다: C♯, D♯, F♯, G♯, A♯ 혹은 D♭, E♭, G♭, A♭, B♭. 이 때 C♯과 D♭는 같은 피치 클래스를 나타낸다. 이는 “enharmonic equivalence”로도 알려져 있다.\n\n과학적 피치 표기\n\n12음 평균율의 음에 이름을 지정하기 위해 피치 클래스를 표시하는 것 외에도 옥타브에 대한 식별자가 필요하다. 과학적 피치 표기법에 따라 각 음은 피치 클래스 이름과 옥타브를 나타내는 숫자로 지정된다. 음 A4는 440Hz의 기본 주파수를 갖는 것으로 결정되어 기준 역할을 한다. 옥타브 수는 피치 클래스 B의 음에서 피치 클래스 C의 음으로 올라갈 때 1씩 증가한다.\n다음 그림은 C3에서 C5까지의 건반과 서양 음악 표기법을 사용하는 해당 음표가 있는 피아노 건반 부분을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F02.png\", width=500)\n\n\n\n\n\ndur = 0.2\nFs = 22050\npitches = range(48,73)\n\nx_chromatic, t = generate_sinusoid_pitches(pitches=pitches, dur=dur, Fs=Fs)\n\nprint('Sinusoidal sonification of the chromatic scale ranging from C3 (p=48) to C5 (p=72):', flush=True)\nipd.display(ipd.Audio(data=x_chromatic, rate=Fs))\n\nSinusoidal sonification of the chromatic scale ranging from C3 (p=48) to C5 (p=72):\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.1.Sheet_Music.html#크로마chroma와-셰퍼드-톤shepard-tones",
    "href": "posts/2. Music Representation/2.1.Sheet_Music.html#크로마chroma와-셰퍼드-톤shepard-tones",
    "title": "2.1. 악보 (Sheet Music)",
    "section": "크로마(Chroma)와 셰퍼드 톤(Shepard Tones)",
    "text": "크로마(Chroma)와 셰퍼드 톤(Shepard Tones)\n크로마란?\n\n피치에 따라 평균율 음계의 모든 음을 순서대로 배열하면, 음계의 모든 음이 같은 간격으로 배열된 평균율의 크로마틱 음계(chromatic scale)를 얻을 수 있다.\n“Chromatic”이라는 용어는 색을 의미하는 그리스어 “chroma”에서 유래했다.\n음악적 맥락에서 크로마(chroma)라는 용어는 12개의 다른 피치 클래시와 밀접한 관련이 있다. 예를 들어, C2와 C5 음은 모두 같은 크로마 값 C를 가지고 있다.\n즉, 크로마 값이 같은 모든 음은 동일한 피치 클래스에 속한다.\n같은 피치클래스에 속하거나 크로마 값이 같은 음은 유사하게 인식된다. 반면에, 다른 피치 클래스에 속하거나 다른 크로마 값을 갖는 음은 서로 다른 것으로 인식된다.\n크로마 값의 주기적 특성은 아래 그림과 같이 크로마 원에 의해 설명된다.\n이 개념을 확장하면, 로저 셰퍼드(1929)의 이름을 딴 셰퍼드의 피치 나선(Shepard’s helix of pitch)은 선형 피치 공간을 하나의 수직선을 따라 옥타브 관련 피치가 놓이도록 원통을 감싸고 있는 나선으로 표현한다. 실린더가 수평면에 투영되면 크로마원이 생성된다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F03.png\", width=500)\n\n\n\n\n셰퍼드 톤\n\nShepard의 피치 나선은 Shepard 톤을 사용하여 음향화할 수 있으며, 각 톤은 옥타브로 구분된 사인파의 가중 중첩이다.\n반음계 위로 올라가는 이 음조를 연주할 때, 계속해서 위로 올라가는 음조의 청각적 환영을 얻는다(펜로즈 계단의 시각적 착시와 유사; 아래 그림).\n\n\nImage(\"../img/2.music_representation/FMP_C1_PenroseStairs.png\", width=200)\n\n\n\n\n\n뒤의 코드 예시에서 인간이 들을 수 있는 사인파 (20~20000헤르츠의 주파수)만 사용해보자. 특정 가중은 사용되지 않는다(모든 사인파는 1의 크기를 가짐).\n마지막으로 셰퍼드 톤은 크로마틱 스케일로 C3 (MIDI pitch 48) 부터 C5 (MIDI pitch 72)까지로 생성된다.\n\n\ndef generate_shepard_tone(freq=440, dur=0.5, Fs=44100, amp=1):\n    \"\"\"Generate Shepard tone\n\n    Args:\n        freq (float): Frequency of Shepard tone (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 0.5)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Shepard tone\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    num_sin = 1\n    x = np.sin(2 * np.pi * freq * t)\n    freq_lower = freq / 2\n    while freq_lower > 20:\n        num_sin += 1\n        x = x + np.sin(2 * np.pi * freq_lower * t)\n        freq_lower = freq_lower / 2\n    freq_upper = freq * 2\n    while freq_upper < 20000:\n        num_sin += 1\n        x = x + np.sin(2 * np.pi * freq_upper * t)\n        freq_upper = freq_upper * 2\n    x = x / num_sin\n    x = amp * x / np.max(x)\n    return x, t\n\ndef f_pitch(p):\n    F_A4 = 440\n    return F_A4 * 2 ** ((p - 69) / 12)\n    \nFs = 44100\ndur = 0.5\n\npitch_start = 48\npitch_end = 72\nscale = []\nfor p in range(pitch_start, pitch_end + 1):\n    freq = f_pitch(p)    \n    s, t = generate_shepard_tone(freq=freq, dur=dur, Fs=Fs, amp = 0.5)\n    scale = np.concatenate((scale, s))\n    \nipd.display(ipd.Audio(scale, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nShepard–Risset Glissando\n\n이산적인 스케일을 사용하는대신 연속적인 셰퍼든 톤의 등락을 생성할 수 있다.: (Shepard–Risset glissando)\n뒤의 코드 예시는 상승하는 glissando를 생성한다.\n첫 째로 기하급수적으로 상승하는 chirp 신호가 정의된다. 이때 순간 주파수(instantaneous frequency)는 정현파 변수의 미분으로 주어진다.\n생성된 chirp 신호는 정확히 1옥타브를 커버한다. 그런 다음 Shepared 톤과 유사하게, 옥타브로 분리된 처프의 중첩(superposition)이 생성된다. 한 옥타브를 커버하고 Shepard–Risset glissando의 끝 부분은 (지각적으로) 시작 부분과 일치한다. 따라서 여러 glissando를 연결하여 논스톱 버전을 얻는다.\n\n\ndef generate_chirp_exp_octave(freq_start=440, dur=8, Fs=44100, amp=1):\n    \"\"\"Generate one octave of a chirp with exponential frequency increase\n\n    Args:\n        freq_start (float): Start frequency of chirp (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n\n    Returns:\n        x (np.ndarray): Chirp signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = np.sin(2 * np.pi * freq_start * np.power(2, t / dur) / np.log(2) * dur)\n    x = amp * x / np.max(x)\n    return x, t\n\n\ndef generate_shepard_glissando(num_octaves=3, dur_octave=8, Fs=44100):\n    \"\"\"Generate several ocatves of a Shepared glissando\n\n    Args:\n        num_octaves (int): Number of octaves (Default value = 3)\n        dur_octave (int): Duration (in seconds) per octave (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n\n    Returns:\n        x (np.ndarray): Shepared glissando\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    freqs_start = 10 * 2**np.arange(0, 11)\n    # Generate Shepard glissando by superimposing chirps that differ by octaves\n    for freq in freqs_start:\n        if freq == 10:\n            x, t = generate_chirp_exp_octave(freq_start=freq, dur=dur_octave, Fs=Fs, amp=1)\n        else:\n            chirp, t = generate_chirp_exp_octave(freq_start=freq, dur=dur_octave, Fs=Fs, amp=1)\n            x = x + chirp\n    x = x / len(freqs_start)\n    # Concatenate several octaves\n    x = np.tile(x, num_octaves)\n    N = len(x)\n    t = np.arange(N) / Fs\n    return x, t\n    \nglissando, t = generate_shepard_glissando(num_octaves=3, dur_octave=8)\nipd.display(ipd.Audio(glissando, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_SheetMusic.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_MusicalNotesPitches.html\nhttps://www.audiolabs-erlangen.de/resources/MIR/FMP/C1/C1S1_ChromaShepard.html\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html",
    "title": "2.2. 기호 표현 (Symbolic Representation)",
    "section": "",
    "text": "음악의 표현 방법 중 기호(심볼릭) 표현에 대해 알아본다. 피아노-롤(piano-roll), 미디(MIDI) 등이 있다."
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#피아노-롤piano-roll-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#피아노-롤piano-roll-표현",
    "title": "2.2. 기호 표현 (Symbolic Representation)",
    "section": "피아노-롤(piano-roll) 표현",
    "text": "피아노-롤(piano-roll) 표현\n\n피아노-롤은 피아노와 관련된 음 정보들을 모아 가시화한 것을 일반적으로 말한다.\n드뷔시와 베토벤 음악의 피아노롤을 아래 영상과 같이 표현할 수 있다.\n\n\nipd.display( ipd.YouTubeVideo(\"LlvUepMa31o\", start=15) )\n\n\n        \n        \n\n\n\nipd.display( ipd.YouTubeVideo(\"Kri2jWr08S4\", start=11) )"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#미디-midi-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#미디-midi-표현",
    "title": "2.2. 기호 표현 (Symbolic Representation)",
    "section": "미디 (MIDI) 표현",
    "text": "미디 (MIDI) 표현\n\n또다른 기호 표현으로는 MIDI(Musical Instrument Digital Interface) 스탠다드가 있다. MIDI는 1980년대 초반 전자 음악 악기 시장의 급성장과 함께 출현했다.\nMIDI 메시지는 음(note) 온셋, 음 오프셋, 강도(intensity or “velocity”)와 같은 정보를 인코딩한다. 컴퓨터에서 MIDI 파일은 MIDI 메시지들과 다른 메타데이터를 보관한다.\nMIDI 노트넘버(MIDI note number)는 0과 127 사이의 정수로 노트의 피치를 인코딩한다. 가장 중요한 것으로는 C4(중간 C)는 MIDI 노트넘버 60이고, A4(concert A440)은 MIDI 노트넘버 69이다. MIDI 노트넘버는 12개로 나누어져있으며 한 옥타브씩 나누어진다 (e.g. 72 = C5, 84 = C6, etc.)\n\n\nImage(\"../img/2.music_representation/FMP_C1_MIDI-NoteNumbers.png\", width=500)\n\n\n\n\n\n키 벨로시티(key velocity)는 0과 127 사이의 정수로 소리의 강도를 조정한다.\nMIDI 채널은 0과 15 사이의 정수로 신디사이저가 특정 악기를 사용하도록 안내한다.\nMIDI는 사분음표를 clock pulses 또는 틱으로 세분화한다. 예를 들어, 분기 음 당 펄스 수(PPQN)를 120으로 정의하면 60개의 틱이 8번째 음의 길이를 나타낸다.\n또한 MIDI는 템포를 BPM으로 인코딩하여 절대적인 시간 정보를 알려준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F13.png\", width=600)\n\n\n\n\n\nmidi_data = pretty_midi.PrettyMIDI(\"../data_FMP/FMP_C1_F01_Beethoven_FateMotive_Sibelius-Tracks.mid\")\nmidi_list = []\n\nfor instrument in midi_data.instruments:\n    for note in instrument.notes:\n        start = note.start\n        end = note.end\n        pitch = note.pitch\n        velocity = note.velocity\n        midi_list.append([start, end, pitch, velocity, instrument.name])\n        \nmidi_list = sorted(midi_list, key=lambda x: (x[0], x[2]))\n\ndf = pd.DataFrame(midi_list, columns=['Start', 'End', 'Pitch', 'Velocity', 'Instrument'])\nhtml = df.to_html(index=False)\nipd.HTML(html)\n\n\n\n  \n    \n      Start\n      End\n      Pitch\n      Velocity\n      Instrument\n    \n  \n  \n    \n      0.25\n      0.50\n      43\n      113\n      Piano\n    \n    \n      0.25\n      0.50\n      55\n      76\n      Piano\n    \n    \n      0.25\n      0.50\n      67\n      76\n      Piano\n    \n    \n      0.50\n      0.75\n      43\n      113\n      Piano\n    \n    \n      0.50\n      0.75\n      55\n      76\n      Piano\n    \n    \n      0.50\n      0.75\n      67\n      76\n      Piano\n    \n    \n      0.75\n      1.00\n      43\n      113\n      Piano\n    \n    \n      0.75\n      1.00\n      55\n      76\n      Piano\n    \n    \n      0.75\n      1.00\n      67\n      76\n      Piano\n    \n    \n      1.00\n      2.00\n      39\n      126\n      Piano\n    \n    \n      1.00\n      2.00\n      51\n      126\n      Piano\n    \n    \n      1.00\n      2.00\n      63\n      70\n      Piano\n    \n    \n      2.25\n      2.50\n      41\n      113\n      Piano\n    \n    \n      2.25\n      2.50\n      53\n      76\n      Piano\n    \n    \n      2.25\n      2.50\n      65\n      76\n      Piano\n    \n    \n      2.50\n      2.75\n      41\n      113\n      Piano\n    \n    \n      2.50\n      2.75\n      53\n      76\n      Piano\n    \n    \n      2.50\n      2.75\n      65\n      76\n      Piano\n    \n    \n      2.75\n      3.00\n      41\n      113\n      Piano\n    \n    \n      2.75\n      3.00\n      53\n      76\n      Piano\n    \n    \n      2.75\n      3.00\n      65\n      76\n      Piano\n    \n    \n      3.00\n      5.00\n      38\n      112\n      Piano\n    \n    \n      3.00\n      5.00\n      50\n      126\n      Piano\n    \n    \n      3.00\n      5.00\n      62\n      71\n      Piano\n    \n  \n\n\n\n\nFs = 22050\naudio_data = midi_data.synthesize(fs=Fs)\nipd.Audio(audio_data, rate=Fs)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef midi_to_list(midi):\n    \"\"\"Convert a midi file to a list of note events\n\n    Args:\n        midi (str or pretty_midi.pretty_midi.PrettyMIDI): Either a path to a midi file or PrettyMIDI object\n\n    Returns:\n        score (list): A list of note events where each note is specified as\n            ``[start, duration, pitch, velocity, label]``\n    \"\"\"\n\n    if isinstance(midi, str):\n        midi_data = pretty_midi.pretty_midi.PrettyMIDI(midi)\n    elif isinstance(midi, pretty_midi.pretty_midi.PrettyMIDI):\n        midi_data = midi\n    else:\n        raise RuntimeError('midi must be a path to a midi file or pretty_midi.PrettyMIDI')\n\n    score = []\n\n    for instrument in midi_data.instruments:\n        for note in instrument.notes:\n            start = note.start\n            duration = note.end - start\n            pitch = note.pitch\n            velocity = note.velocity / 128.\n            score.append([start, duration, pitch, velocity, instrument.name])\n    return score\n\n\ndef visualize_piano_roll(score, xlabel='Time (seconds)', ylabel='Pitch', colors='FMP_1', velocity_alpha=False,\n                         figsize=(12, 4), ax=None, dpi=72):\n    \"\"\"Plot a pianoroll visualization\n    Args:\n        score: List of note events\n        xlabel: Label for x axis (Default value = 'Time (seconds)')\n        ylabel: Label for y axis (Default value = 'Pitch')\n        colors: Several options: 1. string of FMP_COLORMAPS, 2. string of matplotlib colormap,\n            3. list or np.ndarray of matplotlib color specifications,\n            4. dict that assigns labels  to colors (Default value = 'FMP_1')\n        velocity_alpha: Use the velocity value for the alpha value of the corresponding rectangle\n            (Default value = False)\n        figsize: Width, height in inches (Default value = (12)\n        ax: The Axes instance to plot on (Default value = None)\n        dpi: Dots per inch (Default value = 72)\n    Returns:\n        fig: The created matplotlib figure or None if ax was given.\n        ax: The used axes\n    \"\"\"\n    fig = None\n    if ax is None:\n        fig = plt.figure(figsize=figsize, dpi=dpi)\n        ax = plt.subplot(1, 1, 1)\n\n    labels_set = sorted(set([note[4] for note in score]))\n    colors = color_argument_to_dict(colors, labels_set)\n\n    pitch_min = min(note[2] for note in score)\n    pitch_max = max(note[2] for note in score)\n    time_min = min(note[0] for note in score)\n    time_max = max(note[0] + note[1] for note in score)\n\n    for start, duration, pitch, velocity, label in score:\n        if velocity_alpha is False:\n            velocity = None\n        rect = patches.Rectangle((start, pitch - 0.5), duration, 1, linewidth=1,\n                                 edgecolor='k', facecolor=colors[label], alpha=velocity)\n        ax.add_patch(rect)\n\n    ax.set_ylim([pitch_min - 1.5, pitch_max + 1.5])\n    ax.set_xlim([min(time_min, 0), time_max + 0.5])\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.grid()\n    ax.set_axisbelow(True)\n    ax.legend([patches.Patch(linewidth=1, edgecolor='k', facecolor=colors[key]) for key in labels_set],\n              labels_set, loc='upper right', framealpha=1)\n\n    if fig is not None:\n        plt.tight_layout()\n\n    return fig, \n\n\nscore = midi_to_list(midi_data)\nvisualize_piano_roll(score, figsize=(8, 3), velocity_alpha=True);\n\n\n\n\n\nmidi_data = pretty_midi.PrettyMIDI(\"../data_FMP/FMP_C1_F12_Bach_BWV846_Sibelius-Tracks.mid\")\nscore = midi_to_list(midi_data)\nvisualize_piano_roll(score, figsize=(8, 3), velocity_alpha=True);\n\n\n\n\n\nimport music21 as m21\n\ns = m21.converter.parse(\"../data_FMP/FMP_C1_F12_Bach_BWV846_Sibelius-Tracks.mid\")\ns.plot('pianoroll', figureSize=(10, 3))"
  },
  {
    "objectID": "posts/2. Music Representation/2.2.Symbolic_Representation.html#악보적score-표현",
    "href": "posts/2. Music Representation/2.2.Symbolic_Representation.html#악보적score-표현",
    "title": "2.2. 기호 표현 (Symbolic Representation)",
    "section": "악보적(score) 표현",
    "text": "악보적(score) 표현\n\n기호적 악보 표현은 “2.1.Sheet_Music.ipynb”에서 설명한 음악적 기호들을 인코딩한다. (음자리표, 조표 등등) 하지만 이를 악보로 가시화하는 것이 아니라 저장하는데, MusicXML같은 파일로 저장한다.\n아래 그 예시가 있다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F15.png\", width=400)\n\n\n\n\n\ndef xml_to_list(xml):\n    \"\"\"Convert a music xml file to a list of note events\n\n    Args:\n        xml (str or music21.stream.Score): Either a path to a music xml file or a music21.stream.Score\n\n    Returns:\n        score (list): A list of note events where each note is specified as\n            ``[start, duration, pitch, velocity, label]``\n    \"\"\"\n\n    if isinstance(xml, str):\n        xml_data = m21.converter.parse(xml)\n    elif isinstance(xml, m21.stream.Score):\n        xml_data = xml\n    else:\n        raise RuntimeError('midi must be a path to a midi file or music21.stream.Score')\n\n    score = []\n\n    for part in xml_data.parts:\n        instrument = part.getInstrument().instrumentName\n\n        for note in part.flat.notes:\n\n            if note.isChord:\n                start = note.offset\n                duration = note.quarterLength\n\n                for chord_note in note.pitches:\n                    pitch = chord_note.ps\n                    volume = note.volume.realized\n                    score.append([start, duration, pitch, volume, instrument])\n\n            else:\n                start = note.offset\n                duration = note.quarterLength\n                pitch = note.pitch.ps\n                volume = note.volume.realized\n                score.append([start, duration, pitch, volume, instrument])\n\n    score = sorted(score, key=lambda x: (x[0], x[2]))\n    return score\n\n\nxml_data = m21.converter.parse(\"../data_FMP/FMP_C1_F01_Beethoven_FateMotive_Sibelius-Tracks.xml\")\nxml_list = xml_to_list(xml_data)\n\ndf = pd.DataFrame(xml_list[:9], columns=['Start', 'End', 'Pitch', 'Velocity', 'Instrument'])\nhtml = df.to_html(index=False, float_format='%.2f', max_rows=8)\nipd.HTML(html)\n\n\n\n  \n    \n      Start\n      End\n      Pitch\n      Velocity\n      Instrument\n    \n  \n  \n    \n      0.50\n      0.50\n      55.00\n      0.71\n      Piano (2)\n    \n    \n      0.50\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      1.00\n      0.50\n      55.00\n      0.71\n      Piano (2)\n    \n    \n      1.00\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1.50\n      0.50\n      67.00\n      0.71\n      Piano (2)\n    \n    \n      2.00\n      2.00\n      63.00\n      0.71\n      Piano (2)\n    \n    \n      2.50\n      0.50\n      43.00\n      1.00\n      Piano (2)\n    \n    \n      3.00\n      0.50\n      43.00\n      1.00\n      Piano (2)\n    \n  \n\n\n\n\nvisualize_piano_roll(xml_list, figsize=(8, 3), velocity_alpha=True,\n                               xlabel='Time (quarter lengths)');\n\n\n\n\n\nxml_data = m21.converter.parse(\"../data_FMP/FMP_C1_F10_Beethoven_Fifth-MM1-21_Sibelius-Orchestra.xml\")\nxml_list = xml_to_list(xml_data)\n\nvisualize_piano_roll(xml_list, figsize=(10, 7), velocity_alpha=False,\n                               colors='gist_rainbow', xlabel='Time (quarter lengths)');\n\n\n\n\n기호 음악 표현법을 사용하는 파이썬 라이브러리\n\nPrettyMIDI: MIDI 읽기, 컨버팅 등\nmusic21: musicxml파일 다루기\npypianoroll: 피아노롤 비주얼\n\n\n출처:\n\nhttps://musicinformationretrieval.com/\nhttps://www.audiolabs-erlangen.de/FMP\n\n\n구글 Colab 링크"
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "",
    "text": "음악의 표현 방법 중 오디오 표현에 대해 알아본다. 음파(wave), 주파수(frequency), 고조파(harmonics), 강도(intensity), 라우드니스(loudness), 음색(timbre) 등의 중요한 개념을 포함한다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#들을-수-있는-주파수-범위",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#들을-수-있는-주파수-범위",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "들을 수 있는 주파수 범위",
    "text": "들을 수 있는 주파수 범위\n\n정현파(sinusoidal wave)의 주파수가 높을수록 더 높은 소리를 낸다.\n인간의 가청 주파수 범위는 약 20Hz와 20000Hz(20 kHz) 사이이다. 다른 동물들은 다른 청력 범위를 가지고 있다. 예를 들어, 개의 청력 범위의 상단은 약 45kHz이고 고양이의 청력은 64kHz인 반면, 박쥐는 심지어 100kHz 이상의 주파수를 감지할 수 있다. 이는 사람의 청각 능력을 뛰어넘는 초음파를 내는 개 호루라기를 이용해 주변 사람들을 방해하지 않고 동물을 훈련시키고 명령할 수 있는 이유이다.\n다음 실험에서 주파수가 초당 2배(1옥타브) 증가하는 처프(chirp) 신호를 생성한다.\n\n\ndef generate_chirp_exp_octave(freq_start=440, dur=8, Fs=44100, amp=1):\n    \"\"\"Generate one octave of a chirp with exponential frequency increase\n    Args:\n        freq_start (float): Start frequency of chirp (Default value = 440)\n        dur (float): Duration (in seconds) (Default value = 8)\n        Fs (scalar): Sampling rate (Default value = 44100)\n        amp (float): Amplitude of generated signal (Default value = 1)\n    Returns:\n        x (np.ndarray): Chirp signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    x = np.sin(2 * np.pi * freq_start * np.power(2, t / dur) / np.log(2) * dur)\n    x = amp * x / np.max(x)\n    return x, t\n\n\n# 20Hz부터 시작하여 주파수는 총 10초 동안 20480Hz까지 상승한다.\nFs = 44100\ndur = 1\nfreq_start = 20 * 2**np.arange(10)\nfor f in freq_start:\n    if f==freq_start[0]:\n        chirp, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=.25)\n    else:\n        chirp_oct, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=.25)\n        chirp = np.concatenate((chirp, chirp_oct))\n\nipd.display(ipd.Audio(chirp, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# 640Hz부터 시작하여 주파수는 총 10초 동안 20Hz까지 하락한다.\n\nFs = 8000\ndur = 2\nfreq_start = 20 * 2**np.arange(5)\nfor f in freq_start:\n    if f==freq_start[0]:\n        chirp, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=1)\n    else:\n        chirp_oct, t = generate_chirp_exp_octave(freq_start=f, dur=dur, Fs=Fs, amp=1)\n        chirp = np.concatenate((chirp,chirp_oct))    \n        \nchirp = chirp[::-1]    \nipd.display(ipd.Audio(chirp, rate=Fs))\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#피치와-중심-주파수center-frequency",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#피치와-중심-주파수center-frequency",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "피치와 중심 주파수(center frequency)",
    "text": "피치와 중심 주파수(center frequency)\n\n정현파는 음의 음향적 실현의 원형으로 간주될 수 있다. 때때로 정현파에서 나오는 소리를 하모닉 사운드(harmonic sound) 또는 순수 음색(pure tone)이라고 한다.\n주파수의 개념은 소리의 피치를 결정하는 것과 밀접한 관련이 있다. 일반적으로 피치는 소리의 주관적인 속성이다.\n복잡한 혼합음의 경우, 주파수와의 관계가 특히 모호할 수 있다. 그러나 순수 음색의 경우 주파수와 피치의 관계가 명확하다. 예를 들어, 440 Hz의 주파수를 갖는 정현파는 피치 A4에 해당한다. 이 특정한 피치는 콘서트 피치(concert pitch)로 알려져 있으며, 이는 연주를 위해 악기가 튜닝되는 기준 피치로 사용된다.\n주파수의 약간의 변화가 반드시 지각할 수 있는 변화로 이어지는 것은 아니기 때문에, 일반적으로 주파수의 전체 범위를 단일 피치와 연관시킨다.\n두 주파수가 2의 거듭제곱에 의해 차이가 나는 경우, 이는 옥타브의 개념과 연관된다.\n\n예를 들어, 피치 A3(220 Hz)와 피치 A4(440 Hz) 사이의 인식 거리는 피치 A4와 피치 A5(880 Hz) 사이의 인식 거리와 동일하다.\n즉, 피치에 대한 인간의 인식은 본질적으로 로그(logarithm)이다. 이 지각 특성은 이미 로그 주파수 축을 기준으로 옥타브를 12개의 반음으로 세분화하는 평균율(“equal-tempered scale”)을 정의하는 데 사용되었다.\n\n더 공식적으로, MIDI 노트 번호를 사용하여, 다음과 같이 정의된 중심 주파수(center frequency) \\(F_{pitch}(p)\\)(Hz 단위로 측정)를 각 피치 \\(p∈[0:127]\\) 에 연결할 수 있다.\n\n\\(F_{pitch}(p)=2^{(p−69)/12} \\cdot 440\\)\n\nMIDI 노트 번호 \\(p=69\\)는 기준으로 사용되며 피치 A4(440Hz)에 해당된다. 피치 넘버를 12(옥타브) 증가시키면 2배 증가한다. \\(F_{pitch} ( p + 12) = 2 \\cdot F_{pitch} ( p)\\)\n\n\n\n\nImage(\"../img/2.music_representation/FMP_C1_MIDI-NoteNumbers.png\", width=500)\n\n\n\n\n\ndef f_pitch(p):\n    \"\"\"Compute center frequency for (single or array of) MIDI note numbers\n    Args:\n        p (float or np.ndarray): MIDI note numbers\n    Returns:\n        freq_center (float or np.ndarray): Center frequency\n    \"\"\"\n    freq_center = 2 ** ((p - 69) / 12) * 440\n    return freq_center\n\nchroma = ['A ', 'A#', 'B ', 'C ', 'C#', 'D ', 'D#', 'E ', 'F ', 'F#', 'G ', 'G#']\n\nfor p in range(21, 109):\n    print('p = %3d (%2s%1d), freq = %7.2f ' % (p, chroma[(p-69) % 12], (p//12-1), f_pitch(p)))\n\np =  21 (A 0), freq =   27.50 \np =  22 (A#0), freq =   29.14 \np =  23 (B 0), freq =   30.87 \np =  24 (C 1), freq =   32.70 \np =  25 (C#1), freq =   34.65 \np =  26 (D 1), freq =   36.71 \np =  27 (D#1), freq =   38.89 \np =  28 (E 1), freq =   41.20 \np =  29 (F 1), freq =   43.65 \np =  30 (F#1), freq =   46.25 \np =  31 (G 1), freq =   49.00 \np =  32 (G#1), freq =   51.91 \np =  33 (A 1), freq =   55.00 \np =  34 (A#1), freq =   58.27 \np =  35 (B 1), freq =   61.74 \np =  36 (C 2), freq =   65.41 \np =  37 (C#2), freq =   69.30 \np =  38 (D 2), freq =   73.42 \np =  39 (D#2), freq =   77.78 \np =  40 (E 2), freq =   82.41 \np =  41 (F 2), freq =   87.31 \np =  42 (F#2), freq =   92.50 \np =  43 (G 2), freq =   98.00 \np =  44 (G#2), freq =  103.83 \np =  45 (A 2), freq =  110.00 \np =  46 (A#2), freq =  116.54 \np =  47 (B 2), freq =  123.47 \np =  48 (C 3), freq =  130.81 \np =  49 (C#3), freq =  138.59 \np =  50 (D 3), freq =  146.83 \np =  51 (D#3), freq =  155.56 \np =  52 (E 3), freq =  164.81 \np =  53 (F 3), freq =  174.61 \np =  54 (F#3), freq =  185.00 \np =  55 (G 3), freq =  196.00 \np =  56 (G#3), freq =  207.65 \np =  57 (A 3), freq =  220.00 \np =  58 (A#3), freq =  233.08 \np =  59 (B 3), freq =  246.94 \np =  60 (C 4), freq =  261.63 \np =  61 (C#4), freq =  277.18 \np =  62 (D 4), freq =  293.66 \np =  63 (D#4), freq =  311.13 \np =  64 (E 4), freq =  329.63 \np =  65 (F 4), freq =  349.23 \np =  66 (F#4), freq =  369.99 \np =  67 (G 4), freq =  392.00 \np =  68 (G#4), freq =  415.30 \np =  69 (A 4), freq =  440.00 \np =  70 (A#4), freq =  466.16 \np =  71 (B 4), freq =  493.88 \np =  72 (C 5), freq =  523.25 \np =  73 (C#5), freq =  554.37 \np =  74 (D 5), freq =  587.33 \np =  75 (D#5), freq =  622.25 \np =  76 (E 5), freq =  659.26 \np =  77 (F 5), freq =  698.46 \np =  78 (F#5), freq =  739.99 \np =  79 (G 5), freq =  783.99 \np =  80 (G#5), freq =  830.61 \np =  81 (A 5), freq =  880.00 \np =  82 (A#5), freq =  932.33 \np =  83 (B 5), freq =  987.77 \np =  84 (C 6), freq = 1046.50 \np =  85 (C#6), freq = 1108.73 \np =  86 (D 6), freq = 1174.66 \np =  87 (D#6), freq = 1244.51 \np =  88 (E 6), freq = 1318.51 \np =  89 (F 6), freq = 1396.91 \np =  90 (F#6), freq = 1479.98 \np =  91 (G 6), freq = 1567.98 \np =  92 (G#6), freq = 1661.22 \np =  93 (A 6), freq = 1760.00 \np =  94 (A#6), freq = 1864.66 \np =  95 (B 6), freq = 1975.53 \np =  96 (C 7), freq = 2093.00 \np =  97 (C#7), freq = 2217.46 \np =  98 (D 7), freq = 2349.32 \np =  99 (D#7), freq = 2489.02 \np = 100 (E 7), freq = 2637.02 \np = 101 (F 7), freq = 2793.83 \np = 102 (F#7), freq = 2959.96 \np = 103 (G 7), freq = 3135.96 \np = 104 (G#7), freq = 3322.44 \np = 105 (A 7), freq = 3520.00 \np = 106 (A#7), freq = 3729.31 \np = 107 (B 7), freq = 3951.07 \np = 108 (C 8), freq = 4186.01 \n\n\n\n이 공식으로부터, 두 개의 연속된 피치 p+1과 p의 주파수 비율은 일정하다.\n\n\\(F_\\mathrm{pitch}(p+1)/F_\\mathrm{pitch}(p) = 2^{1/12} \\approx 1.059463\\)\n\n반음의 개념을 일반화한 센트(cent)는 음악 간격에 사용되는 로그 단위를 나타낸다. 정의에 따라 옥타브는 \\(1200\\) 센트로 나뉘며, 각 반음은 \\(100\\)센트에 해당한다. 두 주파수(예: \\(\\omega_1\\) 및 \\(\\omega_2\\)) 사이의 센트 차이는 다음과 같다.\n\n\\(\\log_2\\left(\\frac{\\omega_1}{\\omega_2}\\right)\\cdot 1200.\\)\n\n1센트의 간격은 너무 작아서 연속된 음 사이를 지각할 수 없다. 지각할 수 있는 문턱은 사람마다 다르고 음색과 음악적 맥락과 같은 측면에 따라 달라진다.\n경험적으로 일반 성인은 25센트의 작은 피치 차이를 매우 안정적으로 인식할 수 있으며, 10센트의 차이는 훈련된 청취자만이 인식할 수 있다.\n그림에서와 같이, 기준으로 사용되는 \\(440~\\mathrm{Hz}\\)의 정현파와 다양한 차이를 가진 추가 정현파를 생성하여 본다.\n\n\ndef difference_cents(freq_1, freq_2):\n    \"\"\"Difference between two frequency values specified in cents\n\n    Args:\n        freq_1 (float): First frequency\n        freq_2 (float): Second frequency\n\n    Returns:\n        delta (float): Difference in cents\n    \"\"\"\n    delta = np.log2(freq_1 / freq_2) * 1200\n    return delta\n \ndef generate_sinusoid(dur=1, Fs=1000, amp=1, freq=1, phase=0):\n    \"\"\"Generation of sinusoid\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 1)\n        freq (float): Frequency of sinusoid (Default value = 1)\n        phase (float): Phase of sinusoid (Default value = 0)\n\n    Returns:\n        x (np.ndarray): Signal\n        t (np.ndarray): Time axis (in seconds)\n\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    x = amp * np.sin(2*np.pi*(freq*t-phase))\n    return x, t\n\n\ndur = 1\nFs = 4000\npitch = 69\nref = f_pitch(pitch)\nfreq_list = ref + np.array([0,2,5,10,ref])\nfor freq in freq_list:\n    x, t = generate_sinusoid(dur=dur, Fs=Fs, freq=freq)\n    print('freq = %0.1f Hz (MIDI note number 69 + %0.2f cents)' % (freq, difference_cents(freq,ref)))\n    ipd.display(ipd.Audio(data=x, rate=Fs))  \n\nfreq = 440.0 Hz (MIDI note number 69 + 0.00 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 442.0 Hz (MIDI note number 69 + 7.85 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 445.0 Hz (MIDI note number 69 + 19.56 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 450.0 Hz (MIDI note number 69 + 38.91 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nfreq = 880.0 Hz (MIDI note number 69 + 1200.00 cents)\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#데시벨-스케일-decibel-scale",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#데시벨-스케일-decibel-scale",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "데시벨 스케일 (Decibel Scale)",
    "text": "데시벨 스케일 (Decibel Scale)\n\n음악의 중요한 특성은 음량을 나타내는 음악 기호뿐만 아니라 음량을 나타내는 일반적인 용어인 다이나믹(dynamics)과 관련이 있다.\n물리적 관점에서 소리 힘(sound power)은 공기를 통해 모든 방향으로 흐르는 음원에 의해 단위 시간당 얼마나 많은 에너지가 방출되는지를 나타낸다.\n소리 강도/인텐시티(sound intensity)는 단위 면적당 소리 힘을 나타낸다. 실제로 소리 힘과 소리 강도는 인간 청취자와 연관된 극히 작은 값을 보여줄 수 있다.\n예를 들어, 인간이 들을 수 있는 순수 음색(pure tone)의 최소 소리 강도인 청각의 임계값(threshold of hearing, TOH)은 다음과 같이 작다.\n\n\\(I_\\mathrm{TOH}:=10^{-12}~\\mathrm{W}/\\mathrm{m}^2\\)\n\n게다가, 인간이 지각할 수 있는 강도의 범위는 \\(I_\\mathrm{TOP}:=10~\\mathrm{W}/\\mathrm{m}^2\\) (통증 임계값(threshold of pain, TOP))으로 매우 크다.\n실질적인 이유로, 힘과 강도를 표현하기 위해 로그 척도로 전환한다. 더 정확하게는 두 값 사이의 비율을 나타내는 로그 단위인 데시벨(dB) 척도를 사용한다.\n일반적으로 소리 강도의 경우 \\(I_\\mathrm{TOH}\\) 같은 값이 참조 역할을 한다.\n그런 다음 dB로 측정된 강도는 다음과 같이 정의된다.\n\n$ (I) := 10_{10}()$\n\n위의 정의에서 \\(\\mathrm{dB}(I_\\mathrm{TOH})=0\\)를 얻을 수 있고, 강도가 두배로 증가하면 대략 3dB 증가한다:\n\n\\(\\mathrm{dB}(2\\cdot I) = 10\\cdot \\log_{10}(2) + \\mathrm{dB}(I) \\approx 3 + \\mathrm{dB}(I)\\)\n\n데시벨 단위로 강도 값을 지정할 때 강도 수준(intensity levels)도 같이 언급된다.\n다음 표는 \\(\\mathrm{W}/\\mathrm{m}^2\\) 와 데시벨 단위로 몇 가지 일반적인 강도값(intensity value)을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_T01.png\", width=400)\n\n\n\n\n\n예시로 이를 보자.\n\n베토벤 5번 교항곡 시작 부분\n\n\n\nipd.Audio(\"../audio/beeth5_orch_21bars.wav\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ndef compute_power_db(x, Fs, win_len_sec=0.1, power_ref=10**(-12)):\n    \"\"\"Computation of the signal power in dB\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        Fs (scalar): Sampling rate\n        win_len_sec (float): Length (seconds) of the window (Default value = 0.1)\n        power_ref (float): Reference power level (0 dB) (Default value = 10**(-12))\n\n    Returns:\n        power_db (np.ndarray): Signal power in dB\n    \"\"\"\n    win_len = round(win_len_sec * Fs)\n    win = np.ones(win_len) / win_len\n    power_db = 10 * np.log10(np.convolve(x**2, win, mode='same') / power_ref)\n    return power_db\n\n\nFs = 22050\nx, Fs = librosa.load(\"../audio/beeth5_orch_21bars.wav\", sr=Fs, mono=True)\n\nwin_len_sec = 0.2\npower_db = compute_power_db(x, win_len_sec=win_len_sec, Fs=Fs)\n\n\nplot_signal(x, Fs, ylabel='Amplitude', color='gray')\nplot_signal(power_db, Fs, ylabel='Power (dB)', color='red')\nplt.show()"
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#라우드니스loudness",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#라우드니스loudness",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "라우드니스(Loudness)",
    "text": "라우드니스(Loudness)\n\n다이나믹과 소리 강도는 소리가 조용한 것에서 큰 것으로 확장되는 규모로 소리를 정렬할 수 있는 라우드니스(loudness)라고 불리는 지각적 특성과 관련이 있다.\n라우드니스는 주관적인 측정이며, 이는 개별 청취자(예: 나이는 소리에 대한 인간의 귀의 반응에 영향을 미치는 요인 중 하나)뿐만 아니라 지속 시간(duration) 또는 주파수와 같은 다른 소리 특성에도 영향을 미친다.\n\n예를 들어, 사람은 200ms 동안 지속되는 소리가 50ms 동안만 지속되는 유사한 소리보다 더 크게 느껴진다.\n게다가, 강도는 같지만 주파수가 다른 두 소리는 일반적으로 동일한 라우드니스로 인식되지 않는다.\n정상적인 청력을 가진 사람은 2~4kHz 정도의 소리에 가장 민감하며, 낮은 주파수뿐만 아니라 높은 주파수에서도 감도가 감소한다.\n\n정신음향(psychoacoustic) 실험을 바탕으로 주파수에 따른 순수 음색의 라우드니스는 단위 폰(unit phon)으로 결정되고 표현되어 왔다.\n다음 그림은 동일한 음량 윤곽선(equal loudness contours)을 보여준다. 각 윤곽선은 폰(phon)으로 주어진 고정된 음량에 대해 (로그로 간격을 둔) 주파수 축에 대한 소리 강도를 지정한다. 하나의 폰 단위는 1000Hz의 주파수에 대해 정규화되며, 여기서 하나의 폰 값은 dB 단위의 강도 수준과 같다. 0폰의 윤곽선은 주파수에 따라 청각 임계값(threshold of hearing)이 어떻게 달라지는지를 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F21.png\", width=400)\n\n\n\n\n\n윤곽선은 가중치 함수에 의해 대략적으로 설명될 수 있다. 다음 코드 셀에서 동일한 음량 윤곽선에 질적으로 근사하는 함수의 예를 찾을 수 있다.\n\n\ndef compute_equal_loudness_contour(freq_min=30, freq_max=15000, num_points=100):\n    \"\"\"Computation of the equal loudness contour\n\n    Args:\n        freq_min (float): Lowest frequency to be evaluated (Default value = 30)\n        freq_max (float): Highest frequency to be evaluated (Default value = 15000)\n        num_points (int): Number of evaluation points (Default value = 100)\n\n    Returns:\n        equal_loudness_contour (np.ndarray): Equal loudness contour (in dB)\n        freq_range (np.ndarray): Evaluated frequency points\n    \"\"\"\n    freq_range = np.logspace(np.log10(freq_min), np.log10(freq_max), num=num_points)\n    freq = 1000\n    # Function D from https://bar.wikipedia.org/wiki/Datei:Acoustic_weighting_curves.svg\n    h_freq = ((1037918.48 - freq**2)**2 + 1080768.16 * freq**2) / ((9837328 - freq**2)**2 + 11723776 * freq**2)\n    n_freq = (freq / (6.8966888496476 * 10**(-5))) * np.sqrt(h_freq / ((freq**2 + 79919.29) * (freq**2 + 1345600)))\n    h_freq_range = ((1037918.48 - freq_range**2)**2 + 1080768.16 * freq_range**2) / ((9837328 - freq_range**2)**2\n                                                                                     + 11723776 * freq_range**2)\n    n_freq_range = (freq_range / (6.8966888496476 * 10**(-5))) * np.sqrt(h_freq_range / ((freq_range**2 + 79919.29) *\n                                                                         (freq_range**2 + 1345600)))\n    equal_loudness_contour = 20 * np.log10(np.abs(n_freq / n_freq_range))\n    return equal_loudness_contour, freq_range\n\n\nequal_loudness_contour, freq_range = compute_equal_loudness_contour()\n\nplot_signal(equal_loudness_contour, T_coef=freq_range, figsize=(6,3), xlabel='Frequency (Hz)',\n            ylabel='Intensity (dB)', title='Equal loudness contour', color='red')\nplt.xscale('log')\nplt.grid()\nplt.show()\n\n\n\n\n\n동일 힘을 가지는 처프 신호\n\n이제 30Hz에서 시작하여 10000Hz로 끝나는, 주파수가 기하급수적으로 증가하는 차프 신호에 대한 작은 실험을 해보자.\n먼저, 전체 시간 간격에 걸쳐 동일한 강도의 차프 신호를 생성한다. 이 신호를 들을 때는 주파수가 증가함에 따라 신호가 먼저 커지고 약 4000Hz의 주파수를 지나면 다시 부드러워지는 느낌이 든다.\n\n\n\ndef generate_chirp_exp(dur, freq_start, freq_end, Fs=22050):\n    \"\"\"Generation chirp with exponential frequency increase\n\n    Args:\n        dur (float): Length (seconds) of the signal\n        freq_start (float): Start frequency of the chirp\n        freq_end (float): End frequency of the chirp\n        Fs (scalar): Sampling rate (Default value = 22050)\n\n    Returns:\n        x (np.ndarray): Generated chirp signal\n        t (np.ndarray): Time axis (in seconds)\n        freq (np.ndarray): Instant frequency (in Hz)\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    freq = np.exp(np.linspace(np.log(freq_start), np.log(freq_end), N))\n    phases = np.zeros(N)\n    for n in range(1, N):\n        phases[n] = phases[n-1] + 2 * np.pi * freq[n-1] / Fs\n    x = np.sin(phases)\n    return x, t, freq\n\n\nFs = 22050\nfreq_start = 30 \nfreq_end = 10000\ndur = 10\nx, t, freq = generate_chirp_exp(dur, freq_start, freq_end, Fs=Fs)\n\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [3, 2]}, figsize=(7, 5))\nN, H = 1024, 512\nX = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\nplot_matrix(np.log(1+np.abs(X)), Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0], ax[0,1]], \n            title='Spectrogram of chirp', colorbar=True)\n\nwin_len_sec = 0.1\npower_db = compute_power_db(x, win_len_sec=win_len_sec, Fs=Fs)\nplot_signal(power_db, Fs=Fs, ax=ax[1,0], title='Sound power level', ylabel='Power (dB)', color='red')\nax[1,0].set_ylim([103, 137])\nax[1,1].set_axis_off()\nplt.tight_layout()\nplt.show()\n\ndisplay(Audio(x, rate=Fs) )\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n동일 라우드니스를 가지는 처프 신호\n\n둘째로, 위에서 생성된 동일 라우드니스 윤곽에 따라 신호의 진폭(amplitude)을 조정한다.\n이 경우 전체 주파수 범위를 통해 스위핑할 때 결과로 발생하는 처프 신호의 라우드니스가 동일한 것으로 보인다.\n\n\n\ndef generate_chirp_exp_equal_loudness(dur, freq_start, freq_end, Fs=22050):\n    \"\"\"Generation chirp with exponential frequency increase and equal loudness\n\n    Args:\n        dur (float): Length (seconds) of the signal\n        freq_start (float): Starting frequency of the chirp\n        freq_end (float): End frequency of the chirp\n        Fs (scalar): Sampling rate (Default value = 22050)\n\n    Returns:\n        x (np.ndarray): Generated chirp signal\n        t (np.ndarray): Time axis (in seconds)\n        freq (np.ndarray): Instant frequency (in Hz)\n        intensity (np.ndarray): Instant intensity of the signal\n    \"\"\"\n    N = int(dur * Fs)\n    t = np.arange(N) / Fs\n    intensity, freq = compute_equal_loudness_contour(freq_min=freq_start, freq_max=freq_end, num_points=N)\n    amp = 10**(intensity / 20)\n    phases = np.zeros(N)\n    for n in range(1, N):\n        phases[n] = phases[n-1] + 2 * np.pi * freq[n-1] / Fs\n    x = amp * np.sin(phases)\n    return x, t, freq, intensity\n\n\nx_equal_loudness, t, freq, intensity = generate_chirp_exp_equal_loudness(dur, freq_start, freq_end, Fs=Fs)\n\nfig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [1, 0.05], \n                                          'height_ratios': [3, 2]}, figsize=(7, 5))\nN, H = 1024, 512\nX = librosa.stft(x_equal_loudness, n_fft=N, hop_length=H, win_length=N, pad_mode='constant')\nplot_matrix(np.log(1+np.abs(X)), Fs=Fs/H, Fs_F=N/Fs, ax=[ax[0,0], ax[0,1]], \n                     title='Spectrogram of chirp with equal loudness', colorbar=True)\n\nwin_len_sec = 0.1\npower_db = compute_power_db(x_equal_loudness, win_len_sec=win_len_sec, Fs=Fs)\nplot_signal(power_db, Fs=Fs, ax=ax[1,0], title='Sound power level', ylabel='Power (dB)', color='red')\nax[1,0].set_ylim([103, 137])\nax[1,1].set_axis_off()\nplt.tight_layout()\nplt.show()\n\ndisplay( Audio(x_equal_loudness, rate=Fs) )\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프와-adsr-모형envelope-and-adsr-model",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프와-adsr-모형envelope-and-adsr-model",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "엔벨로프와 ADSR 모형(Envelope and ADSR Model)",
    "text": "엔벨로프와 ADSR 모형(Envelope and ADSR Model)\n\n소리의 음색에 영향을 미치는 한 가지 소리 특성은 파형의 엔벨로프(envelope)이며, 이는 진폭에서 극단을 나타내는 매끄러운 곡선으로 간주될 수 있다.\n음향 합성에서 생성되는 신호의 엔벨로프는 어택(attack, A), 디케이(decay, D), 서스테인(sustain, S), 릴리스(release, R) 단계로 구성된 ADSR이라는 모델에 의해 종종 설명된다.\n4단계의 상대적 지속 시간과 진폭은 합성된 음색이 어떻게 들릴지에 큰 영향을 미친다.\n다음 그림은 이상적인 ADSR 모델과 피아노와 바이올린 사운드의 엔벨로프(단음 C4 재생)를 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F22a-23.png\", width=800)\n\n\n\n\n\n그림에서 알 수 있듯이, 하나의 음을 연주하면 이미 주기적인 요소뿐만 아니라 비주기적인 요소를 포함하여 시간이 지남에 따라 지속적으로 변화할 수 있는 특성을 가진 복잡한 음향 혼합물이 생성된다.\n어택(attack) 단계 동안, 소리는 보통 넓은 주파수 범위에 걸쳐 소음 같은 구성 요소로 축적된다. 소리가 시작될 때 소음과 같은 짧은 지속 시간의 소리를 과도음/트랜지언트(transient)라고 한다.\n디케이(decay) 단계 동안, 소리는 안정화되고 일정한 주기 패턴에 도달한다.\n서스테인(sustain) 단계 동안, 에너지는 꽤 일정하게 유지된다.\n릴리스(release) 단계에서는 소리가 사라진다.\n다음 코드 셀에서, 이상화된 ADSR 모델을 생성한다.\n\n\ndef compute_adsr(len_A=10, len_D=10, len_S=60, len_R=10, height_A=1.0, height_S=0.5):\n    \"\"\"Computation of idealized ADSR model\n\n    Args:\n        len_A (int): Length (samples) of A phase (Default value = 10)\n        len_D (int): Length (samples) of D phase (Default value = 10)\n        len_S (int): Length (samples) of S phase (Default value = 60)\n        len_R (int): Length (samples) of R phase (Default value = 10)\n        height_A (float): Height of A phase (Default value = 1.0)\n        height_S (float): Height of S phase (Default value = 0.5)\n\n    Returns:\n        curve_ADSR (np.ndarray): ADSR model\n    \"\"\"\n    curve_A = np.arange(len_A) * height_A / len_A\n    curve_D = height_A - np.arange(len_D) * (height_A - height_S) / len_D\n    curve_S = np.ones(len_S) * height_S\n    curve_R = height_S * (1 - np.arange(1, len_R + 1) / len_R)\n    curve_ADSR = np.concatenate((curve_A, curve_D, curve_S, curve_R))\n    return curve_ADSR\n\n\ncurve_ADSR = compute_adsr(len_A=10, len_D=10, len_S=60, len_R=10, height_A=1.0, height_S=0.5)\n\nplot_signal(curve_ADSR, figsize=(4,2.5), ylabel='Amplitude', title='ADSR model', color='red')\nplt.show()\n\ncurve_ADSR = compute_adsr(len_A=20, len_D=2, len_S=60, len_R=1, height_A=2.0, height_S=1.2)\nplot_signal(curve_ADSR, figsize=(4,2.5), ylabel='Amplitude', title='ADSR model', color='red')\nplt.show()\n\n\n\n\n\n\n\n\nADSR 모델은 단순화된 형태이며 특정 악기에서 생성되는 톤의 진폭 엔벨로프에 대한 의미 있는 근사치만 산출한다.\n예를 들어, 위와 같은 바이올린 소리는 ADSR 모델에 의해 잘 근사되지 않는다.\n\n우선 음량을 점차 늘려가며 부드럽게 연주하기 때문에 어택 국면이 퍼진다. 게다가, 디케이 단계가 없는 것처럼 보이고 그 이후의 서스테인 단계는 일정하지 않다; 대신 진폭 엔벨로프는 규칙적인 방식으로 진동한다. 바이올린 연주자가 활로 현을 켜는 것을 멈추면 릴리즈 단계가 시작된다. 그리고 나서 그 소리는 빠르게 사라진다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프-계산",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#엔벨로프-계산",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "엔벨로프 계산",
    "text": "엔벨로프 계산\n\n파형의 엔벨로프를 계산하는 방법은 여러 가지가 있다. 다음에서는 각 윈도우 섹션에 최대 필터를 적용하여 간단한 슬라이딩 윈도우 방식을 사용한다. 다음 코드 셀에서는 주어진 파형의 상한 엔벨로프와 하한 엔벨로프 및 파형의 크기 엔벨로프를 계산한다.\n\n\ndef compute_envelope(x, win_len_sec=0.01, Fs=4000):\n    \"\"\"Computation of a signal's envelopes\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        win_len_sec (float): Length (seconds) of the window (Default value = 0.01)\n        Fs (scalar): Sampling rate (Default value = 4000)\n\n    Returns:\n        env (np.ndarray): Magnitude envelope\n        env_upper (np.ndarray): Upper envelope\n        env_lower (np.ndarray): Lower envelope\n    \"\"\"\n    win_len_half = round(win_len_sec * Fs * 0.5)\n    N = x.shape[0]\n    env = np.zeros(N)\n    env_upper = np.zeros(N)\n    env_lower = np.zeros(N)\n    for i in range(N):\n        i_start = max(0, i - win_len_half)\n        i_end = min(N, i + win_len_half)\n        env[i] = np.amax(np.abs(x)[i_start:i_end])\n        env_upper[i] = np.amax(x[i_start:i_end])\n        env_lower[i] = np.amin(x[i_start:i_end])\n    return env, env_upper, env_lower\n    \n    \ndef compute_plot_envelope(x, win_len_sec, Fs, figsize=(6, 3), title=''):\n    \"\"\"Computation and subsequent plotting of a signal's envelope\n\n    Args:\n        x (np.ndarray): Signal (waveform) to be analyzed\n        win_len_sec (float): Length (seconds) of the window\n        Fs (scalar): Sampling rate\n        figsize (tuple): Size of the figure (Default value = (6, 3))\n        title (str): Title of the figure (Default value = '')\n\n    Returns:\n        fig (mpl.figure.Figure): Generated figure\n    \"\"\"\n    t = np.arange(x.size)/Fs\n    env, env_upper, env_lower = compute_envelope(x, win_len_sec=win_len_sec, Fs=Fs)\n    fig = plt.figure(figsize=figsize)\n    plt.plot(t, x, color='gray', label='Waveform')\n    plt.plot(t, env_upper, linewidth=2, color='cyan', label='Upper envelope')\n    plt.plot(t, env_lower, linewidth=2, color='blue', label='Lower envelope')\n    plt.plot(t, env, linewidth=2, color='red', label='Magnitude envelope')\n    plt.title(title)\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Amplitude')\n    plt.xlim([t[0], t[-1]])\n    #plt.ylim([-0.7, 0.7])\n    plt.legend(loc='lower right')\n    plt.show()\n    ipd.display(ipd.Audio(data=x, rate=Fs))\n    return fig\n\n\nFs = 11025\nwin_len_sec=0.05\n\nx, Fs = librosa.load(\"../audio/piano_c4.wav\", sr=Fs)\nfig = compute_plot_envelope(x, win_len_sec=win_len_sec, Fs=Fs, title='piano sound')\n\nx, Fs = librosa.load(\"../audio/violin_c4.wav\", sr=Fs)\nfig = compute_plot_envelope(x, win_len_sec=win_len_sec, Fs=Fs, title='violin sound')\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#비브라토-트레몰로-vibrato-and-tremolo",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#비브라토-트레몰로-vibrato-and-tremolo",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "비브라토, 트레몰로 Vibrato and Tremolo",
    "text": "비브라토, 트레몰로 Vibrato and Tremolo\n\n바이올린의 예에서 음색과 관련된 다른 현상들이 나타난다. 예를 들어, 진폭의 주기적인 변화를 관찰할 수 있다. 이러한 진폭 변조는 트레몰로(tremolo)로도 알려져 있다.\n트레몰로의 효과는 진동수의 규칙적인 변화(주파수 변조)로 구성된 음악적 효과인 비브라토와 함께 종종 동반된다.\n현악 이외에도 비브라토는 인간 가수들이 표현을 더하기 위해 주로 사용된다. 트레몰로와 비브라토가 단순히 강도와 주파수의 국소적인 변화라고 할지라도, 그것들이 반드시 전체적인 음조의 음량이나 음조의 지각된 변화를 불러일으키지는 않는다. 오히려, 그것들은 음악적 음색에 영향을 미치는 특징들이다.\n다음 코드 셀에서, 단순한 정현파, 비브라토가 있는 정현파, 트레몰로가 있는 정현파를 생성한다.\n\n\ndef generate_sinusoid_vibrato(dur=5, Fs=1000, amp=0.5, freq=440, vib_amp=1, vib_rate=5):\n    \"\"\"Generation of a sinusoid signal with vibrato\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 0.5)\n        freq (float): Frequency (Hz) of sinusoid (Default value = 440)\n        vib_amp (float): Amplitude (Hz) of the frequency oscillation (Default value = 1)\n        vib_rate (float): Rate (Hz) of the frequency oscillation (Default value = 5)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    freq_vib = freq + vib_amp * np.sin(t * 2 * np.pi * vib_rate)\n    phase_vib = np.zeros(num_samples)\n    for i in range(1, num_samples):\n        phase_vib[i] = phase_vib[i-1] + 2 * np.pi * freq_vib[i-1] / Fs\n    x = amp * np.sin(phase_vib)\n    return x, t\n\ndef generate_sinusoid_tremolo(dur=5, Fs=1000, amp=0.5, freq=440, trem_amp=0.1, trem_rate=5):\n    \"\"\"Generation of a sinusoid signal with tremolo\n\n    Args:\n        dur (float): Duration (in seconds) (Default value = 5)\n        Fs (scalar): Sampling rate (Default value = 1000)\n        amp (float): Amplitude of sinusoid (Default value = 0.5)\n        freq (float): Frequency (Hz) of sinusoid (Default value = 440)\n        trem_amp (float): Amplitude of the amplitude oscillation (Default value = 0.1)\n        trem_rate (float): Rate (Hz) of the amplitude oscillation (Default value = 5)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    amps = amp + trem_amp * np.sin(t * 2 * np.pi * trem_rate)\n    x = amps * np.sin(2*np.pi*(freq*t))\n    return x, t\n\n\nFs = 4000\ndur = 5\nfreq = 220\namp = 0.5\nfigsize = (8,2)\n\nx, t = generate_sinusoid(dur=dur, Fs=Fs, amp=amp, freq=freq)\nx_vib, t = generate_sinusoid_vibrato(dur=dur, Fs=Fs, amp=amp, freq=freq, vib_amp=6, vib_rate=5)\nx_trem, t = generate_sinusoid_tremolo(dur=dur, Fs=Fs, amp=amp, freq=freq, trem_amp=0.3, trem_rate=5)\n\n\nplot_signal(x, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nplot_signal(x_vib, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid with vibrato')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x_vib, rate=Fs))\n\nplot_signal(x_trem, Fs=Fs, figsize=figsize, ylabel='Amplitude', title='Sinusoid with tremolo')\nplt.ylim([-0.9, 0.9])\nplt.show()\nipd.display(ipd.Audio(data=x_trem, rate=Fs))\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#부분음부분파-partials",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#부분음부분파-partials",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "부분음/부분파 (Partials)",
    "text": "부분음/부분파 (Partials)\n\n아마도 음색을 특징 짓는 가장 중요하고 잘 알려진 속성은 특정 부분파(Partials)의 존재와 그 상대적 강점일 것이다.\n부분파는 음악 톤에서 가장 낮은 부분이 기본 주파수(fundamental frequency) 인 지배적 주파수이다.\n소리의 부분파는 스펙트로그램으로 시각화된다. 스펙트로그램(spectrogram)은 시간 경과에 따른 주파수 성분의 강도를 보여준다\n비조화(inharmonicity)는 가장 가까운 이상고조파(ideal harmonic)에서 벗어나는 부분적 정도를 나타낸다.\n명확하게 인식할 수 있는 음정을 가진 음악적 톤과 같은 소리의 경우, 대부분의 부분파는 고조파(harmonics)에 가깝다. 그러나 모든 부분파가 동일한 강도로 발생할 필요는 없다. 다음 그림은 서로 다른 악기에서 재생되는 단일 노트 C4에 대한 스펙트로그램 표현을 보여준다.\n\n\nImage(\"../img/2.music_representation/FMP_C1_F23_FourInstruments.png\", width=800)\n\n\n\n\n\n음의 기본 주파수(261.6Hz)와 고조파(261.6Hz의 정수 배수) 모두 수평 구조로 보인다.\n음악 톤의 디케이는 각각의 부분파에서 그에 상응하는 디케이에 의해 반영된다.\n톤의 에너지의 대부분은 낮은 부분에 포함되어 있고, 높은 부분에 대한 에너지는 낮은 경향이 있다. 이러한 분포는 많은 악기에서 일반적이다. 현악기의 경우, 소리는 풍부한 부분 스펙트럼을 갖는 경향이 있는데, 여기서 많은 에너지가 상부 고조파(harmonics)에도 포함될 수 있다. 이 그림은 또한 트레몰로(특히 플루트의 경우)와 비브라토(특히 바이올린의 경우)를 보여준다.\n\n다른 예\n\n# pure tone\nT = 2.0 # seconds\nf0 = 1047.0\nsr = 22050\nt = np.linspace(0, T, int(T*sr), endpoint=False) # time variable\nx = 0.1*np.sin(2*np.pi*f0*t)\nipd.Audio(x, rate=sr)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[:4096])\nX_mag = np.absolute(X)        # spectral magnitude\nf = np.linspace(0, sr, 4096)  # frequency variable\nplt.figure(figsize=(6, 2))\nplt.title('pure tone')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\n# oboe C6\nx, sr = librosa.load('../audio/oboe_c6.wav')\nprint(x.shape)\nipd.Audio(x, rate=sr)\n\n(23625,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[10000:14096])\nX_mag = np.absolute(X)\nplt.figure(figsize=(6, 2))\nplt.title('oboe C6')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\nx, sr = librosa.load('../audio/clarinet_c6.wav')\nprint(x.shape)\nipd.Audio(x, rate=sr)\n\n(51386,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nX = scipy.fft.fft(x[10000:14096])\nX_mag = np.absolute(X)\nplt.figure(figsize=(6, 2))\nplt.title('clarinet C6')\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\n\nText(0.5, 0, 'Frequency (Hz)')\n\n\n\n\n\n\n부분파의 구성 요소의 상대적 진폭 차이에 주목해보자. 세 신호 모두 거의 동일한 피치와 기본 주파수를 가지고 있지만, 음색은 다르다."
  },
  {
    "objectID": "posts/2. Music Representation/2.3.Audio_Representation.html#missing-fundamental",
    "href": "posts/2. Music Representation/2.3.Audio_Representation.html#missing-fundamental",
    "title": "2.3. 오디오 표현 (Audio Representation)",
    "section": "Missing Fundamental",
    "text": "Missing Fundamental\n\n앞서 말했듯이, 소리의 음색은 결정적으로 고조파에 걸친 신호의 에너지 분포에 따라 달라진다. 또한, 인식된 피치의 지각은 기본 주파수뿐만 아니라 더 높은 고조파와 그들의 관계에 따라 달라진다. 예를 들어, 인간은 이 피치와 관련된 기본 주파수가 완전히 누락된 경우에도 톤의 피치를 감지할 수 있다. 이 현상은 “missing fundamental”로 알려져 있다.\n다음 코드 예제에서는 음의 중심 주파수(center frequency)의 정수 배수인 주파수를 가진 (가중된) 정현파를 추가하여 소리를 생성한다. 특히 순수 톤(MIDI 피치 𝑝), 고조파가 있는 톤, missing fundamental의 고조파가 있는 톤, 두번 째 순수 톤(MIDI 피치 𝑝+12)을 생성한다.\n\n\ndef generate_tone(p=60, weight_harmonic=np.ones([16, 1]), Fs=11025, dur=2):\n    \"\"\"Generation of a tone with harmonics\n\n    Args:\n        p (float): MIDI pitch of the tone (Default value = 60)\n        weight_harmonic (np.ndarray): Weights for the different harmonics (Default value = np.ones([16, 1])\n        Fs (scalar): Sampling frequency (Default value = 11025)\n        dur (float): Duration (seconds) of the signal (Default value = 2)\n\n    Returns:\n        x (np.ndarray): Generated signal\n        t (np.ndarray): Time axis (in seconds)\n    \"\"\"\n    freq = 2 ** ((p - 69) / 12) * 440\n    num_samples = int(Fs * dur)\n    t = np.arange(num_samples) / Fs\n    x = np.zeros(t.shape)\n    for h, w in enumerate(weight_harmonic):\n        x = x + w * np.sin(2 * np.pi * freq * (h + 1) * t)\n    return x, t\n\ndef plot_spectrogram(x, Fs=11025, N=4096, H=2048, figsize=(4, 2)):\n    \"\"\"Computation and subsequent plotting of the spectrogram of a signal\n\n    Args:\n        x: Signal (waveform) to be analyzed\n        Fs: Sampling rate (Default value = 11025)\n        N: FFT length (Default value = 4096)\n        H: Hopsize (Default value = 2048)\n        figsize: Size of the figure (Default value = (4, 2))\n\n    \"\"\"\n    N, H = 2048, 1024\n    X = librosa.stft(x, n_fft=N, hop_length=H, win_length=N, window='hann')\n    Y = np.abs(X)\n    plt.figure(figsize=figsize)\n    librosa.display.specshow(librosa.amplitude_to_db(Y, ref=np.max),\n                             y_axis='linear', x_axis='time', sr=Fs, hop_length=H, cmap='gray_r')\n    plt.ylim([0, 3000])\n    # plt.colorbar(format='%+2.0f dB')\n    plt.xlabel('Time (seconds)')\n    plt.ylabel('Frequency (Hz)')\n    plt.tight_layout()\n    plt.show()\n\n\nFs = 11025\np = 60\n\nprint('Pure tone (p = %s):' % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0.2])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Tone with harmonics (p = %s):' % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Tone with harmonics and missing fundamental (p = %s):'  % p)\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nprint('Pure tone (p = %s):' % (p + 12))\nx, t = generate_tone(Fs=Fs, p=p, weight_harmonic=[0, 0.2])\nplot_spectrogram(x)\nipd.display(ipd.Audio(data=x, rate=Fs))\n\nPure tone (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nTone with harmonics (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nTone with harmonics and missing fundamental (p = 60):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPure tone (p = 72):\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n출처:\n\nhttps://musicinformationretrieval.com/\nhttps://www.audiolabs-erlangen.de/FMP\n\n\n구글 Colab 링크"
  }
]